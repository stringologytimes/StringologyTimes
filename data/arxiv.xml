<?xml version="1.0" encoding="UTF-8"?><articles>
<entry id="1910.06169" xmlns="http://www.w3.org/2005/Atom">
    <id>http://arxiv.org/abs/1910.06169v1</id>
    <updated>2019-10-14T14:25:25Z</updated>
    <published>2019-10-14T14:25:25Z</published>
    <title>The PGM-index: a multicriteria, compressed and learned approach to data
  indexing</title>
    <summary>  The recent introduction of learned indexes has shaken the foundations of the
decades-old field of indexing data structures. Combining, or even replacing,
classic design elements such as B-tree nodes with machine learning models has
proven to give outstanding improvements in the space footprint and time
efficiency of data systems. However, these novel approaches are based on
heuristics, thus they lack any guarantees both in their time and space
requirements. We propose the Piecewise Geometric Model index (shortly,
PGM-index), which achieves guaranteed I/O-optimality in query operations,
learns an optimal number of linear models, and its peculiar recursive
construction makes it a purely learned data structure, rather than a hybrid of
traditional and learned indexes (such as RMI and FITing-tree). We show that the
PGM-index improves the space of the FITing-tree by 63.3% and of the B-tree by
more than four orders of magnitude, while achieving their same or even better
query time efficiency. We complement this result by proposing three variants of
the PGM-index. First, we design a compressed PGM-index that further reduces its
space footprint by exploiting the repetitiveness at the level of the learned
linear models it is composed of. Second, we design a PGM-index that adapts
itself to the distribution of the queries, thus resulting in the first known
distribution-aware learned index to date. Finally, given its flexibility in the
offered space-time trade-offs, we propose the multicriteria PGM-index that
efficiently auto-tune itself in a few seconds over hundreds of millions of keys
to the possibly evolving space-time constraints imposed by the application of
use.
  We remark to the reader that this paper is an extended and improved version
of our previous paper titled "Superseding traditional indexes by orchestrating
learning and geometry" (arXiv:1903.00507).
</summary>
    <author>
      <name>Paolo Ferragina</name>
    </author>
    <author>
      <name>Giorgio Vinciguerra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">We remark to the reader that this paper is an extended and improved
  version of our previous paper titled "Superseding traditional indexes by
  orchestrating learning and geometry" (arXiv:1903.00507)</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.4; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.04640">
    <id>http://arxiv.org/abs/1910.04640v1</id>
    <updated>2019-10-10T15:19:19Z</updated>
    <published>2019-10-10T15:19:19Z</published>
    <title>E2FM: an encrypted and compressed full-text index for collections of
  genomic sequences</title>
    <summary>  Next Generation Sequencing (NGS) platforms and, more generally,
high-throughput technologies are giving rise to an exponential growth in the
size of nucleotide sequence databases. Moreover, many emerging applications of
nucleotide datasets -- as those related to personalized medicine -- require the
compliance with regulations about the storage and processing of sensitive data.
We have designed and carefully engineered E2FM-index, a new full-text index in
minute space which was optimized for compressing and encrypting nucleotide
sequence collections in FASTA format and for performing fast pattern-search
queries. E2FM-index allows to build self-indexes which occupy till to 1/20 of
the storage required by the input FASTA file, thus permitting to save about 95%
of storage when indexing collections of highly similar sequences; moreover, it
can exactly search the built indexes for patterns in times ranging from few
milliseconds to a few hundreds milliseconds, depending on pattern length.
Supplementary material and supporting datasets are available through
Bioinformatics Online and https://figshare.com/s/6246ee9c1bd730a8bf6e.
</summary>
    <author>
      <name>Ferdinando Montecuollo</name>
    </author>
    <author>
      <name>Giovannni Schmid</name>
    </author>
    <author>
      <name>Roberto Tagliaferri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/bioinformatics/btx313</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/bioinformatics/btx313" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages with pseudo-code and experimental results</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bioinformatics, 33(18), 2017, 2808-2817</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.04640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.04640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.04728">
    <id>http://arxiv.org/abs/1910.04728v1</id>
    <updated>2019-10-10T17:41:53Z</updated>
    <published>2019-10-10T17:41:53Z</published>
    <title>LISA: Towards Learned DNA Sequence Search</title>
    <summary>  Next-generation sequencing (NGS) technologies have enabled affordable
sequencing of billions of short DNA fragments at high throughput, paving the
way for population-scale genomics. Genomics data analytics at this scale
requires overcoming performance bottlenecks, such as searching for short DNA
sequences over long reference sequences. In this paper, we introduce LISA
(Learned Indexes for Sequence Analysis), a novel learning-based approach to DNA
sequence search. As a first proof of concept, we focus on accelerating one of
the most essential flavors of the problem, called exact search. LISA builds on
and extends FM-index, which is the state-of-the-art technique widely deployed
in genomics tool-chains. Initial experiments with human genome datasets
indicate that LISA achieves up to a factor of 4X performance speedup against
its traditional counterpart.
</summary>
    <author>
      <name>Darryl Ho</name>
    </author>
    <author>
      <name>Jialin Ding</name>
    </author>
    <author>
      <name>Sanchit Misra</name>
    </author>
    <author>
      <name>Nesime Tatbul</name>
    </author>
    <author>
      <name>Vikram Nathan</name>
    </author>
    <author>
      <name>Vasimuddin Md</name>
    </author>
    <author>
      <name>Tim Kraska</name>
    </author>
    <link href="http://arxiv.org/abs/1910.04728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.04728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.03578">
    <id>http://arxiv.org/abs/1910.03578v1</id>
    <updated>2019-10-08T09:12:47Z</updated>
    <published>2019-10-08T09:12:47Z</published>
    <title>Stack Sorting with Increasing and Decreasing Stacks</title>
    <summary>  We introduce a sorting machine consisting of $k+1$ stacks in series: the
first $k$ stacks can only contain elements in decreasing order from top to
bottom, while the last one has the opposite restriction. This device
generalizes \cite{SM}, which studies the case $k=1$. Here we show that, for
$k=2$, the set of sortable permutations is a class with infinite basis, by
explicitly finding an antichain of minimal nonsortable permutations. This
construction can easily be adapted to each $k \ge 3$. Next we describe an
optimal sorting algorithm, again for the case $k=2$. We then analyze two types
of left-greedy sorting procedures, obtaining complete results in one case and
only some partial results in the other one. We close the paper by discussing a
few open questions.
</summary>
    <author>
      <name>Giulio Cerbai</name>
    </author>
    <author>
      <name>Lapo Cioni</name>
    </author>
    <author>
      <name>Luca Ferrari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.03578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.03578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.02611">
    <id>http://arxiv.org/abs/1910.02611v1</id>
    <updated>2019-10-07T05:15:27Z</updated>
    <published>2019-10-07T05:15:27Z</published>
    <title>RAMBO: Repeated And Merged Bloom Filter for Multiple Set Membership
  Testing (MSMT) in Sub-linear time</title>
    <summary>  Approximate set membership is a common problem with wide applications in
databases, networking, and search. Given a set S and a query q, the task is to
determine whether q in S. The Bloom Filter (BF) is a popular data structure for
approximate membership testing due to its simplicity. In particular, a BF
consists of a bit array that can be incrementally updated. A related problem
concerning this paper is the Multiple Set Membership Testing (MSMT) problem.
Here we are given K different sets, and for any given query q the goal is the
find all of the sets containing the query element. Trivially, a multiple set
membership instance can be reduced to K membership testing instances, each with
the same q, leading to O(K) query time. A simple array of Bloom Filters can
achieve that. In this paper, we show the first non-trivial data-structure for
streaming keys, RAMBO (Repeated And Merged Bloom Filter) that achieves expected
O(sqrt(K) logK) query time with an additional worst case memory cost factor of
O(logK) than the array of Bloom Filters. The proposed data-structure is simply
a count-min sketch arrangement of Bloom Filters and retains all its favorable
properties. We replace the addition operation with a set union and the minimum
operation with a set intersection during estimation.
</summary>
    <author>
      <name>Gaurav Gupta</name>
    </author>
    <author>
      <name>Benjamin Coleman</name>
    </author>
    <author>
      <name>Tharun Medini</name>
    </author>
    <author>
      <name>Vijai Mohan</name>
    </author>
    <author>
      <name>Anshumali Shrivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.02611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.07145">
    <id>http://arxiv.org/abs/1910.07145v2</id>
    <updated>2020-03-21T01:05:37Z</updated>
    <published>2019-10-16T03:14:03Z</published>
    <title>Practical Random Access to Large SLP-Compressed Texts</title>
    <summary>  Grammar-based compression is a popular and powerful approach to compressing
repetitive texts but until recently its relatively poor time-space trade-offs
in real life made it impractical for truly massive datasets such as genomic
databases. In a recent paper (SPIRE 2019) we showed how simple pre-processing
can dramatically improve those trade-offs. Now that grammar-based compression
itself is reasonably scalable, in this paper we turn our attention to one of
the features that make grammar-based compression so attractive: the possibility
of supporting fast random access. In this paper we introduce a new encoding in
which we identify symbols by their offsets among those with the same expansion
sizes, thus tightly integrating our encodings of the symbols in the parse tree
and its shape.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <author>
      <name>Louisa Seelbach Benkner</name>
    </author>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <link href="http://arxiv.org/abs/1910.07145v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07145v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.12370">
    <id>http://arxiv.org/abs/1904.12370v2</id>
    <updated>2019-10-14T14:04:03Z</updated>
    <published>2019-04-28T19:04:24Z</published>
    <title>Compact Fenwick trees for dynamic ranking and selection</title>
    <summary>  The Fenwick tree is a classical implicit data structure that stores an array
in such a way that modifying an element, accessing an element, computing a
prefix sum and performing a predecessor search on prefix sums all take
logarithmic time. We introduce a number of variants which improve the classical
implementation of the tree: in particular, we can reduce its size when an upper
bound on the array element is known, and we can perform much faster predecessor
searches. Our aim is to use our variants to implement an efficient dynamic bit
vector: our structure is able to perform updates, ranking and selection in
logarithmic time, with a space overhead in the order of a few percents,
outperforming existing data structures with the same purpose. Along the way, we
highlight the pernicious interplay between the arithmetic behind the Fenwick
tree and the structure of current CPU caches, suggesting simple solutions that
improve performance significantly.
</summary>
    <author>
      <name>Stefano Marchini</name>
    </author>
    <author>
      <name>Sebastiano Vigna</name>
    </author>
    <link href="http://arxiv.org/abs/1904.12370v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.12370v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.06437">
    <id>http://arxiv.org/abs/1910.06437v2</id>
    <updated>2019-11-14T14:50:14Z</updated>
    <published>2019-10-14T21:44:14Z</published>
    <title>It is high time we let go of the Mersenne Twister</title>
    <summary>  When the Mersenne Twister made his first appearance in 1997 it was a powerful
example of how linear maps on $\mathbf F_2$ could be used to generate
pseudorandom numbers. In particular, the easiness with which generators with
long periods could be defined gave the Mersenne Twister a large following, in
spite of the fact that such long periods are not a measure of quality, and they
require a large amount of memory. Even at the time of its publication, several
defects of the Mersenne Twister were predictable, but they were somewhat
obscured by other interesting properties. Today the Mersenne Twister is the
default generator in C compilers, the Python language, the Maple mathematical
computation system, and in many other environments. Nonetheless, knowledge
accumulated in the last $20$ years suggests that the Mersenne Twister has, in
fact, severe defects, and should never be used as a general-purpose
pseudorandom number generator. Many of these results are folklore, or are
scattered through very specialized literature. This paper surveys these results
for the non-specialist, providing new, simple, understandable examples, and it
is intended as a guide for the final user, or for language implementors, so
that they can take an informed decision about whether to use the Mersenne
Twister or not.
</summary>
    <author>
      <name>Sebastiano Vigna</name>
    </author>
    <link href="http://arxiv.org/abs/1910.06437v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06437v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.06920">
    <id>http://arxiv.org/abs/1910.06920v1</id>
    <updated>2019-10-15T16:45:59Z</updated>
    <published>2019-10-15T16:45:59Z</published>
    <title>Apply Sorting Algorithms to FAST Problem</title>
    <summary>  FAST problem is finding minimum feedback arc set problem in tournaments. In
this paper we present some algorithms that are similar to sorting algorithms
for FAST problem and we analyze them. We present Pseudo_InsertionSort algorithm
for FAST problem and we show that average number of all backward edges in
output of that is equal to ((n^2-5n+8)/4)-2^(1-n). We introduce
Pseudo_MergeSort algorithm and we find the probability of being backward for an
edge. Finally we introduce other algorithms for this problem.
</summary>
    <author>
      <name>Sadra Mohammadshirazi</name>
    </author>
    <author>
      <name>Alireza Bagheri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.06416">
    <id>http://arxiv.org/abs/1910.06416v2</id>
    <updated>2019-11-30T11:47:44Z</updated>
    <published>2019-10-14T20:50:22Z</published>
    <title>RecSplit: Minimal Perfect Hashing via Recursive Splitting</title>
    <summary>  A minimal perfect hash function bijectively maps a key set $S$ out of a
universe $U$ into the first $|S|$ natural numbers. Minimal perfect hash
functions are used, for example, to map irregularly-shaped keys, such as
string, in a compact space so that metadata can then be simply stored in an
array. While it is known that just $1.44$ bits per key are necessary to store a
minimal perfect function, no published technique can go below $2$ bits per key
in practice. We propose a new technique for storing minimal perfect hash
functions with expected linear construction time and expected constant lookup
time that makes it possible to build for the first time, for example,
structures which need $1.56$ bits per key, that is, within $8.3$% of the lower
bound, in less than $2$ ms per key. We show that instances of our construction
are able to simultaneously beat the construction time, space usage and lookup
time of the state-of-the-art data structure reaching $2$ bits per key.
Moreover, we provide parameter choices giving structures which are competitive
with alternative, larger-size data structures in terms of space and lookup
time. The construction of our data structures can be easily parallelized or
mapped on distributed computational units (e.g., within the MapReduce
framework), and structures larger than the available RAM can be directly built
in mass storage.
</summary>
    <author>
      <name>Emmanuel Esposito</name>
    </author>
    <author>
      <name>Thomas Mueller Graf</name>
    </author>
    <author>
      <name>Sebastiano Vigna</name>
    </author>
    <link href="http://arxiv.org/abs/1910.06416v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06416v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.11564">
    <id>http://arxiv.org/abs/1910.11564v1</id>
    <updated>2019-10-25T08:17:10Z</updated>
    <published>2019-10-25T08:17:10Z</published>
    <title>Non-Rectangular Convolutions and (Sub-)Cadences with Three Elements</title>
    <summary>  The discrete acyclic convolution computes the 2n-1 sums sum_{i+j=k; (i,j) in
[0,1,2,...,n-1]^2} (a_i b_j) in O(n log n) time. By using suitable offsets and
setting some of the variables to zero, this method provides a tool to calculate
all non-zero sums sum_{i+j=k; (i,j) in (P cap Z^2)} (a_i b_j) in a rectangle P
with perimeter p in O(p log p) time.
  This paper extends this geometric interpretation in order to allow arbitrary
convex polygons P with k vertices and perimeter p. Also, this extended
algorithm only needs O(k + p(log p)^2 log k) time.
  Additionally, this paper presents fast algorithms for counting sub-cadences
and cadences with 3 elements using this extended method.
</summary>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Julian Pape-Lange</name>
    </author>
    <link href="http://arxiv.org/abs/1910.11564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.10406">
    <id>http://arxiv.org/abs/1910.10406v1</id>
    <updated>2019-10-23T08:24:15Z</updated>
    <published>2019-10-23T08:24:15Z</published>
    <title>Analyzing Trade-offs in Reversible Linear and Binary Search Algorithms</title>
    <summary>  Reversible algorithms are algorithms in which each step represents a partial
injective function; they are useful for performance optimization in reversible
systems. In this study, using Janus, a reversible imperative high-level
programming language, we have developed reversible linear and binary search
algorithms. We have analyzed the non-trivial space-time trade-offs between
them, focusing on the memory usage disregarding original inputs and outputs,
the size of the output garbage disregarding the original inputs, and the
maximum amount of traversal of the input. The programs in this study can easily
be adapted to other reversible programming languages. Our analysis reveals that
the change of the output data and/or the data structure affects the design of
efficient reversible algorithms. For example, the number of input data
traversals depends on whether the search has succeeded or failed, while it
expectedly never changes in corresponding irreversible linear and binary
searches. Our observations indicate the importance of the selection of data
structures and what is regarded as the output with the aim of the reversible
algorithm design.
</summary>
    <author>
      <name>Hiroki Masuda</name>
    </author>
    <author>
      <name>Tetsuo Yokoyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Third Workshop on Software Foundations for Data
  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.10406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.10631">
    <id>http://arxiv.org/abs/1910.10631v1</id>
    <updated>2019-10-23T15:54:53Z</updated>
    <published>2019-10-23T15:54:53Z</published>
    <title>Resolution of the Burrows-Wheeler Transform Conjecture</title>
    <summary>  Burrows-Wheeler Transform (BWT) is an invertible text transformation that
permutes symbols of a text according to the lexicographical order of its
suffixes. BWT is the main component of some of the most popular lossless
compression methods as well as of compressed indexes, central in modern
bioinformatics. The compression ratio of BWT-based compressors, such as bzip2,
is quantified by the number $r$ of maximal equal-letter runs in the BWT. This
is also (up to ${\rm polylog}\,n$ factors, where $n$ is the length of the text)
the space used by the state-of-the-art BWT-based indexes, such as the recent
$r$-index [Gagie et al., SODA 2018]. The output size of virtually every known
compression method is known to be either within a ${\rm polylog}\,n$ factor
from $z$, the size of Lempel-Ziv (LZ77) parsing of the text, or significantly
larger (by a $n^{\epsilon}$ factor for $\epsilon > 0$). The value of $r$ has
resisted, however, all attempts and until now, no non-trivial upper bounds on
$r$ were known.
  In this paper, we show that every text satisfies $r=\mathcal{O}(z\log^2 n)$.
This result has a number of immediate implications: (1) it proves that a large
body of work related to BWT automatically applies to the so-far disjoint field
of Lempel--Ziv indexing and compression, e.g., it is possible to obtain full
functionality of the suffix tree and the suffix array in $\mathcal{O}(z\,{\rm
polylog}\,n)$ space; (2) it lets us relate the number of runs in the BWT of the
text and its reverse; (3) it shows that many fundamental text processing tasks
can be solved in the optimal time assuming that the text is compressible by a
sufficiently large ${\rm polylog}\,n$ factor using LZ77.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <link href="http://arxiv.org/abs/1910.10631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.07819">
    <id>http://arxiv.org/abs/1910.07819v1</id>
    <updated>2019-10-17T10:41:47Z</updated>
    <published>2019-10-17T10:41:47Z</published>
    <title>EvoZip: Efficient Compression of Large Collections of Evolutionary Trees</title>
    <summary>  Phylogenetic trees represent evolutionary relationships among sets of
organisms. Popular phylogenetic reconstruction approaches typically yield
hundreds to thousands of trees on a common leafset. Storing and sharing such
large collection of trees requires considerable amount of space and bandwidth.
Furthermore, the huge size of phylogenetic tree databases can make search and
retrieval operations time-consuming. Phylogenetic compression techniques are
specialized compression techniques that exploit redundant topological
information to achieve better compression of phylogenetic trees. Here, we
present EvoZip, a new approach for phylogenetic tree compression. On average,
EvoZip achieves 71.6% better compression and takes 80.71% less compression time
and 60.47% less decompression time than TreeZip, the current state-of-the-art
algorithm for phylogenetic tree compression. While EvoZip is based on TreeZip,
it betters TreeZip due to (a) an improved bipartition and support list encoding
scheme, (b) use of Deflate compression algorithm, and (c) use of an efficient
tree reconstruction algorithm. EvoZip is freely available online for use by the
scientific community.
</summary>
    <author>
      <name>Balanand Jha</name>
    </author>
    <author>
      <name>David Fern√°ndez-Baca</name>
    </author>
    <author>
      <name>Akshay Deepak</name>
    </author>
    <author>
      <name>Kumar Abhishek</name>
    </author>
    <link href="http://arxiv.org/abs/1910.07819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.07849">
    <id>http://arxiv.org/abs/1910.07849v2</id>
    <updated>2019-10-28T10:22:03Z</updated>
    <published>2019-10-17T12:15:09Z</published>
    <title>Engineering Top-Down Weight-Balanced Trees</title>
    <summary>  Weight-balanced trees are a popular form of self-balancing binary search
trees. Their popularity is due to desirable guarantees, for example regarding
the required work to balance annotated trees.
  While usual weight-balanced trees perform their balancing operations in a
bottom-up fashion after a modification to the tree is completed, there exists a
top-down variant which performs these balancing operations during descend. This
variant has so far received only little attention. We provide an in-depth
analysis and engineering of these top-down weight-balanced trees, demonstrating
their superior performance. We also gaining insights into how the balancing
parameters necessary for a weight-balanced tree should be chosen - with the
surprising observation that it is often beneficial to choose parameters which
are not feasible in the sense of the correctness proofs for the rebalancing
algorithm.
</summary>
    <author>
      <name>Lukas Barth</name>
    </author>
    <author>
      <name>Dorothea Wagner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/1.9781611976007.13</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/1.9781611976007.13" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at ALENEX 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.07849v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07849v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.13479">
    <id>http://arxiv.org/abs/1910.13479v1</id>
    <updated>2019-10-29T18:58:48Z</updated>
    <published>2019-10-29T18:58:48Z</published>
    <title>Practical Repetition-Aware Grammar Compression</title>
    <summary>  The goal of grammar compression is to construct a small sized context free
grammar which uniquely generates the input text data. Among grammar compression
methods, RePair is known for its good practical compression performance.
MR-RePair was recently proposed as an improvement to RePair for constructing
small-sized context free grammar for repetitive text data. However, a compact
encoding scheme has not been discussed for MR-RePair. We propose a practical
encoding method for MR-RePair and show its effectiveness through comparative
experiments. Moreover, we extend MR-RePair to run-length context free grammar
and design a novel variant for it called RL-MR-RePair. We experimentally
demonstrate that a compression scheme consisting of RL-MR-RePair and the
proposed encoding method show good performance on real repetitive datasets.
</summary>
    <author>
      <name>Isamu Furuya</name>
    </author>
    <link href="http://arxiv.org/abs/1910.13479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.13479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.11993">
    <id>http://arxiv.org/abs/1910.11993v1</id>
    <updated>2019-10-26T04:17:37Z</updated>
    <published>2019-10-26T04:17:37Z</published>
    <title>Selection on $X_1+X_2+\cdots + X_m$ with layer-ordered heaps</title>
    <summary>  Selection on $X_1+X_2+\cdots + X_m$ is an important problem with many
applications in areas such as max-convolution, max-product Bayesian inference,
calculating most probable isotopes, and computing non-parametric test
statistics, among others. Faster-than-na\"{i}ve approaches exist for $m=2$:
Johnson \&amp; Mizoguchi (1978) find the smallest $k$ values in $A+B$ with runtime
$O(n \log(n))$. Frederickson \&amp; Johnson (1982) created a method for finding the
$k$ smallest values in $A+B$ with runtime $O(n +
\min(k,n)\log(\frac{k}{\min(k,n)}))$. In 1993, Frederickson published an
optimal algorithm for selection on $A+B$, which runs in $O(n+k)$. In 2018,
Kaplan \emph{et al.} described another optimal algorithm in terms Chazelle's of
soft heaps. No fast methods exist for $m>2$. Johnson \&amp; Mizoguchi (1978)
introduced a method to compute the minimal $k$ terms when $m>2$, but that
method runs in $O(m\cdot n^{\frac{m}{2}} \log(n))$ and is inefficient when $m
\gg 1$.
  In this paper, we introduce the first efficient methods for problems where
$m>2$. We introduce the ``layer-ordered heap,'' a simple special class of heap
with which we produce a new, fast selection algorithm on the Cartesian product.
Using this new algorithm to perform $k$-selection on the Cartesian product of
$m$ arrays of length $n$ has runtime $\in o(m\cdot n + k\cdot m)$. We also
provide implementations of the algorithms proposed and their performance in
practice.
</summary>
    <author>
      <name>Patrick Kreitzberg</name>
    </author>
    <author>
      <name>Kyle Lucke</name>
    </author>
    <author>
      <name>Oliver Serang</name>
    </author>
    <link href="http://arxiv.org/abs/1910.11993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.01851">
    <id>http://arxiv.org/abs/1911.01851v1</id>
    <updated>2019-11-02T14:56:53Z</updated>
    <published>2019-11-02T14:56:53Z</published>
    <title>Lyndon words versus inverse Lyndon words: queries on suffixes and
  bordered words</title>
    <summary>  Lyndon words have been largely investigated and showned to be a useful tool
to prove interesting combinatorial properties of words. In this paper we state
new properties of both Lyndon and inverse Lyndon factorizations of a word $w$,
with the aim of exploring their use in some classical queries on $w$.
  The main property we prove is related to a classical query on words. We prove
that there are relations between the length of the longest common extension (or
longest common prefix) $lcp(x,y)$ of two different suffixes $x,y$ of a word $w$
and the maximum length $\mathcal{M}$ of two consecutive factors of the inverse
Lyndon factorization of $w$. More precisely, $\mathcal{M}$ is an upper bound on
the length of $lcp(x,y)$. This result is in some sense stronger than the
compatibility property, proved by Mantaci, Restivo, Rosone and Sciortino for
the Lyndon factorization and here for the inverse Lyndon factorization.
Roughly, the compatibility property allows us to extend the mutual order
between local suffixes of (inverse) Lyndon factors to the suffixes of the whole
word.
  A main tool used in the proof of the above results is a property that we
state for factors $m_i$ with nonempty borders in an inverse Lyndon
factorization: a nonempty border of $m_i$ cannot be a prefix of the next factor
$m_{i+1}$. The last property we prove shows that if two words share a common
overlap, then their Lyndon factorizations can be used to capture the common
overlap of the two words.
  The above results open to the study of new applications of Lyndon words and
inverse Lyndon words in the field of string comparison.
</summary>
    <author>
      <name>Paola Bonizzoni</name>
    </author>
    <author>
      <name>Clelia De Felice</name>
    </author>
    <author>
      <name>Rocco Zaccagnino</name>
    </author>
    <author>
      <name>Rosalba Zizza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1705.10277</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.01851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.01763">
    <id>http://arxiv.org/abs/1911.01763v1</id>
    <updated>2019-11-05T13:36:15Z</updated>
    <published>2019-11-05T13:36:15Z</published>
    <title>An Efficient Word Lookup System by using Improved Trie Algorithm</title>
    <summary>  Efficiently word storing and searching is an important task in computer
science. An application space complexity, time complexity, and overall
performance depend on this string data. Many word searching data structures and
algorithms exist in the current world but few of them have space compress
ability. Trie is a popular data structure for word searching for its linear
searching capability. It is the basic and important part of various computer
applications such as information retrieval, natural language processing,
database system, compiler, and computer network. But currently, the available
version of trie tree cannot be used widely because of its high memory
requirement. This paper proposes a new Radix trie based data structure for word
storing and searching which can share not only just prefix but also infix and
suffix and thus reduces memory requirement. We propose a new emptiness property
to Radix trie. Proposed trie has character cell reduction capability and it can
dramatically reduce any application runtime memory size. Using it as data tank
to an operating system the overall main memory requirement of a device can be
reduced to a large extent.
</summary>
    <author>
      <name>Rahat Yeasin Emon</name>
    </author>
    <author>
      <name>Sharmistha Chanda Tista</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.01763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="null" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.01644">
    <id>http://arxiv.org/abs/1911.01644v1</id>
    <updated>2019-11-05T06:59:56Z</updated>
    <published>2019-11-05T06:59:56Z</published>
    <title>Fast Multiple Pattern Cartesian Tree Matching</title>
    <summary>  Cartesian tree matching is the problem of finding all substrings in a given
text which have the same Cartesian trees as that of a given pattern. In this
paper, we deal with Cartesian tree matching for the case of multiple patterns.
We present two fingerprinting methods, i.e., the parent-distance encoding and
the binary encoding. By combining an efficient fingerprinting method and a
conventional multiple string matching algorithm, we can efficiently solve
multiple pattern Cartesian tree matching. We propose three practical algorithms
for multiple pattern Cartesian tree matching based on the Wu-Manber algorithm,
the Rabin-Karp algorithm, and the Alpha Skip Search algorithm, respectively. In
the experiments we compare our solutions against the previous algorithm [18].
Our solutions run faster than the previous algorithm as the pattern lengths
increase. Especially, our algorithm based on Wu-Manber runs up to 33 times
faster.
</summary>
    <author>
      <name>Geonmo Gu</name>
    </author>
    <author>
      <name>Siwoo Song</name>
    </author>
    <author>
      <name>Simone Faro</name>
    </author>
    <author>
      <name>Thierry Lecroq</name>
    </author>
    <author>
      <name>Kunsoo Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to WALCOM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.01644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.01414">
    <id>http://arxiv.org/abs/1911.01414v2</id>
    <updated>2019-11-12T18:55:43Z</updated>
    <published>2019-11-04T18:57:04Z</published>
    <title>Counting Small Permutation Patterns</title>
    <summary>  A sample of n generic points in the xy-plane defines a permutation that
relates their ranks along the two axes. Every subset of k points similarly
defines a pattern, which occurs in that permutation. The number of occurrences
of small patterns in a large permutation arises in many areas, including
nonparametric statistics. It is therefore desirable to count them more
efficiently than the straightforward ~O(n^k) time algorithm.
  This work proposes new algorithms for counting patterns. We show that all
patterns of order 2 and 3, as well as eight patterns of order 4, can be counted
in nearly linear time. To that end, we develop an algebraic framework that we
call corner tree formulas. Our approach generalizes the existing methods and
allows a systematic study of their scope.
  Using the machinery of corner trees, we find twenty-three independent linear
combinations of order-4 patterns, that can be computed in time ~O(n). We also
describe an algorithm that counts another 4-pattern, and hence all 4-patterns,
in time ~O(n^(3/2)).
  As a practical application, we provide a nearly linear time computation of a
statistic by Yanagimoto (1970), Bergsma and Dassios (2010). This statistic
yields a natural and strongly consistent variant of Hoeffding's test for
independence of X and Y, given a random sample as above. This improves upon the
so far most efficient ~O(n^2) algorithm.
</summary>
    <author>
      <name>Chaim Even-Zohar</name>
    </author>
    <author>
      <name>Calvin Leng</name>
    </author>
    <link href="http://arxiv.org/abs/1911.01414v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01414v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.01348">
    <id>http://arxiv.org/abs/1911.01348v1</id>
    <updated>2019-11-04T17:18:44Z</updated>
    <published>2019-11-04T17:18:44Z</published>
    <title>Nearly Optimal Static Las Vegas Succinct Dictionary</title>
    <summary>  Given a set $S$ of $n$ (distinct) keys from key space $[U]$, each associated
with a value from $\Sigma$, the \emph{static dictionary} problem asks to
preprocess these (key, value) pairs into a data structure, supporting
value-retrieval queries: for any given $x\in [U]$, $\mathtt{valRet}(x)$ must
return the value associated with $x$ if $x\in S$, or return $\bot$ if $x\notin
S$. The special case where $|\Sigma|=1$ is called the \emph{membership}
problem. The "textbook" solution is to use a hash table, which occupies linear
space and answers each query in constant time. On the other hand, the minimum
possible space to encode all (key, value) pairs is only $\mathtt{OPT}:=
\lceil\lg_2\binom{U}{n}+n\lg_2|\Sigma|\rceil$ bits, which could be much less.
  In this paper, we design a randomized dictionary data structure using
$\mathtt{OPT}+\mathrm{poly}\lg n+O(\lg\lg\lg\lg\lg U)$ bits of space, and it
has \emph{expected constant} query time, assuming the query algorithm can
access an external lookup table of size $n^{0.001}$. The lookup table depends
only on $U$, $n$ and $|\Sigma|$, and not the input. Previously, even for
membership queries and $U\leq n^{O(1)}$, the best known data structure with
constant query time requires $\mathtt{OPT}+n/\mathrm{poly}\lg n$ bits of space
(Pagh [Pag01] and P\v{a}tra\c{s}cu [Pat08]); the best-known using
$\mathtt{OPT}+n^{0.999}$ space has query time $O(\lg n)$; the only known
non-trivial data structure with $\mathtt{OPT}+n^{0.001}$ space has $O(\lg n)$
query time and requires a lookup table of size $\geq n^{2.99}$ (!). Our new
data structure answers open questions by P\v{a}tra\c{s}cu and Thorup
[Pat08,Tho13].
</summary>
    <author>
      <name>Huacheng Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1911.01348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.01169">
    <id>http://arxiv.org/abs/1911.01169v1</id>
    <updated>2019-11-04T12:45:25Z</updated>
    <published>2019-11-04T12:45:25Z</published>
    <title>Optimal Adaptive Detection of Monotone Patterns</title>
    <summary>  We investigate adaptive sublinear algorithms for detecting monotone patterns
in an array. Given fixed $2 \leq k \in \mathbb{N}$ and $\varepsilon > 0$,
consider the problem of finding a length-$k$ increasing subsequence in an array
$f \colon [n] \to \mathbb{R}$, provided that $f$ is $\varepsilon$-far from free
of such subsequences. Recently, it was shown that the non-adaptive query
complexity of the above task is $\Theta((\log n)^{\lfloor \log_2 k \rfloor})$.
In this work, we break the non-adaptive lower bound, presenting an adaptive
algorithm for this problem which makes $O(\log n)$ queries. This is optimal,
matching the classical $\Omega(\log n)$ adaptive lower bound by Fischer [2004]
for monotonicity testing (which corresponds to the case $k=2$), and implying in
particular that the query complexity of testing whether the longest increasing
subsequence (LIS) has constant length is $\Theta(\log n)$.
</summary>
    <author>
      <name>Omri Ben-Eliezer</name>
    </author>
    <author>
      <name>Shoham Letzter</name>
    </author>
    <author>
      <name>Erik Waingarten</name>
    </author>
    <link href="http://arxiv.org/abs/1911.01169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.00044">
    <id>http://arxiv.org/abs/1911.00044v3</id>
    <updated>2020-01-17T16:14:20Z</updated>
    <published>2019-10-31T18:19:55Z</published>
    <title>Edge minimization in de Bruijn graphs</title>
    <summary>  This paper introduces the de Bruijn graph edge minimization problem, which is
related to the compression of de Bruijn graphs: find the order-k de Bruijn
graph with minimum edge count among all orders. We describe an efficient
algorithm that solves this problem. Since the edge minimization problem is
connected to the BWT compression technique called "tunneling", the paper also
describes a way to minimize the length of a tunneled BWT in such a way that
useful properties for sequence analysis are preserved. Although being a
restriction, this is significant progress towards a solution to the open
problem of finding optimal disjoint blocks that minimize space, as stated in
Alanko et al. (DCC 2019).
</summary>
    <author>
      <name>Uwe Baier</name>
    </author>
    <author>
      <name>Thomas B√ºchler</name>
    </author>
    <author>
      <name>Enno Ohlebusch</name>
    </author>
    <author>
      <name>Pascal Weber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Data Compression Conference 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.00044v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00044v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.14508">
    <id>http://arxiv.org/abs/1910.14508v2</id>
    <updated>2019-11-15T17:03:58Z</updated>
    <published>2019-10-31T14:44:42Z</published>
    <title>ALLSAT compressed with wildcards: Frequent Set Mining</title>
    <summary>  Like any simplicial complex the simplicial complex of all frequent sets can
be compressed with wildcards once the maximal frequent sets (=facets) are
known. Namely, the task (a particular kind of ALLSAT problem) is achieved by
the author's recent algorithm Facets-To-Faces. But how to get the facets in the
first place? The novel algorithm Find-All-Facets determines all facets of any
(decidable) finite simplicial complex by replacing costly hypergraph
dualization (Dualize+Advance and its variants) with the cheaper calculation of
the minimal members of certain set families. The latter task is sped up by
Vertical Layout. While all of this concerns arbitrary simplicial complexes, the
impact to Frequent Set Mining (FSM) seems particularly high.
</summary>
    <author>
      <name>Marcel Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">I solicite the help of FSM practitioners for the final version of
  this article!</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.14508v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.14508v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.06347">
    <id>http://arxiv.org/abs/1911.06347v1</id>
    <updated>2019-11-14T19:07:20Z</updated>
    <published>2019-11-14T19:07:20Z</published>
    <title>In Search of the Fastest Concurrent Union-Find Algorithm</title>
    <summary>  Union-Find (or Disjoint-Set Union) is one of the fundamental problems in
computer science; it has been well-studied from both theoretical and practical
perspectives in the sequential case. Recently, there has been mounting interest
in analyzing this problem in the concurrent scenario, and several
asymptotically-efficient algorithms have been proposed. Yet, to date, there is
very little known about the practical performance of concurrent Union-Find.
  This work addresses this gap. We evaluate and analyze the performance of
several concurrent Union-Find algorithms and optimization strategies across a
wide range of platforms (Intel, AMD, and ARM) and workloads (social, random,
and road networks, as well as integrations into more complex algorithms). We
first observe that, due to the limited computational cost, the number of
induced cache misses is the critical determining factor for the performance of
existing algorithms. We introduce new techniques to reduce this cost by storing
node priorities implicitly and by using plain reads and writes in a way that
does not affect the correctness of the algorithms. Finally, we show that
Union-Find implementations are an interesting application for Transactional
Memory (TM): one of the fastest algorithm variants we discovered is a
sequential one that uses coarse-grained locking with the lock elision
optimization to reduce synchronization cost and increase scalability.
</summary>
    <author>
      <name>Dan Alistarh</name>
    </author>
    <author>
      <name>Alexander Fedorov</name>
    </author>
    <author>
      <name>Nikita Koval</name>
    </author>
    <link href="http://arxiv.org/abs/1911.06347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.05676">
    <id>http://arxiv.org/abs/1911.05676v1</id>
    <updated>2019-11-13T17:55:06Z</updated>
    <published>2019-11-13T17:55:06Z</published>
    <title>Enumerative Data Compression with Non-Uniquely Decodable Codes</title>
    <summary>  Non-uniquely decodable codes can be defined as the codes that cannot be
uniquely decoded without additional disambiguation information. These are
mainly the class of non-prefix-free codes, where a codeword can be a prefix of
other(s), and thus, the codeword boundary information is essential for correct
decoding. Although the codeword bit stream consumes significantly less space
when compared to prefix--free codes, the additional disambiguation information
makes it difficult to catch the performance of prefix-free codes in total.
Previous studies considered compression with non-prefix-free codes by
integrating rank/select dictionaries or wavelet trees to mark the code-word
boundaries. In this study we focus on another dimension with a block--wise
enumeration scheme that improves the compression ratios of the previous studies
significantly. Experiments conducted on a known corpus showed that the proposed
scheme successfully represents a source within its entropy, even performing
better than the Huffman and arithmetic coding in some cases. The non-uniquely
decodable codes also provides an intrinsic security feature due to lack of
unique-decodability. We investigate this dimension as an opportunity to provide
compressed data security without (or with less) encryption, and discuss various
possible practical advantages supported by such codes.
</summary>
    <author>
      <name>M. Oƒüuzhan K√ºlekci</name>
    </author>
    <author>
      <name>Yasin √ñzt√ºrk</name>
    </author>
    <author>
      <name>Elif Altunok</name>
    </author>
    <author>
      <name>Can Altƒ±niƒüne</name>
    </author>
    <link href="http://arxiv.org/abs/1911.05676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.05676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.05060">
    <id>http://arxiv.org/abs/1911.05060v1</id>
    <updated>2019-11-12T18:35:28Z</updated>
    <published>2019-11-12T18:35:28Z</published>
    <title>Fully-Dynamic Space-Efficient Dictionaries and Filters with Constant
  Number of Memory Accesses</title>
    <summary>  A fully-dynamic dictionary is a data structure for maintaining sets that
supports insertions, deletions and membership queries. A filter approximates
membership queries with a one-sided error. We present two designs:
  1. The first space-efficient fully-dynamic dictionary that maintains both
sets and random multisets and supports queries, insertions, and deletions with
a constant number of memory accesses in the worst case with high probability.
The comparable dictionary of Arbitman, Naor, and Segev [FOCS 2010] works only
for sets.
  2. By a reduction from our dictionary for random multisets, we obtain a
space-efficient fully-dynamic filter that supports queries, insertions, and
deletions with a constant number of memory accesses in the worst case with high
probability (as long as the false positive probability is $2^{-O(w)}$, where
$w$ denotes the word length). This is the first in-memory space-efficient
fully-dynamic filter design that provably achieves these properties.
  We also present an application of the techniques used to design our
dictionary to the static Retrieval Problem.
</summary>
    <author>
      <name>Ioana O. Bercea</name>
    </author>
    <author>
      <name>Guy Even</name>
    </author>
    <link href="http://arxiv.org/abs/1911.05060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.05060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.03542">
    <id>http://arxiv.org/abs/1911.03542v2</id>
    <updated>2019-12-10T13:59:09Z</updated>
    <published>2019-11-08T21:15:21Z</published>
    <title>Space Efficient Construction of Lyndon Arrays in Linear Time</title>
    <summary>  We present the first linear time algorithm to construct the $2n$-bit version
of the Lyndon array for a string of length $n$ using only $o(n)$ bits of
working space. A simpler variant of this algorithm computes the plain ($n\lg
n$-bit) version of the Lyndon array using only $\mathcal{O}(1)$ words of
additional working space. All previous algorithms are either not linear, or use
at least $n\lg n$ bits of additional working space. Also in practice, our new
algorithms outperform the previous best ones by an order of magnitude, both in
terms of time and space.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Jonas Ellert</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <author>
      <name>Florian Kurpicz</name>
    </author>
    <author>
      <name>Ian Munro</name>
    </author>
    <author>
      <name>Eva Rotenberg</name>
    </author>
    <link href="http://arxiv.org/abs/1911.03542v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03542v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.04202">
    <id>http://arxiv.org/abs/1911.04202v1</id>
    <updated>2019-11-11T12:05:37Z</updated>
    <published>2019-11-11T12:05:37Z</published>
    <title>Dv2v: A Dynamic Variable-to-Variable Compressor</title>
    <summary>  We present Dv2v, a new dynamic (one-pass) variable-to-variable compressor.
Variable-to-variable compression aims at using a modeler that gathers
variable-length input symbols and a variable-length statistical coder that
assigns shorter codewords to the more frequent symbols. In Dv2v, we process the
input text word-wise to gather variable-length symbols that can be either
terminals (new words) or non-terminals, subsequences of words seen before in
the input text. Those input symbols are set in a vocabulary that is kept sorted
by frequency. Therefore, those symbols can be easily encoded with dense codes.
Our Dv2v permits real-time transmission of data, i.e. compression/transmission
can begin as soon as data become available. Our experiments show that Dv2v is
able to overcome the compression ratios of the v2vDC, the state-of-the-art
semi-static variable-to-variable compressor, and to almost reach p7zip values.
It also draws a competitive performance at both compression and decompression.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Antonio Fari√±a</name>
    </author>
    <author>
      <name>Adri√°n G√≥mez-Brand√≥n</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Tirso V. Rodeiro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DCC.2019.00016</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DCC.2019.00016" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dv2v: A Dynamic Variable-to-Variable Compressor. In 2019 Data
  Compression Conference (DCC) (pp. 83-92). IEEE</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.04202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.04202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.04198">
    <id>http://arxiv.org/abs/1911.04198v1</id>
    <updated>2019-11-11T11:54:20Z</updated>
    <published>2019-11-11T11:54:20Z</published>
    <title>GraCT: A Grammar-based Compressed Index for Trajectory Data</title>
    <summary>  We introduce a compressed data structure for the storage of free trajectories
of moving objects (such as ships and planes) that efficiently supports various
spatio-temporal queries. Our structure, dubbed GraCT, stores the absolute
positions of all the objects at regular time intervals (snapshots) using a
$k^2$-tree, which is a space- and time-efficient version of a region quadtree.
Positions between snapshots are represented as logs of relative movements and
compressed using Re-Pair, a grammar-based compressor. The nonterminals of this
grammar are enhanced with MBR information to enable fast queries.
  The GraCT structure of a dataset occupies less than the raw data compressed
with a powerful traditional compressor such as p7zip. Further, instead of
requiring full decompression to access the data like a traditional compressor,
GraCT supports direct access to object trajectories or to their position at
specific time instants, as well as spatial range and nearest-neighbor queries
on time instants and/or time intervals.
  Compared to traditional methods for storing and indexing spatio-temporal
data, GraCT requires two orders of magnitude less space, and is competitive in
query times. In particular, thanks to its compressed representation, the GraCT
structure may reside in main memory in situations where any classical
uncompressed index must resort to disk, thereby being one or two orders of
magnitude faster.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Adri√°n G√≥mez-Brand√≥n</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Jos√© R. Param√°</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2019.01.035</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2019.01.035" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Sciences, 2019, vol. 483, p. 106-135</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.04198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.04198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.03028">
    <id>http://arxiv.org/abs/1911.03028v1</id>
    <updated>2019-11-08T03:55:54Z</updated>
    <published>2019-11-08T03:55:54Z</published>
    <title>Lock-Free Hopscotch Hashing</title>
    <summary>  In this paper we present a lock-free version of Hopscotch Hashing. Hopscotch
Hashing is an open addressing algorithm originally proposed by Herlihy, Shavit,
and Tzafrir, which is known for fast performance and excellent cache locality.
The algorithm allows users of the table to skip or jump over irrelevant
entries, allowing quick search, insertion, and removal of entries. Unlike
traditional linear probing, Hopscotch Hashing is capable of operating under a
high load factor, as probe counts remain small. Our lock-free version improves
on both speed, cache locality, and progress guarantees of the original, being a
chimera of two concurrent hash tables. We compare our data structure to various
other lock-free and blocking hashing algorithms and show that its performance
is in many cases superior to existing strategies. The proposed lock-free
version overcomes some of the drawbacks associated with the original blocking
version, leading to a substantial boost in scalability while maintaining
attractive features like physical deletion or probe-chain compression.
</summary>
    <author>
      <name>Robert Kelly</name>
    </author>
    <author>
      <name>Barak A. Pearlmutter</name>
    </author>
    <author>
      <name>Phil Maguire</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, to appear in APOCS20</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.03028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.02889">
    <id>http://arxiv.org/abs/1911.02889v2</id>
    <updated>2019-11-15T11:28:31Z</updated>
    <published>2019-11-07T13:24:50Z</published>
    <title>Towards Better Compressed Representations</title>
    <summary>  We introduce the problem of computing a parsing where each phrase is of
length at most $m$ and which minimizes the zeroth order entropy of parsing.
Based on the recent theoretical results we devise a heuristic for this problem.
The solution has straightforward application in succinct text representations
and gives practical improvements. Moreover the proposed heuristic yields
structure whose size can be bounded both by $|S|H_{m-1}(S)$ and by
$|S|/m(H_0(S) + \cdots + H_{m-1})$, where $H_{k}(S)$ is the $k$-th order
empirical entropy of $S$. We also consider a similar problem in which the
first-order entropy is minimized.
</summary>
    <author>
      <name>Micha≈Ç Ga≈Ñczorz</name>
    </author>
    <link href="http://arxiv.org/abs/1911.02889v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.02889v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.03035">
    <id>http://arxiv.org/abs/1911.03035v2</id>
    <updated>2020-02-18T18:21:38Z</updated>
    <published>2019-11-08T04:03:40Z</published>
    <title>On the Complexity of BWT-runs Minimization via Alphabet Reordering</title>
    <summary>  The Burrows-Wheeler Transform (BWT) has been an essential tool in text
compression and indexing. First introduced in 1994, it went on to provide the
backbone for the first encoding of the classic suffix tree data structure in
space close to the entropy-based lower bound. Recently, there has been the
development of compact suffix trees in space proportional to "$r$", the number
of runs in the BWT, as well as the appearance of $r$ in the time complexity of
new algorithms. Unlike other popular measures of compression, the parameter $r$
is sensitive to the lexicographic ordering given to the text's alphabet.
Despite several past attempts to exploit this, a provably efficient algorithm
for finding, or approximating, an alphabet ordering which minimizes $r$ has
been open for years.
  We present the first set of results on the computational complexity of
minimizing BWT-runs via alphabet reordering. We prove that the decision version
of this problem is NP-complete and cannot be solved in time $2^{o(\sigma +
\sqrt{n})}$ unless the Exponential Time Hypothesis fails, where $\sigma$ is the
size of the alphabet and $n$ is the length of the text. We also show that the
optimization problem is APX-hard. In doing so, we relate two previously
disparate topics: the optimal traveling salesperson path and the number of runs
in the BWT of a text, providing a surprising connection between problems on
graphs and text compression. Also, by relating recent results in the field of
dictionary compression, we illustrate that an arbitrary alphabet ordering
provides a $O(\log^2 n)$-approximation.
  We provide an optimal linear-time algorithm for the problem of finding a run
minimizing ordering on a subset of symbols (occurring only once) under ordering
constraints, and prove a generalization of this problem to a class of graphs
with BWT like properties called Wheeler graphs is NP-complete.
</summary>
    <author>
      <name>Jason Bentley</name>
    </author>
    <author>
      <name>Daniel Gibney</name>
    </author>
    <author>
      <name>Sharma V. Thankachan</name>
    </author>
    <link href="http://arxiv.org/abs/1911.03035v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03035v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.03195">
    <id>http://arxiv.org/abs/1911.03195v2</id>
    <updated>2019-12-06T12:12:28Z</updated>
    <published>2019-11-08T11:35:29Z</published>
    <title>On dynamic succinct graph representations</title>
    <summary>  We address the problem of representing dynamic graphs using $k^2$-trees. The
$k^2$-tree data structure is one of the succinct data structures proposed for
representing static graphs, and binary relations in general. It relies on
compact representations of bit vectors. Hence, by relying on compact
representations of dynamic bit vectors, we can also represent dynamic graphs.
In this paper we follow instead the ideas by Munro {\em et al.}, and we present
an alternative implementation for representing dynamic graphs using
$k^2$-trees. Our experimental results show that this new implementation is
competitive in practice.
</summary>
    <author>
      <name>Miguel E. Coimbra</name>
    </author>
    <author>
      <name>Alexandre P. Francisco</name>
    </author>
    <author>
      <name>Lu√≠s M. S. Russo</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Susana Ladra</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.03195v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03195v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.11048">
    <id>http://arxiv.org/abs/1911.11048v1</id>
    <updated>2019-11-25T17:00:13Z</updated>
    <published>2019-11-25T17:00:13Z</published>
    <title>Listing Conflicting Triples in Optimal Time</title>
    <summary>  Different sources of information might tell different stories about the
evolutionary history of a given set of species. This leads to (rooted)
phylogenetic trees that "disagree" on triples of species, which we call
"conflict triples". An important subtask of computing consensus trees which is
interesting in its own regard is the enumeration of all conflicts exhibited by
a pair of phylogenetic trees (on the same set of $n$ taxa). As it is possible
that a significant part of the $n^3$ triples are in conflict, the trivial
${\Theta}(n^3)$-time algorithm that checks for each triple whether it
constitutes a conflict, was considered optimal. It turns out, however, that we
can do way better in the case that there are only few conflicts. In particular,
we show that we can enumerate all d conflict triples between a pair of
phylogenetic trees in $O(n + d)$ time. Since any deterministic algorithm has to
spend ${\Theta}(n)$ time reading the input and ${\Theta}(d)$ time writing the
output, no deterministic algorithm can solve this task faster than we do (up to
constant factors).
</summary>
    <author>
      <name>Mathias Weller</name>
    </author>
    <link href="http://arxiv.org/abs/1911.11048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.09498">
    <id>http://arxiv.org/abs/1911.09498v1</id>
    <updated>2019-11-21T14:42:08Z</updated>
    <published>2019-11-21T14:42:08Z</published>
    <title>Implementing the Topological Model Succinctly</title>
    <summary>  We show that the topological model, a semantically rich standard to represent
GIS data, can be encoded succinctly while efficiently answering a number of
topology-related queries. We build on recent succinct planar graph
representations so as to encode a model with $m$ edges within $4m+o(m)$ bits
and answer various queries relating nodes, edges, and faces in $o(\log\log m)$
time, or any time in $\omega(\log m)$ for a few complex ones.
</summary>
    <author>
      <name>Jos√© Fuentes-Sep√∫lveda</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Diego Seco</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-32686-9_35</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-32686-9_35" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. Conference version
  presented at SPIRE 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.09498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.09498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.09370">
    <id>http://arxiv.org/abs/1911.09370v1</id>
    <updated>2019-11-21T09:56:37Z</updated>
    <published>2019-11-21T09:56:37Z</published>
    <title>Energy consumption in compact integer vectors: A study case</title>
    <summary>  In the field of algorithms and data structures analysis and design, most of
the researchers focus only on the space/time trade-off, and little attention
has been paid to energy consumption. Moreover, most of the efforts in the field
of Green Computing have been devoted to hardware-related issues, being green
software in its infancy. Optimizing the usage of computing resources,
minimizing power consumption or increasing battery life are some of the goals
of this field of research.
  As an attempt to address the most recent sustainability challenges, we must
incorporate the energy consumption as a first-class constraint when designing
new compact data structures. Thus, as a preliminary work to reach that goal, we
first need to understand the factors that impact on the energy consumption and
their relation with compression. In this work, we study the energy consumption
required by several integer vector representations. We execute typical
operations over datasets of different nature. We can see that, as commonly
believed, energy consumption is highly related to the time required by the
process, but not always. We analyze other parameters, such as number of
instructions, number of CPU cycles, memory loads, among others.
</summary>
    <author>
      <name>Jos√© Fuentes-Sep√∫lveda</name>
    </author>
    <author>
      <name>Susana Ladra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2019.2949655</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2019.2949655" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access 7, pp. 155625-155636 (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.09370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.09370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.08971">
    <id>http://arxiv.org/abs/1911.08971v1</id>
    <updated>2019-11-20T15:35:20Z</updated>
    <published>2019-11-20T15:35:20Z</published>
    <title>Faster Dynamic Compressed d-ary Relations</title>
    <summary>  The $k^2$-tree is a successful compact representation of binary relations
that exhibit sparseness and/or clustering properties. It can be extended to $d$
dimensions, where it is called a $k^d$-tree. The representation boils down to a
long bitvector. We show that interpreting the $k^d$-tree as a dynamic trie on
the Morton codes of the points, instead of as a dynamic representation of the
bitvector as done in previous work, yields operation times that are below the
lower bound of dynamic bitvectors and offers improved time performance in
practice.
</summary>
    <author>
      <name>Diego Arroyuelo</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-32686-9_30</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-32686-9_30" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. SPIRE 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.08971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.08971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.09077">
    <id>http://arxiv.org/abs/1911.09077v2</id>
    <updated>2019-11-21T21:32:34Z</updated>
    <published>2019-11-20T18:29:30Z</published>
    <title>Grammar Compressed Sequences with Rank/Select Support</title>
    <summary>  Sequence representations supporting not only direct access to their symbols,
but also rank/select operations, are a fundamental building block in many
compressed data structures. Several recent applications need to represent
highly repetitive sequences, and classical statistical compression proves
ineffective. We introduce, instead, grammar-based representations for
repetitive sequences, which use up to 6% of the space needed by statistically
compressed representations, and support direct access and rank/select
operations within tens of microseconds. We demonstrate the impact of our
structures in text indexing applications.
</summary>
    <author>
      <name>Alberto Ord√≥√±ez</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jda.2016.10.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jda.2016.10.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Discrete Algorithms 43, pp. 54-71 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.09077v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.09077v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.08376">
    <id>http://arxiv.org/abs/1911.08376v1</id>
    <updated>2019-11-19T16:18:15Z</updated>
    <published>2019-11-19T16:18:15Z</published>
    <title>Extending General Compact Querieable Representations to GIS Applications</title>
    <summary>  The raster model is commonly used for the representation of images in many
domains, and is especially useful in Geographic Information Systems (GIS) to
store information about continuous variables of the space (elevation,
temperature, etc.). Current representations of raster data are usually designed
for external memory or, when stored in main memory, lack efficient query
capabilities. In this paper we propose compact representations to efficiently
store and query raster datasets in main memory. We present different
representations for binary raster data, general raster data and time-evolving
raster data. We experimentally compare our proposals with traditional storage
mechanisms such as linear quadtrees or compressed GeoTIFF files. Results show
that our structures are up to 10 times smaller than classical linear quadtrees,
and even comparable in space to non-querieable representations of raster data,
while efficiently answering a number of typical queries.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Ana Cerdeira-Pena</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Oscar Pedreira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2019.08.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2019.08.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941,</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Sciences 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.08376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.08376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.09415">
    <id>http://arxiv.org/abs/1907.09415v1</id>
    <updated>2019-07-19T08:56:18Z</updated>
    <published>2019-07-19T08:56:18Z</published>
    <title>Quantum Computing: Lecture Notes</title>
    <summary>  This is a set of lecture notes suitable for a Master's course on quantum
computation and information from the perspective of theoretical computer
science. The first version was written in 2011, with many extensions and
improvements in subsequent years. The first 10 chapters cover the circuit model
and the main quantum algorithms (Deutsch-Jozsa, Simon, Shor, Hidden Subgroup
Problem, Grover, quantum walks, Hamiltonian simulation and HHL). They are
followed by 2 chapters about complexity, 4 chapters about distributed ("Alice
and Bob") settings, and a final chapter about quantum error correction.
Appendices A and B give a brief introduction to the required linear algebra and
some other mathematical and computer science background. All chapters come with
exercises, with some hints provided in Appendix C.
</summary>
    <author>
      <name>Ronald de Wolf</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">QuSoft, CWI and University of Amsterdam</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">165 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.09415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.09415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.08372">
    <id>http://arxiv.org/abs/1911.08372v1</id>
    <updated>2019-11-19T16:07:49Z</updated>
    <published>2019-11-19T16:07:49Z</published>
    <title>Improved Compressed String Dictionaries</title>
    <summary>  We introduce a new family of compressed data structures to efficiently store
and query large string dictionaries in main memory. Our main technique is a
combination of hierarchical Front-coding with ideas from longest-common-prefix
computation in suffix arrays. Our data structures yield relevant space-time
tradeoffs in real-world dictionaries. We focus on two domains where string
dictionaries are extensively used and efficient compression is required: URL
collections, a key element in Web graphs and applications such as Web mining;
and collections of URIs and literals, the basic components of RDF datasets. Our
experiments show that our data structures achieve better compression than the
state-of-the-art alternatives while providing very competitive query times.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Ana Cerdeira-Pena</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3357384.3357972</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3357384.3357972" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 28th ACM International Conference on Information and
  Knowledge Management (CIKM 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.08372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.08372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.07124">
    <id>http://arxiv.org/abs/1911.07124v2</id>
    <updated>2019-12-11T00:15:07Z</updated>
    <published>2019-11-17T01:18:03Z</published>
    <title>Faster Integer Multiplication Using Preprocessing</title>
    <summary>  A New Number Theoretic Transform(NTT), which is a form of FFT, is introduced,
that is faster than FFTs. Also, a multiplication algorithm is introduced that
uses this to perform integer multiplication faster than O(n log n). It uses
preprocessing to achieve an upper bounds of (n log n/(log log n/ log log log
n).
  Also, we explore the possibility of O(n) time multiplication via NTTs that
require only O(n) operations, using preprocessing.
</summary>
    <author>
      <name>Matt Groff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.07124v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07124v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.06985">
    <id>http://arxiv.org/abs/1911.06985v1</id>
    <updated>2019-11-16T08:04:25Z</updated>
    <published>2019-11-16T08:04:25Z</published>
    <title>Constructing the Bijective BWT</title>
    <summary>  The Burrows-Wheeler transform (BWT) is a permutation whose applications are
prevalent in data compression and text indexing. The bijective BWT (BBWT) is a
bijective variant of it. Although it is known that the BWT can be constructed
in linear time for integer alphabets by using a linear time suffix array
construction algorithm, it was up to now only conjectured that the BBWT can
also be constructed in linear time. We confirm this conjecture by proposing a
construction algorithm that is based on SAIS, improving the best known result
of $O(n \lg n /\lg \lg n)$ time to linear.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Juha K√§rkk√§inen</name>
    </author>
    <author>
      <name>Dominik K√∂ppl</name>
    </author>
    <author>
      <name>Marcin Picatkowski</name>
    </author>
    <link href="http://arxiv.org/abs/1911.06985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.10140">
    <id>http://arxiv.org/abs/1912.10140v1</id>
    <updated>2019-12-20T23:12:09Z</updated>
    <published>2019-12-20T23:12:09Z</published>
    <title>String factorisations with maximum or minimum dimension</title>
    <summary>  In this paper we consider two problems concerning string factorisation.
Specifically given a string $w$ and an integer $k$ find a factorisation of $w$
where each factor has length bounded by $k$ and has the minimum (the FmD
problem) or the maximum (the FMD problem) number of different factors. The FmD
has been proved to be NP-hard even if $k=2$ in [9] and for this case we provide
a $3/2$-approximation algorithm. The FMD problem, up to our knowledge has not
been considered in the literature. We show that this problem is NP-hard for any
$k\geq 3$. In view of this we propose a $2$-approximation algorithm (for any
$k$) an exact exponential algorithm. We conclude with some open problems.
</summary>
    <author>
      <name>Angelo Monti</name>
    </author>
    <author>
      <name>Blerina Sinaimeri</name>
    </author>
    <link href="http://arxiv.org/abs/1912.10140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.09783">
    <id>http://arxiv.org/abs/1912.09783v2</id>
    <updated>2020-02-11T08:33:09Z</updated>
    <published>2019-12-20T12:16:29Z</published>
    <title>Circ-Tree: A B+-Tree Variant with Circular Design for Persistent Memory</title>
    <summary>  Several B+-tree variants have been developed to exploit the performance
potential of byte-addressable non-volatile memory (NVM). In this paper, we
attentively investigate the properties of B+-tree and find that, a conventional
B+-tree node is a linear structure in which key-value (KV) pairs are maintained
from the zero offset of the node. These pairs are shifted in a unidirectional
fashion for insertions and deletions. Inserting and deleting one KV pair may
inflict a large amount of write amplifications due to shifting KV pairs. This
badly impairs the performance of in-NVM B+-tree. In this paper, we propose a
novel circular design for B+-tree. With regard to NVM's byte-addressability,
our Circ-tree design embraces tree nodes in a circular structure without a
fixed base address, and bidirectionally shifts KV pairs in a node for
insertions and deletions to minimize write amplifications. We have implemented
a prototype for Circ-Tree and conducted extensive experiments. Experimental
results show that Circ-Tree significantly outperforms two state-of-the-art
in-NVM B+-tree variants, i.e., NV-tree and FAST+FAIR, by up to 1.6x and 8.6x,
respectively, in terms of write performance. The end-to-end comparison by
running YCSB to KV store systems built on NV-tree, FAST+FAIR, and Circ-Tree
reveals that Circ-Tree yields up to 29.3% and 47.4% higher write performance,
respectively, than NV-tree and FAST+FAIR.
</summary>
    <author>
      <name>Chundong Wang</name>
    </author>
    <author>
      <name>Gunavaran Brihadiswarn</name>
    </author>
    <author>
      <name>Xingbin Jiang</name>
    </author>
    <author>
      <name>Sudipta Chattopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.09783v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.09783v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.08258">
    <id>http://arxiv.org/abs/1912.08258v3</id>
    <updated>2020-01-27T18:27:49Z</updated>
    <published>2019-12-17T20:07:53Z</published>
    <title>Xor Filters: Faster and Smaller Than Bloom and Cuckoo Filters</title>
    <summary>  The Bloom filter provides fast approximate set membership while using little
memory. Engineers often use these filters to avoid slow operations such as disk
or network accesses. As an alternative, a cuckoo filter may need less space
than a Bloom filter and it is faster. Chazelle et al. proposed a generalization
of the Bloom filter called the Bloomier filter. Dietzfelbinger and Pagh
described a variation on the Bloomier filter that can be used effectively for
approximate membership queries. It has never been tested empirically, to our
knowledge. We review an efficient implementation of their approach, which we
call the xor filter. We find that xor filters can be faster than Bloom and
cuckoo filters while using less memory. We further show that a more compact
version of xor filters (xor+) can use even less space than highly compact
alternatives (e.g., Golomb-compressed sequences) while providing speeds
competitive with Bloom filters.
</summary>
    <author>
      <name>Thomas Mueller Graf</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <link href="http://arxiv.org/abs/1912.08258v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08258v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.08147">
    <id>http://arxiv.org/abs/1912.08147v1</id>
    <updated>2019-12-17T17:29:45Z</updated>
    <published>2019-12-17T17:29:45Z</published>
    <title>New Bounds on Antipowers in Binary Words</title>
    <summary>  Fici et al. defined a word to be a k-power if it is the concatenation of k
consecutive identical blocks, and an r-antipower if it is the concatenation of
r pairwise distinct blocks of the same size. They defined N(k, r) as the
shortest length l such that every binary word of length l contains either a
k-power or an r-antipower. In this note we obtain some new upper and lower
bounds on N(k, r).
</summary>
    <author>
      <name>Lukas Fleischer</name>
    </author>
    <author>
      <name>Samin Riasat</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/1912.08147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.09625">
    <id>http://arxiv.org/abs/1903.09625v2</id>
    <updated>2019-12-10T17:58:58Z</updated>
    <published>2019-03-22T17:46:15Z</published>
    <title>Matching strings in encoded sequences</title>
    <summary>  We investigate the longest common substring problem for encoded sequences and
its asymptotic behaviour. The main result is a strong law of large numbers for
a re-scaled version of this quantity, which presents an explicit relation with
the R\'enyi entropy of the source. We apply this result to the zero-inflated
contamination model and the stochastic scrabble. In the case of dynamical
systems, this problem is equivalent to the shortest distance between two
observed orbits and its limiting relationship with the correlation dimension of
the pushforward measure. An extension to the shortest distance between orbits
for random dynamical systems is also provided.
</summary>
    <author>
      <name>Adriana Coutinho</name>
    </author>
    <author>
      <name>Rodrigo Lambert</name>
    </author>
    <author>
      <name>J√©r√¥me Rousseau</name>
    </author>
    <link href="http://arxiv.org/abs/1903.09625v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.09625v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.02900">
    <id>http://arxiv.org/abs/1912.02900v2</id>
    <updated>2019-12-21T20:45:16Z</updated>
    <published>2019-12-05T22:13:34Z</published>
    <title>Pinning Down the Strong Wilber 1 Bound for Binary Search Trees</title>
    <summary>  The dynamic optimality conjecture, postulating the existence of an
$O(1)$-competitive online algorithm for binary search trees (BSTs), is among
the most fundamental open problems in dynamic data structures. Despite
extensive work and some notable progress, including, for example, the Tango
Trees (Demaine et al., FOCS 2004), that give the best currently known $O(\log
\log n)$-competitive algorithm, the conjecture remains widely open. One of the
main hurdles towards settling the conjecture is that we currently do not have
approximation algorithms achieving better than an $O(\log \log
n)$-approximation, even in the offline setting. All known non-trivial
algorithms for BST's so far rely on comparing the algorithm's cost with the
so-called Wilber's first bound (WB-1). Therefore, establishing the worst-case
relationship between this bound and the optimal solution cost appears crucial
for further progress, and it is an interesting open question in its own right.
  Our contribution is two-fold. First, we show that the gap between the WB-1
bound and the optimal solution value can be as large as $\Omega(\log \log n/
\log \log \log n)$; in fact, the gap holds even for several stronger variants
of the bound. Second, we provide a simple algorithm, that, given an integer
$D>0$, obtains an $O(D)$-approximation in time $\exp\left(O\left
(n^{1/2^{\Omega(D)}}\log n\right )\right )$. In particular, this gives a
constant-factor approximation sub-exponential time algorithm. Moreover, we
obtain a simpler and cleaner efficient $O(\log \log n)$-approximation algorithm
that can be used in an online setting. Finally, we suggest a new bound, that we
call {\em Guillotine Bound}, that is stronger than WB, while maintaining its
algorithm-friendly nature, that we hope will lead to better algorithms. All our
results use the geometric interpretation of the problem, leading to cleaner and
simpler analysis.
</summary>
    <author>
      <name>Parinya Chalermsook</name>
    </author>
    <author>
      <name>Julia Chuzhoy</name>
    </author>
    <author>
      <name>Thatchaphol Saranurak</name>
    </author>
    <link href="http://arxiv.org/abs/1912.02900v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.02900v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.00692">
    <id>http://arxiv.org/abs/1912.00692v2</id>
    <updated>2019-12-03T13:52:57Z</updated>
    <published>2019-12-02T11:41:02Z</published>
    <title>Gardens of Eden in the Game of Life</title>
    <summary>  We prove that in the Game of Life, if the thickness-four zero-padding of a
rectangular pattern is not an orphan, then the corresponding finite-support
configuration is not a Garden of Eden, and that the preimage of every
finite-support configuration has dense semilinear configurations. In particular
finite-support Gardens of Eden are in co-NP.
</summary>
    <author>
      <name>Ville Salo</name>
    </author>
    <author>
      <name>Ilkka T√∂rm√§</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages + 5 pages of code; some figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.00692v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.00692v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.12464">
    <id>http://arxiv.org/abs/1911.12464v3</id>
    <updated>2020-01-04T12:44:38Z</updated>
    <published>2019-11-27T23:40:26Z</published>
    <title>Words With Few Palindromes, Revisited</title>
    <summary>  In 2013, Fici and Zamboni proved a number of theorems about finite and
infinite words having only a small number of factors that are palindromes. In
this paper we rederive some of their results, and obtain some new ones, by a
different method based on finite automata.
</summary>
    <author>
      <name>Lukas Fleischer</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor typo corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.12464v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.12464v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.11704">
    <id>http://arxiv.org/abs/1911.11704v2</id>
    <updated>2019-12-09T15:41:13Z</updated>
    <published>2019-11-26T17:14:56Z</published>
    <title>Words Avoiding Reversed Factors, Revisited</title>
    <summary>  In 2005, Rampersad and the second author proved a number of theorems about
infinite words x with the property that if w is any sufficiently long finite
factor of x, then its reversal w^R is not a factor of x. In this note we
revisit these results, reproving them in more generality, using machine
computations only. Two different techniques are presented.
</summary>
    <author>
      <name>Lukas Fleischer</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/1911.11704v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11704v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.11637">
    <id>http://arxiv.org/abs/1911.11637v1</id>
    <updated>2019-11-25T08:42:50Z</updated>
    <published>2019-11-25T08:42:50Z</published>
    <title>Fast Fibonacci heaps with worst case extensions</title>
    <summary>  We are concentrating on reducing overhead of heaps based on comparisons with
optimal worstcase behaviour. The paper is inspired by Strict Fibonacci Heaps
[1], where G. S. Brodal, G. Lagogiannis, and R. E. Tarjan implemented the heap
with DecreaseKey and Meld interface in assymptotically optimal worst case times
(based on key comparisons). In the paper [2], the ideas were elaborated and it
was shown that the same asymptotical times could be achieved with a strategy
loosing much less information from previous comparisons. There is big overhead
with maintainance of violation lists in these heaps. We propose simple
alternative reducing this overhead. It allows us to implement fast amortized
Fibonacci heaps, where user could call some methods in variants guaranting
worst case time. If he does so, the heaps are not guaranted to be Fibonacci
until an amortized version of a method is called. Of course we could call worst
case versions all the time, but as there is an overhead with the guarantee,
calling amortized versions is prefered choice if we are not concentrated on
complexity of the separate operation.
  We have shown, we could implement full DecreaseKey-Meld interface, but Meld
interface is not natural for these heaps, so if Meld is not needed, much
simpler implementation suffices. As I don't know application requiring Meld, we
would concentrate on noMeld variant, but we will show the changes could be
applied on Meld including variant as well. The papers [1], [2] shown the heaps
could be implemented on pointer machine model. For fast practical
implementations we would rather use arrays. Our goal is to reduce number of
pointer manipulations. Maintainance of ranks by pointers to rank lists would be
unnecessary overhead.
</summary>
    <author>
      <name>Vladan Majerech</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages at all, 4+2/2 pages of tables, 1 figure. arXiv admin note:
  text overlap with arXiv:1911.04372</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.11637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.11944">
    <id>http://arxiv.org/abs/1912.11944v1</id>
    <updated>2019-12-26T22:51:13Z</updated>
    <published>2019-12-26T22:51:13Z</published>
    <title>On the Reproducibility of Experiments of Indexing Repetitive Document
  Collections</title>
    <summary>  This work introduces a companion reproducible paper with the aim of allowing
the exact replication of the methods, experiments, and results discussed in a
previous work [5]. In that parent paper, we proposed many and varied techniques
for compressing indexes which exploit that highly repetitive collections are
formed mostly of documents that are near-copies of others. More concretely, we
describe a replication framework, called uiHRDC (universal indexes for Highly
Repetitive Document Collections), that allows our original experimental setup
to be easily replicated using various document collections. The corresponding
experimentation is carefully explained, providing precise details about the
parameters that can be tuned for each indexing solution. Finally, note that we
also provide uiHRDC as reproducibility package.
</summary>
    <author>
      <name>Antonio Fari√±a</name>
    </author>
    <author>
      <name>Miguel A. Mart√≠nez-Prieto</name>
    </author>
    <author>
      <name>Francisco Claude</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Juan J. Lastra-D√≠az</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Diego Seco</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.is.2019.03.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.is.2019.03.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. Replication framework
  available at: https://github.com/migumar2/uiHRDC/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Systems; Volume 83, July 2019; pages 181-194</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.11944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.11866">
    <id>http://arxiv.org/abs/1912.11866v1</id>
    <updated>2019-12-26T13:50:46Z</updated>
    <published>2019-12-26T13:50:46Z</published>
    <title>Efficient processing of raster and vector data</title>
    <summary>  In this work, we propose a framework to store and manage spatial data, which
includes new efficient algorithms to perform operations accepting as input a
raster dataset and a vector dataset. More concretely, we present algorithms for
solving a spatial join between a raster and a vector dataset imposing a
restriction on the values of the cells of the raster; and an algorithm for
retrieving K objects of a vector dataset that overlap cells of a raster
dataset, such that the K objects are those overlapping the highest (or lowest)
cell values among all objects.
  The raster data is stored using a compact data structure, which can directly
manipulate compressed data without the need for prior decompression. This leads
to better running times and lower memory consumption. In our experimental
evaluation comparing our solution to other baselines, we obtain the best
space/time trade-offs.
</summary>
    <author>
      <name>Fernando Silva-Coira</name>
    </author>
    <author>
      <name>Jos√© R. Param√°</name>
    </author>
    <author>
      <name>Susana Ladra</name>
    </author>
    <author>
      <name>Juan R. L√≥pez</name>
    </author>
    <author>
      <name>Gilberto Guti√©rrez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0226943</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0226943" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941 To appear in PLOS One (2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.11866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.11388">
    <id>http://arxiv.org/abs/1912.11388v1</id>
    <updated>2019-12-23T03:07:12Z</updated>
    <published>2019-12-23T03:07:12Z</published>
    <title>The Weak Circular Repetition Threshold Over Large Alphabets</title>
    <summary>  The repetition threshold for words on $n$ letters, denoted $\mbox{RT}(n)$, is
the infimum of the set of all $r$ such that there are arbitrarily long $r$-free
words over $n$ letters. A repetition threshold for circular words on $n$
letters can be defined in three natural ways, which gives rise to the weak,
intermediate, and strong circular repetition thresholds for $n$ letters,
denoted $\mbox{CRT}_{\mbox{W}}(n)$, $\mbox{CRT}_{\mbox{I}}(n)$, and
$\mbox{CRT}_{\mbox{S}}(n)$, respectively. Currie and the present authors
conjectured that
$\mbox{CRT}_{\mbox{I}}(n)=\mbox{CRT}_{\mbox{W}}(n)=\mbox{RT}(n)$ for all $n\geq
4$. We prove that $\mbox{CRT}_{\mbox{W}}(n)=\mbox{RT}(n)$ for all $n\geq 45$,
which confirms a weak version of this conjecture for all but finitely many
values of $n$.
</summary>
    <author>
      <name>Lucas Mol</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1911.05779</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.11388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15 (primary), 05C15 (secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.11417">
    <id>http://arxiv.org/abs/1912.11417v1</id>
    <updated>2019-12-24T15:39:46Z</updated>
    <published>2019-12-24T15:39:46Z</published>
    <title>Flat combined Red Black Trees</title>
    <summary>  Flat combining is a concurrency threaded technique whereby one thread
performs all the operations in batch by scanning a queue of operations
to-be-done and performing them together. Flat combining makes sense as long as
k operations each taking O(n) separately can be batched together and done in
less than O(k*n). Red black tree is a balanced binary search tree with
permanent balancing warranties. Operations in red black tree are hard to batch
together: for example inserting nodes in two different branches of the tree
affect different areas of the tree. In this paper we investigate alternatives
to making a flat combine approach work for red black trees.
</summary>
    <author>
      <name>Sergio Sainz-Palacios</name>
    </author>
    <link href="http://arxiv.org/abs/1912.11417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.05239">
    <id>http://arxiv.org/abs/2001.05239v1</id>
    <updated>2020-01-15T11:13:17Z</updated>
    <published>2020-01-15T11:13:17Z</published>
    <title>Optimal Skeleton Huffman Trees Revisited</title>
    <summary>  A skeleton Huffman tree is a Huffman tree in which all disjoint maximal
perfect subtrees are shrunk into leaves. Skeleton Huffman trees, besides saving
storage space, are also used for faster decoding and for speeding up
Huffman-shaped wavelet trees. In 2017 Klein et al. introduced an optimal
skeleton tree: for given symbol frequencies, it has the least number of nodes
among all optimal prefix-free code trees (not necessarily Huffman's) with
shrunk perfect subtrees. Klein et al. described a simple algorithm that, for
fixed codeword lengths, finds a skeleton tree with the least number of nodes;
with this algorithm one can process each set of optimal codeword lengths to
find an optimal skeleton tree. However, there are exponentially many such sets
in the worst case. We describe an $O(n^2\log n)$-time algorithm that, given $n$
symbol frequencies, constructs an optimal skeleton tree and its corresponding
optimal code.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Oleg Merkurev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.05239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.04505">
    <id>http://arxiv.org/abs/2001.04505v1</id>
    <updated>2020-01-13T19:20:14Z</updated>
    <published>2020-01-13T19:20:14Z</published>
    <title>Fast Generation of Big Random Binary Trees</title>
    <summary>  random_tree() is a linear time and space C++ implementation able to create
trees of up to a billion nodes for genetic programming and genetic improvement
experiments. A 3.60GHz CPU can generate more than 18 million random nodes for
GP program trees per second.
</summary>
    <author>
      <name>William B. Langdon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">C++ code:
  http://www.cs.ucl.ac.uk/staff/W.Langdon/ftp/gp-code/rand_tree.cc_r1.43</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.04505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.04760">
    <id>http://arxiv.org/abs/2001.04760v1</id>
    <updated>2020-01-14T13:19:41Z</updated>
    <published>2020-01-14T13:19:41Z</published>
    <title>Simulation computation in grammar-compressed graphs</title>
    <summary>  Like [1], we present an algorithm to compute the simulation of a query
pattern in a graph of labeled nodes and unlabeled edges. However, our algorithm
works on a compressed graph grammar, instead of on the original graph. The
speed-up of our algorithm compared to the algorithm in [1] grows with the size
of the graph and with the compression strength.
</summary>
    <author>
      <name>Stefan B√∂ttcher</name>
    </author>
    <author>
      <name>Rita Hartel</name>
    </author>
    <author>
      <name>Sven Peeters</name>
    </author>
    <link href="http://arxiv.org/abs/2001.04760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.03147">
    <id>http://arxiv.org/abs/2001.03147v1</id>
    <updated>2020-01-09T18:23:11Z</updated>
    <published>2020-01-09T18:23:11Z</published>
    <title>Age-Partitioned Bloom Filters</title>
    <summary>  Bloom filters (BF) are widely used for approximate membership queries over a
set of elements. BF variants allow removals, sets of unbounded size or querying
a sliding window over an unbounded stream. However, for this last case the best
current approaches are dictionary based (e.g., based on Cuckoo Filters or
TinyTable), and it may seem that BF-based approaches will never be competitive
to dictionary-based ones. In this paper we present Age-Partitioned Bloom
Filters, a BF-based approach for duplicate detection in sliding windows that
not only is competitive in time-complexity, but has better space usage than
current dictionary-based approaches (e.g., SWAMP), at the cost of some moderate
slack. APBFs retain the BF simplicity, unlike dictionary-based approaches,
important for hardware-based implementations, and can integrate known
improvements such as double hashing or blocking. We present an Age-Partitioned
Blocked Bloom Filter variant which can operate with 2-3 cache-line accesses per
insertion and around 2-4 per query, even for high accuracy filters.
</summary>
    <author>
      <name>Ariel Shtul</name>
    </author>
    <author>
      <name>Carlos Baquero</name>
    </author>
    <author>
      <name>Paulo S√©rgio Almeida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.03147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.03147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.02172">
    <id>http://arxiv.org/abs/2001.02172v1</id>
    <updated>2020-01-07T16:56:23Z</updated>
    <published>2020-01-07T16:56:23Z</published>
    <title>Data Structure Primitives on Persistent Memory: An Evaluation</title>
    <summary>  Persistent Memory (PM), as already available e.g. with Intel Optane DC
Persistent Memory, represents a very promising, next generation memory solution
with a significant impact on database architectures. Several data structures
for this new technology and its properties have already been proposed. However,
primarily merely complete structures were presented and evaluated hiding the
impact of the individual ideas and PM characteristics. Therefore, in this
paper, we disassemble the structures presented so far, identify their
underlying design primitives, and assign them to appropriate design goals
regarding PM. As a result of our comprehensive experiments on real PM hardware,
we were able to reveal the trade-offs of the primitives at the micro level.
From this, performance profiles could be derived for selected primitives. With
these it is possible to precisely identify their best use cases as well as
vulnerabilities. Beside our general insights regarding PM-based data structure
design, we also discovered new promising combinations not considered in the
literature so far.
</summary>
    <author>
      <name>Philipp G√∂tze</name>
    </author>
    <author>
      <name>Arun Kumar Tharanatha</name>
    </author>
    <author>
      <name>Kai-Uwe Sattler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 15 figures, submitted to PVLDB</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.02172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.02172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.02139">
    <id>http://arxiv.org/abs/2001.02139v2</id>
    <updated>2020-01-17T13:49:47Z</updated>
    <published>2020-01-07T15:55:52Z</published>
    <title>Computing the rearrangement distance of natural genomes</title>
    <summary>  The computation of genomic distances has been a very active field of
computational comparative genomics over the last 25 years. Substantial results
include the polynomial-time computability of the inversion distance by
Hannenhalli and Pevzner in 1995 and the introduction of the double-cut and join
(DCJ) distance by Yancopoulos et al. in 2005. Both results, however, rely on
the assumption that the genomes under comparison contain the same set of unique
markers (syntenic genomic regions, sometimes also referred to as genes). In
2015, Shao, Lin and Moret relax this condition by allowing for duplicate
markers in the analysis. This generalized version of the genomic distance
problem is NP-hard, and they give an ILP solution that is efficient enough to
be applied to real-world datasets. A restriction of their approach is that it
can be applied only to balanced genomes, that have equal numbers of duplicates
of any marker. Therefore it still needs a delicate preprocessing of the input
data in which excessive copies of unbalanced markers have to be removed.
  In this paper we present an algorithm solving the genomic distance problem
for natural genomes, in which any marker may occur an arbitrary number of
times. Our method is based on a new graph data structure, the multi-relational
diagram, that allows an elegant extension of the ILP by Shao, Lin and Moret to
count runs of markers that are under- or over-represented in one genome with
respect to the other and need to be inserted or deleted, respectively. With
this extension, previous restrictions on the genome configurations are lifted,
for the first time enabling an uncompromising rearrangement analysis. Any
marker sequence can directly be used for the distance calculation.
  The evaluation of our approach shows that it can be used to analyze genomes
with up to a few ten thousand markers, which we demonstrate on simulated and
real data.
</summary>
    <author>
      <name>Leonard Bohnenk√§mper</name>
    </author>
    <author>
      <name>Mar√≠lia D. V. Braga</name>
    </author>
    <author>
      <name>Daniel Doerr</name>
    </author>
    <author>
      <name>Jens Stoye</name>
    </author>
    <link href="http://arxiv.org/abs/2001.02139v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.02139v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.01914">
    <id>http://arxiv.org/abs/2001.01914v1</id>
    <updated>2020-01-07T07:22:02Z</updated>
    <published>2020-01-07T07:22:02Z</published>
    <title>Quantum Algorithms for the Most Frequently String Search, Intersection
  of Two String Sequences and Sorting of Strings Problems</title>
    <summary>  We study algorithms for solving three problems on strings. The first one is
the Most Frequently String Search Problem. The problem is the following. Assume
that we have a sequence of $n$ strings of length $k$. The problem is finding
the string that occurs in the sequence most often. We propose a quantum
algorithm that has a query complexity $\tilde{O}(n \sqrt{k})$. This algorithm
shows speed-up comparing with the deterministic algorithm that requires
$\Omega(nk)$ queries. The second one is searching intersection of two sequences
of strings. All strings have the same length $k$. The size of the first set is
$n$ and the size of the second set is $m$. We propose a quantum algorithm that
has a query complexity $\tilde{O}((n+m) \sqrt{k})$. This algorithm shows
speed-up comparing with the deterministic algorithm that requires
$\Omega((n+m)k)$ queries. The third problem is sorting of $n$ strings of length
$k$. On the one hand, it is known that quantum algorithms cannot sort objects
asymptotically faster than classical ones. On the other hand, we focus on
sorting strings that are not arbitrary objects. We propose a quantum algorithm
that has a query complexity $O(n (\log n)^2 \sqrt{k})$. This algorithm shows
speed-up comparing with the deterministic algorithm (radix sort) that requires
$\Omega((n+d)k)$ queries, where $d$ is a size of the alphabet.
</summary>
    <author>
      <name>Kamil Khadiev</name>
    </author>
    <author>
      <name>Artem Ilikaev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-34500-6_17</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-34500-6_17" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">THe paper was presented on TPNC 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">TPNC 2019. Lecture Notes in Computer Science, vol 11934. Springer,
  Cham</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2001.01914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.01661">
    <id>http://arxiv.org/abs/2001.01661v2</id>
    <updated>2020-01-24T11:22:10Z</updated>
    <published>2020-01-06T16:52:44Z</published>
    <title>A Hybrid Approach to Temporal Pattern Matching</title>
    <summary>  The primary objective of graph pattern matching is to find all appearances of
an input graph pattern query in a large data graph. Such appearances are called
matches. In this paper, we are interested in finding matches of interaction
patterns in temporal graphs. To this end, we propose a hybrid approach that
achieves effective filtering of potential matches based both on structure and
time. Our approach exploits a graph representation where edges are ordered by
time. We present experiments with real datasets that illustrate the efficiency
of our approach.
</summary>
    <author>
      <name>Konstantinos Semertzidis</name>
    </author>
    <author>
      <name>Evaggelia Pitoura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.01661v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01661v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.00211">
    <id>http://arxiv.org/abs/2001.00211v1</id>
    <updated>2020-01-01T14:03:31Z</updated>
    <published>2020-01-01T14:03:31Z</published>
    <title>Approximating Text-to-Pattern Hamming Distances</title>
    <summary>  We revisit a fundamental problem in string matching: given a pattern of
length m and a text of length n, both over an alphabet of size $\sigma$,
compute the Hamming distance between the pattern and the text at every
location. Several $(1+\epsilon)$-approximation algorithms have been proposed in
the literature, with running time of the form $O(\epsilon^{-O(1)}n\log n\log
m)$, all using fast Fourier transform (FFT). We describe a simple
$(1+\epsilon)$-approximation algorithm that is faster and does not need FFT.
Combining our approach with additional ideas leads to numerous new results:
  - We obtain the first linear-time approximation algorithm; the running time
is $O(\epsilon^{-2}n)$.
  - We obtain a faster exact algorithm computing all Hamming distances up to a
given threshold k; its running time improves previous results by logarithmic
factors and is linear if $k\le\sqrt m$.
  - We obtain approximation algorithms with better $\epsilon$-dependence using
rectangular matrix multiplication. The time-bound is $\~O(n)$ when the pattern
is sufficiently long: $m\ge \epsilon^{-28}$. Previous algorithms require
$\~O(\epsilon^{-1}n)$ time.
  - When k is not too small, we obtain a truly sublinear-time algorithm to find
all locations with Hamming distance approximately (up to a constant factor)
less than k, in $O((n/k^{\Omega(1)}+occ)n^{o(1)})$ time, where occ is the
output size. The algorithm leads to a property tester, returning true if an
exact match exists and false if the Hamming distance is more than $\delta m$ at
every location, running in $\~O(\delta^{-1/3}n^{2/3}+\delta^{-1}n/m)$ time.
  - We obtain a streaming algorithm to report all locations with Hamming
distance approximately less than k, using $\~O(\epsilon^{-2}\sqrt k)$ space.
Previously, streaming algorithms were known for the exact problem with \~O(k)
space or for the approximate problem with $\~O(\epsilon^{-O(1)}\sqrt m)$ space.
</summary>
    <author>
      <name>Timothy M. Chan</name>
    </author>
    <author>
      <name>Shay Golan</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.00218">
    <id>http://arxiv.org/abs/2001.00218v3</id>
    <updated>2020-02-22T16:09:43Z</updated>
    <published>2020-01-01T15:04:43Z</published>
    <title>Lossless Compression of Deep Neural Networks</title>
    <summary>  Deep neural networks have been successful in many predictive modeling tasks,
such as image and language recognition, where large neural networks are often
used to obtain good accuracy. Consequently, it is challenging to deploy these
networks under limited computational resources, such as in mobile devices. In
this work, we introduce an algorithm that removes units and layers of a neural
network while not changing the output that is produced, which thus implies a
lossless compression. This algorithm, which we denote as LEO (Lossless
Expressiveness Optimization), relies on Mixed-Integer Linear Programming (MILP)
to identify Rectified Linear Units (ReLUs) with linear behavior over the input
domain. By using L1 regularization to induce such behavior, we can benefit from
training over a larger architecture than we would later use in the environment
where the trained neural network is deployed.
</summary>
    <author>
      <name>Thiago Serra</name>
    </author>
    <author>
      <name>Abhinav Kumar</name>
    </author>
    <author>
      <name>Srikumar Ramalingam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CPAIOR 2020 (to appear)</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.00218v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00218v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.03459">
    <id>http://arxiv.org/abs/2002.03459v1</id>
    <updated>2020-02-09T21:58:36Z</updated>
    <published>2020-02-09T21:58:36Z</published>
    <title>Approximating Text-to-Pattern Distance via Dimensionality Reduction</title>
    <summary>  Text-to-pattern distance is a fundamental problem in string matching, where
given a pattern of length $m$ and a text of length $n$, over integer alphabet,
we are asked to compute the distance between pattern and text at every
location. The distance function can be e.g. Hamming distance or $\ell_p$
distance for some parameter $p > 0$. Almost all state-of-the-art exact and
approximate algorithms developed in the past $\sim 40$ years were using FFT as
a black-box. In this work we present $\widetilde{O}(n/\varepsilon^2)$ time
algorithms for $(1\pm\varepsilon)$-approximation of $\ell_2$ distances, and
$\widetilde{O}(n/\varepsilon^3)$ algorithm for approximation of Hamming and
$\ell_1$ distances, all without use of FFT. This is independent to the very
recent development by Chan et al. [STOC 2020], where $O(n/\varepsilon^2)$
algorithm for Hamming distances not using FFT was presented -- although their
algorithm is much more "combinatorial", our techniques apply to other norms
than Hamming.
</summary>
    <author>
      <name>Przemys≈Çaw Uzna≈Ñski</name>
    </author>
    <link href="http://arxiv.org/abs/2002.03459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1601.05706">
    <id>http://arxiv.org/abs/1601.05706v1</id>
    <updated>2016-01-21T16:52:27Z</updated>
    <published>2016-01-21T16:52:27Z</published>
    <title>Pachinko</title>
    <summary>  Inspired by the Japanese game Pachinko, we study simple (perfectly
"inelastic" collisions) dynamics of a unit ball falling amidst point obstacles
(pins) in the plane. A classic example is that a checkerboard grid of pins
produces the binomial distribution, but what probability distributions result
from different pin placements? In the 50-50 model, where the pins form a subset
of this grid, not all probability distributions are possible, but surprisingly
the uniform distribution is possible for $\{1,2,4,8,16\}$ possible drop
locations. Furthermore, every probability distribution can be approximated
arbitrarily closely, and every dyadic probability distribution can be divided
by a suitable power of $2$ and then constructed exactly (along with extra
"junk" outputs). In a more general model, if a ball hits a pin off center, it
falls left or right accordingly. Then we prove a universality result: any
distribution of $n$ dyadic probabilities, each specified by $k$ bits, can be
constructed using $O(n k^2)$ pins, which is close to the information-theoretic
lower bound of $\Omega(n k)$.
</summary>
    <author>
      <name>Hugo A. Akitaya</name>
    </author>
    <author>
      <name>Erik D. Demaine</name>
    </author>
    <author>
      <name>Martin L. Demaine</name>
    </author>
    <author>
      <name>Adam Hesterberg</name>
    </author>
    <author>
      <name>Ferran Hurtado</name>
    </author>
    <author>
      <name>Jason S. Ku</name>
    </author>
    <author>
      <name>Jayson Lynch</name>
    </author>
    <link href="http://arxiv.org/abs/1601.05706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.05706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.11218">
    <id>http://arxiv.org/abs/2001.11218v2</id>
    <updated>2020-03-16T09:53:32Z</updated>
    <published>2020-01-30T09:08:41Z</published>
    <title>Reconstructing Words from Right-Bounded-Block Words</title>
    <summary>  A reconstruction problem of words from scattered factors asks for the minimal
information, like multisets of scattered factors of a given length or the
number of occurrences of scattered factors from a given set, necessary to
uniquely determine a word. We show that a word $w \in \{a, b\}^{*}$ can be
reconstructed from the number of occurrences of at most $\min(|w|_a, |w|_b)+ 1$
scattered factors of the form $a^{i} b$. Moreover, we generalize the result to
alphabets of the form $\{1,\ldots,q\}$ by showing that at most $
\sum^{q-1}_{i=1} |w|_i (q-i+1)$ scattered factors suffices to reconstruct $w$.
Both results improve on the upper bounds known so far. Complexity time bounds
on reconstruction algorithms are also considered here.
</summary>
    <author>
      <name>Pamela Fleischmann</name>
    </author>
    <author>
      <name>Marie Lejeune</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Dirk Nowotka</name>
    </author>
    <author>
      <name>Michel Rigo</name>
    </author>
    <link href="http://arxiv.org/abs/2001.11218v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11218v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.11763">
    <id>http://arxiv.org/abs/2001.11763v1</id>
    <updated>2020-01-31T10:51:37Z</updated>
    <published>2020-01-31T10:51:37Z</published>
    <title>Lengths of extremal square-free ternary words</title>
    <summary>  A square-free word $w$ over a fixed alphabet $\Sigma$ is extremal if every
word obtained from $w$ by inserting a single letter from $\Sigma$ (at any
position) contains a square. Grytczuk et al. recently introduced the concept of
extremal square-free word, and demonstrated that there are arbitrarily long
extremal square-free ternary words. We find all lengths which admit an extremal
square-free ternary word. In particular, we show that there is an extremal
square-free ternary word of every sufficiently large length. We also solve the
analogous problem for circular words.
</summary>
    <author>
      <name>Lucas Mol</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <link href="http://arxiv.org/abs/2001.11763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.11732">
    <id>http://arxiv.org/abs/2001.11732v1</id>
    <updated>2020-01-31T09:27:41Z</updated>
    <published>2020-01-31T09:27:41Z</published>
    <title>On the binomial equivalence classes of finite words</title>
    <summary>  Two finite words $u$ and $v$ are $k$-binomially equivalent if, for each word
$x$ of length at most $k$, $x$ appears the same number of times as a
subsequence (i.e., as a scattered subword) of both $u$ and $v$. This notion
generalizes abelian equivalence. In this paper, we study the equivalence
classes induced by the $k$-binomial equivalence with a special focus on the
cardinalities of the classes. We provide an algorithm generating the
$2$-binomial equivalence class of a word. For $k \geq 2$ and alphabet of $3$ or
more symbols, the language made of lexicographically least elements of every
$k$-binomial equivalence class and the language of singletons, i.e., the words
whose $k$-binomial equivalence class is restricted to a single element, are
shown to be non context-free. As a consequence of our discussions, we also
prove that the submonoid generated by the generators of the free nil-$2$ group
on $m$ generators is isomorphic to the quotient of the free monoid $\{ 1,
\ldots , m\}^{*}$ by the $2$-binomial equivalence.
</summary>
    <author>
      <name>Marie Lejeune</name>
    </author>
    <author>
      <name>Michel Rigo</name>
    </author>
    <author>
      <name>Matthieu Rosenfeld</name>
    </author>
    <link href="http://arxiv.org/abs/2001.11732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.08516">
    <id>http://arxiv.org/abs/2001.08516v1</id>
    <updated>2020-01-23T13:57:47Z</updated>
    <published>2020-01-23T13:57:47Z</published>
    <title>Communication-Efficient String Sorting</title>
    <summary>  There has been surprisingly little work on algorithms for sorting strings on
distributed-memory parallel machines. We develop efficient algorithms for this
problem based on the multi-way merging principle. These algorithms inspect only
characters that are needed to determine the sorting order. Moreover,
communication volume is reduced by also communicating (roughly) only those
characters and by communicating repetitions of the same prefixes only once.
Experiments on up to 1280 cores reveal that these algorithm are often more than
five times faster than previous algorithms.
</summary>
    <author>
      <name>Timo Bingmann</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <author>
      <name>Matthias Schimek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version to appear at IPDPS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.08516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.08516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.08679">
    <id>http://arxiv.org/abs/2001.08679v1</id>
    <updated>2020-01-23T17:20:36Z</updated>
    <published>2020-01-23T17:20:36Z</published>
    <title>$O(\log \log n)$ Worst-Case Local Decoding and Update Efficiency for
  Data Compression</title>
    <summary>  This paper addresses the problem of data compression with local decoding and
local update. A compression scheme has worst-case local decoding $d_{wc}$ if
any bit of the raw file can be recovered by probing at most $d_{wc}$ bits of
the compressed sequence, and has update efficiency of $u_{wc}$ if a single bit
of the raw file can be updated by modifying at most $u_{wc}$ bits of the
compressed sequence. This article provides an entropy-achieving compression
scheme for memoryless sources that simultaneously achieves $ O(\log\log n) $
local decoding and update efficiency. Key to this achievability result is a
novel succinct data structure for sparse sequences which allows efficient local
decoding and local update. Under general assumptions on the local decoder and
update algorithms, a converse result shows that $d_{wc}$ and $u_{wc}$ must grow
as $ \Omega(\log\log n) $.
</summary>
    <author>
      <name>Shashank Vatedka</name>
    </author>
    <author>
      <name>Venkat Chandar</name>
    </author>
    <author>
      <name>Aslan Tchamkerten</name>
    </author>
    <link href="http://arxiv.org/abs/2001.08679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.08679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.06864">
    <id>http://arxiv.org/abs/2001.06864v1</id>
    <updated>2020-01-19T16:58:58Z</updated>
    <published>2020-01-19T16:58:58Z</published>
    <title>Chaining with overlaps revisited</title>
    <summary>  Chaining algorithms aim to form a semi-global alignment of two sequences
based on a set of anchoring local alignments as input. Depending on the
optimization criteria and the exact definition of a chain, there are several
$O(n \log n)$ time algorithms to solve this problem optimally, where $n$ is the
number of input anchors.
  In this paper, we focus on a formulation allowing the anchors to overlap in a
chain. This formulation was studied by Shibuya and Kurochin (WABI 2003), but
their algorithm comes with no proof of correctness. We revisit and modify their
algorithm to consider a strict definition of precedence relation on anchors,
adding the required derivation to convince on the correctness of the resulting
algorithm that runs in $O(n \log^2 n)$ time on anchors formed by exact matches.
With the more relaxed definition of precedence relation considered by Shibuya
and Kurochin or when anchors are non-nested such as matches of uniform length
($k$-mers), the algorithm takes $O(n \log n)$ time.
  We also establish a connection between chaining with overlaps to the widely
studied longest common subsequence (LCS) problem.
</summary>
    <author>
      <name>Veli M√§kinen</name>
    </author>
    <author>
      <name>Kristoffer Sahlin</name>
    </author>
    <link href="http://arxiv.org/abs/2001.06864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.06864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.05976">
    <id>http://arxiv.org/abs/2001.05976v1</id>
    <updated>2020-01-16T18:20:04Z</updated>
    <published>2020-01-16T18:20:04Z</published>
    <title>Generalised Pattern Matching Revisited</title>
    <summary>  In the problem of $\texttt{Generalised Pattern Matching}\ (\texttt{GPM})$
[STOC'94, Muthukrishnan and Palem], we are given a text $T$ of length $n$ over
an alphabet $\Sigma_T$, a pattern $P$ of length $m$ over an alphabet
$\Sigma_P$, and a matching relationship $\subseteq \Sigma_T \times \Sigma_P$,
and must return all substrings of $T$ that match $P$ (reporting) or the number
of mismatches between each substring of $T$ of length $m$ and $P$ (counting).
In this work, we improve over all previously known algorithms for this problem
for various parameters describing the input instance:
  * $\mathcal{D}\,$ being the maximum number of characters that match a fixed
character,
  * $\mathcal{S}\,$ being the number of pairs of matching characters,
  * $\mathcal{I}\,$ being the total number of disjoint intervals of characters
that match the $m$ characters of the pattern $P$.
  At the heart of our new deterministic upper bounds for $\mathcal{D}\,$ and
$\mathcal{S}\,$ lies a faster construction of superimposed codes, which solves
an open problem posed in [FOCS'97, Indyk] and can be of independent interest.
To conclude, we demonstrate first lower bounds for $\texttt{GPM}$. We start by
showing that any deterministic or Monte Carlo algorithm for $\texttt{GPM}$ must
use $\Omega(\mathcal{S})$ time, and then proceed to show higher lower bounds
for combinatorial algorithms. These bounds show that our algorithms are almost
optimal, unless a radically new approach is developed.
</summary>
    <author>
      <name>Bart≈Çomiej Dudek</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <link href="http://arxiv.org/abs/2001.05976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.05671">
    <id>http://arxiv.org/abs/2001.05671v1</id>
    <updated>2020-01-16T06:30:29Z</updated>
    <published>2020-01-16T06:30:29Z</published>
    <title>Faster STR-EC-LCS Computation</title>
    <summary>  The longest common subsequence (LCS) problem is a central problem in
stringology that finds the longest common subsequence of given two strings $A$
and $B$. More recently, a set of four constrained LCS problems (called
generalized constrained LCS problem) were proposed by Chen and Chao [J. Comb.
Optim, 2011]. In this paper, we consider the substring-excluding constrained
LCS (STR-EC-LCS) problem. A string $Z$ is said to be an STR-EC-LCS of two given
strings $A$ and $B$ excluding $P$ if, $Z$ is one of the longest common
subsequences of $A$ and $B$ that does not contain $P$ as a substring. Wang et
al. proposed a dynamic programming solution which computes an STR-EC-LCS in
$O(mnr)$ time and space where $m = |A|, n = |B|, r = |P|$ [Inf. Process. Lett.,
2013]. In this paper, we show a new solution for the STR-EC-LCS problem. Our
algorithm computes an STR-EC-LCS in $O(n|\Sigma| + (L+1)(m-L+1)r)$ time where
$|\Sigma| \leq \min\{m, n\}$ denotes the set of distinct characters occurring
in both $A$ and $B$, and $L$ is the length of the STR-EC-LCS. This algorithm is
faster than the $O(mnr)$-time algorithm for short/long STR-EC-LCS (namely, $L
\in O(1)$ or $m-L \in O(1)$), and is at least as efficient as the $O(mnr)$-time
algorithm for all cases.
</summary>
    <author>
      <name>Kohei Yamada</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2001.05671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.06265">
    <id>http://arxiv.org/abs/2002.06265v1</id>
    <updated>2020-02-14T22:02:10Z</updated>
    <published>2020-02-14T22:02:10Z</published>
    <title>On Extensions of Maximal Repeats in Compressed Strings</title>
    <summary>  This paper provides an upper bound for several subsets of maximal repeats and
maximal pairs in compressed strings and also presents a formerly unknown
relationship between maximal pairs and the run-length Burrows-Wheeler
transform.
  This relationship is used to obtain a different proof for the Burrows-Wheeler
conjecture which has recently been proven by Kempa and Kociumaka in "Resolution
of the Burrows-Wheeler Transform Conjecture".
  More formally, this paper proves that a string $S$ with $z$ LZ77-factors and
without $q$-th powers has at most $73(\log_2 |S|)(z+2)^2$ runs in the
run-length Burrows-Wheeler transform and the number of arcs in the compacted
directed acyclic word graph of $S$ is bounded from above by $18q(1+\log_q
|S|)(z+2)^2$.
</summary>
    <author>
      <name>Julian Pape-Lange</name>
    </author>
    <link href="http://arxiv.org/abs/2002.06265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.06764">
    <id>http://arxiv.org/abs/2002.06764v1</id>
    <updated>2020-02-17T04:16:05Z</updated>
    <published>2020-02-17T04:16:05Z</published>
    <title>Computing Covers under Substring Consistent Equivalence Relations</title>
    <summary>  Covers are a kind of quasiperiodicity in strings. A string $C$ is a cover of
another string $T$ if any position of $T$ is inside some occurrence of $C$ in
$T$. The literature has proposed linear-time algorithms computing longest and
shortest cover arrays taking border arrays as input. An equivalence relation
$\approx$ over strings is called a substring consistent equivalence relation
(SCER) iff $X \approx Y$ implies (1) $|X| = |Y|$ and (2) $X[i:j] \approx
Y[i:j]$ for all $1 \le i \le j \le |X|$. In this paper, we generalize the
notion of covers for SCERs and prove that existing algorithms to compute the
shortest cover array and the longest cover array of a string $T$ under the
identity relation will work for any SCERs taking the accordingly generalized
border arrays.
</summary>
    <author>
      <name>Natsumi Kikuchi</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.06764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.06786">
    <id>http://arxiv.org/abs/2002.06786v1</id>
    <updated>2020-02-17T06:08:01Z</updated>
    <published>2020-02-17T06:08:01Z</published>
    <title>DAWGs for parameterized matching: online construction and related
  indexing structures</title>
    <summary>  Two strings $x$ and $y$ over $\Sigma \cup \Pi$ of equal length are said to
parameterized match (p-match) if there is a renaming bijection $f:\Sigma \cup
\Pi \rightarrow \Sigma \cup \Pi$ that is identity on $\Sigma$ and transforms
$x$ to $y$ (or vice versa). The p-matching problem is to look for substrings in
a text that p-match a given pattern. In this paper, we propose parameterized
suffix automata (p-suffix automata) and parameterized directed acyclic word
graphs (PDAWGs) which are the p-matching versions of suffix automata and DAWGs.
While suffix automata and DAWGs are equivalent for standard strings, we show
that p-suffix automata can have $\Theta(n^2)$ nodes and edges but PDAWGs have
only $O(n)$ nodes and edges, where $n$ is the length of an input string. We
also give $O(n |\Pi| \log (|\Pi| + |\Sigma|))$-time $O(n)$-space algorithm that
builds the PDAWG in a left-to-right online manner. We then show that an
implicit representation for the PDAWG can be built in $O(n \log (|\Pi| +
|\Sigma|))$ time and $O(n)$ space from left to right. As a byproduct, it is
shown that the parameterized suffix tree for the reversed string can also be
built in the same time and space, in a right-to-left online manner. We also
discuss parameterized compact DAWGs.
</summary>
    <author>
      <name>Katsuhito Nakashima</name>
    </author>
    <author>
      <name>Noriki Fujisato</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.06786v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06786v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.06796">
    <id>http://arxiv.org/abs/2002.06796v1</id>
    <updated>2020-02-17T06:39:57Z</updated>
    <published>2020-02-17T06:39:57Z</published>
    <title>Detecting $k$-(Sub-)Cadences and Equidistant Subsequence Occurrences</title>
    <summary>  The equidistant subsequence pattern matching problem is considered. Given a
pattern string $P$ and a text string $T$, we say that $P$ is an
\emph{equidistant subsequence} of $T$ if $P$ is a subsequence of the text such
that consecutive symbols of $P$ in the occurrence are equally spaced. We can
consider the problem of equidistant subsequences as generalizations of
(sub-)cadences. We give bit-parallel algorithms that yield $o(n^2)$ time
algorithms for finding $k$-(sub-)cadences and equidistant subsequences.
Furthermore, $O(n\log^2 n)$ and $O(n\log n)$ time algorithms, respectively for
equidistant and Abelian equidistant matching for the case $|P| = 3$, are shown.
The algorithms make use of a technique that was recently introduced which can
efficiently compute convolutions with linear constraints.
</summary>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <link href="http://arxiv.org/abs/2002.06796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.05599">
    <id>http://arxiv.org/abs/2002.05599v1</id>
    <updated>2020-02-13T16:22:11Z</updated>
    <published>2020-02-13T16:22:11Z</published>
    <title>Engineering Faster Sorters for Small Sets of Items</title>
    <summary>  Sorting a set of items is a task that can be useful by itself or as a
building block for more complex operations. That is why a lot of effort has
been put into finding sorting algorithms that sort large sets as fast as
possible. But the more sophisticated the algorithms become, the less efficient
they are for small sets of items due to large constant factors. We aim to
determine if there is a faster way than insertion sort to sort small sets of
items to provide a more efficient base case sorter. We looked at sorting
networks, at how they can improve the speed of sorting few elements, and how to
implement them in an efficient manner by using conditional moves. Since sorting
networks need to be implemented explicitly for each set size, providing
networks for larger sizes becomes less efficient due to increased code sizes.
To also enable the sorting of slightly larger base cases, we adapted sample
sort to Register Sample Sort, to break down those larger sets into sizes that
can in turn be sorted by sorting networks. From our experiments we found that
when sorting only small sets, the sorting networks outperform insertion sort by
a factor of at least 1.76 for any array size between six and sixteen, and by a
factor of 2.72 on average across all machines and array sizes. When integrating
sorting networks as a base case sorter into Quicksort, we achieved far less
performance improvements, which is probably due to the networks having a larger
code size and cluttering the L1 instruction cache. But for x86 machines with a
larger L1 instruction cache of 64 KiB or more, we obtained speedups of 12.7%
when using sorting networks as a base case sorter in std::sort. In conclusion,
the desired improvement in speed could only be achieved under special
circumstances, but the results clearly show the potential of using conditional
moves in the field of sorting algorithms.
</summary>
    <author>
      <name>Timo Bingmann</name>
    </author>
    <author>
      <name>Jasper Marianczuk</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <link href="http://arxiv.org/abs/2002.05599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.05600">
    <id>http://arxiv.org/abs/2002.05600v2</id>
    <updated>2020-02-14T11:42:55Z</updated>
    <published>2020-02-13T16:22:26Z</published>
    <title>On Two Measures of Distance between Fully-Labelled Trees</title>
    <summary>  The last decade brought a significant increase in the amount of data and a
variety of new inference methods for reconstructing the detailed evolutionary
history of various cancers. This brings the need of designing efficient
procedures for comparing rooted trees representing the evolution of mutations
in tumor phylogenies. Bernardini et al. [CPM 2019] recently introduced a notion
of the rearrangement distance for fully-labelled trees motivated by this
necessity. This notion originates from two operations: one that permutes the
labels of the nodes, the other that affects the topology of the tree. Each
operation alone defines a distance that can be computed in polynomial time,
while the actual rearrangement distance, that combines the two, was proven to
be NP-hard.
  We answer two open question left unanswered by the previous work. First, what
is the complexity of computing the permutation distance? Second, is there a
constant-factor approximation algorithm for estimating the rearrangement
distance between two arbitrary trees? We answer the first one by showing, via a
two-way reduction, that calculating the permutation distance between two trees
on $n$ nodes is equivalent, up to polylogarithmic factors, to finding the
largest cardinality matching in a sparse bipartite graph. In particular, by
plugging in the algorithm of Liu and Sidford [ArXiv 2019], we obtain an
$O(n^{11/8})$ time algorithm for computing the permutation distance between two
trees on $n$ nodes. Then we answer the second question positively, and design a
linear-time constant-factor approximation algorithm that does not need any
assumption on the trees.
</summary>
    <author>
      <name>Giulia Bernardini</name>
    </author>
    <author>
      <name>Paola Bonizzoni</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.05600v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05600v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.04979">
    <id>http://arxiv.org/abs/2002.04979v1</id>
    <updated>2020-02-12T13:37:23Z</updated>
    <published>2020-02-12T13:37:23Z</published>
    <title>On Rearrangement of Items Stored in Stacks</title>
    <summary>  There are $n \ge 2$ stacks, each filled with $d$ items (its full capacity),
and one empty stack with capacity $d$. A robot arm, in one stack operation
(move), may pop one item from the top of a non-empty stack and subsequently
push it into a stack that is not at capacity. In a {\em labeled} problem, all
$nd$ items are distinguishable and are initially randomly scattered in the $n$
stacks. The items must be rearranged using pop-and-push moves so that at the
end, the $k^{\rm th}$ stack holds items $(k-1)d +1, \ldots, kd$, in that order,
from the top to the bottom for all $1 \le k \le n$. In an {\em unlabeled}
problem, the $nd$ items are of $n$ types of $d$ each. The goal is to rearrange
items so that items of type $k$ are located in the $k^{\rm th}$ stack for all
$1 \le k \le n$. In carrying out the rearrangement, a natural question is to
find the least number of required pop-and-push moves.
  In terms of the required number of moves for solving the rearrangement
problems, the labeled and unlabeled version have lower bounds $\Omega(nd +
nd{\frac{\log d}{\log n}})$ and $\Omega(nd)$, respectively. Our main
contribution is the design of an algorithm with a guaranteed upper bound of
$O(nd)$ for both versions when $d \le cn$ for arbitrary fixed positive number
$c$. In addition, a subroutine for a problem that we call the Rubik table
problem is of independent interest, with applications to problems including
multi-robot motion planning.
</summary>
    <author>
      <name>Mario Szegedy</name>
    </author>
    <author>
      <name>Jingjin Yu</name>
    </author>
    <link href="http://arxiv.org/abs/2002.04979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.04979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.05034">
    <id>http://arxiv.org/abs/2002.05034v2</id>
    <updated>2020-03-18T14:25:21Z</updated>
    <published>2020-02-12T14:50:40Z</published>
    <title>Uniform Linked Lists Contraction</title>
    <summary>  We present a parallel algorithm (EREW PRAM algorithm) for linked lists
contraction. We show that when we contract a linked list from size $n$ to size
$n/c$ for a suitable constant $c$ we can pack the linked list into an array of
size $n/d$ for a constant $1 &lt; d\leq c$ in the time of 3 coloring the list.
Thus for a set of linked lists with a total of $n$ elements and the longest
list has $l$ elements our algorithm contracts them in $O(n\log
i/p+(\log^{(i)}n+\log i )\log \log l+ \log l)$ time, for an arbitrary
constructible integer $i$, with $p$ processors on the EREW PRAM, where
$\log^{(1)} n =\log n$ and $\log^{(t)}n=\log \log^{(t-1)} n$ and $\log^*n=\min
\{ i|\log^{(i)} n &lt; 10\}$. When $i$ is a constant we get time
$O(n/p+\log^{(i)}n\log \log l+\log l)$. Thus when $l=\Omega (\log^{(c)}n)$ for
any constant $c$ we achieve $O(n/p+\log l)$ time. The previous best
deterministic EREW PRAM algorithm has time $O(n/p+\log n)$ and best CRCW PRAM
algorithm has time $O(n/p+\log n/\log \log n+\log l)$.
  Keywords: Parallel algorithms, linked list, linked list contraction, uniform
linked list contraction, EREW PRAM.
</summary>
    <author>
      <name>Yijie Han</name>
    </author>
    <link href="http://arxiv.org/abs/2002.05034v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05034v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W10, 68W40" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.03965">
    <id>http://arxiv.org/abs/2002.03965v2</id>
    <updated>2020-02-16T17:57:08Z</updated>
    <published>2020-02-10T17:27:34Z</published>
    <title>Palindromic k-Factorization in Pure Linear Time</title>
    <summary>  Given a string $s$ of length $n$ over a general alphabet and an integer $k$,
the problem is to decide whether $s$ is a concatenation of $k$ nonempty
palindromes. Two previously known solutions for this problem work in time
$O(kn)$ and $O(n\log n)$ respectively. Here we settle the complexity of this
problem in the word-RAM model, presenting an $O(n)$-time online deciding
algorithm. The algorithm simultaneously finds the minimum odd number of factors
and the minimum even number of factors in a factorization of a string into
nonempty palindromes. We also demonstrate how to get an explicit factorization
of $s$ into $k$ palindromes with an $O(n)$-time offline postprocessing.
</summary>
    <author>
      <name>Mikhail Rubinchik</name>
    </author>
    <author>
      <name>Arseny M. Shur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to CPM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.03965v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03965v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.03057">
    <id>http://arxiv.org/abs/2002.03057v3</id>
    <updated>2020-02-19T11:50:25Z</updated>
    <published>2020-02-08T00:54:19Z</published>
    <title>The Bloom Tree</title>
    <summary>  We introduce a data structure that allows for efficient (probabilistic)
presence proofs and non-probabilistic absence proofs in a bandwidth efficient
and secure way. The Bloom tree combines the idea of Bloom filters with that of
Merkle trees. Bloom filters are used to verify the presence, or absence of
elements in a set. In the case of the Bloom tree, we are interested to verify
and transmit the presence, or absence of an element in a secure and bandwidth
efficient way to another party. Instead of sending the whole Bloom filter to
check for the presence, or absence of an element, the Bloom tree achieves
efficient verification by using a compact Merkle multiproof.
</summary>
    <author>
      <name>Lum Ramabaja</name>
    </author>
    <author>
      <name>Arber Avdullahu</name>
    </author>
    <link href="http://arxiv.org/abs/2002.03057v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03057v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.11157">
    <id>http://arxiv.org/abs/2002.11157v1</id>
    <updated>2020-02-25T19:56:08Z</updated>
    <published>2020-02-25T19:56:08Z</published>
    <title>2-Dimensional Palindromes with $k$ Mismatches</title>
    <summary>  This paper extends the problem of 2-dimensional palindrome search into the
area of approximate matching. Using the Hamming distance as the measure, we
search for 2D palindromes that allow up to $k$ mismatches. We consider two
different definitions of 2D palindromes and describe efficient algorithms for
both of them. The first definition implies a square, while the second
definition (also known as a \emph{centrosymmetric factor}), can be any
rectangular shape. Given a text of size $n \times m$, the time complexity of
the first algorithm is $O(nm (\log m + \log n + k))$ and for the second
algorithm it is $O(nm(\log m + k) + occ)$ where $occ$ is the size of the
output.
</summary>
    <author>
      <name>Dina Sokol</name>
    </author>
    <link href="http://arxiv.org/abs/2002.11157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.11342">
    <id>http://arxiv.org/abs/2002.11342v1</id>
    <updated>2020-02-26T08:04:35Z</updated>
    <published>2020-02-26T08:04:35Z</published>
    <title>Streaming with Oracle: New Streaming Algorithms for Edit Distance and
  LCS</title>
    <summary>  The edit distance (ED) and longest common subsequence (LCS) are two
fundamental problems which quantify how similar two strings are to one another.
In this paper, we consider these problems in the streaming model where one
string is available via oracle queries and the other string comes as a stream
of characters. Our main contribution is a constant factor approximation
algorithm in this setting for ED with memory $O(n^{\delta})$ for any $\delta >
0$. In addition to this, we present an upper bound of $\tilde O(\sqrt{n})$ on
the memory needed to approximate ED or LCS within a factor $1+o(1)$ in our
setting. All our algorithms run in a single pass.
  For approximating ED within a constant factor, we discover yet another
application of triangle inequality, this time in the context of streaming
algorithms. Triangle inequality has been previously used to obtain subquadratic
time approximation algorithms for ED. Our technique is novel and elegantly
utilizes triangle inequality to save memory at the expense of an exponential
increase in the runtime.
</summary>
    <author>
      <name>Alireza Farhadi</name>
    </author>
    <author>
      <name>MohammadTaghi Hajiaghayi</name>
    </author>
    <author>
      <name>Aviad Rubinstein</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <link href="http://arxiv.org/abs/2002.11342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.11691">
    <id>http://arxiv.org/abs/2002.11691v1</id>
    <updated>2020-02-26T18:32:32Z</updated>
    <published>2020-02-26T18:32:32Z</published>
    <title>Bitvectors with runs and the successor/predecessor problem</title>
    <summary>  The successor and predecessor problem consists of obtaining the closest value
in a set of integers, greater/smaller than a given value. This problem has
interesting applications, like the intersection of inverted lists. It can be
easily modeled by using a bitvector of size $n$ and its operations rank and
select. However, there is a practical approach, which keeps the best
theoretical bounds, and allows to solve successor and predecessor more
efficiently. Based on that technique, we designed a novel compact data
structure for bitvectors with $k$ runs that achieves access, rank, and
successor/predecessor in $O(1)$ time by consuming space $O(\sqrt{kn})$ bits. In
practice, it obtains a compression ratio of $0.04\%-26.33\%$ when the runs are
larger than $100$, and becomes the fastest technique, which considers
compressibility, in successor/predecessor queries. Besides, we present a
recursive variant of our structure, which tends to $O(k)$ bits and takes
$O(\log \frac{n}{k})$ time.
</summary>
    <author>
      <name>Adri√°n G√≥mez-Brand√≥n</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2020 Data Compression Conference (DCC)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.11691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.10303">
    <id>http://arxiv.org/abs/2002.10303v1</id>
    <updated>2020-02-24T15:20:33Z</updated>
    <published>2020-02-24T15:20:33Z</published>
    <title>Wheeler Languages</title>
    <summary>  The recently introduced class of Wheeler graphs, inspired by the
Burrows-Wheeler Transform (BWT) of a given string, admits an efficient index
data structure for searching for subpaths with a given path label, and lifts
the applicability of the Burrows-Wheeler transform from strings to languages.
In this paper we study the regular languages accepted by automata having a
Wheeler graph as transition function, and prove results on determination,
Myhill_Nerode characterization, decidability, and closure properties for this
class of languages.
</summary>
    <author>
      <name>Jarno Alanko</name>
    </author>
    <author>
      <name>Giovanna D'Agostino</name>
    </author>
    <author>
      <name>Alberto Policriti</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/2002.10303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.09707">
    <id>http://arxiv.org/abs/2002.09707v1</id>
    <updated>2020-02-22T14:18:53Z</updated>
    <published>2020-02-22T14:18:53Z</published>
    <title>Compression with wildcards: All spanning trees</title>
    <summary>  By processing all minimal cutsets of a graph G, and by using novel wildcards,
all spanning trees of G can be compactly encoded. Thus, different from all
previous enumeration schemes, the spanning trees are not generated one-by-one.
The Mathematica implementation of one of our algorithms generated for a random
(11,50)-graph its 819'603'181 spanning trees, in bundles of size about 400,
within 52 seconds.
</summary>
    <author>
      <name>Marcel Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.09707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.09511">
    <id>http://arxiv.org/abs/2002.09511v3</id>
    <updated>2020-03-02T11:44:07Z</updated>
    <published>2020-02-21T19:16:07Z</published>
    <title>Chronofold: a data structure for versioned text</title>
    <summary>  Chronofold is a replicated data structure for versioned text, based on the
extended Causal Tree model. Past models of this kind either retrofitted local
linear orders to a distributed system (the OT approach) or employed distributed
data models locally (the CRDT approach). That caused either extreme fragility
in a distributed setting or egregious overheads in local use. Overall, that
local/distributed impedance mismatch is cognitively taxing and causes lots of
complexity. We solve that by using subjective linear orders locally at each
replica, while inter-replica communication uses a distributed model. A separate
translation layer insulates local data structures from the distributed
environment. We modify the Lamport timestamping scheme to make that translation
as trivial as possible. We believe our approach has applications beyond the
domain of collaborative editing.
</summary>
    <author>
      <name>Victor Grishchenko</name>
    </author>
    <author>
      <name>Mikhail Patrakeev</name>
    </author>
    <link href="http://arxiv.org/abs/2002.09511v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09511v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.09041">
    <id>http://arxiv.org/abs/2002.09041v1</id>
    <updated>2020-02-20T22:15:41Z</updated>
    <published>2020-02-20T22:15:41Z</published>
    <title>Compressed Data Structures for Binary Relations in Practice</title>
    <summary>  Binary relations are commonly used in Computer Science for modeling data. In
addition to classical representations using matrices or lists, some compressed
data structures have recently been proposed to represent binary relations in
compact space, such as the $k^2$-tree and the Binary Relation Wavelet Tree
(BRWT). Knowing their storage needs, supported operations and time performance
is key for enabling an appropriate choice of data representation given a domain
or application, its data distribution and typical operations that are computed
over the data.
  In this work, we present an empirical comparison among several compressed
representations for binary relations. We analyze their space usage and the
speed of their operations using different (synthetic and real) data
distributions. We include both neighborhood and set operations, also proposing
algorithms for set operations for the BRWT, which were not presented before in
the literature. We conclude that there is not a clear choice that outperforms
the rest, but we give some recommendations of usage of each compact
representation depending on the data distribution and types of operations
performed over the data. We also include a scalability study of the data
representations.
</summary>
    <author>
      <name>Carlos Quijada-Fuentes</name>
    </author>
    <author>
      <name>Miguel R. Penabad</name>
    </author>
    <author>
      <name>Susana Ladra</name>
    </author>
    <author>
      <name>Gilberto Guti√©rrez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2020.2970983</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2020.2970983" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access 8, pp. 25949-25963 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.09041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.08498">
    <id>http://arxiv.org/abs/2002.08498v3</id>
    <updated>2020-03-13T13:52:20Z</updated>
    <published>2020-02-19T23:33:39Z</published>
    <title>Space Efficient Deterministic Approximation of String Measures</title>
    <summary>  We study approximation algorithms for the following three string measures
that are widely used in practice: edit distance, longest common subsequence,
and longest increasing sequence.\ All three problems can be solved exactly by
standard algorithms that run in polynomial time with roughly $O(n)$ space,
where $n$ is the input length, and our goal is to design deterministic
approximation algorithms that run in polynomial time with significantly smaller
space. Towards this, we design several algorithms that achieve $1+\epsilon$ or
$1-\epsilon$ approximation for all three problems, where $\epsilon>0$ can be
any constant. Our algorithms use space $n^{\delta}$ for any constant $\delta>0$
and have running time essentially the same as or slightly more than the
standard algorithms. Our algorithms significantly improve previous results in
terms of space complexity, where all known results need to use space at least
$\Omega(\sqrt{n})$. Some of our algorithms can also be adapted to work in the
asymmetric streaming model \cite{saks2013space}, and output the corresponding
sequence.
  Our algorithms are based on the idea of using recursion as in Savitch's
theorem \cite{Savitch70}, and a careful modification of previous techniques to
make the recursion work. Along the way we also give a new logspace reduction
from longest common subsequence to longest increasing sequence, which may be of
independent interest.
</summary>
    <author>
      <name>Kuan Cheng</name>
    </author>
    <author>
      <name>Zhengzhong Jin</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Yu Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/2002.08498v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08498v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.08061">
    <id>http://arxiv.org/abs/2002.08061v1</id>
    <updated>2020-02-19T08:51:38Z</updated>
    <published>2020-02-19T08:51:38Z</published>
    <title>Translating Between Wavelet Tree and Wavelet Matrix Construction</title>
    <summary>  The wavelet tree (Grossi et al. [SODA, 2003]) and wavelet matrix (Claude et
al. [Inf. Syst., 2015]) are compact data structures with many applications such
as text indexing or computational geometry. By continuing the recent research
of Fischer et al. [ALENEX, 2018], we explore the similarities and differences
of these heavily related data structures with focus on their construction. We
develop a data structure to modify construction algorithms for either the
wavelet tree or matrix to construct instead the other. This modification is
efficient, in that it does not worsen the asymptotic time and space
requirements of any known wavelet tree or wavelet matrix construction
algorithm.
</summary>
    <author>
      <name>Patrick Dinklage</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper originally submitted to and presented at the Prague Stringology
  Conference 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.08061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.08004">
    <id>http://arxiv.org/abs/2002.08004v1</id>
    <updated>2020-02-19T04:58:41Z</updated>
    <published>2020-02-19T04:58:41Z</published>
    <title>Fast and linear-time string matching algorithms based on the distances
  of $q$-gram occurrences</title>
    <summary>  Given a text $T$ of length $n$ and a pattern $P$ of length $m$, the string
matching problem is a task to find all occurrences of $P$ in $T$. In this
study, we propose an algorithm that solves this problem in $O((n + m)q)$ time
considering the distance between two adjacent occurrences of the same $q$-gram
contained in $P$. We also propose a theoretical improvement of it which runs in
$O(n + m)$ time, though it is not necessarily faster in practice. We compare
the execution times of our and existing algorithms on various kinds of real and
artificial datasets such as an English text, a genome sequence and a Fibonacci
string. The experimental results show that our algorithm is as fast as the
state-of-the-art algorithms in many cases, particularly when a pattern
frequently appears in a text.
</summary>
    <author>
      <name>Satoshi Kobayashi</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <link href="http://arxiv.org/abs/2002.08004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.03801">
    <id>http://arxiv.org/abs/2003.03801v1</id>
    <updated>2020-03-08T15:38:08Z</updated>
    <published>2020-03-08T15:38:08Z</published>
    <title>Multiset Synchronization with Counting Cuckoo Filters</title>
    <summary>  Set synchronization is a fundamental task in distributed applications and
implementations. Existing methods that synchronize simple sets are mainly based
on compact data structures such as Bloom filter and its variants. However,
these methods are infeasible to synchronize a pair of multisets which allow an
element to appear for multiple times. To this end, in this paper, we propose to
leverage the counting cuckoo filter (CCF), a novel variant of cuckoo filter, to
represent and thereafter synchronize a pair of multisets. The cuckoo filter
(CF) is a minimized hash table that uses cuckoo hashing to resolve collisions.
CF has an array of buckets, each of which has multiple slots to store element
fingerprints. Based on CF, CCF extends each slot as two fields, the fingerprint
field and the counter field. The fingerprint field records the fingerprint of
element which is stored by this slot; while the counter field counts the
multiplicity of the stored element. With such a design, CCF is competent to
represent any multiset. After generating and exchanging the respective CCFs
which represent the local multi-sets, we propose the query-based and the
decoding-based methods to identify the different elements between the given
multisets. The comprehensive evaluation results indicate that CCF outperforms
the counting Bloom filter (CBF) when they are used to synchronize multisets, in
terms of both synchronization accuracy and the space-efficiency, at the cost of
a little higher time-consumption.
</summary>
    <author>
      <name>Shangsen Li</name>
    </author>
    <author>
      <name>Lailong Luo</name>
    </author>
    <author>
      <name>Deke Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2003.03801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.03801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.03959">
    <id>http://arxiv.org/abs/2003.03959v1</id>
    <updated>2020-03-09T07:55:06Z</updated>
    <published>2020-03-09T07:55:06Z</published>
    <title>Adaptive Fibonacci and Pairing Heaps</title>
    <summary>  This brief note presents two adaptive heap data structures and conjectures on
running times.
</summary>
    <author>
      <name>Andrew Frohmader</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.03959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.03959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.03222">
    <id>http://arxiv.org/abs/2003.03222v1</id>
    <updated>2020-03-05T07:43:53Z</updated>
    <published>2020-03-05T07:43:53Z</published>
    <title>Generating a Gray code for prefix normal words in amortized
  polylogarithmic time per word</title>
    <summary>  A prefix normal word is a binary word with the property that no substring has
more 1s than the prefix of the same length. By proving that the set of prefix
normal words is a bubble language, we can exhaustively list all prefix normal
words of length n as a combinatorial Gray code, where successive strings differ
by at most two swaps or bit flips. This Gray code can be generated in O(log^2
n) amortized time per word, while the best generation algorithm hitherto has
O(n) running time per word. We also present a membership tester for prefix
normal words, as well as a novel characterization of bubble languages.
</summary>
    <author>
      <name>P√©ter Burcsi</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Zsuzsanna Lipt√°k</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <author>
      <name>Joe Sawada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1401.6346</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.03222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.03222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.02336">
    <id>http://arxiv.org/abs/2003.02336v1</id>
    <updated>2020-03-04T21:31:42Z</updated>
    <published>2020-03-04T21:31:42Z</published>
    <title>Approximating Optimal Bidirectional Macro Schemes</title>
    <summary>  Lempel-Ziv is an easy-to-compute member of a wide family of so-called macro
schemes; it restricts pointers to go in one direction only. Optimal
bidirectional macro schemes are NP-complete to find, but they may provide much
better compression on highly repetitive sequences. We consider the problem of
approximating optimal bidirectional macro schemes. We describe a simulated
annealing algorithm that usually converges quickly. Moreover, in some cases, we
obtain bidirectional macro schemes that are provably a 2-approximation of the
optimal. We test our algorithm on a number of artificial repetitive texts and
verify that it is efficient in practice and outperforms Lempel-Ziv, sometimes
by a wide margin.
</summary>
    <author>
      <name>Lu√≠s M. S. Russo</name>
    </author>
    <author>
      <name>Ana D. Correia</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Alexandre P. Francisco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.02336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.02016">
    <id>http://arxiv.org/abs/2003.02016v1</id>
    <updated>2020-03-04T11:48:05Z</updated>
    <published>2020-03-04T11:48:05Z</published>
    <title>Time-Space Tradeoffs for Finding a Long Common Substring</title>
    <summary>  We consider the problem of finding, given two documents of total length $n$,
a longest string occurring as a substring of both documents. This problem,
known as the Longest Common Substring (LCS) problem, has a classic $O(n)$-time
solution dating back to the discovery of suffix trees (Weiner, 1973) and their
efficient construction for integer alphabets (Farach-Colton, 1997). However,
these solutions require $\Theta(n)$ space, which is prohibitive in many
applications. To address this issue, Starikovskaya and Vildh{\o}j (CPM 2013)
showed that for $n^{2/3} \le s \le n^{1-o(1)}$, the LCS problem can be solved
in $O(s)$ space and $O(\frac{n^2}{s})$ time. Kociumaka et al. (ESA 2014)
generalized this tradeoff to $1 \leq s \leq n$, thus providing a smooth
time-space tradeoff from constant to linear space. In this paper, we obtain a
significant speed-up for instances where the length $L$ of the sought LCS is
large. For $1 \leq s \leq n$, we show that the LCS problem can be solved in
$O(s)$ space and $\tilde{O}(\frac{n^2}{L\cdot s}+n)$ time. The result is based
on techniques originating from the LCS with Mismatches problem (Flouri et al.,
2015; Charalampopoulos et al., CPM 2018), on space-efficient locally consistent
parsing (Birenzwige et al., SODA 2020), and on the structure of maximal
repetitions (runs) in the input documents.
</summary>
    <author>
      <name>Stav Ben Nun</name>
    </author>
    <author>
      <name>Shay Golan</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Matan Kraus</name>
    </author>
    <link href="http://arxiv.org/abs/2003.02016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.12570">
    <id>http://arxiv.org/abs/2002.12570v1</id>
    <updated>2020-02-28T06:51:40Z</updated>
    <published>2020-02-28T06:51:40Z</published>
    <title>Learning Directly from Grammar Compressed Text</title>
    <summary>  Neural networks using numerous text data have been successfully applied to a
variety of tasks. While massive text data is usually compressed using
techniques such as grammar compression, almost all of the previous machine
learning methods assume already decompressed sequence data as their input. In
this paper, we propose a method to directly apply neural sequence models to
text data compressed with grammar compression algorithms without decompression.
To encode the unique symbols that appear in compression rules, we introduce
composer modules to incrementally encode the symbols into vector
representations. Through experiments on real datasets, we empirically showed
that the proposal model can achieve both memory and computational efficiency
while maintaining moderate performance.
</summary>
    <author>
      <name>Yoichi Sasaki</name>
    </author>
    <author>
      <name>Kosuke Akimoto</name>
    </author>
    <author>
      <name>Takanori Maehara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 Postscript figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.12570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.12570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.01203">
    <id>http://arxiv.org/abs/2003.01203v1</id>
    <updated>2020-03-02T21:43:46Z</updated>
    <published>2020-03-02T21:43:46Z</published>
    <title>Concurrent Disjoint Set Union</title>
    <summary>  We develop and analyze concurrent algorithms for the disjoint set union
(union-find) problem in the shared memory, asynchronous multiprocessor model of
computation, with CAS (compare and swap) or DCAS (double compare and swap) as
the synchronization primitive. We give a deterministic bounded wait-free
algorithm that uses DCAS and has a total work bound of $O(m \cdot (\log(np/m +
1) + \alpha(n, m/(np)))$ for a problem with $n$ elements and $m$ operations
solved by $p$ processes, where $\alpha$ is a functional inverse of Ackermann's
function. We give two randomized algorithms that use only CAS and have the same
work bound in expectation. The analysis of the second randomized algorithm is
valid even if the scheduler is adversarial. Our DCAS and randomized algorithms
take $O(\log n)$ steps per operation, worst-case for the DCAS algorithm,
high-probability for the randomized algorithms. Our work and step bounds grow
only logarithmically with $p$, making our algorithms truly scalable. We prove
that for a class of symmetric algorithms that includes ours, no better step or
work bound is possible.
</summary>
    <author>
      <name>Siddhartha V. Jayanti</name>
    </author>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, combines ideas in two previous PODC papers</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.01203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.12662">
    <id>http://arxiv.org/abs/2002.12662v1</id>
    <updated>2020-02-28T11:33:30Z</updated>
    <published>2020-02-28T11:33:30Z</published>
    <title>Fast Indexes for Gapped Pattern Matching</title>
    <summary>  We describe indexes for searching large data sets for variable-length-gapped
(VLG) patterns. VLG patterns are composed of two or more subpatterns, between
each adjacent pair of which is a gap-constraint specifying upper and lower
bounds on the distance allowed between subpatterns. VLG patterns have numerous
applications in computational biology (motif search), information retrieval
(e.g., for language models, snippet generation, machine translation) and
capture a useful subclass of the regular expressions commonly used in practice
for searching source code. Our best approach provides search speeds several
times faster than prior art across a broad range of patterns and texts.
</summary>
    <author>
      <name>Manuel C√°ceres</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Chile</arxiv:affiliation>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Helsinki</arxiv:affiliation>
    </author>
    <author>
      <name>Bella Zhukova</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Helsinki</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-38919-2_40</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-38919-2_40" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research is supported by Academy of Finland through grant 319454
  and has received funding from the European Union's Horizon 2020 research and
  innovation programme under the Marie Sklodowska-Curie Actions
  H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SOFSEM 2020: Theory and Practice of Computer Science</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.12662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.12662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.12050">
    <id>http://arxiv.org/abs/2002.12050v2</id>
    <updated>2020-02-28T13:20:14Z</updated>
    <published>2020-02-27T11:48:33Z</published>
    <title>Semantrix: A Compressed Semantic Matrix</title>
    <summary>  We present a compact data structure to represent both the duration and length
of homogeneous segments of trajectories from moving objects in a way that, as a
data warehouse, it allows us to efficiently answer cumulative queries. The
division of trajectories into relevant segments has been studied in the
literature under the topic of Trajectory Segmentation. In this paper, we design
a data structure to compactly represent them and the algorithms to answer the
more relevant queries. We experimentally evaluate our proposal in the real
context of an enterprise with mobile workers (truck drivers) where we aim at
analyzing the time they spend in different activities. To test our proposal
under higher stress conditions we generated a huge amount of synthetic
realistic trajectories and evaluated our system with those data to have a good
idea about its space needs and its efficiency when answering different types of
queries.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Antonio Fari√±a</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Tirso V. Rodeiro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, Data Compression Conference 2020. This research has
  received funding from the European Union's Horizon 2020 research and
  innovation programme under the Marie Sk{\l}odowska-Curie Actions
  H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.12050v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.12050v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.11622">
    <id>http://arxiv.org/abs/2002.11622v1</id>
    <updated>2020-02-26T17:03:28Z</updated>
    <published>2020-02-26T17:03:28Z</published>
    <title>Revisiting compact RDF stores based on k2-trees</title>
    <summary>  We present a new compact representation to efficiently store and query large
RDF datasets in main memory. Our proposal, called BMatrix, is based on the
k2-tree, a data structure devised to represent binary matrices in a compressed
way, and aims at improving the results of previous state-of-the-art
alternatives, especially in datasets with a relatively large number of
predicates. We introduce our technique, together with some improvements on the
basic k2-tree that can be applied to our solution in order to boost
compression. Experimental results in the flagship RDF dataset DBPedia show that
our proposal achieves better compression than existing alternatives, while
yielding competitive query times, particularly in the most frequent triple
patterns and in queries with unbound predicate, in which we outperform existing
solutions.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Ana Cerdeira-Pena</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Antonio Fari√±a</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.11622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.08211">
    <id>http://arxiv.org/abs/2003.08211v2</id>
    <updated>2020-03-19T01:23:39Z</updated>
    <published>2020-03-17T04:26:35Z</published>
    <title>An Efficient Implementation of Manacher's Algorithm</title>
    <summary>  Manacher's algorithm has been shown to be optimal to the longest palindromic
substring problem. Many of the existing implementations of this algorithm,
however, unanimously required in-memory construction of an augmented string
that is twice as long as the original string. Although it has found widespread
use, we found that this preprocessing is neither economic nor necessary. We
present a more efficient implementation of Manacher's algorithm based on index
mapping that makes the string augmentation process obsolete.
</summary>
    <author>
      <name>Shoupu Wan</name>
    </author>
    <link href="http://arxiv.org/abs/2003.08211v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08211v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.6; F.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.08097">
    <id>http://arxiv.org/abs/2003.08097v1</id>
    <updated>2020-03-18T08:47:16Z</updated>
    <published>2020-03-18T08:47:16Z</published>
    <title>Grammar compression with probabilistic context-free grammar</title>
    <summary>  We propose a new approach for universal lossless text compression, based on
grammar compression. In the literature, a target string $T$ has been compressed
as a context-free grammar $G$ in Chomsky normal form satisfying $L(G) = \{T\}$.
Such a grammar is often called a \emph{straight-line program} (SLP). In this
paper, we consider a probabilistic grammar $G$ that generates $T$, but not
necessarily as a unique element of $L(G)$. In order to recover the original
text $T$ unambiguously, we keep both the grammar $G$ and the derivation tree of
$T$ from the start symbol in $G$, in compressed form. We show some simple
evidence that our proposal is indeed more efficient than SLPs for certain
texts, both from theoretical and practical points of view.
</summary>
    <author>
      <name>Hiroaki Naganuma</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <author>
      <name>Naoki Kobayashi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, accepted for poster presentation at DCC 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.08097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.07285">
    <id>http://arxiv.org/abs/2003.07285v1</id>
    <updated>2020-03-16T15:45:53Z</updated>
    <published>2020-03-16T15:45:53Z</published>
    <title>Approximating LCS in Linear Time: Beating the $\sqrt{n}$ Barrier</title>
    <summary>  Longest common subsequence (LCS) is one of the most fundamental problems in
combinatorial optimization. Apart from theoretical importance, LCS has enormous
applications in bioinformatics, revision control systems, and data comparison
programs. Although a simple dynamic program computes LCS in quadratic time, it
has been recently proven that the problem admits a conditional lower bound and
may not be solved in truly subquadratic time. In addition to this, LCS is
notoriously hard with respect to approximation algorithms. Apart from a trivial
sampling technique that obtains a $n^{x}$ approximation solution in time
$O(n^{2-2x})$ nothing else is known for LCS. This is in sharp contrast to its
dual problem edit distance for which several linear time solutions are obtained
in the past two decades.
</summary>
    <author>
      <name>MohammadTaghi Hajiaghayi</name>
    </author>
    <author>
      <name>Masoud Seddighin</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <author>
      <name>Xiaorui Sun</name>
    </author>
    <link href="http://arxiv.org/abs/2003.07285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.07285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.06742">
    <id>http://arxiv.org/abs/2003.06742v1</id>
    <updated>2020-03-15T03:00:13Z</updated>
    <published>2020-03-15T03:00:13Z</published>
    <title>Four-Dimensional Dominance Range Reporting in Linear Space</title>
    <summary>  In this paper we study the four-dimensional dominance range reporting problem
and present data structures with linear or almost-linear space usage. Our
results can be also used to answer four-dimensional queries that are bounded on
five sides. The first data structure presented in this paper uses linear space
and answers queries in $O(\log^{1+\varepsilon}n + k\log^{\varepsilon} n)$ time,
where $k$ is the number of reported points, $n$ is the number of points in the
data structure, and $\varepsilon$ is an arbitrarily small positive constant.
Our second data structure uses $O(n \log^{\varepsilon} n)$ space and answers
queries in $O(\log n+k)$ time.
  These are the first data structures for this problem that use linear (resp.
$O(n\log^{\varepsilon} n)$) space and answer queries in poly-logarithmic time.
For comparison the fastest previously known linear-space or
$O(n\log^{\varepsilon} n)$-space data structure supports queries in
$O(n^{\varepsilon} + k)$ time (Bentley and Mauer, 1980). Our results can be
generalized to $d\ge 4$ dimensions. For example, we can answer $d$-dimensional
dominance range reporting queries in $O(\log\log n (\log n/\log\log n)^{d-3} +
k)$ time using $O(n\log^{d-4+\varepsilon}n)$ space. Compared to the fastest
previously known result (Chan, 2013), our data structure reduces the space
usage by $O(\log n)$ without increasing the query time.
</summary>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a SoCG'20 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.06742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.06691">
    <id>http://arxiv.org/abs/2003.06691v1</id>
    <updated>2020-03-14T19:44:44Z</updated>
    <published>2020-03-14T19:44:44Z</published>
    <title>Shorter Labels for Routing in Trees</title>
    <summary>  A routing labeling scheme assigns a binary string, called a label, to each
node in a network, and chooses a distinct port number from $\{1,\ldots,d\}$ for
every edge outgoing from a node of degree $d$. Then, given the labels of $u$
and $w$ and no other information about the network, it should be possible to
determine the port number corresponding to the first edge on the shortest path
from $u$ to $w$. In their seminal paper, Thorup and Zwick [SPAA 2001] designed
several routing methods for general weighted networks. An important technical
ingredient in their paper that according to the authors ``may be of independent
practical and theoretical interest'' is a routing labeling scheme for trees of
arbitrary degrees. For a tree on $n$ nodes, their scheme constructs labels
consisting of $(1+o(1))\log n$ bits such that the sought port number can be
computed in constant time. Looking closer at their construction, the labels
consist of $\log n + O(\log n\cdot \log\log\log n / \log\log n)$ bits. Given
that the only known lower bound is $\log n+\Omega(\log\log n)$, a natural
question that has been asked for other labeling problems in trees is to
determine the asymptotics of the smaller-order term.
  We make the first (and significant) progress in 19 years on determining the
correct second-order term for the length of a label in a routing labeling
scheme for trees on $n$ nodes. We design such a scheme with labels of length
$\log n+O((\log\log n)^{2})$. Furthermore, we modify the scheme to allow for
computing the port number in constant time at the expense of slightly
increasing the length to $\log n+O((\log\log n)^{3})$.
</summary>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Wojciech Janczewski</name>
    </author>
    <author>
      <name>Jakub ≈Åopusza≈Ñski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.06691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.04629">
    <id>http://arxiv.org/abs/2003.04629v1</id>
    <updated>2020-03-10T10:51:05Z</updated>
    <published>2020-03-10T10:51:05Z</published>
    <title>Scattered Factor-Universality of Words</title>
    <summary>  A word $u=u_1\dots u_n$ is a scattered factor of a word $w$ if $u$ can be
obtained from $w$ by deleting some of its letters: there exist the (potentially
empty) words $v_0,v_1,..,v_n$ such that $w = v_0u_1v_1...u_nv_n$. The set of
all scattered factors up to length $k$ of a word is called its full
$k$-spectrum. Firstly, we show an algorithm deciding whether the $k$-spectra
for given $k$ of two words are equal or not, running in optimal time. Secondly,
we consider a notion of scattered-factors universality: the word $w$, with
$\letters(w)=\Sigma$, is called $k$-universal if its $k$-spectrum includes all
words of length $k$ over the alphabet $\Sigma$; we extend this notion to
$k$-circular universality. After a series of preliminary combinatorial results,
we present an algorithm computing, for a given $k'$-universal word $w$ the
minimal $i$ such that $w^i$ is $k$-universal for some $k>k'$. Several other
connected problems~are~also~considered.
</summary>
    <author>
      <name>Laura Barker</name>
    </author>
    <author>
      <name>Pamela Fleischmann</name>
    </author>
    <author>
      <name>Katharina Harwardt</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Dirk Nowotka</name>
    </author>
    <link href="http://arxiv.org/abs/2003.04629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.04629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.06794">
    <id>http://arxiv.org/abs/1909.06794v2</id>
    <updated>2019-10-01T05:13:05Z</updated>
    <published>2019-09-15T12:48:00Z</published>
    <title>Run-Length Encoding in a Finite Universe</title>
    <summary>  Text compression schemes and compact data structures usually combine
sophisticated probability models with basic coding methods whose average
codeword length closely match the entropy of known distributions. In the
frequent case where basic coding represents run-lengths of outcomes that have
probability $p$, i.e. the geometric distribution $\Pr(i)=p^i(1-p)$, a
\emph{Golomb code} is an optimal instantaneous code, which has the additional
advantage that codewords can be computed using only an integer parameter
calculated from $p$, without need for a large or sophisticated data structure.
Golomb coding does not, however, gracefully handle the case where run-lengths
are bounded by a known integer~$n$. In this case, codewords allocated for the
case $i>n$ are wasted. While negligible for large $n$, this makes Golomb coding
unattractive in situations where $n$ is recurrently small, e.g., when
representing many short lists of integers drawn from limited ranges, or when
the range of $n$ is narrowed down by a recursive algorithm. We address the
problem of choosing a code for this case, considering efficiency from both
information-theoretic and computational perspectives, and arrive at a simple
code that allows computing a codeword using only $O(1)$ simple computer
operations and $O(1)$ machine words. We demonstrate experimentally that the
resulting representation length is very close (equal in a majority of tested
cases) to the optimal Huffman code, to the extent that the expected difference
is practically negligible. We describe efficient branch-free implementation of
encoding and decoding.
</summary>
    <author>
      <name>N. Jesper Larsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SPIRE 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.06794v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.06794v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.06444">
    <id>http://arxiv.org/abs/1909.06444v3</id>
    <updated>2019-10-14T07:57:50Z</updated>
    <published>2019-09-13T20:51:30Z</published>
    <title>Local Decode and Update for Big Data Compression</title>
    <summary>  This paper investigates data compression that simultaneously allows local
decoding and local update. The main result is a universal compression scheme
for memoryless sources with the following features. The rate can be made
arbitrarily close to the entropy of the underlying source, contiguous fragments
of the source can be recovered or updated by probing or modifying a number of
codeword bits that is on average linear in the size of the fragment, and the
overall encoding and decoding complexity is quasilinear in the blocklength of
the source. In particular, the local decoding or update of a single message
symbol can be performed by probing or modifying a constant number of codeword
bits. This latter part improves over previous best known results for which
local decodability or update efficiency grows logarithmically with blocklength.
</summary>
    <author>
      <name>Shashank Vatedka</name>
    </author>
    <author>
      <name>Aslan Tchamkerten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 4 figures. v2: updated references</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.06444v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.06444v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.07538">
    <id>http://arxiv.org/abs/1909.07538v2</id>
    <updated>2019-11-05T07:36:36Z</updated>
    <published>2019-09-17T01:08:27Z</published>
    <title>Generalized Dictionary Matching under Substring Consistent Equivalence
  Relations</title>
    <summary>  Given a set of patterns called a dictionary and a text, the dictionary
matching problem is a task to find all occurrence positions of all patterns in
the text. The dictionary matching problem can be solved efficiently by using
the Aho-Corasick algorithm. Recently, Matsuoka et al. [TCS, 2016] proposed a
generalization of pattern matching problem under substring consistent
equivalence relations and presented a generalization of the Knuth-Morris-Pratt
algorithm to solve this problem. An equivalence relation $\approx$ is a
substring consistent equivalence relation (SCER) if for two strings $X,Y$, $X
\approx Y$ implies $|X| = |Y|$ and $X[i:j] \approx Y[i:j]$ for all $1 \le i \le
j \le |X|$. In this paper, we propose a generalization of the dictionary
matching problem and present a generalization of the Aho-Corasick algorithm for
the dictionary matching under SCER. We present an algorithm that constructs
SCER automata and an algorithm that performs dictionary matching under SCER by
using the automata. Moreover, we show the time and space complexity of our
algorithms with respect to the size of input strings.
</summary>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.07538v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.07538v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.08006">
    <id>http://arxiv.org/abs/1909.08006v1</id>
    <updated>2019-09-17T18:10:04Z</updated>
    <published>2019-09-17T18:10:04Z</published>
    <title>Leyenda: An Adaptive, Hybrid Sorting Algorithm for Large Scale Data with
  Limited Memory</title>
    <summary>  Sorting is the one of the fundamental tasks of modern data management
systems. With Disk I/O being the most-accused performance bottleneck and more
computation-intensive workloads, it has come to our attention that in
heterogeneous environment, performance bottleneck may vary among different
infrastructure. As a result, sort kernels need to be adaptive to changing
hardware conditions. In this paper, we propose Leyenda, a hybrid, parallel and
efficient Radix Most-Significant-Bit (MSB) MergeSort algorithm, with
utilization of local thread-level CPU cache and efficient disk/memory I/O.
Leyenda is capable of performing either internal or external sort efficiently,
based on different I/O and processing conditions. We benchmarked Leyenda with
three different workloads from Sort Benchmark, targeting three unique use
cases, including internal, partially in-memory and external sort, and we found
Leyenda to outperform GNU's parallel in-memory quick/merge sort implementations
by up to three times. Leyenda is also ranked the second best external sort
algorithm on ACM 2019 SIGMOD programming contest and forth overall.
</summary>
    <author>
      <name>Yuanjing Shi</name>
    </author>
    <author>
      <name>Zhaoxing Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.08006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.08006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.11433">
    <id>http://arxiv.org/abs/1909.11433v1</id>
    <updated>2019-09-25T12:19:38Z</updated>
    <published>2019-09-25T12:19:38Z</published>
    <title>Weighted Shortest Common Supersequence Problem Revisited</title>
    <summary>  A weighted string, also known as a position weight matrix, is a sequence of
probability distributions over some alphabet. We revisit the Weighted Shortest
Common Supersequence (WSCS) problem, introduced by Amir et al. [SPIRE 2011],
that is, the SCS problem on weighted strings. In the WSCS problem, we are given
two weighted strings $W_1$ and $W_2$ and a threshold $\mathit{Freq}$ on
probability, and we are asked to compute the shortest (standard) string $S$
such that both $W_1$ and $W_2$ match subsequences of $S$ (not necessarily the
same) with probability at least $\mathit{Freq}$. Amir et al. showed that this
problem is NP-complete if the probabilities, including the threshold
$\mathit{Freq}$, are represented by their logarithms (encoded in binary). We
present an algorithm that solves the WSCS problem for two weighted strings of
length $n$ over a constant-sized alphabet in $\mathcal{O}(n^2\sqrt{z} \log{z})$
time. Notably, our upper bound matches known conditional lower bounds stating
that the WSCS problem cannot be solved in $\mathcal{O}(n^{2-\varepsilon})$ time
or in $\mathcal{O}^*(z^{0.5-\varepsilon})$ time unless there is a breakthrough
improving upon long-standing upper bounds for fundamental NP-hard problems
(CNF-SAT and Subset Sum, respectively). We also discover a fundamental
difference between the WSCS problem and the Weighted Longest Common Subsequence
(WLCS) problem, introduced by Amir et al. [JDA 2010]. We show that the WLCS
problem cannot be solved in $\mathcal{O}(n^{f(z)})$ time, for any function
$f(z)$, unless $\mathrm{P}=\mathrm{NP}$.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszy≈Ñski</name>
    </author>
    <author>
      <name>Tomasz Wale≈Ñ</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SPIRE'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.11433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.11433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.11577">
    <id>http://arxiv.org/abs/1909.11577v1</id>
    <updated>2019-09-25T16:08:19Z</updated>
    <published>2019-09-25T16:08:19Z</published>
    <title>Internal Dictionary Matching</title>
    <summary>  We introduce data structures answering queries concerning the occurrences of
patterns from a given dictionary $\mathcal{D}$ in fragments of a given string
$T$ of length $n$. The dictionary is internal in the sense that each pattern in
$\mathcal{D}$ is given as a fragment of $T$. This way, $\mathcal{D}$ takes
space proportional to the number of patterns $d=|\mathcal{D}|$ rather than
their total length, which could be $\Theta(n\cdot d)$.
  In particular, we consider the following types of queries: reporting and
counting all occurrences of patterns from $\mathcal{D}$ in a fragment $T[i..j]$
and reporting distinct patterns from $\mathcal{D}$ that occur in $T[i..j]$. We
show how to construct, in $\mathcal{O}((n+d) \log^{\mathcal{O}(1)} n)$ time, a
data structure that answers each of these queries in time
$\mathcal{O}(\log^{\mathcal{O}(1)} n+|output|)$.
  The case of counting patterns is much more involved and needs a combination
of a locally consistent parsing with orthogonal range searching. Reporting
distinct patterns, on the other hand, uses the structure of maximal repetitions
in strings. Finally, we provide tight---up to subpolynomial factors---upper and
lower bounds for the case of a dynamic dictionary.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Manal Mohamed</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Wale≈Ñ</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A short version of this paper was accepted for presentation at ISAAC
  2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.11577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.11577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.11930">
    <id>http://arxiv.org/abs/1909.11930v2</id>
    <updated>2020-01-15T12:05:06Z</updated>
    <published>2019-09-26T06:34:01Z</published>
    <title>String Indexing with Compressed Patterns</title>
    <summary>  Given a string $S$ of length $n$, the classic string indexing problem is to
preprocess $S$ into a compact data structure that supports efficient subsequent
pattern queries. In this paper we consider the basic variant where the pattern
is given in compressed form and the goal is to achieve query time that is fast
in terms of the compressed size of the pattern. This captures the common
client-server scenario, where a client submits a query and communicates it in
compressed form to a server. Instead of the server decompressing the query
before processing it, we consider how to efficiently process the compressed
query directly. Our main result is a novel linear space data structure that
achieves near-optimal query time for patterns compressed with the classic
Lempel-Ziv compression scheme. Along the way we develop several data structural
techniques of independent interest, including a novel data structure that
compactly encodes all LZ77 compressed suffixes of a string in linear space and
a general decomposition of tries that reduces the search time from logarithmic
in the size of the trie to logarithmic in the length of the pattern.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <author>
      <name>Teresa Anna Steiner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures; added figure for section 5, included discussion
  of open problems, revision of explanations in sections 3, 4 and 5, fixed
  typos</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.11930v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.11930v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.13670">
    <id>http://arxiv.org/abs/1909.13670v4</id>
    <updated>2019-11-08T18:23:08Z</updated>
    <published>2019-09-23T02:21:18Z</published>
    <title>RECIPE : Converting Concurrent DRAM Indexes to Persistent-Memory Indexes</title>
    <summary>  We present Recipe, a principled approach for converting concurrent DRAM
indexes into crash-consistent indexes for persistent memory (PM). The main
insight behind Recipe is that isolation provided by a certain class of
concurrent in-memory indexes can be translated with small changes to
crash-consistency when the same index is used in PM. We present a set of
conditions that enable the identification of this class of DRAM indexes, and
the actions to be taken to convert each index to be persistent. Based on these
conditions and conversion actions, we modify five different DRAM indexes based
on B+ trees, tries, radix trees, and hash tables to their crash-consistent PM
counterparts. The effort involved in this conversion is minimal, requiring
30-200 lines of code. We evaluated the converted PM indexes on Intel DC
Persistent Memory, and found that they outperform state-of-the-art,
hand-crafted PM indexes in multi-threaded workloads by up-to 5.2x. For example,
we built P-CLHT, our PM implementation of the CLHT hash table by modifying only
30 LOC. When running YCSB workloads, P-CLHT performs up to 2.4x better than
Cacheline-Conscious Extendible Hashing (CCEH), the state-of-the-art PM hash
table.
</summary>
    <author>
      <name>Se Kwon Lee</name>
    </author>
    <author>
      <name>Jayashree Mohan</name>
    </author>
    <author>
      <name>Sanidhya Kashyap</name>
    </author>
    <author>
      <name>Taesoo Kim</name>
    </author>
    <author>
      <name>Vijay Chidambaram</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3341301.3359635</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3341301.3359635" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3pages: Added one more reference</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.13670v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.13670v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.02151">
    <id>http://arxiv.org/abs/1910.02151v2</id>
    <updated>2020-03-08T20:06:38Z</updated>
    <published>2019-10-04T21:17:16Z</published>
    <title>Towards a Definitive Measure of Repetitiveness</title>
    <summary>  Unlike in statistical compression, where Shannon's entropy is a definitive
lower bound, no such clear measure exists for the compressibility of repetitive
sequences. Since statistical entropy does not capture repetitiveness, ad-hoc
measures like the size $z$ of the Lempel--Ziv parse are frequently used to
estimate repetitiveness. Recently, a more principled measure, the size $\gamma$
of the smallest string \emph{attractor}, was introduced. The measure $\gamma$
lower bounds all the previous relevant ones, yet length-$n$ strings can be
represented and efficiently indexed within space
$O(\gamma\log\frac{n}{\gamma})$, which also upper bounds most measures. While
$\gamma$ is certainly a better measure of repetitiveness than $z$, it is
NP-complete to compute, and no $o(\gamma\log n)$-space representation of
strings is known.
  In this paper, we study a smaller measure, $\delta \le \gamma$, which can be
computed in linear time. We show that $\delta$ better captures the
compressibility of repetitive strings. For every length $n$ and every value
$\delta \ge 2$, we construct a string such that $\gamma = \Omega(\delta \log
\frac{n}{\delta})$. Still, we show a representation of any string $S$ in
$O(\delta\log\frac{n}{\delta})$ space that supports direct access to any
character $S[i]$ in time $O(\log\frac{n}{\delta})$ and finds the $occ$
occurrences of any pattern $P[1..m]$ in time $O(m\log n + occ\log^\epsilon n)$.
Further, we prove that no $o(\delta\log n)$-space representation exists: for
every $n$ and every $2\le \delta\le n^{1-\epsilon}$, we exhibit a string family
whose elements can only be encoded in $\Omega(\delta\log \frac{n}{\delta})$
space. We complete our characterization of $\delta$ by showing that the
smallest context-free grammar can be of size $\Omega(\delta \log^2 n / \log\log
n)$. No such separation is known for $\gamma$.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1910.02151v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02151v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.02851">
    <id>http://arxiv.org/abs/1910.02851v1</id>
    <updated>2019-10-07T15:19:44Z</updated>
    <published>2019-10-07T15:19:44Z</published>
    <title>ER-index: a referential index for encrypted genomic databases</title>
    <summary>  Huge DBMSs storing genomic information are being created and engineerized for
doing large-scale, comprehensive and in-depth analysis of human beings and
their diseases. However, recent regulations like the GDPR require that
sensitive data are stored and elaborated thanks to privacy-by-design methods
and software. We designed and implemented ER-index, a new full-text index in
minute space which was optimized for compressing and encrypting collections of
genomic sequences, and for performing on them fast pattern-search queries. Our
new index complements the E2FM-index, which was introduced to compress and
encrypt collections of nucleotide sequences without relying on a reference
sequence. When used on collections of highly similar sequences, the ER-index
allows to obtain compression ratios which are an order of magnitude smaller
than those achieved with the E2FM-index, but maintaining its very good search
performance. Moreover, thanks to the ER-index multi-user and multiple-keys
encryption model, a single index can store the sequences related to a
population of individuals so that users may perform search operations only on
the sequences to which they were granted access. The ER-index C++ source code
plus scripts and data to assess the tool performance are available at:
https://github.com/EncryptedIndexes/erindex.
</summary>
    <author>
      <name>Ferdinando Montecuollo</name>
    </author>
    <author>
      <name>Giovannni Schmid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages with detailed pseudocodes</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.02851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P15, 68P25, 68P30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.2; E.3; H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.08111">
    <id>http://arxiv.org/abs/1908.08111v1</id>
    <updated>2019-08-21T20:31:34Z</updated>
    <published>2019-08-21T20:31:34Z</published>
    <title>Engineering Faster Sorters for Small Sets of Items</title>
    <summary>  Sorting a set of items is a task that can be useful by itself or as a
building block for more complex operations. The more sophisticated and fast
sorting algorithms become asymptotically, the less efficient they are for small
sets of items due to large constant factor. This thesis aims to determine if
there is a faster way to sort base case sizes than using insertion sort. For
that we looked at sorting networks and how to implement them efficiently.
Because sorting networks need to be implemented explicitly for each input size,
providing networks for larger sizes becomess less efficient. That is why we
modified Super Scalar Sample Sort to break down larger sets into sizes that can
in turn be sorted by sorting networks. We show that the task of sorting only
small sets can be greatly improved by at least 25% when using sorting networks
compared to insertion sort, but that when integrating them into other sorting
algorithms the speed-up is hindered by the limited L1 instruction cache size.
On a machine with 64KiB of L1 instruction cache we achieved over 6% of
improvement when using sorting networks as a base case sorter instead of
insertion sort.
</summary>
    <author>
      <name>Jasper Marianczuk</name>
    </author>
    <link href="http://arxiv.org/abs/1908.08111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.08762">
    <id>http://arxiv.org/abs/1908.08762v1</id>
    <updated>2019-08-23T11:27:47Z</updated>
    <published>2019-08-23T11:27:47Z</published>
    <title>Revisiting Consistent Hashing with Bounded Loads</title>
    <summary>  Dynamic load balancing lies at the heart of distributed caching. Here, the
goal is to assign objects (load) to servers (computing nodes) in a way that
provides load balancing while at the same time dynamically adjusts to the
addition or removal of servers. One essential requirement is that the
assignment time (or hashing time) should be independent of the number of
servers. Addition or removal of small servers should not require us to
recompute the complete assignment. A popular and widely adopted solution is the
two-decade-old Consistent Hashing (CH). Recently, an elegant extension was
provided to account for server bounds. In this paper, we identify that existing
methodologies for CH and its variants suffer from cascaded overflow, leading to
poor load balancing. This cascading effect leads to decreasing performance of
the hashing procedure with increasing load. To overcome the cascading effect,
we propose a simple solution to CH based on recent advances in fast minwise
hashing. We show, both theoretically and empirically, that our proposed
solution is significantly superior for load balancing and is optimal in many
senses.
</summary>
    <author>
      <name>John Chen</name>
    </author>
    <author>
      <name>Ben Coleman</name>
    </author>
    <author>
      <name>Anshumali Shrivastava</name>
    </author>
    <link href="http://arxiv.org/abs/1908.08762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.09125">
    <id>http://arxiv.org/abs/1908.09125v2</id>
    <updated>2020-03-04T10:00:03Z</updated>
    <published>2019-08-24T11:26:37Z</published>
    <title>When a Dollar Makes a BWT</title>
    <summary>  The Burrows-Wheeler-Transform (BWT) is a reversible string transformation
which plays a central role in text compression and is fundamental in many
modern bioinformatics applications. The BWT is a permutation of the characters,
which is in general better compressible and allows to answer several different
query types more efficiently than the original string.
  It is easy to see that not every string is a BWT image, and exact
characterizations of BWT images are known. We investigate a related
combinatorial question. In many applications, a sentinel character dollar is
added to mark the end of the string, and thus the BWT of a string ending with
dollar contains exactly one dollar-character. Given a string w, we ask in which
positions, if any, the dollar-character can be inserted to turn w into the BWT
image of a word ending with dollar. We show that this depends only on the
standard permutation of w and present a O(n log n)-time algorithm for
identifying all such positions, improving on the naive quadratic time
algorithm. We also give a combinatorial characterization of such positions and
develop bounds on their number and value.
</summary>
    <author>
      <name>Sara Giuliani</name>
    </author>
    <author>
      <name>Zsuzsanna Lipt√°k</name>
    </author>
    <author>
      <name>Romeo Rizzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at ICTCS 2019 (20th Italian Conference on Theoretical
  Computer Science, 9-11 Sept. 2019, Como, Italy)</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.09125v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.09125v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.01960">
    <id>http://arxiv.org/abs/1902.01960v2</id>
    <updated>2019-02-25T19:39:39Z</updated>
    <published>2019-02-05T22:29:03Z</published>
    <title>On the Hardness and Inapproximability of Recognizing Wheeler Graphs</title>
    <summary>  In recent years several compressed indexes based on variants of the
Burrows-Wheeler transformation have been introduced. Some of these index
structures far more complex than a single string, as was originally done with
the FM-index [Ferragina and Manzini, J. ACM 2005]. As such, there has been an
effort to better understand under which conditions such an indexing scheme is
possible. This led to the introduction of Wheeler graphs [Gagie it et al.,
Theor. Comput. Sci., 2017]. A Wheeler graph is a directed graph with edge
labels which satisfies two simple axioms. Wheeler graphs can be indexed in a
way which is space efficient and allows for fast traversal. Gagie et al. showed
that de Bruijn graphs, generalized compressed suffix arrays, and several other
BWT related structures can be represented as Wheeler graphs. Here we answer the
open question of whether or not there exists an efficient algorithm for
recognizing if a graph is a Wheeler graph. We demonstrate:(i) Recognizing if a
graph is a Wheeler graph is NP-complete for any edge label alphabet of size
$\sigma \geq 2$, even for DAGs. It can be solved in linear time for $\sigma
=1$; (ii) An optimization variant called Wheeler Graph Violation (WGV) which
aims to remove the minimum number of edges needed to obtain a Wheeler graph is
APX-hard, even for DAGs. Hence, unless P = NP, there exists constant $C > 1$
such that there is no $C$-approximation algorithm. We show conditioned on the
Unique Games Conjecture, for every constant $C \geq 1$, it is NP-hard to find a
$C$-approximation to WGV; (iii) The Wheeler Subgraph problem (WS) which aims to
find the largest Wheeler subgraph is in APX for $\sigma=O(1)$; (iv) For the
above problems there exist efficient exponential time exact algorithms, relying
on graph isomorphism being computed in strictly sub-exponential time; (v) A
class of graphs where the recognition problem is polynomial time solvable.
</summary>
    <author>
      <name>Daniel Gibney</name>
    </author>
    <author>
      <name>Sharma V. Thankachan</name>
    </author>
    <link href="http://arxiv.org/abs/1902.01960v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01960v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.10159">
    <id>http://arxiv.org/abs/1908.10159v1</id>
    <updated>2019-08-27T12:33:32Z</updated>
    <published>2019-08-27T12:33:32Z</published>
    <title>Partial Sums on the Ultra-Wide Word RAM</title>
    <summary>  We consider the classic partial sums problem on the ultra-wide word RAM model
of computation. This model extends the classic $w$-bit word RAM model with
special ultrawords of length $w^2$ bits that support standard arithmetic and
boolean operation and scattered memory access operations that can access $w$
(non-contiguous) locations in memory. The ultra-wide word RAM model captures
(and idealizes) modern vector processor architectures.
  Our main result is a new in-place data structure for the partial sum problem
that only stores a constant number of ultraword in addition to the input and
supports operations in doubly logarithmic time. This matches the best known
time bounds for the problem (among polynomial space data structures) while
improving the space from superlinear to a constant number of ultrawords. Our
results are based on a simple and elegant in-place word RAM data structure,
known as the Fenwick tree. Our main technical contribution is a new efficient
parallel ultra-wide word RAM implementation of the Fenwick tree, which is
likely of independent interest.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <author>
      <name>Frederik Rye Skjoldjensen</name>
    </author>
    <link href="http://arxiv.org/abs/1908.10159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.10644">
    <id>http://arxiv.org/abs/1908.10644v1</id>
    <updated>2019-08-28T11:13:03Z</updated>
    <published>2019-08-28T11:13:03Z</published>
    <title>Bloom filter variants for multiple sets: a comparative assessment</title>
    <summary>  In this paper we compare two probabilistic data structures for association
queries derived from the well-known Bloom filter: the shifting Bloom filter
(ShBF), and the spatial Bloom filter (SBF). With respect to the original data
structure, both variants add the ability to store multiple subsets in the same
filter, using different strategies. We analyse the performance of the two data
structures with respect to false positive probability, and the inter-set error
probability (the probability for an element in the set of being recognised as
belonging to the wrong subset). As part of our analysis, we extended the
functionality of the shifting Bloom filter, optimising the filter for any
non-trivial number of subsets. We propose a new generalised ShBF definition
with applications outside of our specific domain, and present new probability
formulas. Results of the comparison show that the ShBF provides better space
efficiency, but at a significantly higher computational cost than the SBF.
</summary>
    <author>
      <name>Luca Calderoni</name>
    </author>
    <author>
      <name>Dario Maio</name>
    </author>
    <author>
      <name>Paolo Palmieri</name>
    </author>
    <link href="http://arxiv.org/abs/1908.10644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.10843">
    <id>http://arxiv.org/abs/1908.10843v2</id>
    <updated>2020-01-08T00:30:51Z</updated>
    <published>2019-08-26T21:05:26Z</published>
    <title>An Incompressibility Theorem for Automatic Complexity</title>
    <summary>  Shallit and Wang showed that the automatic complexity $A(x)\ge n/13$ for
almost all $x\in\{0,1\}^n$. They also stated that Holger Petersen had informed
them that the constant 13 can be reduced to 7. Here we show that it can be
reduced to $2+\epsilon$ for any $\epsilon>0$.
</summary>
    <author>
      <name>Bj√∏rn Kjos-Hanssen</name>
    </author>
    <link href="http://arxiv.org/abs/1908.10843v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10843v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.11181">
    <id>http://arxiv.org/abs/1908.11181v2</id>
    <updated>2019-10-31T17:48:20Z</updated>
    <published>2019-08-29T12:36:15Z</published>
    <title>Compacted binary trees admit a stretched exponential</title>
    <summary>  A compacted binary tree is a directed acyclic graph encoding a binary tree in
which common subtrees are factored and shared, such that they are represented
only once. We show that the number of compacted binary trees of size $n$ grows
asymptotically like $$\Theta\left( n! \, 4^n e^{3a_1n^{1/3}} n^{3/4} \right),$$
where $a_1\approx-2.338$ is the largest root of the Airy function. Our method
involves a new two parameter recurrence which yields an algorithm of quadratic
arithmetic complexity. We use empirical methods to estimate the values of all
terms defined by the recurrence, then we prove by induction that these
estimates are sufficiently accurate for large $n$ to determine the asymptotic
form. Our results also lead to new bounds on the number of minimal finite
automata recognizing a finite language on a binary alphabet. As a consequence,
these also exhibit a stretched exponential.
</summary>
    <author>
      <name>Andrew Elvey Price</name>
    </author>
    <author>
      <name>Wenjie Fang</name>
    </author>
    <author>
      <name>Michael Wallner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.11181v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.11181v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05C30, 05A16, 05C20, 05C05" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.10598">
    <id>http://arxiv.org/abs/1908.10598v1</id>
    <updated>2019-08-28T08:29:14Z</updated>
    <published>2019-08-28T08:29:14Z</published>
    <title>Techniques for Inverted Index Compression</title>
    <summary>  The data structure at the core of large-scale search engines is the inverted
index, which is essentially a collection of sorted integer sequences called
inverted lists. Because of the many documents indexed by such engines and
stringent performance requirements imposed by the heavy load of queries, the
inverted index stores billions of integers that must be searched efficiently.
In this scenario, index compression is essential because it leads to a better
exploitation of the computer memory hierarchy for faster query processing and,
at the same time, allows reducing the number of storage machines. The aim of
this article is twofold: first, surveying the encoding algorithms suitable for
inverted index compression and, second, characterizing the performance of the
inverted index through experimentation.
</summary>
    <author>
      <name>Giulio Ermanno Pibiri</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <link href="http://arxiv.org/abs/1908.10598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.02804">
    <id>http://arxiv.org/abs/1909.02804v2</id>
    <updated>2019-09-13T06:16:46Z</updated>
    <published>2019-09-06T10:17:30Z</published>
    <title>Minimal Unique Substrings and Minimal Absent Words in a Sliding Window</title>
    <summary>  A substring $u$ of a string $T$ is called a minimal unique substring (MUS) of
$T$ if $u$ occurs exactly once in $T$ and any proper substring of $u$ occurs at
least twice in $T$. A string $w$ is called a minimal absent word (MAW) of $T$
if $w$ does not occur in $T$ and any proper substring of $w$ occurs in $T$. In
this paper, we study the problems of computing MUSs and MAWs in a sliding
window over a given string $T$. We first show how the set of MUSs can change in
a sliding window over $T$, and present an $O(n\log\sigma)$-time and
$O(d)$-space algorithm to compute MUSs in a sliding window of width $d$ over
$T$, where $\sigma$ is the maximum number of distinct characters in every
window. We then give tight upper and lower bounds on the maximum number of
changes in the set of MAWs in a sliding window over $T$. Our bounds improve on
the previous results in [Crochemore et al., 2017].
</summary>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Yuki Kuhara</name>
    </author>
    <author>
      <name>Tooru Akagi</name>
    </author>
    <author>
      <name>Yuta Fujishige</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1909.02804v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02804v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.02741">
    <id>http://arxiv.org/abs/1908.02741v4</id>
    <updated>2019-10-10T17:58:10Z</updated>
    <published>2019-08-07T17:35:50Z</published>
    <title>Parallel Finger Search Structures</title>
    <summary>  In this paper we present two versions of a parallel finger structure FS on p
processors that supports searches, insertions and deletions, and has a finger
at each end. This is to our knowledge the first implementation of a parallel
search structure that is work-optimal with respect to the finger bound and yet
has very good parallelism (within a factor of O( (log p)^2 ) of optimal). We
utilize an extended implicit batching framework that transparently facilitates
the use of FS by any parallel program P that is modelled by a dynamically
generated DAG D where each node is either a unit-time instruction or a call to
FS.
  The total work done by either version of FS is bounded by the finger bound
F[L] (for some linearization L of D ), i.e. each operation on an item with
finger distance r takes O( log r + 1 ) amortized work; it is cheaper for items
closer to a finger. Running P using the simpler version takes O( ( T[1] + F[L]
) / p + T[inf] + d * ( (log p)^2 + log n ) ) time on a greedy scheduler, where
T[1],T[inf] are the size and span of D respectively, and n is the maximum
number of items in FS, and d is the maximum number of calls to FS along any
path in D. Using the faster version, this is reduced to O( ( T[1] + F[L] ) / p
+ T[inf] + d * (log p)^2 + s[L] ) time, where s[L] is the weighted span of D
where each call to FS is weighted by its cost according to F[L]. We also sketch
how to extend FS to support a fixed number of movable fingers.
  The data structures in our paper fit into the dynamic multithreading
paradigm, and their performance bounds are directly composable with other data
structures given in the same paradigm. Also, the results can be translated to
practical implementations using work-stealing schedulers.
</summary>
    <author>
      <name>Seth Gilbert</name>
    </author>
    <author>
      <name>Wei Quan Lim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper published in DISC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.02741v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02741v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.03169">
    <id>http://arxiv.org/abs/1908.03169v4</id>
    <updated>2020-02-06T20:38:44Z</updated>
    <published>2019-08-08T17:02:16Z</published>
    <title>The repetition threshold for binary rich words</title>
    <summary>  A word of length $n$ is rich if it contains $n$ nonempty palindromic factors.
An infinite word is rich if all of its finite factors are rich. Baranwal and
Shallit produced an infinite binary rich word with critical exponent
$2+\sqrt{2}/2$ ($\approx 2.707$) and conjectured that this was the least
possible critical exponent for infinite binary rich words (i.e., that the
repetition threshold for binary rich words is $2+\sqrt{2}/2$). In this article,
we give a structure theorem for infinite binary rich words that avoid
$14/5$-powers (i.e., repetitions with exponent at least 2.8). As a consequence,
we deduce that the repetition threshold for binary rich words is
$2+\sqrt{2}/2$, as conjectured by Baranwal and Shallit. This resolves an open
problem of Vesti for the binary alphabet; the problem remains open for larger
alphabets.
</summary>
    <author>
      <name>James D. Currie</name>
    </author>
    <author>
      <name>Lucas Mol</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.03169v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.03169v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.04686">
    <id>http://arxiv.org/abs/1908.04686v1</id>
    <updated>2019-08-12T10:58:49Z</updated>
    <published>2019-08-12T10:58:49Z</published>
    <title>Space-Efficient Construction of Compressed Suffix Trees</title>
    <summary>  We show how to build several data structures of central importance to string
processing, taking as input the Burrows-Wheeler transform (BWT) and using small
extra working space. Let $n$ be the text length and $\sigma$ be the alphabet
size. We first provide two algorithms that enumerate all LCP values and suffix
tree intervals in $O(n\log\sigma)$ time using just $o(n\log\sigma)$ bits of
working space on top of the input BWT. Using these algorithms as building
blocks, for any parameter $0 &lt; \epsilon \leq 1$ we show how to build the PLCP
bitvector and the balanced parentheses representation of the suffix tree
topology in $O\left(n(\log\sigma + \epsilon^{-1}\cdot \log\log n)\right)$ time
using at most $n\log\sigma \cdot(\epsilon + o(1))$ bits of working space on top
of the input BWT and the output. In particular, this implies that we can build
a compressed suffix tree from the BWT using just succinct working space (i.e.
$o(n\log\sigma)$ bits) and any time in $\Theta(n\log\sigma) + \omega(n\log\log
n)$. This improves the previous most space-efficient algorithms, which worked
in $O(n)$ bits and $O(n\log n)$ time. We also consider the problem of merging
BWTs of string collections, and provide a solution running in $O(n\log\sigma)$
time and using just $o(n\log\sigma)$ bits of working space. An efficient
implementation of our LCP construction and BWT merge algorithms use (in RAM) as
few as $n$ bits on top of a packed representation of the input/output and
process data as fast as $2.92$ megabases per second.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1901.05226</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.04686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.04517">
    <id>http://arxiv.org/abs/1908.04517v1</id>
    <updated>2019-08-13T07:14:01Z</updated>
    <published>2019-08-13T07:14:01Z</published>
    <title>Beyond the Inverted Index</title>
    <summary>  In this paper, a new data structure named group-list is proposed. The
group-list is as simple as the inverted index. However, the group-list divides
document identifiers in an inverted index into groups, which makes it more
efficient when it is used to perform the intersection or union operation on
document identifiers. The experimental results on a synthetic dataset show that
the group-list outperforms the inverted index.
</summary>
    <author>
      <name>Zhi-Hong Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, and 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.04517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.04937">
    <id>http://arxiv.org/abs/1908.04937v1</id>
    <updated>2019-08-14T03:14:21Z</updated>
    <published>2019-08-14T03:14:21Z</published>
    <title>Fast Cartesian Tree Matching</title>
    <summary>  Cartesian tree matching is the problem of finding all substrings of a given
text which have the same Cartesian trees as that of a given pattern. So far
there is one linear-time solution for Cartesian tree matching, which is based
on the KMP algorithm. We improve the running time of the previous solution by
introducing new representations. We present the framework of a binary
filtration method and an efficient verification technique for Cartesian tree
matching. Any exact string matching algorithm can be used as a filtration for
Cartesian tree matching on our framework. We also present a SIMD solution for
Cartesian tree matching suitable for short patterns. By experiments we show
that known string matching algorithms combined on our framework of binary
filtration and efficient verification produce algorithms of good performances
for Cartesian tree matching.
</summary>
    <author>
      <name>Siwoo Song</name>
    </author>
    <author>
      <name>Cheol Ryu</name>
    </author>
    <author>
      <name>Simone Faro</name>
    </author>
    <author>
      <name>Thierry Lecroq</name>
    </author>
    <author>
      <name>Kunsoo Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures, Submitted to SPIRE 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.04937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.04933">
    <id>http://arxiv.org/abs/1908.04933v3</id>
    <updated>2019-11-16T07:48:28Z</updated>
    <published>2019-08-14T02:42:36Z</published>
    <title>Re-Pair In Small Space</title>
    <summary>  Re-Pair is a grammar compression scheme with favorably good compression
rates. The computation of Re-Pair comes with the cost of maintaining large
frequency tables, which makes it hard to compute Re-Pair on large scale data
sets. As a solution for this problem we present, given a text of length $n$
whose characters are drawn from an integer alphabet, an $O(n^2) \cap O(n^2 \lg
\log_\tau n \lg \lg \lg n / \log_\tau n)$ time algorithm computing Re-Pair in
$n \lg \max(n,\tau)$ bits of space including the text space, where $\tau$ is
the number of terminals and non-terminals. The algorithm works in the restore
model, supporting the recovery of the original input in the time for the
Re-Pair computation with $O(\lg n)$ additional bits of working space. We give
variants of our solution working in parallel or in the external memory model.
</summary>
    <author>
      <name>Dominik K√∂ppl</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Isamu Furuya</name>
    </author>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <author>
      <name>Kensuke Sakai</name>
    </author>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <link href="http://arxiv.org/abs/1908.04933v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04933v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.04810">
    <id>http://arxiv.org/abs/1908.04810v1</id>
    <updated>2019-08-13T18:21:04Z</updated>
    <published>2019-08-13T18:21:04Z</published>
    <title>On Occupancy Moments and Bloom Filter Efficiency</title>
    <summary>  Two multivariate committee distributions are shown to belong to Berg's family
of factorial series distributions and Kemp's family of generalized
hypergeometric factorial moment distributions. Exact moment formulas, upper and
lower bounds, and statistical parameter estimators are provided for the classic
occupancy and committee distributions. The derived moment equations are used to
determine exact formulas for the false-positive rate and efficiency of Bloom
filters -- probabilistic data structures used to solve the set membership
problem. This study reveals that the conventional Bloom filter analysis
overestimates the number of hash functions required to minimize the
false-positive rate, and shows that Bloom filter efficiency is monotonic in the
number of hash functions.
</summary>
    <author>
      <name>Jonathan Burns</name>
    </author>
    <link href="http://arxiv.org/abs/1908.04810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60C05, 68R05 (Primary) 94A24, 33C20 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.04056">
    <id>http://arxiv.org/abs/1908.04056v1</id>
    <updated>2019-08-12T08:58:39Z</updated>
    <published>2019-08-12T08:58:39Z</published>
    <title>New Results on Nyldon Words Derived Using an Algorithm from Hall Set
  Theory</title>
    <summary>  Grinberg defined Nyldon words as those words which cannot be factorized into
a sequence of lexicographically nondecreasing smaller Nyldon words. He was
inspired by Lyndon words, defined the same way except with "nondecreasing"
replaced by "nonincreasing." Charlier, Philibert, and Stipulanti proved that,
like Lyndon words, any word has a unique nondecreasing factorization into
Nyldon words. They also show that the Nyldon words form a right Lazard set, and
equivalently, a right Hall set. In this paper, we provide a new proof of unique
factorization into Nyldon words related to Hall set theory and resolve several
questions of Charlier et al. In particular, we prove that Nyldon words of a
fixed length form a circular code, we prove a result on factorizing powers of
words into Nyldon words, and we investigate the Lazard procedure for generating
Nyldon words.
</summary>
    <author>
      <name>Swapnil Garg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.04056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.05930">
    <id>http://arxiv.org/abs/1908.05930v1</id>
    <updated>2019-08-16T11:11:06Z</updated>
    <published>2019-08-16T11:11:06Z</published>
    <title>Efficient Online String Matching Based on Characters Distance Text
  Sampling</title>
    <summary>  Searching for all occurrences of a pattern in a text is a fundamental problem
in computer science with applications in many other fields, like natural
language processing, information retrieval and computational biology. Sampled
string matching is an efficient approach recently introduced in order to
overcome the prohibitive space requirements of an index construction, on the
one hand, and drastically reduce searching time for the online solutions, on
the other hand. In this paper we present a new algorithm for the sampled string
matching problem, based on a characters distance sampling approach. The main
idea is to sample the distances between consecutive occurrences of a given
pivot character and then to search online the sampled data for any occurrence
of the sampled pattern, before verifying the original text. From a theoretical
point of view we prove that, under suitable conditions, our solution can
achieve both linear worst-case time complexity and optimal average-time
complexity. From a practical point of view it turns out that our solution shows
a sub-linear behaviour in practice and speeds up online searching by a factor
of up to 9, using limited additional space whose amount goes from 11% to 2.8%
of the text size, with a gain up to 50% if compared with previous solutions.
</summary>
    <author>
      <name>Simone Faro</name>
    </author>
    <author>
      <name>Arianna Pavone</name>
    </author>
    <author>
      <name>Francesco Pio Marino</name>
    </author>
    <link href="http://arxiv.org/abs/1908.05930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.05930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.06428">
    <id>http://arxiv.org/abs/1908.06428v1</id>
    <updated>2019-08-18T11:38:09Z</updated>
    <published>2019-08-18T11:38:09Z</published>
    <title>The smallest grammar problem revisited</title>
    <summary>  In a seminal paper of Charikar et al. on the smallest grammar problem, the
authors derive upper and lower bounds on the approximation ratios for several
grammar-based compressors, but in all cases there is a gap between the lower
and upper bound. Here the gaps for $\mathsf{LZ78}$ and $\mathsf{BISECTION}$ are
closed by showing that the approximation ratio of $\mathsf{LZ78}$ is $\Theta(
(n/\log n)^{2/3})$, whereas the approximation ratio of $\mathsf{BISECTION}$ is
$\Theta(\sqrt{n/\log n})$. In addition, the lower bound for $\mathsf{RePair}$
is improved from $\Omega(\sqrt{\log n})$ to $\Omega(\log n/\log\log n)$.
Finally, results of Arpe and Reischuk relating grammar-based compression for
arbitrary alphabets and binary alphabets are improved.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Momoko Hirayama</name>
    </author>
    <author>
      <name>Danny Hucke</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Artur Jez</name>
    </author>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Carl Philipp Reh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A short version of this paper appeared in the Proceedings of SPIRE
  2016. This work has been supported by the DFG research project LO 748/10-1
  (QUANT-KOMP)</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.06428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.06428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.10984">
    <id>http://arxiv.org/abs/1907.10984v1</id>
    <updated>2019-07-25T11:48:57Z</updated>
    <published>2019-07-25T11:48:57Z</published>
    <title>Enumerating Range Modes</title>
    <summary>  We consider the range mode problem where given a sequence and a query range
in it, we want to find items with maximum frequency in the range. We give time-
and space- efficient algorithms for this problem. Our algorithms are efficient
for small maximum frequency cases. We also consider a natural generalization of
the problem: the range mode enumeration problem, for which there has been no
known efficient algorithms. Our algorithms have query time complexities which
is linear to the output size plus small terms.
</summary>
    <author>
      <name>Kentaro Sumigawa</name>
    </author>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <link href="http://arxiv.org/abs/1907.10984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.10874">
    <id>http://arxiv.org/abs/1907.10874v1</id>
    <updated>2019-07-25T07:37:42Z</updated>
    <published>2019-07-25T07:37:42Z</published>
    <title>How to Store a Random Walk</title>
    <summary>  Motivated by storage applications, we study the following data structure
problem: An encoder wishes to store a collection of jointly-distributed files
$\overline{X}:=(X_1,X_2,\ldots, X_n) \sim \mu$ which are \emph{correlated}
($H_\mu(\overline{X}) \ll \sum_i H_\mu(X_i)$), using as little (expected)
memory as possible, such that each individual file $X_i$ can be recovered
quickly with few (ideally constant) memory accesses.
  In the case of independent random files, a dramatic result by \Pat (FOCS'08)
and subsequently by Dodis, \Pat and Thorup (STOC'10) shows that it is possible
to store $\overline{X}$ using just a \emph{constant} number of extra bits
beyond the information-theoretic minimum space, while at the same time decoding
each $X_i$ in constant time. However, in the (realistic) case where the files
are correlated, much weaker results are known, requiring at least
$\Omega(n/poly\lg n)$ extra bits for constant decoding time, even for "simple"
joint distributions $\mu$.
  We focus on the natural case of compressing\emph{Markov chains}, i.e.,
storing a length-$n$ random walk on any (possibly directed) graph $G$. Denoting
by $\kappa(G,n)$ the number of length-$n$ walks on $G$, we show that there is a
succinct data structure storing a random walk using $\lg_2 \kappa(G,n) + O(\lg
n)$ bits of space, such that any vertex along the walk can be decoded in $O(1)$
time on a word-RAM. For the harder task of matching the \emph{point-wise}
optimal space of the walk, i.e., the empirical entropy $\sum_{i=1}^{n-1} \lg
(deg(v_i))$, we present a data structure with $O(1)$ extra bits at the price of
$O(\lg n)$ decoding time, and show that any improvement on this would lead to
an improved solution on the long-standing Dictionary problem. All of our data
structures support the \emph{online} version of the problem with constant
update and query time.
</summary>
    <author>
      <name>Emanuele Viola</name>
    </author>
    <author>
      <name>Omri Weinstein</name>
    </author>
    <author>
      <name>Huacheng Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1907.10874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.11232">
    <id>http://arxiv.org/abs/1907.11232v1</id>
    <updated>2019-07-24T20:50:16Z</updated>
    <published>2019-07-24T20:50:16Z</published>
    <title>Exhaustive Exact String Matching: The Analysis of the Full Human Genome</title>
    <summary>  Exact string matching has been a fundamental problem in computer science for
decades because of many practical applications. Some are related to common
procedures, such as searching in files and text editors, or, more recently, to
more advanced problems such as pattern detection in Artificial Intelligence and
Bioinformatics. Tens of algorithms and methodologies have been developed for
pattern matching and several programming languages, packages, applications and
online systems exist that can perform exact string matching in biological
sequences. These techniques, however, are limited to searching for specific and
predefined strings in a sequence. In this paper a novel methodology (called
Ex2SM) is presented, which is a pipeline of execution of advanced data
structures and algorithms, explicitly designed for text mining, that can detect
every possible repeated string in multivariate biological sequences. In
contrast to known algorithms in literature, the methodology presented here is
string agnostic, i.e., it does not require an input string to search for it,
rather it can detect every string that exists at least twice, regardless of its
attributes such as length, frequency, alphabet, overlapping etc. The complexity
of the problem solved and the potential of the proposed methodology is
demonstrated with the experimental analysis performed on the entire human
genome. More specifically, all repeated strings with a length of up to 50
characters have been detected, an achievement which is practically impossible
using other algorithms due to the exponential number of possible permutations
of such long strings.
</summary>
    <author>
      <name>Konstantinos F. Xylogiannopoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3341161.3343517</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3341161.3343517" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted for publication at IEEE/ACM ASONAM 2019 conference,
  Vancouver, BC, Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.11232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.12034">
    <id>http://arxiv.org/abs/1907.12034v2</id>
    <updated>2019-10-30T07:58:05Z</updated>
    <published>2019-07-28T07:32:58Z</published>
    <title>Minimal Absent Words in Rooted and Unrooted Trees</title>
    <summary>  We extend the theory of minimal absent words to (rooted and unrooted) trees,
having edges labeled by letters from an alphabet $\Sigma$ of cardinality
$\sigma$. We show that the set $\text{MAW}(T)$ of minimal absent words of a
rooted (resp. unrooted) tree $T$ with $n$ nodes has cardinality $O(n\sigma)$
(resp. $O(n^{2}\sigma)$), and we show that these bounds are realized. Then, we
exhibit algorithms to compute all minimal absent words in a rooted (resp.
unrooted) tree in output-sensitive time $O(n+|\text{MAW}(T)|)$ (resp.
$O(n^{2}+|\text{MAW}(T)|)$ assuming an integer alphabet of size polynomial in
$n$.
</summary>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a slightly modified version of the paper that appeared in the
  proceedings of SPIRE 2019, which contained an error in the example showed in
  Fig.1, now corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.12034v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12034v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.00848">
    <id>http://arxiv.org/abs/1908.00848v1</id>
    <updated>2019-08-02T13:33:47Z</updated>
    <published>2019-08-02T13:33:47Z</published>
    <title>Competitive Online Search Trees on Trees</title>
    <summary>  We consider the design of adaptive data structures for searching elements of
a tree-structured space. We use a natural generalization of the rotation-based
online binary search tree model in which the underlying search space is the set
of vertices of a tree. This model is based on a simple structure for
decomposing graphs, previously known under several names including elimination
trees, vertex rankings, and tubings. The model is equivalent to the classical
binary search tree model exactly when the underlying tree is a path. We
describe an online $O(\log \log n)$-competitive search tree data structure in
this model, matching the best known competitive ratio of binary search trees.
Our method is inspired by Tango trees, an online binary search tree algorithm,
but critically needs several new notions including one which we call
Steiner-closed search trees, which may be of independent interest. Moreover our
technique is based on a novel use of two levels of decomposition, first from
search space to a set of Steiner-closed trees, and secondly from these trees
into paths.
</summary>
    <author>
      <name>Prosenjit Bose</name>
    </author>
    <author>
      <name>Jean Cardinal</name>
    </author>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Grigorios Koumoutsos</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <link href="http://arxiv.org/abs/1908.00848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.00848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.00563">
    <id>http://arxiv.org/abs/1908.00563v1</id>
    <updated>2019-08-01T18:08:19Z</updated>
    <published>2019-08-01T18:08:19Z</published>
    <title>Dynamic Optimality Refuted -- For Tournament Heaps</title>
    <summary>  We prove a separation between offline and online algorithms for finger-based
tournament heaps undergoing key modifications. These heaps are implemented by
binary trees with keys stored on leaves, and intermediate nodes tracking the
min of their respective subtrees. They represent a natural starting point for
studying self-adjusting heaps due to the need to access the root-to-leaf path
upon modifications. We combine previous studies on the competitive ratios of
unordered binary search trees by [Fredman WADS2011] and on order-by-next
request by [Mart\'inez-Roura TCS2000] and [Munro ESA2000] to show that for any
number of fingers, tournament heaps cannot handle a sequence of modify-key
operations with competitive ratio in $o(\sqrt{\log{n}})$. Critical to this
analysis is the characterization of the modifications that a heap can undergo
upon an access. There are $\exp(\Theta(n \log{n}))$ valid heaps on $n$ keys,
but only $\exp(\Theta(n))$ binary search trees. We parameterize the
modification power through the well-studied concept of fingers: additional
pointers the data structure can manipulate arbitrarily. Here we demonstrate
that fingers can be significantly more powerful than servers moving on a static
tree by showing that access to $k$ fingers allow an offline algorithm to handle
any access sequence with amortized cost $O(\log_{k}(n) + 2^{\lg^{*}n})$.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Richard Peng</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <author>
      <name>Lingyi Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1908.00563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.00563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.01664">
    <id>http://arxiv.org/abs/1908.01664v1</id>
    <updated>2019-08-05T14:54:19Z</updated>
    <published>2019-08-05T14:54:19Z</published>
    <title>On the cyclic regularities of strings</title>
    <summary>  Regularities in strings are often related to periods and covers, which have
extensively been studied, and algorithms for their efficient computation have
broad application. In this paper we concentrate on computing cyclic
regularities of strings, in particular, we propose several efficient algorithms
for computing: (i) cyclic periodicity; (ii) all cyclic periodicity; (iii)
maximal local cyclic periodicity; (iv) cyclic covers.
</summary>
    <author>
      <name>Oluwole Ajala</name>
    </author>
    <author>
      <name>Miznah Alshammary</name>
    </author>
    <author>
      <name>Mai Alzamel</name>
    </author>
    <author>
      <name>Jia Gao</name>
    </author>
    <author>
      <name>Costas Iliopoulos</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Bruce Watson</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.01263">
    <id>http://arxiv.org/abs/1908.01263v1</id>
    <updated>2019-08-04T03:31:16Z</updated>
    <published>2019-08-04T03:31:16Z</published>
    <title>Matching reads to many genomes with the $r$-index</title>
    <summary>  The $r$-index is a tool for compressed indexing of genomic databases for
exact pattern matching, which can be used to completely align reads that
perfectly match some part of a genome in the database or to find seeds for
reads that do not. This paper shows how to download and install the programs
ri-buildfasta and ri-align; how to call ri-buildfasta on a FASTA file to build
an $r$-index for that file; and how to query that index with ri-align.
  Availability: The source code for these programs is released under GPLv3 and
available at https://github.com/alshai/r-index .
</summary>
    <author>
      <name>Taher Mun</name>
    </author>
    <author>
      <name>Alan Kuhnle</name>
    </author>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Ben Langmead</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.02211">
    <id>http://arxiv.org/abs/1908.02211v2</id>
    <updated>2019-11-29T13:06:54Z</updated>
    <published>2019-08-06T15:25:54Z</published>
    <title>An Index for Sequencing Reads Based on The Colored de Bruijn Graph</title>
    <summary>  In this article, we show how to transform a colored de Bruijn graph (dBG)
into a practical index for processing massive sets of sequencing reads. Similar
to previous works, we encode an instance of a colored dBG of the set using BOSS
and a color matrix C. To reduce the space requirements, we devise an algorithm
that produces a smaller and more sparse version of C. The novelties in this
algorithm are (i) an incomplete coloring of the graph and (ii) a greedy
coloring approach that tries to reuse the same colors for different strings
when possible. We also propose two algorithms that work on top of the index;
one is for reconstructing reads, and the other is for contig assembly.
Experimental results show that our data structure uses about half the space of
the plain representation of the set (1 Byte per DNA symbol) and that more than
99% of the reads can be reconstructed just from the index.
</summary>
    <author>
      <name>Diego Diaz-Dom√≠nguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.02211v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02211v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.01812">
    <id>http://arxiv.org/abs/1908.01812v2</id>
    <updated>2020-01-09T15:28:33Z</updated>
    <published>2019-08-05T19:25:57Z</published>
    <title>Optimal Joins using Compact Data Structures</title>
    <summary>  Worst-case optimal join algorithms have gained a lot of attention in the
database literature. We now count with several algorithms that are optimal in
the worst case, and many of them have been implemented and validated in
practice. However, the implementation of these algorithms often requires an
enhanced indexing structure: to achieve optimality we either need to build
completely new indexes, or we must populate the database with several
instantiations of indexes such as B$+$-trees. Either way, this means spending
an extra amount of storage space that may be non-negligible.
  We show that optimal algorithms can be obtained directly from a
representation that regards the relations as point sets in variable-dimensional
grids, without the need of extra storage. Our representation is a compact quad
tree for the static indexes, and a dynamic quadtree sharing subtrees (which we
dub a qdag) for intermediate results. We develop a compositional algorithm to
process full join queries under this representation, and show that the running
time of this algorithm is worst-case optimal in data complexity. Remarkably, we
can extend our framework to evaluate more expressive queries from relational
algebra by introducing a lazy version of qdags (lqdags). Once again, we can
show that the running time of our algorithms is worst-case optimal.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Juan L. Reutter</name>
    </author>
    <author>
      <name>Javiel Rojas-Ledesma</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01812v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01812v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.07330">
    <id>http://arxiv.org/abs/1812.07330v1</id>
    <updated>2018-12-18T12:39:42Z</updated>
    <published>2018-12-18T12:39:42Z</published>
    <title>Computing the $k$-binomial complexity of the Thue--Morse word</title>
    <summary>  Two words are $k$-binomially equivalent whenever they share the same
subwords, i.e., subsequences, of length at most $k$ with the same
multiplicities. This is a refinement of both abelian equivalence and the Simon
congruence. The $k$-binomial complexity of an infinite word $\mathbf{x}$ maps
the integer $n$ to the number of classes in the quotient, by this $k$-binomial
equivalence relation, of the set of factors of length $n$ occurring in
$\mathbf{x}$. This complexity measure has not been investigated very much. In
this paper, we characterize the $k$-binomial complexity of the Thue--Morse
word. The result is striking, compared to more familiar complexity functions.
Although the Thue--Morse word is aperiodic, its $k$-binomial complexity
eventually takes only two values. In this paper, we first obtain general
results about the number of occurrences of subwords appearing in iterates of
the form $\Psi^\ell(w)$ for an arbitrary morphism $\Psi$. We also thoroughly
describe the factors of the Thue--Morse word by introducing a relevant new
equivalence relation.
</summary>
    <author>
      <name>Marie Lejeune</name>
    </author>
    <author>
      <name>Julien Leroy</name>
    </author>
    <author>
      <name>Michel Rigo</name>
    </author>
    <link href="http://arxiv.org/abs/1812.07330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.07330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.07223">
    <id>http://arxiv.org/abs/1905.07223v1</id>
    <updated>2019-05-17T12:24:01Z</updated>
    <published>2019-05-17T12:24:01Z</published>
    <title>Separating many words by counting occurrences of factors</title>
    <summary>  For a given language $L$, we study the languages $X$ such that for all
distinct words $u, v \in L$, there exists a word $x \in X$ that appears a
different number of times as a factor in $u$ and in $v$. In particular, we are
interested in the following question: For which languages $L$ does there exist
a finite language $X$ satisfying the above condition? We answer this question
for all regular languages and for all sets of factors of infinite words.
</summary>
    <author>
      <name>Aleksi Saarela</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages. Full version of an article to appear in the proceedings of
  the 23rd International Conference on Developments in Language Theory (DLT
  2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.07223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.08579">
    <id>http://arxiv.org/abs/1907.08579v1</id>
    <updated>2019-07-19T17:17:44Z</updated>
    <published>2019-07-19T17:17:44Z</published>
    <title>On Approximate Range Mode and Range Selection</title>
    <summary>  For any $\epsilon \in (0,1)$, a $(1+\epsilon)$-approximate range mode query
asks for the position of an element whose frequency in the query range is at
most a factor $(1+\epsilon)$ smaller than the true mode. For this problem, we
design an $O(n/\epsilon)$ bit data structure supporting queries in
$O(\lg(1/\epsilon))$ time. This is an encoding data structure which does not
require access to the input sequence; we prove the space cost is asymptotically
optimal for constant $\epsilon$. Our solution improves the previous best result
of Greve et al. (Cell Probe Lower Bounds and Approximations for Range Mode,
ICALP'10) by reducing the space cost by a factor of $\lg n$ while achieving the
same query time. We also design an $O(n)$-word dynamic data structure that
answers queries in $O(\lg n /\lg\lg n)$ time and supports insertions and
deletions in $O(\lg n)$ time, for any constant $\epsilon \in (0,1)$. This is
the first result on dynamic approximate range mode; it can also be used to
obtain the first static data structure for approximate 3-sided range mode
queries in two dimensions.
  We also consider approximate range selection. For any $\alpha \in (0,1/2)$,
an $\alpha$-approximate range selection query asks for the position of an
element whose rank in the query range is in $[k - \alpha s, k + \alpha s]$,
where $k$ is a rank given by the query and $s$ is the size of the query range.
When $\alpha$ is a constant, we design an $O(n)$-bit encoding data structure
that can answer queries in constant time and prove this space cost is
asymptotically optimal. The previous best result by Krizanc et al. (Range Mode
and Range Median Queries on Lists and Trees, Nordic Journal of Computing, 2005)
uses $O(n\lg n)$ bits, or $O(n)$ words, to achieve constant approximation for
range median only. Thus we not only improve the space cost, but also provide
support for any arbitrary $k$ given at query time.
</summary>
    <author>
      <name>Hicham El-Zein</name>
    </author>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Bryce Sandlund</name>
    </author>
    <link href="http://arxiv.org/abs/1907.08579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.08579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.08355">
    <id>http://arxiv.org/abs/1907.08355v2</id>
    <updated>2019-11-09T23:21:07Z</updated>
    <published>2019-07-19T03:02:25Z</published>
    <title>Data Structures Meet Cryptography: 3SUM with Preprocessing</title>
    <summary>  This paper shows several connections between data structure problems and
cryptography against preprocessing attacks. Our results span data structure
upper bounds, cryptographic applications, and data structure lower bounds, as
summarized next.
  First, we apply Fiat--Naor inversion, a technique with cryptographic origins,
to obtain a data structure upper bound. In particular, our technique yields a
suite of algorithms with space $S$ and (online) time $T$ for a preprocessing
version of the $N$-input 3SUM problem where $S^3\cdot T = \widetilde{O}(N^6)$.
This disproves a strong conjecture (Goldstein et al., WADS 2017) that there is
no data structure that solves this problem for $S=N^{2-\delta}$ and $T =
N^{1-\delta}$ for any constant $\delta>0$.
  Secondly, we show equivalence between lower bounds for a broad class of
(static) data structure problems and one-way functions in the random oracle
model that resist a very strong form of preprocessing attack. Concretely, given
a random function $F: [N] \to [N]$ (accessed as an oracle) we show how to
compile it into a function $G^F: [N^2] \to [N^2]$ which resists $S$-bit
preprocessing attacks that run in query time $T$ where
$ST=O(N^{2-\varepsilon})$ (assuming a corresponding data structure lower bound
on 3SUM). In contrast, a classical result of Hellman tells us that $F$ itself
can be more easily inverted, say with $N^{2/3}$-bit preprocessing in $N^{2/3}$
time. We also show that much stronger lower bounds follow from the hardness of
kSUM. Our results can be equivalently interpreted as security against
adversaries that are very non-uniform, or have large auxiliary input, or as
security in the face of a powerfully backdoored random oracle.
  Thirdly, we give lower bounds for 3SUM and a range of geometric problems
which match the best known lower bounds for static data structure problems
(Larsen, FOCS 2012).
</summary>
    <author>
      <name>Alexander Golovnev</name>
    </author>
    <author>
      <name>Siyao Guo</name>
    </author>
    <author>
      <name>Thibaut Horel</name>
    </author>
    <author>
      <name>Sunoo Park</name>
    </author>
    <author>
      <name>Vinod Vaikuntanathan</name>
    </author>
    <link href="http://arxiv.org/abs/1907.08355v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.08355v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.08246">
    <id>http://arxiv.org/abs/1907.08246v1</id>
    <updated>2019-07-18T18:55:41Z</updated>
    <published>2019-07-18T18:55:41Z</published>
    <title>Finding First and Most-Beautiful Queens by Integer Programming</title>
    <summary>  The n-queens puzzle is a well-known combinatorial problem that requires to
place n queens on an n x n chessboard so that no two queens can attack each
other. Since the 19th century, this problem was studied by many mathematicians
and computer scientists. While finding any solution to the n-queens puzzle is
rather straightforward, it is very challenging to find the lexicographically
first (or smallest) feasible solution. Solutions for this type are known in the
literature for n &lt;= 55, while for some larger chessboards only partial
solutions are known. The present paper was motivated by the question of whether
Integer Linear Programming (ILP) can be used to compute solutions for some open
instances. We describe alternative ILP-based solution approaches, and show that
they are indeed able to compute (sometimes in unexpectedly-short computing
times) many new lexicographically optimal solutions for n ranging from 56 to
115. One of the proposed algorithms is a pure cutting plane method based on a
combinatorial variant of classical Gomory cuts. We also address an intriguing
"lexicographic bottleneck" (or min-max) variant of the problem that requires
finding a most beautiful (in a well defined sense) placement, and report its
solution for n up to 176.
</summary>
    <author>
      <name>Matteo Fischetti</name>
    </author>
    <author>
      <name>Domenico Salvagnin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.08246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.08246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R05" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.09271">
    <id>http://arxiv.org/abs/1907.09271v1</id>
    <updated>2019-07-22T12:29:03Z</updated>
    <published>2019-07-22T12:29:03Z</published>
    <title>Succinct Representation for (Non)Deterministic Finite Automata</title>
    <summary>  Deterministic finite automata are one of the simplest and most practical
models of computation studied in automata theory. Their conceptual extension is
the non-deterministic finite automata which also have plenty of applications.
In this article, we study these models through the lens of succinct data
structures where our ultimate goal is to encode these mathematical objects
using information-theoretically optimal number of bits along with supporting
queries on them efficiently. Towards this goal, we first design a succinct data
structure for representing any deterministic finite automaton $\mathcal{D}$
having $n$ states over a $\sigma$-letter alphabet $\Sigma$ using $(\sigma-1)
n\log n + O(n \log \sigma)$ bits of space, which can determine, given an input
string $x$ over $\Sigma$, whether $\mathcal{D}$ accepts $x$ in $O(|x| \log
\sigma)$ time, using constant words of working space. When the input
deterministic finite automaton is acyclic, not only we can improve the above
space-bound significantly to $(\sigma -1) (n-1)\log n+ 3n + O(\log^2 \sigma) +
o(n)$ bits, we also obtain optimal query time for string acceptance checking.
More specifically, using our succinct representation, we can check if a given
input string $x$ can be accepted by the acyclic deterministic finite automaton
using time proportional to the length of $x$, hence, the optimal query time. We
also exhibit a succinct data structure for representing a non-deterministic
finite automaton $\mathcal{N}$ having $n$ states over a $\sigma$-letter
alphabet $\Sigma$ using $\sigma n^2+n$ bits of space, such that given an input
string $x$, we can decide whether $\mathcal{N}$ accepts $x$ efficiently in
$O(n^2|x|)$ time. Finally, we also provide time and space-efficient algorithms
for performing several standard operations such as union, intersection, and
complement on the languages accepted by deterministic finite automata.
</summary>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <link href="http://arxiv.org/abs/1907.09271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.09271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.09280">
    <id>http://arxiv.org/abs/1907.09280v1</id>
    <updated>2019-07-22T12:41:59Z</updated>
    <published>2019-07-22T12:41:59Z</published>
    <title>Optimal In-place Algorithms for Basic Graph Problems</title>
    <summary>  We present linear time {\it in-place} algorithms for several basic and
fundamental graph problems including the well-known graph search methods (like
depth-first search, breadth-first search, maximum cardinality search),
connectivity problems (like biconnectivity, $2$-edge connectivity),
decomposition problem (like chain decomposition) among various others,
improving the running time (by polynomial multiplicative factor) of the recent
results of Chakraborty et al. [ESA, 2018] who designed $O(n^3 \lg n)$ time
in-place algorithms for a strict subset of the above mentioned problems. The
running times of all our algorithms are essentially optimal as they run in
linear time. One of the main ideas behind obtaining these algorithms is the
detection and careful exploitation of sortedness present in the input
representation for any graph without loss of generality. This observation alone
is powerful enough to design some basic linear time in-place algorithms, but
more non-trivial graph problems require extra techniques which, we believe, may
find other applications while designing in-place algorithms for different graph
problems in the future.
</summary>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <link href="http://arxiv.org/abs/1907.09280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.09280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0706.4107">
    <id>http://arxiv.org/abs/0706.4107v1</id>
    <updated>2007-06-27T22:04:40Z</updated>
    <published>2007-06-27T22:04:40Z</published>
    <title>Radix Sorting With No Extra Space</title>
    <summary>  It is well known that n integers in the range [1,n^c] can be sorted in O(n)
time in the RAM model using radix sorting. More generally, integers in any
range [1,U] can be sorted in O(n sqrt{loglog n}) time. However, these
algorithms use O(n) words of extra memory. Is this necessary?
  We present a simple, stable, integer sorting algorithm for words of size
O(log n), which works in O(n) time and uses only O(1) words of extra memory on
a RAM model. This is the integer sorting case most useful in practice. We
extend this result with same bounds to the case when the keys are read-only,
which is of theoretical interest. Another interesting question is the case of
arbitrary c. Here we present a black-box transformation from any RAM sorting
algorithm to a sorting algorithm which uses only O(1) extra space and has the
same running time. This settles the complexity of in-place sorting in terms of
the complexity of sorting.
</summary>
    <author>
      <name>Gianni Franceschini</name>
    </author>
    <author>
      <name>S. Muthukrishnan</name>
    </author>
    <author>
      <name>Mihai Patrascu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of paper accepted to ESA 2007. (17 pages)</arxiv:comment>
    <link href="http://arxiv.org/abs/0706.4107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.4107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.10444">
    <id>http://arxiv.org/abs/1907.10444v1</id>
    <updated>2019-07-24T13:39:59Z</updated>
    <published>2019-07-24T13:39:59Z</published>
    <title>Constant Delay Traversal of Grammar-Compressed Graphs with Bounded Rank</title>
    <summary>  We present a pointer-based data structure for constant time traversal of the
edges of an edge-labeled (alphabet $\Sigma$) directed hypergraph (a graph where
edges can be incident to more than two vertices, and the incident vertices are
ordered) given as hyperedge-replacement grammar $G$. It is assumed that the
grammar has a fixed rank $\kappa$ (maximal number of vertices connected to a
nonterminal hyperedge) and that each vertex of the represented graph is
incident to at most one $\sigma$-edge per direction ($\sigma \in \Sigma$).
Precomputing the data structure needs $O(|G||\Sigma|\kappa r h)$ space and
$O(|G||\Sigma|\kappa rh^2)$ time, where $h$ is the height of the derivation
tree of $G$ and $r$ is the maximal rank of a terminal edge occurring in the
grammar.
</summary>
    <author>
      <name>Sebastian Maneth</name>
    </author>
    <author>
      <name>Fabian Peternek</name>
    </author>
    <link href="http://arxiv.org/abs/1907.10444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.11206">
    <id>http://arxiv.org/abs/1907.11206v1</id>
    <updated>2019-07-25T17:11:11Z</updated>
    <published>2019-07-25T17:11:11Z</published>
    <title>The Strong 3SUM-INDEXING Conjecture is False</title>
    <summary>  In the 3SUM-Indexing problem the goal is to preprocess two lists of elements
from $U$, $A=(a_1,a_2,\ldots,a_n)$ and $B=(b_1,b_2,...,b_n)$, such that given
an element $c\in U$ one can quickly determine whether there exists a pair
$(a,b)\in A \times B$ where $a+b=c$. Goldstein et al.~[WADS'2017] conjectured
that there is no algorithm for 3SUM-Indexing which uses $n^{2-\Omega(1)}$ space
and $n^{1-\Omega(1)}$ query time.
  We show that the conjecture is false by reducing the 3SUM-Indexing problem to
the problem of inverting functions, and then applying an algorithm of Fiat and
Naor [SICOMP'1999] for inverting functions.
</summary>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <link href="http://arxiv.org/abs/1907.11206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.05369">
    <id>http://arxiv.org/abs/1907.05369v1</id>
    <updated>2019-07-04T21:14:28Z</updated>
    <published>2019-07-04T21:14:28Z</published>
    <title>Abelian-square factors and binary words</title>
    <summary>  In this work, we affirm the conjecture proposed by Gabriele Fici and Filippo
Mignosi at the 10th Conference on Combinatorics on Words.
</summary>
    <author>
      <name>Salah Triki</name>
    </author>
    <link href="http://arxiv.org/abs/1907.05369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.06310">
    <id>http://arxiv.org/abs/1907.06310v1</id>
    <updated>2019-07-15T02:02:40Z</updated>
    <published>2019-07-15T02:02:40Z</published>
    <title>New Paths from Splay to Dynamic Optimality</title>
    <summary>  Consider the task of performing a sequence of searches in a binary search
tree. After each search, an algorithm is allowed to arbitrarily restructure the
tree, at a cost proportional to the amount of restructuring performed. The cost
of an execution is the sum of the time spent searching and the time spent
optimizing those searches with restructuring operations. This notion was
introduced by Sleator and Tarjan in (JACM, 1985), along with an algorithm and a
conjecture. The algorithm, Splay, is an elegant procedure for performing
adjustments while moving searched items to the top of the tree. The conjecture,
called "dynamic optimality," is that the cost of splaying is always within a
constant factor of the optimal algorithm for performing searches. The
conjecture stands to this day. In this work, we attempt to lay the foundations
for a proof of the dynamic optimality conjecture.
</summary>
    <author>
      <name>Caleb C. Levy</name>
    </author>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An earlier version of this work appeared in the Proceedings of the
  Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms. arXiv admin note:
  text overlap with arXiv:1907.06309</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.06310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.06310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.06309">
    <id>http://arxiv.org/abs/1907.06309v1</id>
    <updated>2019-07-15T01:45:56Z</updated>
    <published>2019-07-15T01:45:56Z</published>
    <title>Splaying Preorders and Postorders</title>
    <summary>  Let $T$ be a binary search tree. We prove two results about the behavior of
the Splay algorithm (Sleator and Tarjan 1985). Our first result is that
inserting keys into an empty binary search tree via splaying in the order of
either $T$'s preorder or $T$'s postorder takes linear time. Our proof uses the
fact that preorders and postorders are pattern-avoiding: i.e. they contain no
subsequences that are order-isomorphic to $(2,3,1)$ and $(3,1,2)$,
respectively. Pattern-avoidance implies certain constraints on the manner in
which items are inserted. We exploit this structure with a simple potential
function that counts inserted nodes lying on access paths to uninserted nodes.
Our methods can likely be extended to permutations that avoid more general
patterns. Second, if $T'$ is any other binary search tree with the same keys as
$T$ and $T$ is weight-balanced (Nievergelt and Reingold 1973), then splaying
$T$'s preorder sequence or $T$'s postorder sequence starting from $T'$ takes
linear time. To prove this, we demonstrate that preorders and postorders of
balanced search trees do not contain many large "jumps" in symmetric order, and
exploit this fact by using the dynamic finger theorem (Cole et al. 2000). Both
of our results provide further evidence in favor of the elusive "dynamic
optimality conjecture."
</summary>
    <author>
      <name>Caleb C. Levy</name>
    </author>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <link href="http://arxiv.org/abs/1907.06309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.06309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.08142">
    <id>http://arxiv.org/abs/1907.08142v1</id>
    <updated>2019-07-18T16:24:30Z</updated>
    <published>2019-07-18T16:24:30Z</published>
    <title>Stack sorting with restricted stacks</title>
    <summary>  The (classical) problem of characterizing and enumerating permutations that
can be sorted using two stacks connected in series is still largely open. In
the present paper we address a related problem, in which we impose restrictions
both on the procedure and on the stacks. More precisely, we consider a greedy
algorithm where we perform the rightmost legal operation (here "rightmost"
refers to the usual representation of stack sorting problems). Moreover, the
first stack is required to be $\sigma$-avoiding, for some permutation $\sigma$,
meaning that, at each step, the elements maintained in the stack avoid the
pattern $\sigma$ when read from top to bottom. Since the set of permutations
which can be sorted by such a device (which we call $\sigma$-machine) is not
always a class, it would be interesting to understand when it happens. We will
prove that the set of $\sigma$-machines whose associated sortable permutations
are not a class is counted by Catalan numbers. Moreover, we will analyze two
specific $\sigma$-machines in full details (namely when $\sigma=321$ and
$\sigma=123$), providing for each of them a complete characterization and
enumeration of sortable permutations.
</summary>
    <author>
      <name>Giulio Cerbai</name>
    </author>
    <author>
      <name>Anders Claesson</name>
    </author>
    <author>
      <name>Luca Ferrari</name>
    </author>
    <link href="http://arxiv.org/abs/1907.08142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.08142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.07795">
    <id>http://arxiv.org/abs/1907.07795v1</id>
    <updated>2019-07-17T22:11:22Z</updated>
    <published>2019-07-17T22:11:22Z</published>
    <title>Efficient computation of the Jacobi symbol</title>
    <summary>  The family of left-to-right GCD algorithms reduces input numbers by
repeatedly subtracting the smaller number, or multiple of the smaller number,
from the larger number. This paper describes how to extend any such algorithm
to compute the Jacobi symbol, using a single table lookup per reduction. For
both quadratic time GCD algorithms (Euclid, Lehmer) and subquadratic algorithms
(Knuth, Sch\"onhage, M\"oller), the additional cost is linear, roughly one
table lookup per quotient in the quotient sequence. This method was used for
the 2010 rewrite of the Jacobi symbol computation in GMP.
</summary>
    <author>
      <name>Niels M√∂ller</name>
    </author>
    <link href="http://arxiv.org/abs/1907.07795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.07795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11Y16" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.00192">
    <id>http://arxiv.org/abs/1907.00192v1</id>
    <updated>2019-06-29T12:09:18Z</updated>
    <published>2019-06-29T12:09:18Z</published>
    <title>Recurrence along directions in multidimensional words</title>
    <summary>  In this paper we introduce and study new notions of uniform recurrence in
multidimensional words. A $d$-dimensional word is called \emph{uniformly
recurrent} if for all $(s_1,\ldots,s_d)\in\N^d$ there exists $n\in\N$ such that
each block of size $(n,\ldots,n)$ contains the prefix of size
$(s_1,\ldots,s_d)$. We are interested in a modification of this property.
Namely, we ask that for each rational direction $(q_1,\ldots,q_d)$, each
rectangular prefix occurs along this direction in positions
$\ell(q_1,\ldots,q_d)$ with bounded gaps. Such words are called \emph{uniformly
recurrent along all directions}. We provide several constructions of
multidimensional words satisfying this condition, and more generally, a series
of four increasingly stronger conditions. In particular, we study the uniform
recurrence along directions of multidimentional rotation words and of fixed
points of square morphisms.
</summary>
    <author>
      <name>√âmilie Charlier</name>
    </author>
    <author>
      <name>Svetlana Puzynina</name>
    </author>
    <author>
      <name>√âlise Vandomme</name>
    </author>
    <link href="http://arxiv.org/abs/1907.00192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.08731">
    <id>http://arxiv.org/abs/1804.08731v2</id>
    <updated>2018-07-16T16:09:31Z</updated>
    <published>2018-04-23T20:40:40Z</published>
    <title>Longest Common Substring Made Fully Dynamic</title>
    <summary>  In the longest common substring (LCS) problem, we are given two strings $S$
and $T$, each of length at most $n$, and we are asked to find a longest string
occurring as a fragment of both $S$ and $T$. This is a classical and
well-studied problem in computer science with a known $\mathcal{O}(n)$-time
solution. In the fully dynamic version of the problem, edit operations are
allowed in either of the two strings, and we are asked to report an LCS after
each such operation. We present the first solution to this problem that
requires sublinear time per edit operation. In particular, we show how to
return an LCS in $\tilde{\mathcal{O}}(n^{2/3})$ time (or
$\tilde{\mathcal{O}}(\sqrt{n})$ time if edits are allowed in only one of the
two strings) after each operation using $\tilde{\mathcal{O}}(n)$ space.
  This line of research was recently initiated by the authors [SPIRE 2017] in a
somewhat restricted dynamic variant. An $\tilde{\mathcal{O}}(n)$-sized data
structure that returns an LCS of the two strings after a single edit operation
(that is reverted afterwards) in $\tilde{\mathcal{O}}(1)$ time was presented.
At CPM 2018, three papers studied analogously restricted dynamic variants of
problems on strings. We show that our techniques can be used to obtain fully
dynamic algorithms for several classical problems on strings, namely, computing
the longest repeat, the longest palindrome and the longest Lyndon substring of
a string. The only previously known sublinear-time dynamic algorithms for
problems on strings were obtained for maintaining a dynamic collection of
strings for comparison queries and for pattern matching with the most recent
advances made by Gawrychowski et al. [SODA 2018] and by Clifford et al. [STACS
2018].
</summary>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08731v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08731v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.00425">
    <id>http://arxiv.org/abs/1808.00425v1</id>
    <updated>2018-08-01T17:10:42Z</updated>
    <published>2018-08-01T17:10:42Z</published>
    <title>List Decoding with Double Samplers</title>
    <summary>  We develop the notion of "double samplers", first introduced by Dinur and
Kaufman [Proc. 58th FOCS, 2017], which are samplers with additional
combinatorial properties, and whose existence we prove using high dimensional
expanders.
  We show how double samplers give a generic way of amplifying distance in a
way that enables efficient list-decoding. There are many error correcting code
constructions that achieve large distance by starting with a base code $C$ with
moderate distance, and then amplifying the distance using a sampler, e.g., the
ABNNR code construction [IEEE Trans. Inform. Theory, 38(2):509--516, 1992.]. We
show that if the sampler is part of a larger double sampler then the
construction has an efficient list-decoding algorithm and the list decoding
algorithm is oblivious to the base code $C$ (i.e., it runs the unique decoder
for $C$ in a black box way).
  Our list-decoding algorithm works as follows: it uses a local voting scheme
from which it constructs a unique games constraint graph. The constraint graph
is an expander, so we can solve unique games efficiently. These solutions are
the output of the list decoder. This is a novel use of a unique games algorithm
as a subroutine in a decoding procedure, as opposed to the more common
situation in which unique games are used for demonstrating hardness results.
  Double samplers and high dimensional expanders are akin to pseudorandom
objects in their utility, but they greatly exceed random objects in their
combinatorial properties. We believe that these objects hold significant
potential for coding theoretic constructions and view this work as
demonstrating the power of double samplers in this context.
</summary>
    <author>
      <name>Irit Dinur</name>
    </author>
    <author>
      <name>Prahladh Harsha</name>
    </author>
    <author>
      <name>Tali Kaufman</name>
    </author>
    <author>
      <name>Inbal Livni Navon</name>
    </author>
    <author>
      <name>Amnon Ta Shma</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.03530">
    <id>http://arxiv.org/abs/1803.03530v1</id>
    <updated>2018-03-08T02:45:15Z</updated>
    <published>2018-03-08T02:45:15Z</published>
    <title>Synchronization Strings: Efficient and Fast Deterministic Constructions
  over Small Alphabets</title>
    <summary>  Synchronization strings are recently introduced by Haeupler and Shahrasbi
(STOC 2017) in the study of codes for correcting insertion and deletion errors
(insdel codes). They showed that for any parameter $\varepsilon>0$,
synchronization strings of arbitrary length exist over an alphabet whose size
depends only on $\varepsilon$. Specifically, they obtained an alphabet size of
$O(\varepsilon^{-4})$, which left an open question on where the minimal size of
such alphabets lies between $\Omega(\varepsilon^{-1})$ and
$O(\varepsilon^{-4})$. In this work, we partially bridge this gap by providing
an improved lower bound of $\Omega(\varepsilon^{-3/2})$, and an improved upper
bound of $O(\varepsilon^{-2})$. We also provide fast explicit constructions of
synchronization strings over small alphabets.
  Further, along the lines of previous work on similar combinatorial objects,
we study the extremal question of the smallest possible alphabet size over
which synchronization strings can exist for some constant $\varepsilon &lt; 1$. We
show that one can construct $\varepsilon$-synchronization strings over
alphabets of size four while no such string exists over binary alphabets. This
reduces the extremal question to whether synchronization strings exist over
ternary alphabets.
</summary>
    <author>
      <name>Kuan Cheng</name>
    </author>
    <author>
      <name>Bernhard Haeupler</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Amirbehshad Shahrasbi</name>
    </author>
    <author>
      <name>Ke Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages. arXiv admin note: substantial text overlap with
  arXiv:1710.07356</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.03530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.08343">
    <id>http://arxiv.org/abs/1904.08343v1</id>
    <updated>2019-04-17T16:12:54Z</updated>
    <published>2019-04-17T16:12:54Z</published>
    <title>The power word problem</title>
    <summary>  In this work we introduce a new succinct variant of the word problem in a
finitely generated group $G$, which we call the power word problem: the input
word may contain powers $p^x$, where $p$ is a finite word over generators of
$G$ and $x$ is a binary encoded integer. The power word problem is a
restriction of the compressed word problem, where the input word is represented
by a straight-line program (i.e., an algebraic circuit over $G$). The main
result of the paper states that the power word problem for a finitely generated
free group $F$ is AC$^0$-Turing-reducible to the word problem for $F$.
Moreover, the following hardness result is shown: For a wreath product $G \wr
\mathbb{Z}$, where $G$ is either free of rank at least two or finite
non-solvable, the power word problem is complete for coNP. This contrasts with
the situation where $G$ is abelian: then the power word problem is shown to be
in TC$^0$.
</summary>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Armin Wei√ü</name>
    </author>
    <link href="http://arxiv.org/abs/1904.08343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.11423">
    <id>http://arxiv.org/abs/1906.11423v1</id>
    <updated>2019-06-27T03:33:23Z</updated>
    <published>2019-06-27T03:33:23Z</published>
    <title>Vector Programming Using Generative Recursion</title>
    <summary>  Vector programming is an important topic in many Introduction to Computer
Science courses. Despite the importance of vectors, learning vector programming
is a source of frustration for many students. Much of the frustration is rooted
in discovering the source of bugs that are manifested as out-of-bounds
indexing. The problem is that such bugs are, sometimes, rooted in incorrectly
computing an index. Other times, however, these errors are rooted in mistaken
reasoning about how to correctly process a vector. Unfortunately, either way,
all too often beginners are left adrift to resolve indexing errors on their
own. This article extends the work done on vector programming using vector
intervals and structural recursion to using generative recursion. As for
problems solved using structural recursion, vector intervals provide beginners
with a useful framework for designing code that properly indexes vectors. This
article presents the methodology and concrete examples that others may use to
build their own CS1 modules involving vector programming using any programming
language.
</summary>
    <author>
      <name>Marco T. Moraz√°n</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Seton Hall University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.295.3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.295.3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TFPIE 2018, arXiv:1906.10757. arXiv admin note: text
  overlap with arXiv:1805.05124</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 295, 2019, pp. 35-51</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.11423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.11423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.01600">
    <id>http://arxiv.org/abs/1907.01600v1</id>
    <updated>2019-07-02T19:45:34Z</updated>
    <published>2019-07-02T19:45:34Z</published>
    <title>Approximate Similarity Search Under Edit Distance Using
  Locality-Sensitive Hashing</title>
    <summary>  Edit distance similarity search, also called approximate pattern matching, is
a fundamental problem with widespread applications. The goal of the problem is
to preprocess n strings of length d to quickly answer queries q of the form: if
there is a database string within edit distance r of q, return a database
string within edit distance cr of q. A data structure solving this problem is
analyzed using two criteria: the amount of extra space used in preprocessing,
and the expected time to answer a query.
  Previous approaches to this problem have either used trie-based methods,
which give exact solutions at the cost of expensive queries, or embeddings,
which only work for large (superconstant) values of c.
  In this work we achieve the first bounds for any approximation factor c, via
a simple and easy-to-implement hash function. This gives a running time of
$\tilde{O}(d3^rn^{1/c})$, with space $\tilde{O}(3^r n^{1 + 1/c} + dn)$. We show
how to apply these ideas to the closely-related Approximate Nearest Neighbor
problem for edit distance, obtaining similar time bounds.
</summary>
    <author>
      <name>Samuel McCauley</name>
    </author>
    <link href="http://arxiv.org/abs/1907.01600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.01631">
    <id>http://arxiv.org/abs/1907.01631v1</id>
    <updated>2019-07-02T20:55:47Z</updated>
    <published>2019-07-02T20:55:47Z</published>
    <title>Cache-Friendly Search Trees; or, In Which Everything Beats std::set</title>
    <summary>  While a lot of work in theoretical computer science has gone into optimizing
the runtime and space usage of data structures, such work very often neglects a
very important component of modern computers: the cache. In doing so, very
often, data structures are developed that achieve theoretically-good runtimes
but are slow in practice due to a large number of cache misses. In 1999, Frigo
et al. introduced the notion of a cache-oblivious algorithm: an algorithm that
uses the cache to its advantage, regardless of the size or structure of said
cache. Since then, various authors have designed cache-oblivious algorithms and
data structures for problems from matrix multiplication to array sorting. We
focus in this work on cache-oblivious search trees; i.e. implementing an
ordered dictionary in a cache-friendly manner. We will start by presenting an
overview of cache-oblivious data structures, especially cache-oblivious search
trees. We then give practical results using these cache-oblivious structures on
modern-day machinery, comparing them to the standard std::set and other
cache-friendly dictionaries such as B-trees.
</summary>
    <author>
      <name>Jeffrey Barratt</name>
    </author>
    <author>
      <name>Brian Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1907.01631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.01815">
    <id>http://arxiv.org/abs/1907.01815v2</id>
    <updated>2020-01-13T12:29:55Z</updated>
    <published>2019-07-03T09:35:53Z</published>
    <title>Circular Pattern Matching with $k$ Mismatches</title>
    <summary>  The $k$-mismatch problem consists in computing the Hamming distance between a
pattern $P$ of length $m$ and every length-$m$ substring of a text $T$ of
length $n$, if this distance is no more than $k$. In many real-world
applications, any cyclic rotation of $P$ is a relevant pattern, and thus one is
interested in computing the minimal distance of every length-$m$ substring of
$T$ and any cyclic rotation of $P$. This is the circular pattern matching with
$k$ mismatches ($k$-CPM) problem. A multitude of papers have been devoted to
solving this problem but, to the best of our knowledge, only average-case upper
bounds are known. In this paper, we present the first non-trivial worst-case
upper bounds for the $k$-CPM problem. Specifically, we show an $O(nk)$-time
algorithm and an $O(n+\frac{n}{m}\,k^4)$-time algorithm. The latter algorithm
applies in an extended way a technique that was very recently developed for the
$k$-mismatch problem [Bringmann et al., SODA 2019].
  A preliminary version of this work appeared at FCT 2019. In this version we
improve the time complexity of the main algorithm from $O(n+\frac{n}{m}\,k^5)$
to $O(n+\frac{n}{m}\,k^4)$.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszy≈Ñski</name>
    </author>
    <author>
      <name>Tomasz Wale≈Ñ</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper from FCT 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.01815v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01815v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.02308">
    <id>http://arxiv.org/abs/1907.02308v1</id>
    <updated>2019-07-04T09:56:34Z</updated>
    <published>2019-07-04T09:56:34Z</published>
    <title>The Alternating BWT: an algorithmic perspective</title>
    <summary>  The Burrows-Wheeler Transform (BWT) is a word transformation introduced in
1994 for Data Compression. It has become a fundamental tool for designing
self-indexing data structures, with important applications in several area in
science and engineering. The Alternating Burrows-Wheeler Transform (ABWT) is
another transformation recently introduced in [Gessel et al. 2012] and studied
in the field of Combinatorics on Words. It is analogous to the BWT, except that
it uses an alternating lexicographical order instead of the usual one. Building
on results in [Giancarlo et al. 2018], where we have shown that BWT and ABWT
are part of a larger class of reversible transformations, here we provide a
combinatorial and algorithmic study of the novel transform ABWT. We establish a
deep analogy between BWT and ABWT by proving they are the only ones in the
above mentioned class to be rank-invertible, a novel notion guaranteeing
efficient invertibility. In addition, we show that the backward-search
procedure can be efficiently generalized to the ABWT; this result implies that
also the ABWT can be used as a basis for efficient compressed full text
indices. Finally, we prove that the ABWT can be efficiently computed by using a
combination of the Difference Cover suffix sorting algorithm
[K\"{a}rkk\"{a}inen et al., 2006] with a linear time algorithm for finding the
minimal cyclic rotation of a word with respect to the alternating
lexicographical order.
</summary>
    <author>
      <name>Raffaele Giancarlo</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Antonio Restivo</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <link href="http://arxiv.org/abs/1907.02308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.06273">
    <id>http://arxiv.org/abs/1811.06273v1</id>
    <updated>2018-11-15T10:12:09Z</updated>
    <published>2018-11-15T10:12:09Z</published>
    <title>On Infinite Prefix Normal Words</title>
    <summary>  Prefix normal words are binary words that have no factor with more $1$s than
the prefix of the same length. Finite prefix normal words were introduced in
[Fici and Lipt\'ak, DLT 2011]. In this paper, we study infinite prefix normal
words and explore their relationship to some known classes of infinite binary
words. In particular, we establish a connection between prefix normal words and
Sturmian words, between prefix normal words and abelian complexity, and between
prefix normality and lexicographic order.
</summary>
    <author>
      <name>Ferdinando Cicalese</name>
    </author>
    <author>
      <name>Zsuzsanna Lipt√°k</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 4 figures, accepted at SOFSEM 2019 (45th International
  Conference on Current Trends in Theory and Practice of Computer Science,
  Nov\'y Smokovec, Slovakia, January 27-30, 2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.06273v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06273v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.03235">
    <id>http://arxiv.org/abs/1907.03235v3</id>
    <updated>2019-12-03T12:05:34Z</updated>
    <published>2019-07-07T07:33:13Z</published>
    <title>Bidirectional Text Compression in External Memory</title>
    <summary>  Bidirectional compression algorithms work by substituting repeated substrings
by references that, unlike in the famous LZ77-scheme, can point to either
direction. We present such an algorithm that is particularly suited for an
external memory implementation. We evaluate it experimentally on large data
sets of size up to 128 GiB (using only 16 GiB of RAM) and show that it is
significantly faster than all known LZ77 compressors, while producing a roughly
similar number of factors. We also introduce an external memory decompressor
for texts compressed with any uni- or bidirectional compression scheme.
</summary>
    <author>
      <name>Patrick Dinklage</name>
    </author>
    <author>
      <name>Jonas Ellert</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Dominik K√∂ppl</name>
    </author>
    <author>
      <name>Manuel Penschuck</name>
    </author>
    <link href="http://arxiv.org/abs/1907.03235v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.03235v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.04752">
    <id>http://arxiv.org/abs/1907.04752v1</id>
    <updated>2019-07-10T14:29:22Z</updated>
    <published>2019-07-10T14:29:22Z</published>
    <title>Sparse Regular Expression Matching</title>
    <summary>  We present the first algorithm for regular expression matching that can take
advantage of sparsity in the input instance. Our main result is a new algorithm
that solves regular expression matching in $O\left(\Delta \log \log
\frac{nm}{\Delta} + n + m\right)$ time, where $m$ is the number of positions in
the regular expression, $n$ is the length of the string, and $\Delta$ is the
\emph{density} of the instance, defined as the total number of active states in
a simulation of the position automaton. This measure is a lower bound on the
total number of active states in simulations of all classic polynomial sized
finite automata. Our bound improves the best known bounds for regular
expression matching by almost a linear factor in the density of the problem.
The key component in the result is a novel linear space representation of the
position automaton that supports state-set transition computation in
near-linear time in the size of the input and output state sets.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <link href="http://arxiv.org/abs/1907.04752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.04660">
    <id>http://arxiv.org/abs/1907.04660v1</id>
    <updated>2019-07-10T12:21:12Z</updated>
    <published>2019-07-10T12:21:12Z</published>
    <title>String Attractors and Combinatorics on Words</title>
    <summary>  The notion of \emph{string attractor} has recently been introduced in
[Prezza, 2017] and studied in [Kempa and Prezza, 2018] to provide a unifying
framework for known dictionary-based compressors. A string attractor for a word
$w=w[1]w[2]\cdots w[n]$ is a subset $\Gamma$ of the positions $\{1,\ldots,n\}$,
such that all distinct factors of $w$ have an occurrence crossing at least one
of the elements of $\Gamma$. While finding the smallest string attractor for a
word is a NP-complete problem, it has been proved in [Kempa and Prezza, 2018]
that dictionary compressors can be interpreted as algorithms approximating the
smallest string attractor for a given word.
  In this paper we explore the notion of string attractor from a combinatorial
point of view, by focusing on several families of finite words. The results
presented in the paper suggest that the notion of string attractor can be used
to define new tools to investigate combinatorial properties of the words.
</summary>
    <author>
      <name>Sabrina Mantaci</name>
    </author>
    <author>
      <name>Antonio Restivo</name>
    </author>
    <author>
      <name>Giuseppe Romana</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <link href="http://arxiv.org/abs/1907.04660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.04405">
    <id>http://arxiv.org/abs/1907.04405v1</id>
    <updated>2019-07-09T20:42:46Z</updated>
    <published>2019-07-09T20:42:46Z</published>
    <title>$L_p$ Pattern Matching in a Stream</title>
    <summary>  We consider the problem of computing distance between a pattern of length $n$
and all $n$-length subwords of a text in the streaming model.
  In the streaming setting, only the Hamming distance ($L_0$) has been studied.
It is known that computing the Hamming distance between a pattern and a
streaming text exactly requires $\Omega(n)$ space. Therefore, to develop
sublinear-space solutions, one must relax their requirements. One possibility
to do so is to compute only the distances bounded by a threshold $k$,
see~[SODA'19, Clifford, Kociumaka, Porat]. The motivation for this variant of
this problem is that we are interested in subwords of the text that are similar
to the pattern, i.e. in subwords such that the distance between them and the
pattern is relatively small.
  On the other hand, the main application of the streaming setting is
processing large-scale data, such as biological data. Recent advances in
hardware technology allow generating such data at a very high speed, but
unfortunately, the produced data may contain about 10\% of noise~[Biol.
Direct.'07, Klebanov and Yakovlev]. To analyse such data, it is not sufficient
to consider small distances only. A possible workaround for this issue is
$(1\pm\varepsilon)$-approximation. This line of research was initiated in
[ICALP'16, Clifford and Starikovskaya] who gave a
$(1\pm\varepsilon)$-approximation algorithm with space
$\widetilde{O}(\varepsilon^{-5}\sqrt{n})$.
  In this work, we show a suite of new streaming algorithms for computing the
Hamming, $L_1$, $L_2$ and general $L_p$ ($0 &lt; p \le 2$) distances between the
pattern and the text. Our results significantly extend over the previous result
in this setting. In particular, for the Hamming distance case and for the $L_p$
distance when $0 &lt; p \le 1$ we show a streaming algorithm that uses
$\widetilde{O}(\varepsilon^{-2}\sqrt{n})$ space for polynomial-size alphabets.
</summary>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <author>
      <name>Michal Svagerka</name>
    </author>
    <author>
      <name>Przemys≈Çaw Uzna≈Ñski</name>
    </author>
    <link href="http://arxiv.org/abs/1907.04405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.05486">
    <id>http://arxiv.org/abs/1906.05486v1</id>
    <updated>2019-06-13T05:24:44Z</updated>
    <published>2019-06-13T05:24:44Z</published>
    <title>On Longest Common Property Preserved Substring Queries</title>
    <summary>  We revisit the problem of longest common property preserving substring
queries introduced by~Ayad et al. (SPIRE 2018, arXiv 2018). We consider a
generalized and unified on-line setting, where we are given a set $X$ of $k$
strings of total length $n$ that can be pre-processed so that, given a query
string $y$ and a positive integer $k'\leq k$, we can determine the longest
substring of $y$ that satisfies some specific property and is common to at
least $k'$ strings in $X$. Ayad et al. considered the longest square-free
substring in an on-line setting and the longest periodic and palindromic
substring in an off-line setting. In this paper, we give efficient solutions in
the on-line setting for finding the longest common square, periodic,
palindromic, and Lyndon substrings. More precisely, we show that $X$ can be
pre-processed in $O(n)$ time resulting in a data structure of $O(n)$ size that
answers queries in $O(|y|\log\sigma)$ time and $O(1)$ working space, where
$\sigma$ is the size of the alphabet, and the common substring must be a
square, a periodic substring, a palindrome, or a Lyndon word.
</summary>
    <author>
      <name>Kazuki Kai</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">minor change from version submitted to SPIRE 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.05384">
    <id>http://arxiv.org/abs/1906.05384v1</id>
    <updated>2019-06-12T21:28:51Z</updated>
    <published>2019-06-12T21:28:51Z</published>
    <title>Loop Programming Practices that Simplify Quicksort Implementations</title>
    <summary>  Quicksort algorithm with Hoare's partition scheme is traditionally
implemented with nested loops. In this article, we present loop programming and
refactoring techniques that lead to simplified implementation for Hoare's
quicksort algorithm consisting of a single loop. We believe that the techniques
are beneficial for general programming and may be used for the discovery of
more novel algorithms.
</summary>
    <author>
      <name>Shoupu Wan</name>
    </author>
    <link href="http://arxiv.org/abs/1906.05384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.06015">
    <id>http://arxiv.org/abs/1906.06015v1</id>
    <updated>2019-06-14T04:31:12Z</updated>
    <published>2019-06-14T04:31:12Z</published>
    <title>Dynamic Path-Decomposed Tries</title>
    <summary>  A keyword dictionary is an associative array whose keys are strings. Recent
applications handling massive keyword dictionaries in main memory have a need
for a space-efficient implementation. When limited to static applications,
there are a number of highly-compressed keyword dictionaries based on the
advancements of practical succinct data structures. However, as most succinct
data structures are only efficient in the static case, it is still difficult to
implement a keyword dictionary that is space efficient and dynamic. In this
article, we propose such a keyword dictionary. Our main idea is to embrace the
path decomposition technique, which was proposed for constructing
cache-friendly tries. To store the path-decomposed trie in small memory, we
design data structures based on recent compact hash trie representations.
Exhaustive experiments on real-world datasets reveal that our dynamic keyword
dictionary needs up to 68% less space than the existing smallest ones.
</summary>
    <author>
      <name>Shunsuke Kanda</name>
    </author>
    <author>
      <name>Dominik K√∂ppl</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <author>
      <name>Kazuhiro Morita</name>
    </author>
    <author>
      <name>Masao Fuketa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.06015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.06015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.06965">
    <id>http://arxiv.org/abs/1906.06965v2</id>
    <updated>2019-07-29T14:45:41Z</updated>
    <published>2019-06-17T11:38:42Z</published>
    <title>Matching Patterns with Variables</title>
    <summary>  A pattern p (i.e., a string of variables and terminals) matches a word w, if
w can be obtained by uniformly replacing the variables of p by terminal words.
The respective matching problem, i.e., deciding whether or not a given pattern
matches a given word, is generally NP-complete, but can be solved in
polynomial-time for classes of patterns with restricted structure. In this
paper we overview a series of recent results related to efficient matching for
patterns with variables, as well as a series of extensions of this problem.
</summary>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Markus L. Schmid</name>
    </author>
    <link href="http://arxiv.org/abs/1906.06965v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.06965v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.07874">
    <id>http://arxiv.org/abs/1906.07874v1</id>
    <updated>2019-06-19T01:45:10Z</updated>
    <published>2019-06-19T01:45:10Z</published>
    <title>Space Efficient Algorithms for Breadth-Depth Search</title>
    <summary>  Continuing the recent trend, in this article we design several
space-efficient algorithms for two well-known graph search methods. Both these
search methods share the same name {\it breadth-depth search} (henceforth {\sf
BDS}), although they work entirely in different fashion. The classical
implementation for these graph search methods takes $O(m+n)$ time and $O(n \lg
n)$ bits of space in the standard word RAM model (with word size being
$\Theta(\lg n)$ bits), where $m$ and $n$ denotes the number of edges and
vertices of the input graph respectively. Our goal here is to beat the space
bound of the classical implementations, and design $o(n \lg n)$ space
algorithms for these search methods by paying little to no penalty in the
running time. Note that our space bounds (i.e., with $o(n \lg n)$ bits of
space) do not even allow us to explicitly store the required information to
implement the classical algorithms, yet our algorithms visits and reports all
the vertices of the input graph in correct order.
</summary>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Anish Mukherjee</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, This work will appear in FCT 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.07874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.07874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.07871">
    <id>http://arxiv.org/abs/1906.07871v1</id>
    <updated>2019-06-19T01:34:47Z</updated>
    <published>2019-06-19T01:34:47Z</published>
    <title>Indexing Graph Search Trees and Applications</title>
    <summary>  We consider the problem of compactly representing the Depth First Search
(DFS) tree of a given undirected or directed graph having $n$ vertices and $m$
edges while supporting various DFS related queries efficiently in the RAM with
logarithmic word size. We study this problem in two well-known models: {\it
indexing} and {\it encoding} models. While most of these queries can be
supported easily in constant time using $O(n \lg n)$ bits\footnote{We use $\lg$
to denote logarithm to the base $2$.} of extra space, our goal here is, more
specifically, to beat this trivial $O(n \lg n)$ bit space bound, yet not
compromise too much on the running time of these queries. In the {\it indexing}
model, the space bound of our solution involves the quantity $m$, hence, we
obtain different bounds for sparse and dense graphs respectively. In the {\it
encoding} model, we first give a space lower bound, followed by an almost
optimal data structure with extremely fast query time. Central to our algorithm
is a partitioning of the DFS tree into connected subtrees, and a compact way to
store these connections. Finally, we also apply these techniques to compactly
index the shortest path structure, biconnectivity structures among others.
</summary>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, Preliminary version of this paper will appear in MFCS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.07871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.07871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.09732">
    <id>http://arxiv.org/abs/1906.09732v1</id>
    <updated>2019-06-24T05:33:41Z</updated>
    <published>2019-06-24T05:33:41Z</published>
    <title>Dynamic Palindrome Detection</title>
    <summary>  Lately, there is a growing interest in dynamic string matching problems.
Specifically, the dynamic Longest Common Factor problem has been researched and
some interesting results has been reached. In this paper we examine another
classic string problem in a dynamic setting - finding the longest palindrome
substring of a given string. We show that the longest palindrome can be
maintained in poly-logarithmic time per symbol edit.
</summary>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Itai Boneh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1806.02718 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.09732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.09732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.10535">
    <id>http://arxiv.org/abs/1906.10535v2</id>
    <updated>2019-09-13T10:19:31Z</updated>
    <published>2019-06-25T13:56:56Z</published>
    <title>Pseudo-solutions of word equations</title>
    <summary>  We present a framework which allows a uniform approach to the recently
introduced concept of pseudo-repetitions on words in the morphic case. This
framework is at the same time more general and simpler. We introduce the
concept of a pseudo-solution and a pseudo-rank of an equation. In particular,
this allows to prove that if a classical equation forces periodicity then it
also forces pseudo-periodicity. Consequently, there is no need to investigate
generalizations of important equations one by one.
</summary>
    <author>
      <name>≈†tƒõp√°n Holub</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">small corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.10535v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10535v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.11030">
    <id>http://arxiv.org/abs/1906.11030v2</id>
    <updated>2019-12-28T13:33:02Z</updated>
    <published>2019-06-26T12:34:14Z</published>
    <title>Combinatorial Algorithms for String Sanitization</title>
    <summary>  String data are often disseminated to support applications such as
location-based service provision or DNA sequence analysis. This dissemination,
however, may expose sensitive patterns that model confidential knowledge. In
this paper, we consider the problem of sanitizing a string by concealing the
occurrences of sensitive patterns, while maintaining data utility, in two
settings that are relevant to many common string processing tasks.
  In the first setting, we aim to generate the minimal-length string that
preserves the order of appearance and frequency of all non-sensitive patterns.
Such a string allows accurately performing tasks based on the sequential nature
and pattern frequencies of the string. To construct such a string, we propose a
time-optimal algorithm, TFS-ALGO. We also propose another time-optimal
algorithm, PFS-ALGO, which preserves a partial order of appearance of
non-sensitive patterns but produces a much shorter string that can be analyzed
more efficiently. The strings produced by either of these algorithms are
constructed by concatenating non-sensitive parts of the input string. However,
it is possible to detect the sensitive patterns by ``reversing'' the
concatenation operations. In response, we propose a heuristic, MCSR-ALGO, which
replaces letters in the strings output by the algorithms with carefully
selected letters, so that sensitive patterns are not reinstated, implausible
patterns are not introduced, and occurrences of spurious patterns are
prevented. In the second setting, we aim to generate a string that is at
minimal edit distance from the original string, in addition to preserving the
order of appearance and frequency of all non-sensitive patterns. To construct
such a string, we propose an algorithm, ETFS-ALGO, based on solving specific
instances of approximate regular expression matching.
</summary>
    <author>
      <name>Giulia Bernardini</name>
    </author>
    <author>
      <name>Huiping Chen</name>
    </author>
    <author>
      <name>Alessio Conte</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Grigorios Loukides</name>
    </author>
    <author>
      <name>Nadia Pisanti</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <author>
      <name>Michelle Sweering</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper accepted to ECML/PKDD 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.11030v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.11030v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.11062">
    <id>http://arxiv.org/abs/1906.11062v1</id>
    <updated>2019-06-24T18:57:57Z</updated>
    <published>2019-06-24T18:57:57Z</published>
    <title>Survey of Information Encoding Techniques for DNA</title>
    <summary>  Key to DNA storage is encoding the information to a sequence of nucleotides
before it can be synthesised for storage. Definition of such an encoding or
mapping must adhere to multiple design restrictions. First, not all possible
sequences of nucleotides can be synthesised. Homopolymers, e.g., sequences of
the same nucleotide, of a length of more than two, for example, cannot be
synthesised without potential errors. Similarly, the G-C content of the
resulting sequences should be higher than 50\%. Second, given that synthesis is
expensive, the encoding must map as many bits as possible to one nucleotide.
Third, the synthesis (as well as the sequencing) is error prone, leading to
substitutions, deletions and insertions. An encoding must therefore be designed
to be resilient to errors through error correction codes or replication.
Fourth, for the purpose of computation and selective retrieval, encodings
should result in substantially different sequences across all data, even for
very similar data. In the following we discuss the history and evolution of
encodings.
</summary>
    <author>
      <name>Thomas Heinis</name>
    </author>
    <link href="http://arxiv.org/abs/1906.11062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.11062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.00809">
    <id>http://arxiv.org/abs/1906.00809v1</id>
    <updated>2019-06-03T13:45:43Z</updated>
    <published>2019-06-03T13:45:43Z</published>
    <title>Rpair: Rescaling RePair with Rsync</title>
    <summary>  Data compression is a powerful tool for managing massive but repetitive
datasets, especially schemes such as grammar-based compression that support
computation over the data without decompressing it. In the best case such a
scheme takes a dataset so big that it must be stored on disk and shrinks it
enough that it can be stored and processed in internal memory. Even then,
however, the scheme is essentially useless unless it can be built on the
original dataset reasonably quickly while keeping the dataset on disk. In this
paper we show how we can preprocess such datasets with context-triggered
piecewise hashing such that afterwards we can apply RePair and other
grammar-based compressors more easily. We first give our algorithm, then show
how a variant of it can be used to approximate the LZ77 parse, then leverage
that to prove theoretical bounds on compression, and finally give experimental
evidence that our approach is competitive in practice.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <link href="http://arxiv.org/abs/1906.00809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.00563">
    <id>http://arxiv.org/abs/1906.00563v1</id>
    <updated>2019-06-03T04:17:38Z</updated>
    <published>2019-06-03T04:17:38Z</published>
    <title>Direct Linear Time Construction of Parameterized Suffix and LCP Arrays
  for Constant Alphabets</title>
    <summary>  We present the first worst-case linear time algorithm that directly computes
the parameterized suffix and LCP arrays for constant sized alphabets. Previous
algorithms either required quadratic time or the parameterized suffix tree to
be built first. More formally, for a string over static alphabet $\Sigma$ and
parameterized alphabet $\Pi$, our algorithm runs in $O(n\pi)$ time and $O(n)$
words of space, where $\pi$ is the number of distinct symbols of $\Pi$ in the
string.
</summary>
    <author>
      <name>Noriki Fujisato</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to SPIRE 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.00563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.01346">
    <id>http://arxiv.org/abs/1906.01346v1</id>
    <updated>2019-06-04T11:07:29Z</updated>
    <published>2019-06-04T11:07:29Z</published>
    <title>Characteristic Parameters and Special Trapezoidal Words</title>
    <summary>  Following earlier work by Aldo de Luca and others, we study trapezoidal words
and their prefixes, with respect to their characteristic parameters $K$ and $R$
(length of shortest unrepeated suffix, and shortest length without right
special factors, respectively), as well as their symmetric versions $H$ and
$L$. We consider the distinction between closed (i.e., periodic-like) and open
prefixes, and between Sturmian and non-Sturmian ones. Our main results
characterize right special and strictly bispecial trapezoidal words, as done by
de Luca and Mignosi for Sturmian words.
</summary>
    <author>
      <name>Alma D'Aniello</name>
    </author>
    <author>
      <name>Alessandro De Luca</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages; to appear in LNCS proceedings for WORDS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.01346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.00665">
    <id>http://arxiv.org/abs/1906.00665v1</id>
    <updated>2019-06-03T09:41:57Z</updated>
    <published>2019-06-03T09:41:57Z</published>
    <title>Every nonnegative real number is an abelian critical exponent</title>
    <summary>  The abelian critical exponent of an infinite word $w$ is defined as the
maximum ratio between the exponent and the period of an abelian power occurring
in $w$. It was shown by Fici et al. that the set of finite abelian critical
exponents of Sturmian words coincides with the Lagrange spectrum. This spectrum
contains every large enough positive real number. We construct words whose
abelian critical exponents fill the remaining gaps, that is, we prove that for
each nonnegative real number $\theta$ there exists an infinite word having
abelian critical exponent $\theta$. We also extend this result to the
$k$-abelian setting.
</summary>
    <author>
      <name>Jarkko Peltom√§ki</name>
    </author>
    <author>
      <name>Markus A. Whiteland</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-28796-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-28796-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 12th International Conference, WORDS, Lecture
  Notes in Computer Science Vol. 11682, pp. 275-285 (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.00665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.03689">
    <id>http://arxiv.org/abs/1906.03689v1</id>
    <updated>2019-06-09T18:50:25Z</updated>
    <published>2019-06-09T18:50:25Z</published>
    <title>Borders, Palindrome Prefixes, and Square Prefixes</title>
    <summary>  We show that the number of length-$n$ words over a $k$-letter alphabet having
no even palindromic prefix is the same as the number of length-$n$ unbordered
words, by constructing an explicit bijection between the two sets. A similar
result holds for those words having no odd palindromic prefix, again by
constructing a certain bijection. Using known results on borders, it follows
that the number of length-$n$ words having no even (resp., odd) palindromic
prefix is asymptotically $\gamma_k \cdot k^n$ for some positive constant
$\gamma_k$. We obtain an analogous result for words having no nontrivial
palindromic prefix. Finally, we obtain similar results for words having no
square prefix, thus proving a 2013 conjecture of Chaffin, Linderman, Sloane,
and Wilks.
</summary>
    <author>
      <name>Daniel Gabric</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/1906.03689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.04346">
    <id>http://arxiv.org/abs/1607.04346v2</id>
    <updated>2016-11-14T02:20:36Z</updated>
    <published>2016-07-15T00:16:58Z</published>
    <title>Space-Efficient Construction of Compressed Indexes in Deterministic
  Linear Time</title>
    <summary>  We show that the compressed suffix array and the compressed suffix tree of a
string $T$ can be built in $O(n)$ deterministic time using $O(n\log\sigma)$
bits of space, where $n$ is the string length and $\sigma$ is the alphabet
size. Previously described deterministic algorithms either run in time that
depends on the alphabet size or need $\omega(n\log \sigma)$ bits of working
space. Our result has immediate applications to other problems, such as
yielding the first linear-time LZ77 and LZ78 parsing algorithms that use $O(n
\log\sigma)$ bits.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper to appear at SODA 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04346v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04346v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.09120">
    <id>http://arxiv.org/abs/1812.09120v1</id>
    <updated>2018-12-21T13:52:54Z</updated>
    <published>2018-12-21T13:52:54Z</published>
    <title>Lower bounds for text indexing with mismatches and differences</title>
    <summary>  In this paper we study lower bounds for the fundamental problem of text
indexing with mismatches and differences. In this problem we are given a long
string of length $n$, the "text", and the task is to preprocess it into a data
structure such that given a query string $Q$, one can quickly identify
substrings that are within Hamming or edit distance at most $k$ from $Q$. This
problem is at the core of various problems arising in biology and text
processing. While exact text indexing allows linear-size data structures with
linear query time, text indexing with $k$ mismatches (or $k$ differences) seems
to be much harder: All known data structures have exponential dependency on $k$
either in the space, or in the time bound. We provide conditional and
pointer-machine lower bounds that make a step toward explaining this
phenomenon. We start by demonstrating lower bounds for $k = \Theta(\log n)$. We
show that assuming the Strong Exponential Time Hypothesis, any data structure
for text indexing that can be constructed in polynomial time cannot have
$\mathcal{O}(n^{1-\delta})$ query time, for any $\delta>0$. This bound also
extends to the setting where we only ask for $(1+\varepsilon)$-approximate
solutions for text indexing. However, in many applications the value of $k$ is
rather small, and one might hope that for small~$k$ we can develop more
efficient solutions. We show that this would require a radically new approach
as using the current methods one cannot avoid exponential dependency on $k$
either in the space, or in the time bound for all even $\frac{8}{\sqrt{3}}
\sqrt{\log n} \le k = o(\log n)$. Our lower bounds also apply to the dictionary
look-up problem, where instead of a text one is given a set of strings.
</summary>
    <author>
      <name>Vincent Cohen-Addad</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Feuilloley</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIF</arxiv:affiliation>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DI-ENS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SODA 2019, Jan 2019, San Diego, United States</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.09120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.09120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.04897">
    <id>http://arxiv.org/abs/1906.04897v1</id>
    <updated>2019-05-20T01:53:05Z</updated>
    <published>2019-05-20T01:53:05Z</published>
    <title>Prefix Block-Interchanges on Binary and Ternary Strings</title>
    <summary>  The genome rearrangement problem computes the minimum number of operations
that are required to sort all elements of a permutation. A block-interchange
operation exchanges two blocks of a permutation which are not necessarily
adjacent and in a prefix block-interchange, one block is always the prefix of
that permutation. In this paper, we focus on applying prefix block-interchanges
on binary and ternary strings. We present upper bounds to group and sort a
given binary/ternary string. We also provide upper bounds for a different
version of the block-interchange operation which we refer to as the `restricted
prefix block-interchange'. We observe that our obtained upper bound for
restricted prefix block-interchange operations on binary strings is better than
that of other genome rearrangement operations to group fully normalized binary
strings. Consequently, we provide a linear-time algorithm to solve the problem
of grouping binary normalized strings by restricted prefix block-interchanges.
We also provide a polynomial time algorithm to group normalized ternary strings
by prefix block-interchange operations. Finally, we provide a classification
for ternary strings based on the required number of prefix block-interchange
operations.
</summary>
    <author>
      <name>Md. Khaledur Rahman</name>
    </author>
    <author>
      <name>M. Sohel Rahman</name>
    </author>
    <link href="http://arxiv.org/abs/1906.04897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.04897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.05208">
    <id>http://arxiv.org/abs/1906.05208v1</id>
    <updated>2019-06-12T15:24:07Z</updated>
    <published>2019-06-12T15:24:07Z</published>
    <title>Sorted Top-k in Rounds</title>
    <summary>  We consider the sorted top-$k$ problem whose goal is to recover the top-$k$
items with the correct order out of $n$ items using pairwise comparisons. In
many applications, multiple rounds of interaction can be costly. We restrict
our attention to algorithms with a constant number of rounds $r$ and try to
minimize the sample complexity, i.e. the number of comparisons.
  When the comparisons are noiseless, we characterize how the optimal sample
complexity depends on the number of rounds (up to a polylogarithmic factor for
general $r$ and up to a constant factor for $r=1$ or 2). In particular, the
sample complexity is $\Theta(n^2)$ for $r=1$, $\Theta(n\sqrt{k} + n^{4/3})$ for
$r=2$ and $\tilde{\Theta}\left(n^{2/r} k^{(r-1)/r} + n\right)$ for $r \geq 3$.
  We extend our results of sorted top-$k$ to the noisy case where each
comparison is correct with probability $2/3$. When $r=1$ or 2, we show that the
sample complexity gets an extra $\Theta(\log(k))$ factor when we transition
from the noiseless case to the noisy case.
  We also prove new results for top-$k$ and sorting in the noisy case. We
believe our techniques can be generally useful for understanding the trade-off
between round complexities and sample complexities of rank aggregation
problems.
</summary>
    <author>
      <name>Mark Braverman</name>
    </author>
    <author>
      <name>Jieming Mao</name>
    </author>
    <author>
      <name>Yuval Peres</name>
    </author>
    <link href="http://arxiv.org/abs/1906.05208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.05266">
    <id>http://arxiv.org/abs/1906.05266v1</id>
    <updated>2019-06-12T17:47:17Z</updated>
    <published>2019-06-12T17:47:17Z</published>
    <title>The Tandem Duplication Distance is NP-hard</title>
    <summary>  In computational biology, tandem duplication is an important biological
phenomenon which can occur either at the genome or at the DNA level. A tandem
duplication takes a copy of a genome segment and inserts it right after the
segment - this can be represented as the string operation $AXB \Rightarrow
AXXB$. For example, Tandem exon duplications have been found in many species
such as human, fly or worm, and have been largely studied in computational
biology. The Tandem Duplication (TD) distance problem we investigate in this
paper is defined as follows: given two strings $S$ and $T$ over the same
alphabet, compute the smallest sequence of tandem duplications required to
convert $S$ to $T$. The natural question of whether the TD distance can be
computed in polynomial time was posed in 2004 by Leupold et al. and had
remained open, despite the fact that tandem duplications have received much
attention ever since. In this paper, we prove that this problem is NP-hard. We
further show that this hardness holds even if all characters of $S$ are
distinct. This is known as the exemplar TD distance, which is of special
relevance in bioinformatics. One of the tools we develop for the reduction is a
new problem called the Cost-Effective Subgraph, for which we obtain
W[1]-hardness results that might be of independent interest. We finally show
that computing the exemplar TD distance between $S$ and $T$ is fixed-parameter
tractable. Our results open the door to many other questions, and we conclude
with several open problems.
</summary>
    <author>
      <name>Manuel Lafond</name>
    </author>
    <author>
      <name>Binhai Zhu</name>
    </author>
    <author>
      <name>Peng Zou</name>
    </author>
    <link href="http://arxiv.org/abs/1906.05266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.07455">
    <id>http://arxiv.org/abs/1905.07455v3</id>
    <updated>2019-10-23T14:56:50Z</updated>
    <published>2019-05-16T12:39:10Z</published>
    <title>Speeding up the Karatsuba algorithm</title>
    <summary>  This paper describes an $\sim {\cal O}(n)$ pre-compute technique to speed up
the Karatsuba algorithm for multiplying two numbers.
</summary>
    <author>
      <name>Satish Ramakrishna</name>
    </author>
    <author>
      <name>Kamesh Aiyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.07455v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07455v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.08563">
    <id>http://arxiv.org/abs/1905.08563v2</id>
    <updated>2019-10-09T17:22:17Z</updated>
    <published>2019-05-21T11:44:49Z</published>
    <title>Memory lower bounds for deterministic self-stabilization</title>
    <summary>  In the context of self-stabilization, a \emph{silent} algorithm guarantees
that the register of every node does not change once the algorithm has
stabilized. At the end of the 90's, Dolev et al. [Acta Inf. '99] showed that,
for finding the centers of a graph, for electing a leader, or for constructing
a spanning tree, every silent algorithm must use a memory of $\Omega(\log n)$
bits per register in $n$-node networks. Similarly, Korman et al. [Dist. Comp.
'07] proved, using the notion of proof-labeling-scheme, that, for constructing
a minimum-weight spanning trees (MST), every silent algorithm must use a memory
of $\Omega(\log^2n)$ bits per register. It follows that requiring the algorithm
to be silent has a cost in terms of memory space, while, in the context of
self-stabilization, where every node constantly checks the states of its
neighbors, the silence property can be of limited practical interest. In fact,
it is known that relaxing this requirement results in algorithms with smaller
space-complexity.
  In this paper, we are aiming at measuring how much gain in terms of memory
can be expected by using arbitrary self-stabilizing algorithms, not necessarily
silent. To our knowledge, the only known lower bound on the memory requirement
for general algorithms, also established at the end of the 90's, is due to
Beauquier et al.~[PODC '99] who proved that registers of constant size are not
sufficient for leader election algorithms. We improve this result by
establishing a tight lower bound of $\Theta(\log \Delta+\log \log n)$ bits per
register for self-stabilizing algorithms solving $(\Delta+1)$-coloring or
constructing a spanning tree in networks of maximum degree~$\Delta$. The lower
bound $\Omega(\log \log n)$ bits per register also holds for leader election.
</summary>
    <author>
      <name>L√©lia Blin</name>
    </author>
    <author>
      <name>Laurent Feuilloley</name>
    </author>
    <author>
      <name>Gabriel Le Bouder</name>
    </author>
    <link href="http://arxiv.org/abs/1905.08563v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08563v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.08974">
    <id>http://arxiv.org/abs/1905.08974v1</id>
    <updated>2019-05-22T06:15:31Z</updated>
    <published>2019-05-22T06:15:31Z</published>
    <title>Cartesian Tree Matching and Indexing</title>
    <summary>  We introduce a new metric of match, called Cartesian tree matching, which
means that two strings match if they have the same Cartesian trees. Based on
Cartesian tree matching, we define single pattern matching for a text of length
n and a pattern of length m, and multiple pattern matching for a text of length
n and k patterns of total length m. We present an O(n+m) time algorithm for
single pattern matching, and an O((n+m) log k) deterministic time or O(n+m)
randomized time algorithm for multiple pattern matching. We also define an
index data structure called Cartesian suffix tree, and present an O(n)
randomized time algorithm to build the Cartesian suffix tree. Our efficient
algorithms for Cartesian tree matching use a representation of the Cartesian
tree, called the parent-distance representation.
</summary>
    <author>
      <name>Sung Gwan Park</name>
    </author>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Kunsoo Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures, Submitted to CPM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.08974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.08977">
    <id>http://arxiv.org/abs/1905.08977v1</id>
    <updated>2019-05-22T06:39:48Z</updated>
    <published>2019-05-22T06:39:48Z</published>
    <title>A Memory-Efficient Sketch Method for Estimating High Similarities in
  Streaming Sets</title>
    <summary>  Estimating set similarity and detecting highly similar sets are fundamental
problems in areas such as databases, machine learning, and information
retrieval. MinHash is a well-known technique for approximating Jaccard
similarity of sets and has been successfully used for many applications such as
similarity search and large scale learning. Its two compressed versions, b-bit
MinHash and Odd Sketch, can significantly reduce the memory usage of the
original MinHash method, especially for estimating high similarities (i.e.,
similarities around 1). Although MinHash can be applied to static sets as well
as streaming sets, of which elements are given in a streaming fashion and
cardinality is unknown or even infinite, unfortunately, b-bit MinHash and Odd
Sketch fail to deal with streaming data. To solve this problem, we design a
memory efficient sketch method, MaxLogHash, to accurately estimate Jaccard
similarities in streaming sets. Compared to MinHash, our method uses smaller
sized registers (each register consists of less than 7 bits) to build a compact
sketch for each set. We also provide a simple yet accurate estimator for
inferring Jaccard similarity from MaxLogHash sketches. In addition, we derive
formulas for bounding the estimation error and determine the smallest necessary
memory usage (i.e., the number of registers used for a MaxLogHash sketch) for
the desired accuracy. We conduct experiments on a variety of datasets, and
experimental results show that our method MaxLogHash is about 5 times more
memory efficient than MinHash with the same accuracy and computational cost for
estimating high similarities.
</summary>
    <author>
      <name>Pinghui Wang</name>
    </author>
    <author>
      <name>Yiyan Qi</name>
    </author>
    <author>
      <name>Yuanming Zhang</name>
    </author>
    <author>
      <name>Qiaozhu Zhai</name>
    </author>
    <author>
      <name>Chenxu Wang</name>
    </author>
    <author>
      <name>John C. S. Lui</name>
    </author>
    <author>
      <name>Xiaohong Guan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3292500.3330825</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3292500.3330825" rel="related"/>
    <link href="http://arxiv.org/abs/1905.08977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.09656">
    <id>http://arxiv.org/abs/1905.09656v1</id>
    <updated>2019-05-23T13:50:56Z</updated>
    <published>2019-05-23T13:50:56Z</published>
    <title>On the Average Case of MergeInsertion</title>
    <summary>  MergeInsertion, also known as the Ford-Johnson algorithm, is a sorting
algorithm which, up to today, for many input sizes achieves the best known
upper bound on the number of comparisons. Indeed, it gets extremely close to
the information-theoretic lower bound. While the worst-case behavior is well
understood, only little is known about the average case.
  This work takes a closer look at the average case behavior. In particular, we
establish an upper bound of $n \log n - 1.4005n + o(n)$ comparisons. We also
give an exact description of the probability distribution of the length of the
chain a given element is inserted into and use it to approximate the average
number of comparisons numerically. Moreover, we compute the exact average
number of comparisons for $n$ up to 148.
  Furthermore, we experimentally explore the impact of different decision trees
for binary insertion. To conclude, we conduct experiments showing that a
slightly different insertion order leads to a better average case and we
compare the algorithm to the recent combination with (1,2)-Insertionsort by
Iwama and Teruyama.
</summary>
    <author>
      <name>Florian Stober</name>
    </author>
    <author>
      <name>Armin Wei√ü</name>
    </author>
    <link href="http://arxiv.org/abs/1905.09656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.09656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1108.0866">
    <id>http://arxiv.org/abs/1108.0866v1</id>
    <updated>2011-08-03T15:24:49Z</updated>
    <published>2011-08-03T15:24:49Z</published>
    <title>Towards Optimal Sorting of 16 Elements</title>
    <summary>  One of the fundamental problem in the theory of sorting is to find the
pessimistic number of comparisons sufficient to sort a given number of
elements. Currently 16 is the lowest number of elements for which we do not
know the exact value. We know that 46 comparisons suffices and that 44 do not.
There is an open question if 45 comparisons are sufficient. We present an
attempt to resolve that problem by performing an exhaustive computer search. We
also present an algorithm for counting linear extensions which substantially
speeds up computations.
</summary>
    <author>
      <name>Marcin Peczarski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures, 2 tables. First submitted to IWOCA 2010, 21st
  International Workshop on Combinatorial Algorithms. Submission was rejected</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Acta Universitatis Sapientiae, Informatica, 4(2) (2012) 215-224</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1108.0866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.0866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.12987">
    <id>http://arxiv.org/abs/1905.12987v2</id>
    <updated>2019-07-26T19:34:44Z</updated>
    <published>2019-05-30T12:01:00Z</published>
    <title>Inducing the Lyndon Array</title>
    <summary>  In this paper we propose a variant of the induced suffix sorting algorithm by
Nong (TOIS, 2013) that computes simultaneously the Lyndon array and the suffix
array of a text in $O(n)$ time using $\sigma + O(1)$ words of working space,
where $n$ is the length of the text and $\sigma$ is the alphabet size. Our
result improves the previous best space requirement for linear time computation
of the Lyndon array. In fact, all the known linear algorithms for Lyndon array
computation use suffix sorting as a preprocessing step and use $O(n)$ words of
working space in addition to the Lyndon array and suffix array. Experimental
results with real and synthetic datasets show that our algorithm is not only
space-efficient but also fast in practice.
</summary>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Sabrina Mantaci</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <author>
      <name>Guilherme P. Telles</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SPIRE'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.12987v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.12987v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.12854">
    <id>http://arxiv.org/abs/1905.12854v3</id>
    <updated>2019-09-15T05:28:41Z</updated>
    <published>2019-05-30T05:07:09Z</published>
    <title>Compact Data Structures for Shortest Unique Substring Queries</title>
    <summary>  Given a string T of length n, a substring u = T[i.. j] of T is called a
shortest unique substring (SUS) for an interval [s, t] if (a) u occurs exactly
once in T, (b) u contains the interval [s, t] (i.e. i \leq s \leq t \leq j),
and (c) every substring v of T with |v| &lt; |u| containing [s, t] occurs at least
twice in T. Given a query interval [s, t] \subset [1, n], the interval SUS
problem is to output all the SUSs for the interval [s, t]. In this article, we
propose a 4n + o(n) bits data structure answering an interval SUS query in
output-sensitive O(occ) time, where occ is the number of returned SUSs.
Additionally, we focus on the point SUS problem, which is the interval SUS
problem for s = t. Here, we propose a \lceil (log2 3 + 1)n \rceil + o(n) bits
data structure answering a point SUS query in the same output-sensitive time.
</summary>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Dominik K√∂ppl</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1905.12854v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.12854v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.13064">
    <id>http://arxiv.org/abs/1905.13064v4</id>
    <updated>2019-06-09T19:51:19Z</updated>
    <published>2019-05-30T14:12:03Z</published>
    <title>The Bloom Clock</title>
    <summary>  The bloom clock is a space-efficient, probabilistic data structure designed
to determine the partial order of events in highly distributed systems. The
bloom clock, like the vector clock, can autonomously detect causality
violations by comparing its logical timestamps. Unlike the vector clock, the
space complexity of the bloom clock does not depend on the number of nodes in a
system. Instead it depends on a set of chosen parameters that determine its
confidence interval, i.e. false positive rate. To reduce the space complexity
from which the vector clock suffers, the bloom clock uses a 'moving window' in
which the partial order of events can be inferred with high confidence. If two
clocks are not comparable, the bloom clock can always deduce it, i.e. false
negatives are not possible. If two clocks are comparable, the bloom clock can
calculate the confidence of that statement, i.e. it can compute the false
positive rate between comparable pairs of clocks. By choosing an acceptable
threshold for the false positive rate, the bloom clock can properly compare the
order of its timestamps, with that of other nodes in a highly accurate and
space efficient way.
</summary>
    <author>
      <name>Lum Ramabaja</name>
    </author>
    <link href="http://arxiv.org/abs/1905.13064v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.13064v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.07224">
    <id>http://arxiv.org/abs/1905.07224v1</id>
    <updated>2019-05-17T12:24:17Z</updated>
    <published>2019-05-17T12:24:17Z</published>
    <title>Parallel decompression of gzip-compressed files and random access to DNA
  sequences</title>
    <summary>  Decompressing a file made by the gzip program at an arbitrary location is in
principle impossible, due to the nature of the DEFLATE compression algorithm.
Consequently, no existing program can take advantage of parallelism to rapidly
decompress large gzip-compressed files. This is an unsatisfactory bottleneck,
especially for the analysis of large sequencing data experiments. Here we
propose a parallel algorithm and an implementation, pugz, that performs fast
and exact decompression of any text file. We show that pugz is an order of
magnitude faster than gunzip, and 5x faster than a highly-optimized sequential
implementation (libdeflate). We also study the related problem of random access
to compressed data. We give simple models and experimental results that shed
light on the structure of gzip-compressed files containing DNA sequences.
Preliminary results show that random access to sequences within a
gzip-compressed FASTQ file is almost always feasible at low compression levels,
yet is approximate at higher compression levels.
</summary>
    <author>
      <name>Ma√´l Kerbiriou</name>
    </author>
    <author>
      <name>Rayan Chikhi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HiCOMB'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.07224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.12135">
    <id>http://arxiv.org/abs/1904.12135v1</id>
    <updated>2019-04-27T09:31:32Z</updated>
    <published>2019-04-27T09:31:32Z</published>
    <title>About Fibonacci trees. I</title>
    <summary>  In this first paper, we look at the following question: are the properties of
the Fibonacci tree still true if we consider a finitely generated tree by the
same rules but rooted at a black node? The direct answer is no, but new
properties arise, a bit more complex than in the case of a tree rooted at a
white node, but still of interest.
</summary>
    <author>
      <name>Maurice Margenstern</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.12135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.12135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.00369">
    <id>http://arxiv.org/abs/1905.00369v3</id>
    <updated>2019-11-15T11:46:12Z</updated>
    <published>2019-05-01T16:33:11Z</published>
    <title>Fast hashing with Strong Concentration Bounds</title>
    <summary>  Previous work on tabulation hashing by P{\v a}tra{\c s}cu and Thorup from
STOC'11 on simple tabulation and from SODA'13 on twisted tabulation offered
Chernoff-style concentration bounds on hash based sums, e.g., the number of
balls/keys hashing to a given bin, but under some quite severe restrictions on
the expected values of these sums. The basic idea in tabulation hashing is to
view a key as consisting of $c=O(1)$ characters, e.g., a 64-bit key as $c=8$
characters of 8-bits. The character domain $\Sigma$ should be small enough that
character tables of size $|\Sigma|$ fit in fast cache. The schemes then use
$O(1)$ tables of this size, so the space of tabulation hashing is
$O(|\Sigma|)$. However, the concentration bounds by P{\v a}tra{\c s}cu and
Thorup only apply if the expected sums are $\ll |\Sigma|$.
  To see the problem, consider the very simple case where we use tabulation
hashing to throw $n$ balls into $m$ bins and want to analyse the number of
balls in a given bin. With their concentration bounds, we are fine if $n=m$,
for then the expected value is $1$. However, if $m=2$, as when tossing $n$
unbiased coins, the expected value $n/2$ is $\gg |\Sigma|$ for large data sets,
e.g., data sets that do not fit in fast cache.
  To handle expectations that go beyond the limits of our small space, we need
a much more advanced analysis of simple tabulation, plus a new tabulation
technique that we call \emph{tabulation-permutation} hashing which is at most
twice as slow as simple tabulation. No other hashing scheme of comparable speed
offers similar Chernoff-style concentration bounds.
</summary>
    <author>
      <name>Anders Aamand</name>
    </author>
    <author>
      <name>Jakob B. T. Knudsen</name>
    </author>
    <author>
      <name>Mathias B. T. Knudsen</name>
    </author>
    <author>
      <name>Peter M. R. Rasmussen</name>
    </author>
    <author>
      <name>Mikkel Thorup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.00369v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.00369v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.00163">
    <id>http://arxiv.org/abs/1905.00163v1</id>
    <updated>2019-05-01T02:22:14Z</updated>
    <published>2019-05-01T02:22:14Z</published>
    <title>Separate Chaining Meets Compact Hashing</title>
    <summary>  While separate chaining is a common strategy for resolving collisions in a
hash table taught in most textbooks, compact hashing is a less common technique
for saving space when hashing integers whose domain is relatively small with
respect to the problem size. It is widely believed that hash tables waste a
considerable amount of memory, as they either leave allocated space untouched
(open addressing) or store additional pointers (separate chaining). For the
former, Cleary introduced the compact hashing technique that stores only a part
of a key to save space. However, as can be seen by the line of research
focusing on compact hash tables with open addressing, there is additional
information, called displacement, required for restoring a key. There are
several representations of this displacement information with different space
and time trade-offs. In this article, we introduce a separate chaining hash
table that applies the compact hashing technique without the need for the
displacement information. Practical evaluations reveal that insertions in this
hash table are faster or use less space than all previously known compact hash
tables on modern computer architectures when storing sufficiently large
satellite data.
</summary>
    <author>
      <name>Dominik K√∂ppl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of the local proceedings of the 173th SIGAL, 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.00163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.00163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.00118">
    <id>http://arxiv.org/abs/1905.00118v3</id>
    <updated>2020-02-26T02:21:40Z</updated>
    <published>2019-04-30T22:04:28Z</published>
    <title>Using Non-Linear Difference Equations to Study Quicksort Algorithms</title>
    <summary>  Using non-linear difference equations, combined with symbolic computations,
we make a detailed study of the running times of numerous variants of the
celebrated Quicksort algorithms, where we consider the variants of single-pivot
and multi-pivot Quicksort algorithms as discrete probability problems. With
non-linear difference equations, recurrence relations and experimental
mathematics techniques, explicit expressions for expectations, variances and
even higher moments of their numbers of comparisons and swaps can be obtained.
For some variants, Monte Carlo experiments are performed, the numerical results
are demonstrated and the scaled limiting distribution is also discussed.
</summary>
    <author>
      <name>Yukun Yao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.00118v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.00118v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.01254">
    <id>http://arxiv.org/abs/1905.01254v1</id>
    <updated>2019-05-03T16:19:31Z</updated>
    <published>2019-05-03T16:19:31Z</published>
    <title>RLE edit distance in near optimal time</title>
    <summary>  We show that the edit distance between two run-length encoded strings of
compressed lengths $m$ and $n$ respectively, can be computed in
$\mathcal{O}(mn\log(mn))$ time. This improves the previous record by a factor
of $\mathcal{O}(n/\log(mn))$. The running time of our algorithm is within
subpolynomial factors of being optimal, subject to the standard SETH-hardness
assumption. This effectively closes a line of algorithmic research first
started in 1993.
</summary>
    <author>
      <name>Rapha√´l Clifford</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Daniel P. Martin</name>
    </author>
    <author>
      <name>Przemys≈Çaw Uzna≈Ñski</name>
    </author>
    <link href="http://arxiv.org/abs/1905.01254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.01340">
    <id>http://arxiv.org/abs/1905.01340v1</id>
    <updated>2019-05-03T18:40:09Z</updated>
    <published>2019-05-03T18:40:09Z</published>
    <title>Palindromic Ziv-Lempel and Crochemore Factorizations of $m$-Bonacci
  Infinite Words</title>
    <summary>  We introduce a variation of the Ziv-Lempel and Crochemore factorizations of
words by requiring each factor to be a palindrome. We compute these
factorizations for the Fibonacci word, and more generally, for all $m$-bonacci
words.
</summary>
    <author>
      <name>Marieh Jahannia</name>
    </author>
    <author>
      <name>Morteza Mohammad-noori</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <author>
      <name>Manon Stipulanti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.01340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.02589">
    <id>http://arxiv.org/abs/1905.02589v1</id>
    <updated>2019-05-07T14:01:52Z</updated>
    <published>2019-05-07T14:01:52Z</published>
    <title>Order-Preserving Pattern Matching Indeterminate Strings</title>
    <summary>  Given an indeterminate string pattern $p$ and an indeterminate string text
$t$, the problem of order-preserving pattern matching with character
uncertainties ($\mu$OPPM) is to find all substrings of $t$ that satisfy one of
the possible orderings defined by $p$. When the text and pattern are
determinate strings, we are in the presence of the well-studied exact
order-preserving pattern matching (OPPM) problem with diverse applications on
time series analysis. Despite its relevance, the exact OPPM problem suffers
from two major drawbacks: 1) the inability to deal with indetermination in the
text, thus preventing the analysis of noisy time series; and 2) the inability
to deal with indetermination in the pattern, thus imposing the strict
satisfaction of the orders among all pattern positions. This paper provides the
first polynomial algorithm to answer the $\mu$OPPM problem when indetermination
is observed on the pattern or text. Given two strings with length $m$ and
$O(r)$ uncertain characters per string position, we show that the $\mu$OPPM
problem can be solved in $O(mr\lg r)$ time when one string is indeterminate and
$r\in\mathbb{N}^+$. Mappings into satisfiability problems are provided when
indetermination is observed on both the pattern and the text, and results
concerning the general problem complexity are presented as well, with $\mu$OPPM
problem proved to be NP-hard in general.
</summary>
    <author>
      <name>Diogo Costa</name>
    </author>
    <author>
      <name>Lu√≠s M. S. Russo</name>
    </author>
    <author>
      <name>Rui Henriques</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Alexandre P. Francisco</name>
    </author>
    <link href="http://arxiv.org/abs/1905.02589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.02589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.02322">
    <id>http://arxiv.org/abs/1905.02322v1</id>
    <updated>2019-05-07T02:08:05Z</updated>
    <published>2019-05-07T02:08:05Z</published>
    <title>Orthogonal Range Reporting and Rectangle Stabbing for Fat Rectangles</title>
    <summary>  In this paper we study two geometric data structure problems in the special
case when input objects or queries are fat rectangles. We show that in this
case a significant improvement compared to the general case can be achieved.
  We describe data structures that answer two- and three-dimensional orthogonal
range reporting queries in the case when the query range is a \emph{fat}
rectangle. Our two-dimensional data structure uses $O(n)$ words and supports
queries in $O(\log\log U +k)$ time, where $n$ is the number of points in the
data structure, $U$ is the size of the universe and $k$ is the number of points
in the query range. Our three-dimensional data structure needs
$O(n\log^{\varepsilon}U)$ words of space and answers queries in $O(\log \log U
+ k)$ time. We also consider the rectangle stabbing problem on a set of
three-dimensional fat rectangles. Our data structure uses $O(n)$ space and
answers stabbing queries in $O(\log U\log\log U +k)$ time.
</summary>
    <author>
      <name>Timothy M. Chan</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Michiel Smid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of a WADS'19 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.02322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.02322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.02298">
    <id>http://arxiv.org/abs/1905.02298v1</id>
    <updated>2019-05-07T00:26:49Z</updated>
    <published>2019-05-07T00:26:49Z</published>
    <title>Even Faster Elastic-Degenerate String Matching via Fast Matrix
  Multiplication</title>
    <summary>  An elastic-degenerate (ED) string is a sequence of $n$ sets of strings of
total length $N$, which was recently proposed to model a set of similar
sequences. The ED string matching (EDSM) problem is to find all occurrences of
a pattern of length $m$ in an ED text. The EDSM problem has recently received
some attention in the combinatorial pattern matching community, and an
$\mathcal{O}(nm^{1.5}\sqrt{\log m} + N)$-time algorithm is known [Aoyama et
al., CPM 2018]. The standard assumption in the prior work on this question is
that $N$ is substantially larger than both $n$ and $m$, and thus we would like
to have a linear dependency on the former. Under this assumption, the natural
open problem is whether we can decrease the 1.5 exponent in the time
complexity, similarly as in the related (but, to the best of our knowledge, not
equivalent) word break problem [Backurs and Indyk, FOCS 2016].
  Our starting point is a conditional lower bound for the EDSM problem. We use
the popular combinatorial Boolean matrix multiplication (BMM) conjecture
stating that there is no truly subcubic combinatorial algorithm for BMM [Abboud
and Williams, FOCS 2014]. By designing an appropriate reduction we show that a
combinatorial algorithm solving the EDSM problem in
$\mathcal{O}(nm^{1.5-\epsilon} + N)$ time, for any $\epsilon>0$, refutes this
conjecture. Of course, the notion of combinatorial algorithms is not clearly
defined, so our reduction should be understood as an indication that decreasing
the exponent requires fast matrix multiplication.
  Two standard tools used in algorithms on strings are string periodicity and
fast Fourier transform. Our main technical contribution is that we successfully
combine these tools with fast matrix multiplication to design a
non-combinatorial $\mathcal{O}(nm^{1.381} + N)$-time algorithm for EDSM. To the
best of our knowledge, we are the first to do so.
</summary>
    <author>
      <name>Giulia Bernardini</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Nadia Pisanti</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of paper in ICALP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.02298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.02298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.06138">
    <id>http://arxiv.org/abs/1905.06138v2</id>
    <updated>2020-01-02T12:16:32Z</updated>
    <published>2019-05-15T12:33:32Z</published>
    <title>Abelian periods of factors of Sturmian words</title>
    <summary>  We study the abelian period sets of Sturmian words, which are codings of
irrational rotations on a one-dimensional torus. The main result states that
the minimum abelian period of a factor of a Sturmian word of angle $\alpha$
with continued fraction expansion $[0; a_1, a_2, \ldots]$ is either $tq_k$ with
$1 \leq t \leq a_{k+1}$ (a multiple of a denominator $q_k$ of a convergent of
$\alpha$) or $q_{k,\ell}$ (a denominator $q_{k,\ell}$ of a semiconvergent of
$\alpha$). This result generalizes a result of Fici et. al stating that the
abelian period set of the Fibonacci word is the set of Fibonacci numbers. A
characterization of the Fibonacci word in terms of its abelian period set is
obtained as a corollary.
</summary>
    <author>
      <name>Jarkko Peltom√§ki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.06138v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06138v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15, 11A55" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.05459">
    <id>http://arxiv.org/abs/1904.05459v2</id>
    <updated>2019-05-09T08:42:20Z</updated>
    <published>2019-04-10T21:52:07Z</published>
    <title>Constant factor approximations to edit distance on far input pairs in
  nearly linear time</title>
    <summary>  For any $T \geq 1$, there are constants $R=R(T) \geq 1$ and
$\zeta=\zeta(T)>0$ and a randomized algorithm that takes as input an integer
$n$ and two strings $x,y$ of length at most $n$, and runs in time
$O(n^{1+\frac{1}{T}})$ and outputs an upper bound $U$ on the edit distance
$ED(x,y)$ that with high probability, satisfies $U \leq
R(ED(x,y)+n^{1-\zeta})$. In particular, on any input with $ED(x,y) \geq
n^{1-\zeta}$ the algorithm outputs a constant factor approximation with high
probability.
  A similar result has been proven independently by Brakensiek and Rubinstein
(2019).
</summary>
    <author>
      <name>Michal Kouck√Ω</name>
    </author>
    <author>
      <name>Michael E. Saks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected typos. Revised argument in Section 4.9, results unchanged</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05459v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05459v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.05459">
    <id>http://arxiv.org/abs/1904.05459v2</id>
    <updated>2019-05-09T08:42:20Z</updated>
    <published>2019-04-10T21:52:07Z</published>
    <title>Constant factor approximations to edit distance on far input pairs in
  nearly linear time</title>
    <summary>  For any $T \geq 1$, there are constants $R=R(T) \geq 1$ and
$\zeta=\zeta(T)>0$ and a randomized algorithm that takes as input an integer
$n$ and two strings $x,y$ of length at most $n$, and runs in time
$O(n^{1+\frac{1}{T}})$ and outputs an upper bound $U$ on the edit distance
$ED(x,y)$ that with high probability, satisfies $U \leq
R(ED(x,y)+n^{1-\zeta})$. In particular, on any input with $ED(x,y) \geq
n^{1-\zeta}$ the algorithm outputs a constant factor approximation with high
probability.
  A similar result has been proven independently by Brakensiek and Rubinstein
(2019).
</summary>
    <author>
      <name>Michal Kouck√Ω</name>
    </author>
    <author>
      <name>Michael E. Saks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected typos. Revised argument in Section 4.9, results unchanged</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05459v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05459v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.05390">
    <id>http://arxiv.org/abs/1904.05390v2</id>
    <updated>2020-01-28T18:54:41Z</updated>
    <published>2019-04-10T18:55:56Z</published>
    <title>Constant-factor approximation of near-linear edit distance in
  near-linear time</title>
    <summary>  We show that the edit distance between two strings of length $n$ can be
computed within a factor of $f(\epsilon)$ in $n^{1+\epsilon}$ time as long as
the edit distance is at least $n^{1-\delta}$ for some $\delta(\epsilon) > 0$.
</summary>
    <author>
      <name>Joshua Brakensiek</name>
    </author>
    <author>
      <name>Aviad Rubinstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05390v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05390v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.05452">
    <id>http://arxiv.org/abs/1904.05452v1</id>
    <updated>2019-04-10T21:41:35Z</updated>
    <published>2019-04-10T21:41:35Z</published>
    <title>What Storage Access Privacy is Achievable with Small Overhead?</title>
    <summary>  Oblivious RAM (ORAM) and private information retrieval (PIR) are classic
cryptographic primitives used to hide the access pattern to data whose storage
has been outsourced to an untrusted server. Unfortunately, both primitives
require considerable overhead compared to plaintext access. For large-scale
storage infrastructure with highly frequent access requests, the degradation in
response time and the exorbitant increase in resource costs incurred by either
ORAM or PIR prevent their usage. In an ideal scenario, a privacy-preserving
storage protocols with small overhead would be implemented for these heavily
trafficked storage systems to avoid negatively impacting either performance
and/or costs. In this work, we study the problem of the best $\mathit{storage\
access\ privacy}$ that is achievable with only $\mathit{small\ overhead}$ over
plaintext access.
  To answer this question, we consider $\mathit{differential\ privacy\ access}$
which is a generalization of the $\mathit{oblivious\ access}$ security notion
that are considered by ORAM and PIR. Quite surprisingly, we present strong
evidence that constant overhead storage schemes may only be achieved with
privacy budgets of $\epsilon = \Omega(\log n)$. We present asymptotically
optimal constructions for differentially private variants of both ORAM and PIR
with privacy budgets $\epsilon = \Theta(\log n)$ with only $O(1)$ overhead. In
addition, we consider a more complex storage primitive called key-value storage
in which data is indexed by keys from a large universe (as opposed to
consecutive integers in ORAM and PIR). We present a differentially private
key-value storage scheme with $\epsilon = \Theta(\log n)$ and $O(\log\log n)$
overhead. This construction uses a new oblivious, two-choice hashing scheme
that may be of independent interest.
</summary>
    <author>
      <name>Sarvar Patel</name>
    </author>
    <author>
      <name>Giuseppe Persiano</name>
    </author>
    <author>
      <name>Kevin Yeo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at PODS'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.07467">
    <id>http://arxiv.org/abs/1904.07467v2</id>
    <updated>2019-11-16T07:37:22Z</updated>
    <published>2019-04-16T05:20:47Z</published>
    <title>Dynamic Packed Compact Tries Revisited</title>
    <summary>  Given a dynamic set $K$ of $k$ strings of total length $n$ whose characters
are drawn from an alphabet of size $\sigma$, a keyword dictionary is a data
structure built on $K$ that provides lookup, prefix search, and update
operations on $K$. Under the assumption that $\alpha = w/ \lg \sigma$
characters fit into a single machine word of $w$ bits, we propose a keyword
dictionary that represents $K$ in either $n \lg \sigma + \Theta(k \lg n)$ or
$|T| \lg \sigma + \Theta(k w)$ bits of space, where $|T|$ is the number of
nodes of a trie representing $K$. It supports all operations in $O(m / \alpha +
\lg \alpha)$ expected time on an input string of length $m$ in the word RAM
model. An exhaustive practical evaluation highlights the practical usefulness
of the proposed data structure, especially for prefix searches - one of the
most essential keyword dictionary operations.
</summary>
    <author>
      <name>Kazuya Tsuruta</name>
    </author>
    <author>
      <name>Dominik K√∂ppl</name>
    </author>
    <author>
      <name>Shunsuke Kanda</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1904.07467v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07467v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.07619">
    <id>http://arxiv.org/abs/1904.07619v3</id>
    <updated>2020-02-27T09:28:13Z</updated>
    <published>2019-04-16T12:27:24Z</published>
    <title>Compressed Indexes for Fast Search of Semantic Data</title>
    <summary>  The sheer increase in volume of RDF data demands efficient solutions for the
triple indexing problem, that is devising a compressed data structure to
compactly represent RDF triples by guaranteeing, at the same time, fast pattern
matching operations. This problem lies at the heart of delivering good
practical performance for the resolution of complex SPARQL queries on large RDF
datasets. In this work, we propose a trie-based index layout to solve the
problem and introduce two novel techniques to reduce its space of
representation for improved effectiveness. The extensive experimental analysis
conducted over a wide range of publicly available real-world datasets, reveals
that our best space/time trade-off configuration substantially outperforms
existing solutions at the state-of-the-art, by taking 30-60% less space and
speeding up query execution by a factor of 2-81x.
</summary>
    <author>
      <name>Raffaele Perego</name>
    </author>
    <author>
      <name>Giulio Ermanno Pibiri</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TKDE.2020.2966609</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TKDE.2020.2966609" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in IEEE Transactions on Knowledge and Data Engineering
  (TKDE), 14 January 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.07619v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07619v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.10028">
    <id>http://arxiv.org/abs/1904.10028v1</id>
    <updated>2019-04-22T18:52:01Z</updated>
    <published>2019-04-22T18:52:01Z</published>
    <title>Repetitions in infinite palindrome-rich words</title>
    <summary>  Rich words are characterized by containing the maximum possible number of
distinct palindromes. Several characteristic properties of rich words have been
studied; yet the analysis of repetitions in rich words still involves some
interesting open problems. We address lower bounds on the repetition threshold
of infinite rich words over 2 and 3-letter alphabets, and construct a candidate
infinite rich word over the alphabet $\Sigma_2=\{0,1\}$ with a small critical
exponent of $2+\sqrt{2}/2$. This represents the first progress on an open
problem of Vesti from 2017.
</summary>
    <author>
      <name>Aseem Raj Baranwal</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-28796-2_7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-28796-2_7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.10028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.10028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.09157">
    <id>http://arxiv.org/abs/1904.09157v1</id>
    <updated>2019-04-19T11:53:45Z</updated>
    <published>2019-04-19T11:53:45Z</published>
    <title>New results on pseudosquare avoidance</title>
    <summary>  We start by considering binary words containing the minimum possible numbers
of squares and antisquares (where an antisquare is a word of the form $x
\overline{x}$), and we completely classify which possibilities can occur. We
consider avoiding $x p(x)$, where $p$ is any permutation of the underlying
alphabet, and $x t(x)$, where $t$ is any transformation of the underlying
alphabet. Finally, we prove the existence of an infinite binary word
simultaneously avoiding all occurrences of $x h(x)$ for every nonerasing
morphism $h$ and all sufficiently large words $x$.
</summary>
    <author>
      <name>Tim Ng</name>
    </author>
    <author>
      <name>Pascal Ochem</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/1904.09157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.09125">
    <id>http://arxiv.org/abs/1904.09125v2</id>
    <updated>2019-05-24T08:57:45Z</updated>
    <published>2019-04-19T09:09:49Z</published>
    <title>k-Spectra of weakly-c-Balanced Words</title>
    <summary>  A word $u$ is a scattered factor of $w$ if $u$ can be obtained from $w$ by
deleting some of its letters. That is, there exist the (potentially empty)
words $u_1,u_2,..., u_n$, and $v_0,v_1,..,v_n$ such that $u = u_1u_2...u_n$ and
$w = v_0u_1v_1u_2v_2...u_nv_n$. We consider the set of length-$k$ scattered
factors of a given word w, called here $k$-spectrum and denoted
$\ScatFact_k(w)$. We prove a series of properties of the sets $\ScatFact_k(w)$
for binary strictly balanced and, respectively, $c$-balanced words $w$, i.e.,
words over a two-letter alphabet where the number of occurrences of each letter
is the same, or, respectively, one letter has $c$-more occurrences than the
other. In particular, we consider the question which cardinalities $n=
|\ScatFact_k(w)|$ are obtainable, for a positive integer $k$, when $w$ is
either a strictly balanced binary word of length $2k$, or a $c$-balanced binary
word of length $2k-c$. We also consider the problem of reconstructing words
from their $k$-spectra.
</summary>
    <author>
      <name>Joel D. Day</name>
    </author>
    <author>
      <name>Pamela Fleischmann</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Dirk Nowotka</name>
    </author>
    <link href="http://arxiv.org/abs/1904.09125v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09125v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.13369">
    <id>http://arxiv.org/abs/1904.13369v2</id>
    <updated>2019-06-22T14:34:56Z</updated>
    <published>2019-04-30T17:04:08Z</published>
    <title>Constrained Orthogonal Segment Stabbing</title>
    <summary>  Let $S$ and $D$ each be a set of orthogonal line segments in the plane. A
line segment $s\in S$ \emph{stabs} a line segment $s'\in D$ if $s\cap
s'\neq\emptyset$. It is known that the problem of stabbing the line segments in
$D$ with the minimum number of line segments of $S$ is NP-hard. However, no
better than $O(\log |S\cup D|)$-approximation is known for the problem. In this
paper, we introduce a constrained version of this problem in which every
horizontal line segment of $S\cup D$ intersects a common vertical line. We
study several versions of the problem, depending on which line segments are
used for stabbing and which line segments must be stabbed. We obtain several
NP-hardness and constant approximation results for these versions. Our finding
implies, the problem remains NP-hard even under the extra assumption on input,
but small constant approximation algorithms can be designed.
</summary>
    <author>
      <name>Sayan Bandyapadhyay</name>
    </author>
    <author>
      <name>Saeed Mehrabi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear at CCCG 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.13369v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.13369v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1606.06617">
    <id>http://arxiv.org/abs/1606.06617v4</id>
    <updated>2017-10-10T08:23:45Z</updated>
    <published>2016-06-21T15:26:55Z</published>
    <title>A Self-Index on Block Trees</title>
    <summary>  The Block Tree is a recently proposed data structure that reaches compression
close to Lempel-Ziv while supporting efficient direct access to text
substrings. In this paper we show how a self-index can be built on top of a
Block Tree so that it provides efficient pattern searches while using space
proportional to that of the original data structure. More precisely, if a
Lempel-Ziv parse cuts a text of length $n$ into $z$ non-overlapping phrases,
then our index uses $O(z\log(n/z))$ words and finds the $occ$ occurrences of a
pattern of length $m$ in time $O(m\log n+occ\log^\epsilon n)$ for any constant
$\epsilon>0$.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 3 is the final SPIRE 2017 version. Version 4 corrects some
  errors and typos, the important one about what is inserted in the grid. It
  also improves the time when O(w) space can be used</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06617v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06617v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.12312">
    <id>http://arxiv.org/abs/1903.12312v1</id>
    <updated>2019-03-29T01:09:50Z</updated>
    <published>2019-03-29T01:09:50Z</published>
    <title>Data structures to represent sets of k-long DNA sequences</title>
    <summary>  The analysis of biological sequencing data has been one of the biggest
applications of string algorithms. The approaches used in many such
applications are based on the analysis of k-mers, which are short fixed-length
strings present in a dataset. While these approaches are rather diverse,
storing and querying k-mer sets has emerged as a shared underlying component.
Sets of k-mers have unique features and applications that, over the last ten
years, have resulted in many specialized approaches for their representation.
In this survey, we give a unified presentation and comparison of the data
structures that have been proposed to store and query k-mer sets. We hope this
survey will not only serve as a resource for researchers in the field but also
make the area more accessible to outsiders
</summary>
    <author>
      <name>Rayan Chikhi</name>
    </author>
    <author>
      <name>Jan Holub</name>
    </author>
    <author>
      <name>Paul Medvedev</name>
    </author>
    <link href="http://arxiv.org/abs/1903.12312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.12312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.12449">
    <id>http://arxiv.org/abs/1903.12449v1</id>
    <updated>2019-03-29T11:05:51Z</updated>
    <published>2019-03-29T11:05:51Z</published>
    <title>Multiplication method for factoring natural numbers</title>
    <summary>  We offer multiplication method for factoring big natural numbers which
extends the group of the Fermat's and Lehman's factorization algorithms and has
run-time complexity $O(n^{1/3})$. This paper is argued the finiteness of
proposed algorithm depending on the value of the factorizable number n. We
provide here comparative tests results of related algorithms on a large amount
of computational checks. We describe identified advantages of the proposed
algorithm over others. The possibilities of algorithm optimization for reducing
the complexity of factorization are also shown here.
</summary>
    <author>
      <name>Igor Nesiolovskiy</name>
    </author>
    <author>
      <name>Artem Nesiolovskiy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.12449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.12449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.12525">
    <id>http://arxiv.org/abs/1903.12525v1</id>
    <updated>2019-03-17T20:19:23Z</updated>
    <published>2019-03-17T20:19:23Z</published>
    <title>Shed More Light on Bloom Filter's Variants</title>
    <summary>  Bloom Filter is a probabilistic membership data structure and it is
excessively used data structure for membership query. Bloom Filter becomes the
predominant data structure in approximate membership filtering. Bloom Filter
extremely enhances the query response time, and the response time is very fast.
Bloom filter (BF) is used to detect whether an element belongs to a given set
or not. The Bloom Filter returns True Positive (TP), False Positive (FP), or
True Negative (TN). The Bloom Filter is widely adapted in numerous areas to
enhance the performance of a system. In this paper, we present a) in-depth
insight on the Bloom Filter,and b) the prominent variants of the Bloom Filters.
</summary>
    <author>
      <name>Ripon Patgiri</name>
    </author>
    <author>
      <name>Sabuzima Nayak</name>
    </author>
    <author>
      <name>Samir Kumar Borgohain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 Figures, 1 Table, Proceedings of the 2018 International
  Conference on Information and Knowledge Engineering (IKE'18), pp. 14-21</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2018 International Conference on Information
  and Knowledge Engineering (IKE'18), pp. 14-21, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.12525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.12525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1202.3311">
    <id>http://arxiv.org/abs/1202.3311v1</id>
    <updated>2012-02-15T14:13:02Z</updated>
    <published>2012-02-15T14:13:02Z</published>
    <title>Speeding-up $q$-gram mining on grammar-based compressed texts</title>
    <summary>  We present an efficient algorithm for calculating $q$-gram frequencies on
strings represented in compressed form, namely, as a straight line program
(SLP). Given an SLP $\mathcal{T}$ of size $n$ that represents string $T$, the
algorithm computes the occurrence frequencies of all $q$-grams in $T$, by
reducing the problem to the weighted $q$-gram frequencies problem on a
trie-like structure of size $m = |T|-\mathit{dup}(q,\mathcal{T})$, where
$\mathit{dup}(q,\mathcal{T})$ is a quantity that represents the amount of
redundancy that the SLP captures with respect to $q$-grams. The reduced problem
can be solved in linear time. Since $m = O(qn)$, the running time of our
algorithm is $O(\min\{|T|-\mathit{dup}(q,\mathcal{T}),qn\})$, improving our
previous $O(qn)$ algorithm when $q = \Omega(|T|/n)$.
</summary>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-31265-6_18</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-31265-6_18" rel="related"/>
    <link href="http://arxiv.org/abs/1202.3311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1304.5373">
    <id>http://arxiv.org/abs/1304.5373v2</id>
    <updated>2014-06-06T13:40:56Z</updated>
    <published>2013-04-19T11:11:36Z</published>
    <title>Compact q-gram Profiling of Compressed Strings</title>
    <summary>  We consider the problem of computing the q-gram profile of a string \str of
size $N$ compressed by a context-free grammar with $n$ production rules. We
present an algorithm that runs in $O(N-\alpha)$ expected time and uses
$O(n+q+\kq)$ space, where $N-\alpha\leq qn$ is the exact number of characters
decompressed by the algorithm and $\kq\leq N-\alpha$ is the number of distinct
q-grams in $\str$. This simultaneously matches the current best known time
bound and improves the best known space bound. Our space bound is
asymptotically optimal in the sense that any algorithm storing the grammar and
the q-gram profile must use $\Omega(n+q+\kq)$ space. To achieve this we
introduce the q-gram graph that space-efficiently captures the structure of a
string with respect to its q-grams, and show how to construct it from a
grammar.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Patrick Hagge Cording</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <link href="http://arxiv.org/abs/1304.5373v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.5373v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.02809">
    <id>http://arxiv.org/abs/1904.02809v2</id>
    <updated>2019-07-02T09:49:48Z</updated>
    <published>2019-04-04T22:20:12Z</published>
    <title>Proving tree algorithms for succinct data structures</title>
    <summary>  Succinct data structures give space-efficient representations of large
amounts of data without sacrificing performance. They rely one cleverly
designed data representations and algorithms. We present here the formalization
in Coq/SSReflect of two different tree-based succinct representations and their
accompanying algorithms. One is the Level-Order Unary Degree Sequence, which
encodes the structure of a tree in breadth-first order as a sequence of bits,
where access operations can be defined in terms of Rank and Select, which work
in constant time for static bit sequences. The other represents dynamic bit
sequences as binary balanced trees, where Rank and Select present a low
logarithmic overhead compared to their static versions, and with efficient
insertion and deletion. The two can be stacked to provide a dynamic
representation of dictionaries for instance. While both representations are
well-known, we believe this to be their first formalization and a needed step
towards provably-safe implementations of big data.
</summary>
    <author>
      <name>Reynald Affeldt</name>
    </author>
    <author>
      <name>Jacques Garrigue</name>
    </author>
    <author>
      <name>Xuanrui Qi</name>
    </author>
    <author>
      <name>Kazunari Tanaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 10th International Conference on Interactive Theorem
  Proving (ITP 2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.02809v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.02809v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.3.1; E.1; D.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.08162">
    <id>http://arxiv.org/abs/1811.08162v1</id>
    <updated>2018-11-20T10:12:55Z</updated>
    <published>2018-11-20T10:12:55Z</published>
    <title>DeepZip: Lossless Data Compression using Recurrent Neural Networks</title>
    <summary>  Sequential data is being generated at an unprecedented pace in various forms,
including text and genomic data. This creates the need for efficient
compression mechanisms to enable better storage, transmission and processing of
such data. To solve this problem, many of the existing compressors attempt to
learn models for the data and perform prediction-based compression. Since
neural networks are known as universal function approximators with the
capability to learn arbitrarily complex mappings, and in practice show
excellent performance in prediction tasks, we explore and devise methods to
compress sequential data using neural network predictors. We combine recurrent
neural network predictors with an arithmetic coder and losslessly compress a
variety of synthetic, text and genomic datasets. The proposed compressor
outperforms Gzip on the real datasets and achieves near-optimal compression for
the synthetic datasets. The results also help understand why and where neural
networks are good alternatives for traditional finite context models
</summary>
    <author>
      <name>Mohit Goyal</name>
    </author>
    <author>
      <name>Kedar Tatwawadi</name>
    </author>
    <author>
      <name>Shubham Chandak</name>
    </author>
    <author>
      <name>Idoia Ochoa</name>
    </author>
    <link href="http://arxiv.org/abs/1811.08162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.08162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.04228">
    <id>http://arxiv.org/abs/1904.04228v2</id>
    <updated>2019-05-05T21:52:51Z</updated>
    <published>2019-04-08T17:56:21Z</published>
    <title>String Synchronizing Sets: Sublinear-Time BWT Construction and Optimal
  LCE Data Structure</title>
    <summary>  Burrows-Wheeler transform (BWT) is an invertible text transformation that,
given a text $T$ of length $n$, permutes its symbols according to the
lexicographic order of suffixes of $T$. BWT is one of the most heavily studied
algorithms in data compression with numerous applications in indexing, sequence
analysis, and bioinformatics. Its construction is a bottleneck in many
scenarios, and settling the complexity of this task is one of the most
important unsolved problems in sequence analysis that has remained open for 25
years. Given a binary string of length $n$, occupying $O(n/\log n)$ machine
words, the BWT construction algorithm due to Hon et al. (SIAM J. Comput., 2009)
runs in $O(n)$ time and $O(n/\log n)$ space. Recent advancements (Belazzougui,
STOC 2014, and Munro et al., SODA 2017) focus on removing the alphabet-size
dependency in the time complexity, but they still require $\Omega(n)$ time.
  In this paper, we propose the first algorithm that breaks the $O(n)$-time
barrier for BWT construction. Given a binary string of length $n$, our
procedure builds the Burrows-Wheeler transform in $O(n/\sqrt{\log n})$ time and
$O(n/\log n)$ space. We complement this result with a conditional lower bound
proving that any further progress in the time complexity of BWT construction
would yield faster algorithms for the very well studied problem of counting
inversions: it would improve the state-of-the-art $O(m\sqrt{\log m})$-time
solution by Chan and P\v{a}tra\c{s}cu (SODA 2010). Our algorithm is based on a
novel concept of string synchronizing sets, which is of independent interest.
As one of the applications, we show that this technique lets us design a data
structure of the optimal size $O(n/\log n)$ that answers Longest Common
Extension queries (LCE queries) in $O(1)$ time and, furthermore, can be
deterministically constructed in the optimal $O(n/\log n)$ time.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper accepted to STOC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.04228v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.04228v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.04513">
    <id>http://arxiv.org/abs/1904.04513v3</id>
    <updated>2019-07-02T10:26:27Z</updated>
    <published>2019-04-09T08:05:42Z</published>
    <title>Suffix Trees, DAWGs and CDAWGs for Forward and Backward Tries</title>
    <summary>  The suffix tree, DAWG, and CDAWG are fundamental indexing structures of a
string, with a number of applications in bioinformatics, information retrieval,
data mining, etc. An edge-labeled rooted tree (trie) is a natural
generalization of a string, which can also be seen as a compact representation
of a set of strings. Breslauer [TCS 191(1-2): 131-144, 1998] proposed the
suffix tree for a backward trie, where the strings in the trie are read in the
leaf-to-root direction. In contrast to a backward trie, we call a usual trie as
a forward trie. Despite a few follow-up works after Breslauer's paper, indexing
forward/backward tries is not well understood yet. In this paper, we show a
full perspective on the sizes of indexing structures such as suffix trees,
DAWGs, and CDAWGs for forward and backward tries. In particular, we show that
the size of the DAWG for a forward trie with $n$ nodes is $\Omega(\sigma n)$,
where $\sigma$ is the number of distinct characters in the trie. This becomes
$\Omega(n^2)$ for a large alphabet. Still, we show that there is a compact
$O(n)$-space representation of the DAWG for a forward trie over any alphabet,
and present an $O(n)$-time and space algorithm to construct such a
representation of the DAWG for a given forward trie.
</summary>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <link href="http://arxiv.org/abs/1904.04513v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.04513v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1001.1565">
    <id>http://arxiv.org/abs/1001.1565v3</id>
    <updated>2013-10-29T08:44:10Z</updated>
    <published>2010-01-11T20:29:41Z</published>
    <title>Random Access to Grammar Compressed Strings</title>
    <summary>  Grammar based compression, where one replaces a long string by a small
context-free grammar that generates the string, is a simple and powerful
paradigm that captures many popular compression schemes. In this paper, we
present a novel grammar representation that allows efficient random access to
any character or substring without decompressing the string.
  Let $S$ be a string of length $N$ compressed into a context-free grammar
$\mathcal{S}$ of size $n$. We present two representations of $\mathcal{S}$
achieving $O(\log N)$ random access time, and either $O(n\cdot \alpha_k(n))$
construction time and space on the pointer machine model, or $O(n)$
construction time and space on the RAM. Here, $\alpha_k(n)$ is the inverse of
the $k^{th}$ row of Ackermann's function. Our representations also efficiently
support decompression of any substring in $S$: we can decompress any substring
of length $m$ in the same complexity as a single random access query and
additional $O(m)$ time. Combining these results with fast algorithms for
uncompressed approximate string matching leads to several efficient algorithms
for approximate string matching on grammar-compressed strings without
decompression. For instance, we can find all approximate occurrences of a
pattern $P$ with at most $k$ errors in time $O(n(\min\{|P|k, k^4 + |P|\} + \log
N) + occ)$, where $occ$ is the number of occurrences of $P$ in $S$. Finally, we
generalize our results to navigation and other operations on grammar-compressed
ordered trees.
  All of the above bounds significantly improve the currently best known
results. To achieve these bounds, we introduce several new techniques and data
structures of independent interest, including a predecessor data structure, two
"biased" weighted ancestor data structures, and a compact representation of
heavy paths in grammars.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version in SODA 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.1565v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.1565v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1203.1080">
    <id>http://arxiv.org/abs/1203.1080v2</id>
    <updated>2012-05-03T15:45:08Z</updated>
    <published>2012-03-06T00:52:14Z</published>
    <title>Data Structure Lower Bounds on Random Access to Grammar-Compressed
  Strings</title>
    <summary>  In this paper we investigate the problem of building a static data structure
that represents a string s using space close to its compressed size, and allows
fast access to individual characters of s. This type of structures was
investigated by the recent paper of Bille et al. Let n be the size of a
context-free grammar that derives a unique string s of length L. (Note that L
might be exponential in n.) Bille et al. showed a data structure that uses
space O(n) and allows to query for the i-th character of s using running time
O(log L). Their data structure works on a word RAM with a word size of logL
bits. Here we prove that for such data structures, if the space is poly(n),
then the query time must be at least (log L)^{1-\epsilon}/log S where S is the
space used, for any constant eps>0. As a function of n, our lower bound is
\Omega(n^{1/2-\epsilon}). Our proof holds in the cell-probe model with a word
size of log L bits, so in particular it holds in the word RAM model. We show
that no lower bound significantly better than n^{1/2-\epsilon} can be achieved
in the cell-probe model, since there is a data structure in the cell-probe
model that uses O(n) space and achieves O(\sqrt{n log n}) query time. The "bad"
setting of parameters occurs roughly when L=2^{\sqrt{n}}. We also prove a lower
bound for the case of not-as-compressible strings, where, say,
L=n^{1+\epsilon}. For this case, we prove that if the space is n polylog(n),
then the query time must be at least \Omega(log n/loglog n).
  The proof works by reduction to communication complexity, namely to the LSD
problem, recently employed by Patrascu and others. We prove lower bounds also
for the case of LZ-compression and Burrows-Wheeler (BWT) compression. All of
our lower bounds hold even when the strings are over an alphabet of size 2 and
hold even for randomized data structures with 2-sided error.
</summary>
    <author>
      <name>Shiteng Chen</name>
    </author>
    <author>
      <name>Elad Verbin</name>
    </author>
    <author>
      <name>Wei Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to ICALP 2012, with strengthened results included</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.1080v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.1080v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1207.4607">
    <id>http://arxiv.org/abs/1207.4607v1</id>
    <updated>2012-07-19T10:28:56Z</updated>
    <published>2012-07-19T10:28:56Z</published>
    <title>Efficient LZ78 factorization of grammar compressed text</title>
    <summary>  We present an efficient algorithm for computing the LZ78 factorization of a
text, where the text is represented as a straight line program (SLP), which is
a context free grammar in the Chomsky normal form that generates a single
string. Given an SLP of size $n$ representing a text $S$ of length $N$, our
algorithm computes the LZ78 factorization of $T$ in $O(n\sqrt{N}+m\log N)$ time
and $O(n\sqrt{N}+m)$ space, where $m$ is the number of resulting LZ78 factors.
We also show how to improve the algorithm so that the $n\sqrt{N}$ term in the
time and space complexities becomes either $nL$, where $L$ is the length of the
longest LZ78 factor, or $(N - \alpha)$ where $\alpha \geq 0$ is a quantity
which depends on the amount of redundancy that the SLP captures with respect to
substrings of $S$ of a certain length. Since $m = O(N/\log_\sigma N)$ where
$\sigma$ is the alphabet size, the latter is asymptotically at least as fast as
a linear time algorithm which runs on the uncompressed string when $\sigma$ is
constant, and can be more efficient when the text is compressible, i.e. when
$m$ and $n$ are small.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-34109-0_10</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-34109-0_10" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SPIRE 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.4607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.03561">
    <id>http://arxiv.org/abs/1808.03561v1</id>
    <updated>2018-08-10T14:26:29Z</updated>
    <published>2018-08-10T14:26:29Z</published>
    <title>Finding a Small Number of Colourful Components</title>
    <summary>  A partition $(V_1,\ldots,V_k)$ of the vertex set of a graph $G$ with a (not
necessarily proper) colouring $c$ is colourful if no two vertices in any $V_i$
have the same colour and every set $V_i$ induces a connected graph. The
COLOURFUL PARTITION problem is to decide whether a coloured graph $(G,c)$ has a
colourful partition of size at most $k$. This problem is closely related to the
COLOURFUL COMPONENTS problem, which is to decide whether a graph can be
modified into a graph whose connected components form a colourful partition by
deleting at most $p$ edges. Nevertheless we show that COLOURFUL PARTITION and
COLOURFUL COMPONENTS may have different complexities for restricted instances.
We tighten known NP-hardness results for both problems and in addition we prove
new hardness and tractability results for COLOURFUL PARTITION. Using these
results we complete our paper with a thorough parameterized study of COLOURFUL
PARTITION.
</summary>
    <author>
      <name>Laurent Bulteau</name>
    </author>
    <author>
      <name>Konrad K. Dabrowski</name>
    </author>
    <author>
      <name>Guillaume Fertin</name>
    </author>
    <author>
      <name>Matthew Johnson</name>
    </author>
    <author>
      <name>Daniel Paulusma</name>
    </author>
    <author>
      <name>Stephane Vialette</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0910.3123">
    <id>http://arxiv.org/abs/0910.3123v2</id>
    <updated>2010-02-19T09:19:42Z</updated>
    <published>2009-10-16T13:50:12Z</published>
    <title>Wee LCP</title>
    <summary>  We prove that longest common prefix (LCP) information can be stored in much
less space than previously known. More precisely, we show that in the presence
of the text and the suffix array, o(n) additional bits are sufficient to answer
LCP-queries asymptotically in the same time that is needed to retrieve an entry
from the suffix array. This yields the smallest compressed suffix tree with
sub-logarithmic navigation time.
</summary>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <link href="http://arxiv.org/abs/0910.3123v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.3123v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.10081">
    <id>http://arxiv.org/abs/1903.10081v8</id>
    <updated>2019-10-05T16:47:14Z</updated>
    <published>2019-03-24T23:38:27Z</published>
    <title>Determining satisfiability of 3-SAT in polynomial time</title>
    <summary>  In this paper, we provide a polynomial time (and space), algorithm that
determines satisfiability of 3-SAT. The complexity analysis for the algorithm
takes into account no efficiency and yet provides a low enough bound, that
efficient versions are practical with respect to today's hardware. We accompany
this paper with a serial version of the algorithm without non-trivial
efficiencies (link: polynomial3sat.org).
</summary>
    <author>
      <name>Ortho Flint</name>
    </author>
    <author>
      <name>Asanka Wickramasinghe</name>
    </author>
    <author>
      <name>Jason Brasse</name>
    </author>
    <author>
      <name>Christopher Fowler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Versions 1-7 are missing cases in the theorem. Version 8 is complete</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.10081v8" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10081v8" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.10583">
    <id>http://arxiv.org/abs/1903.10583v1</id>
    <updated>2019-03-25T20:21:17Z</updated>
    <published>2019-03-25T20:21:17Z</published>
    <title>Algorithms to compute the Burrows-Wheeler Similarity Distribution</title>
    <summary>  The Burrows-Wheeler transform (BWT) is a well studied text transformation
widely used in data compression and text indexing. The BWT of two strings can
also provide similarity measures between them, based on the observation that
the more their symbols are intermixed in the transformation, the more the
strings are similar. In this article we present two new algorithms to compute
similarity measures based on the BWT for string collections. In particular, we
present practical and theoretical improvements to the computation of the
Burrows-Wheeler similarity distribution for all pairs of strings in a
collection. Our algorithms take advantage of the BWT computed for the
concatenation of all strings, and use compressed data structures that allow
reducing the running time with a small memory footprint, as shown by a set of
experiments with real and artificial datasets.
</summary>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Guilherme P. Telles</name>
    </author>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Liang Zhao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2019.03.012</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2019.03.012" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to TCS</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.10583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0908.0239">
    <id>http://arxiv.org/abs/0908.0239v1</id>
    <updated>2009-08-03T13:10:48Z</updated>
    <published>2009-08-03T13:10:48Z</published>
    <title>On Bijective Variants of the Burrows-Wheeler Transform</title>
    <summary>  The sort transform (ST) is a modification of the Burrows-Wheeler transform
(BWT). Both transformations map an arbitrary word of length n to a pair
consisting of a word of length n and an index between 1 and n. The BWT sorts
all rotation conjugates of the input word, whereas the ST of order k only uses
the first k letters for sorting all such conjugates. If two conjugates start
with the same prefix of length k, then the indices of the rotations are used
for tie-breaking. Both transforms output the sequence of the last letters of
the sorted list and the index of the input within the sorted list. In this
paper, we discuss a bijective variant of the BWT (due to Scott), proving its
correctness and relations to other results due to Gessel and Reutenauer (1993)
and Crochemore, Desarmenien, and Perrin (2005). Further, we present a novel
bijective variant of the ST.
</summary>
    <author>
      <name>Manfred Kufleitner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, presented at the Prague Stringology Conference 2009 (PSC
  2009)</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.0239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1410.3438">
    <id>http://arxiv.org/abs/1410.3438v2</id>
    <updated>2015-06-29T13:25:45Z</updated>
    <published>2014-10-13T19:02:40Z</published>
    <title>Efficient and Compact Representations of Prefix Codes</title>
    <summary>  Most of the attention in statistical compression is given to the space used
by the compressed sequence, a problem completely solved with optimal prefix
codes. However, in many applications, the storage space used to represent the
prefix code itself can be an issue. In this paper we introduce and compare
several techniques to store prefix codes. Let $N$ be the sequence length and
$n$ be the alphabet size. Then a naive storage of an optimal prefix code uses
$O(n\log n)$ bits. Our first technique shows how to use $O(n\log\log(N/n))$
bits to store the optimal prefix code. Then we introduce an approximate
technique that, for any $0&lt;\epsilon&lt;1/2$, takes $O(n \log \log (1 / \epsilon))$
bits to store a prefix code with average codeword length within an additive
$\epsilon$ of the minimum. Finally, a second approximation takes, for any
constant $c > 1$, $O(n^{1 / c} \log n)$ bits to store a prefix code with
average codeword length at most $c$ times the minimum. In all cases, our data
structures allow encoding and decoding of any symbol in $O(1)$ time. We
experimentally compare our new techniques with the state of the art, showing
that we achieve 6--8-fold space reductions, at the price of a slower encoding
(2.5--8 times slower) and decoding (12--24 times slower). The approximations
further reduce this space and improve the time significantly, up to recovering
the speed of classical implementations, for a moderate penalty in the average
code length. As a byproduct, we compare various heuristic, approximate, and
optimal algorithms to generate length-restricted codes, showing that the
optimal ones are clearly superior and practical enough to be implemented.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Alberto Ord√≥√±ez</name>
    </author>
    <link href="http://arxiv.org/abs/1410.3438v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.3438v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1412.0967">
    <id>http://arxiv.org/abs/1412.0967v1</id>
    <updated>2014-12-02T16:37:37Z</updated>
    <published>2014-12-02T16:37:37Z</published>
    <title>Queries on LZ-Bounded Encodings</title>
    <summary>  We describe a data structure that stores a string $S$ in space similar to
that of its Lempel-Ziv encoding and efficiently supports access, rank and
select queries. These queries are fundamental for implementing succinct and
compressed data structures, such as compressed trees and graphs. We show that
our data structure can be built in a scalable manner and is both small and fast
in practice compared to other data structures supporting such queries.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Juha K√§rkk√§inen</name>
    </author>
    <author>
      <name>Alberto Ord√≥√±ez</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/1412.0967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.0967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1606.05031">
    <id>http://arxiv.org/abs/1606.05031v1</id>
    <updated>2016-06-16T02:55:39Z</updated>
    <published>2016-06-16T02:55:39Z</published>
    <title>Scalable Partial Least Squares Regression on Grammar-Compressed Data
  Matrices</title>
    <summary>  With massive high-dimensional data now commonplace in research and industry,
there is a strong and growing demand for more scalable computational techniques
for data analysis and knowledge discovery. Key to turning these data into
knowledge is the ability to learn statistical models with high
interpretability. Current methods for learning statistical models either
produce models that are not interpretable or have prohibitive computational
costs when applied to massive data. In this paper we address this need by
presenting a scalable algorithm for partial least squares regression (PLS),
which we call compression-based PLS (cPLS), to learn predictive linear models
with a high interpretability from massive high-dimensional data. We propose a
novel grammar-compressed representation of data matrices that supports fast row
and column access while the data matrix is in a compressed form. The original
data matrix is grammar-compressed and then the linear model in PLS is learned
on the compressed data matrix, which results in a significant reduction in
working space, greatly improving scalability. We experimentally test cPLS on
its ability to learn linear models for classification, regression and feature
extraction with various massive high-dimensional data, and show that cPLS
performs superiorly in terms of prediction accuracy, computational efficiency,
and interpretability.
</summary>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <author>
      <name>Hiroto Saigo</name>
    </author>
    <author>
      <name>Yoshihiro Yamanishi</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be appeared in the Proceedings of KDD'16</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1611.01479">
    <id>http://arxiv.org/abs/1611.01479v1</id>
    <updated>2016-11-04T18:25:21Z</updated>
    <published>2016-11-04T18:25:21Z</published>
    <title>Space-Efficient Re-Pair Compression</title>
    <summary>  Re-Pair is an effective grammar-based compression scheme achieving strong
compression rates in practice. Let $n$, $\sigma$, and $d$ be the text length,
alphabet size, and dictionary size of the final grammar, respectively. In their
original paper, the authors show how to compute the Re-Pair grammar in expected
linear time and $5n + 4\sigma^2 + 4d + \sqrt{n}$ words of working space on top
of the text. In this work, we propose two algorithms improving on the space of
their original solution. Our model assumes a memory word of $\lceil\log_2
n\rceil$ bits and a re-writable input text composed by $n$ such words. Our
first algorithm runs in expected $\mathcal O(n/\epsilon)$ time and uses
$(1+\epsilon)n +\sqrt n$ words of space on top of the text for any parameter
$0&lt;\epsilon \leq 1$ chosen in advance. Our second algorithm runs in expected
$\mathcal O(n\log n)$ time and improves the space to $n +\sqrt n$ words.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1611.01479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1704.08558">
    <id>http://arxiv.org/abs/1704.08558v1</id>
    <updated>2017-04-27T13:28:45Z</updated>
    <published>2017-04-27T13:28:45Z</published>
    <title>Practical and Effective Re-Pair Compression</title>
    <summary>  Re-Pair is an efficient grammar compressor that operates by recursively
replacing high-frequency character pairs with new grammar symbols. The most
space-efficient linear-time algorithm computing Re-Pair uses
$(1+\epsilon)n+\sqrt n$ words on top of the re-writable text (of length $n$ and
stored in $n$ words), for any constant $\epsilon>0$; in practice however, this
solution uses complex sub-procedures preventing it from being practical. In
this paper, we present an implementation of the above-mentioned result making
use of more practical solutions; our tool further improves the working space to
$(1.5+\epsilon)n$ words (text included), for some small constant $\epsilon$. As
a second contribution, we focus on compact representations of the output
grammar. The lower bound for storing a grammar with $d$ rules is
$\log(d!)+2d\approx d\log d+0.557 d$ bits, and the most efficient encoding
algorithm in the literature uses at most $d\log d + 2d$ bits and runs in
$\mathcal O(d^{1.5})$ time. We describe a linear-time heuristic maximizing the
compressibility of the output Re-Pair grammar. On real datasets, our grammar
encoding uses---on average---only $2.8\%$ more bits than the
information-theoretic minimum. In half of the tested cases, our compressor
improves the output size of 7-Zip with maximum compression rate turned on.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1704.08558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.09538">
    <id>http://arxiv.org/abs/1705.09538v2</id>
    <updated>2017-07-25T11:15:37Z</updated>
    <published>2017-05-26T11:33:05Z</published>
    <title>On Two LZ78-style Grammars: Compression Bounds and Compressed-Space
  Computation</title>
    <summary>  We investigate two closely related LZ78-based compression schemes: LZMW (an
old scheme by Miller and Wegman) and LZD (a recent variant by Goto et al.).
Both LZD and LZMW naturally produce a grammar for a string of length $n$; we
show that the size of this grammar can be larger than the size of the smallest
grammar by a factor $\Omega(n^{\frac{1}3})$ but is always within a factor
$O((\frac{n}{\log n})^{\frac{2}{3}})$. In addition, we show that the standard
algorithms using $\Theta(z)$ working space to construct the LZD and LZMW
parsings, where $z$ is the size of the parsing, work in $\Omega(n^{\frac{5}4})$
time in the worst case. We then describe a new Las Vegas LZD/LZMW parsing
algorithm that uses $O (z \log n)$ space and $O(n + z \log^2 n)$ time w.h.p..
</summary>
    <author>
      <name>Golnaz Badkobeh</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, accepted to SPIRE 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.09538v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09538v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.07967">
    <id>http://arxiv.org/abs/1903.07967v1</id>
    <updated>2019-03-19T12:45:58Z</updated>
    <published>2019-03-19T12:45:58Z</published>
    <title>A New Lower Bound for Semigroup Orthogonal Range Searching</title>
    <summary>  We report the first improvement in the space-time trade-off of lower bounds
for the orthogonal range searching problem in the semigroup model, since
Chazelle's result from 1990. This is one of the very fundamental problems in
range searching with a long history. Previously, Andrew Yao's influential
result had shown that the problem is already non-trivial in one
dimension~\cite{Yao-1Dlb}: using $m$ units of space, the query time $Q(n)$ must
be $\Omega( \alpha(m,n) + \frac{n}{m-n+1})$ where $\alpha(\cdot,\cdot)$ is the
inverse Ackermann's function, a very slowly growing function.
  In $d$ dimensions, Bernard Chazelle~\cite{Chazelle.LB.II} proved that the
query time must be $Q(n) = \Omega( (\log_\beta n)^{d-1})$ where $\beta = 2m/n$.
Chazelle's lower bound is known to be tight for when space consumption is
`high' i.e., $m = \Omega(n \log^{d+\varepsilon}n)$. We have two main results.
The first is a lower bound that shows Chazelle's lower bound was not tight for
`low space': we prove that we must have $m (n) = \Omega(n (\log n \log\log
n)^{d-1})$. Our lower bound does not close the gap to the existing data
structures, however, our second result is that our analysis is tight. Thus, we
believe the gap is in fact natural since lower bounds are proven for idempotent
semigroups while the data structures are built for general semigroups and thus
they cannot assume (and use) the properties of an idempotent semigroup. As a
result, we believe to close the gap one must study lower bounds for
non-idempotent semigroups or building data structures for idempotent
semigroups. We develope significantly new ideas for both of our results that
could be useful in pursuing either of these directions.
</summary>
    <author>
      <name>Peyman Afshani</name>
    </author>
    <link href="http://arxiv.org/abs/1903.07967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.07775">
    <id>http://arxiv.org/abs/1903.07775v1</id>
    <updated>2019-03-19T00:04:32Z</updated>
    <published>2019-03-19T00:04:32Z</published>
    <title>QuickSort: Improved right-tail asymptotics for the limiting
  distribution, and large deviations</title>
    <summary>  We substantially refine asymptotic logarithmic upper bounds produced by
Svante Janson (2015) on the right tail of the limiting QuickSort distribution
function $F$ and by Fill and Hung (2018) on the right tails of the
corresponding density $f$ and of the absolute derivatives of $f$ of each order.
For example, we establish an upper bound on $\log[1 - F(x)]$ that matches
conjectured asymptotics of Knessl and Szpankowski (1999) through terms of order
$(\log x)^2$; the corresponding order for the Janson (2015) bound is the lead
order, $x \log x$.
  Using the refined asymptotic bounds on $F$, we derive right-tail large
deviation (LD) results for the distribution of the number of comparisons
required by QuickSort that substantially sharpen the two-sided LD results of
McDiarmid and Hayward (1996).
</summary>
    <author>
      <name>James Allen Fill</name>
    </author>
    <author>
      <name>Wei-Chun Hung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages; submitted for publication in January, 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.07775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P10 (Primary) 60E05, 60C05 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1304.0528">
    <id>http://arxiv.org/abs/1304.0528v1</id>
    <updated>2013-04-02T04:41:33Z</updated>
    <published>2013-04-02T04:41:33Z</published>
    <title>Efficient repeat finding via suffix arrays</title>
    <summary>  We solve the problem of finding interspersed maximal repeats using a suffix
array construction. As it is well known, all the functionality of suffix trees
can be handled by suffix arrays, gaining practicality. Our solution improves
the suffix tree based approaches for the repeat finding problem, being
particularly well suited for very large inputs. We prove the corrrectness and
complexity of the algorithms.
</summary>
    <author>
      <name>Veronica Becher</name>
    </author>
    <author>
      <name>Alejandro Deymonnaz</name>
    </author>
    <author>
      <name>Pablo Ariel Heiber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bioinformatics, 25(14):1746-1753, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.0528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.0528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.10382">
    <id>http://arxiv.org/abs/1705.10382v4</id>
    <updated>2017-07-11T18:21:03Z</updated>
    <published>2017-05-29T20:24:07Z</published>
    <title>Optimal-Time Text Indexing in BWT-runs Bounded Space</title>
    <summary>  Indexing highly repetitive texts --- such as genomic databases, software
repositories and versioned text collections --- has become an important problem
since the turn of the millennium. A relevant compressibility measure for
repetitive texts is $r$, the number of runs in their Burrows-Wheeler Transform
(BWT). One of the earliest indexes for repetitive collections, the Run-Length
FM-index, used $O(r)$ space and was able to efficiently count the number of
occurrences of a pattern of length $m$ in the text (in loglogarithmic time per
pattern symbol, with current techniques). However, it was unable to locate the
positions of those occurrences efficiently within a space bounded in terms of
$r$. Since then, a number of other indexes with space bounded by other measures
of repetitiveness --- the number of phrases in the Lempel-Ziv parse, the size
of the smallest grammar generating the text, the size of the smallest automaton
recognizing the text factors --- have been proposed for efficiently locating,
but not directly counting, the occurrences of a pattern. In this paper we close
this long-standing problem, showing how to extend the Run-Length FM-index so
that it can locate the $occ$ occurrences efficiently within $O(r)$ space (in
loglogarithmic time each), and reaching optimal time $O(m+occ)$ within
$O(r\log(n/r))$ space, on a RAM machine of $w=\Omega(\log n)$ bits. Within
$O(r\log (n/r))$ space, our index can also count in optimal time $O(m)$.
Raising the space to $O(r w\log_\sigma(n/r))$, we support count and locate in
$O(m\log(\sigma)/w)$ and $O(m\log(\sigma)/w+occ)$ time, which is optimal in the
packed setting and had not been obtained before in compressed space. We also
describe a structure using $O(r\log(n/r))$ space that replaces the text and
extracts any text substring of length $\ell$ in almost-optimal time
$O(\log(n/r)+\ell\log(\sigma)/w)$. (...continues...)
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1705.10382v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10382v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.02517">
    <id>http://arxiv.org/abs/1809.02517v2</id>
    <updated>2019-01-22T12:10:23Z</updated>
    <published>2018-09-07T14:54:53Z</published>
    <title>Streaming dictionary matching with mismatches</title>
    <summary>  In the $k$-mismatch problem we are given a pattern of length $m$ and a text
and must find all locations where the Hamming distance between the pattern and
the text is at most $k$. A series of recent breakthroughs have resulted in an
ultra-efficient streaming algorithm for this problem that requires only $O(k
\log \frac{m}{k})$ space [Clifford, Kociumaka, Porat, SODA 2019]. In this work,
we consider a strictly harder problem called dictionary matching with $k$
mismatches, where we are given a dictionary of $d$ patterns of lengths $\le m$
and must find all their $k$-mismatch occurrences in the text, and show the
first streaming algorithm for it. The algorithm uses $O(k d \log^k d \,
\mathrm{polylog} \, m)$ space and processes each position of the text in $O(k
\log^{k} d \, \mathrm{polylog} \, m + occ)$ time, where $occ$ is the number of
$k$-mismatch occurrences of the patterns that end at this position. The
algorithm is randomised and outputs correct answers with high probability.
</summary>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <link href="http://arxiv.org/abs/1809.02517v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.02517v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1702.07577">
    <id>http://arxiv.org/abs/1702.07577v1</id>
    <updated>2017-02-24T13:41:29Z</updated>
    <published>2017-02-24T13:41:29Z</published>
    <title>Compression with the tudocomp Framework</title>
    <summary>  We present a framework facilitating the implementation and comparison of text
compression algorithms. We evaluate its features by a case study on two novel
compression algorithms based on the Lempel-Ziv compression schemes that perform
well on highly repetitive texts.
</summary>
    <author>
      <name>Patrick Dinklage</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Dominik K√∂ppl</name>
    </author>
    <author>
      <name>Marvin L√∂bel</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <link href="http://arxiv.org/abs/1702.07577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.03560">
    <id>http://arxiv.org/abs/1903.03560v1</id>
    <updated>2019-03-08T17:16:26Z</updated>
    <published>2019-03-08T17:16:26Z</published>
    <title>Belga B-trees</title>
    <summary>  We revisit self-adjusting external memory tree data structures, which combine
the optimal (and practical) worst-case I/O performances of B-trees, while
adapting to the online distribution of queries. Our approach is analogous to
undergoing efforts in the BST model, where Tango Trees (Demaine et al. 2007)
were shown to be $O(\log\log N)$-competitive with the runtime of the best
offline binary search tree on every sequence of searches. Here we formalize the
B-Tree model as a natural generalization of the BST model. We prove lower
bounds for the B-Tree model, and introduce a B-Tree model data structure, the
Belga B-tree, that executes any sequence of searches within a $O(\log \log N)$
factor of the best offline B-tree model algorithm, provided $B=\log^{O(1)}N$.
We also show how to transform any static BST into a static B-tree which is
faster by a $\Theta(\log B)$ factor; the transformation is randomized and we
show that randomization is necessary to obtain any significant speedup.
</summary>
    <author>
      <name>Erik D. Demaine</name>
    </author>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Grigorios Koumoutsos</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <link href="http://arxiv.org/abs/1903.03560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.03560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.04214">
    <id>http://arxiv.org/abs/1903.04214v3</id>
    <updated>2020-02-07T15:48:24Z</updated>
    <published>2019-03-11T11:00:38Z</published>
    <title>How far away must forced letters be so that squares are still avoidable?</title>
    <summary>  We describe a new non-constructive technique to show that squares are
avoidable by an infinite word even if we force some letters from the alphabet
to appear at certain occurrences. We show that as long as forced positions are
at distance at least 19 (resp. 3, resp. 2) from each other then we can avoid
squares over 3 letters (resp. 4 letters, resp. 6 or more letters). We can also
deduce exponential lower bounds on the number of solutions. For our main
Theorem to be applicable, we need to check the existence of some languages and
we explain how to verify that they exist with a computer. We hope that this
technique could be applied to other avoidability questions where the good
approach seems to be non-constructive (e.g., the Thue-list coloring number of
the infinite path).
</summary>
    <author>
      <name>Matthieu Rosenfeld</name>
    </author>
    <link href="http://arxiv.org/abs/1903.04214v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.04214v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.04936">
    <id>http://arxiv.org/abs/1903.04936v1</id>
    <updated>2019-03-12T14:05:07Z</updated>
    <published>2019-03-12T14:05:07Z</published>
    <title>The k-d tree data structure and a proof for neighborhood computation in
  expected logarithmic time</title>
    <summary>  For practical applications, any neighborhood concept imposed on a finite
point set P is not of any use if it cannot be computed efficiently. Thus, in
this paper, we give an introduction to the data structure of k-d trees, first
presented by Friedman, Bentley, and Finkel in 1977. After a short introduction
to the data structure (Section 1), we turn to the proof of efficiency by
Friedman and his colleagues (Section 2). The main contribution of this paper is
the translation of the proof of Freedman, Bentley, and Finkel into modern terms
and the elaboration of the proof.
</summary>
    <author>
      <name>Martin Skrodzki</name>
    </author>
    <link href="http://arxiv.org/abs/1903.04936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.04936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.05442">
    <id>http://arxiv.org/abs/1903.05442v2</id>
    <updated>2019-12-18T17:06:43Z</updated>
    <published>2019-03-13T12:20:37Z</published>
    <title>Maximal State Complexity and Generalized de Bruijn Words</title>
    <summary>  We compute the exact maximum state complexity for the language consisting of
$m$ words of length $N$, and characterize languages achieving the maximum. We
also consider a special case, namely languages $C(w)$ consisting of the
conjugates of a single word $w$. The words for which the maximum state
complexity of $C(w)$ is achieved turn out to be a natural generalization of de
Bruijn words. We show that generalized de Bruijn words exist for each length
and consider the number of them.
</summary>
    <author>
      <name>Daniel Gabric</name>
    </author>
    <author>
      <name>≈†tƒõp√°n Holub</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected and extended version</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.05442v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.05442v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.06570">
    <id>http://arxiv.org/abs/1903.06570v1</id>
    <updated>2019-03-15T14:26:59Z</updated>
    <published>2019-03-15T14:26:59Z</published>
    <title>scaleBF: A High Scalable Membership Filter using 3D Bloom Filter</title>
    <summary>  Bloom Filter is extensively deployed data structure in various applications
and research domain since its inception. Bloom Filter is able to reduce the
space consumption in an order of magnitude. Thus, Bloom Filter is used to keep
information of a very large scale data. There are numerous variants of Bloom
Filters available, however, scalability is a serious dilemma of Bloom Filter
for years. To solve this dilemma, there are also diverse variants of Bloom
Filter. However, the time complexity and space complexity become the key issue
again. In this paper, we present a novel Bloom Filter to address the
scalability issue without compromising the performance, called scaleBF. scaleBF
deploys many 3D Bloom Filter to filter the set of items. In this paper, we
theoretically compare the contemporary Bloom Filter for scalability and scaleBF
outperforms in terms of time complexity.
</summary>
    <author>
      <name>Ripon Patgiri</name>
    </author>
    <author>
      <name>Sabuzima Nayak</name>
    </author>
    <author>
      <name>Samir Kumar Borgohain</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14569/IJACSA.2018.091277</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14569/IJACSA.2018.091277" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, 3 Figures, 1 Table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Science and
  Applications(IJACSA), Volume 9 Issue 12, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.06570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.06570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.06290">
    <id>http://arxiv.org/abs/1903.06290v2</id>
    <updated>2020-03-24T02:30:35Z</updated>
    <published>2019-03-14T22:46:55Z</published>
    <title>Fast Algorithms for the Shortest Unique Palindromic Substring Problem on
  Run-Length Encoded Strings</title>
    <summary>  For a string $S$, a palindromic substring $S[i..j]$ is said to be a
\emph{shortest unique palindromic substring} ($\mathit{SUPS}$) for an interval
$[s, t]$ in $S$, if $S[i..j]$ occurs exactly once in $S$, the interval $[i, j]$
contains $[s, t]$, and every palindromic substring containing $[s, t]$ which is
shorter than $S[i..j]$ occurs at least twice in $S$. In this paper, we study
the problem of answering $\mathit{SUPS}$ queries on run-length encoded strings.
We show how to preprocess a given run-length encoded string $\mathit{RLE}_{S}$
of size $m$ in $O(m)$ space and $O(m \log \sigma_{\mathit{RLE}_{S}} + m
\sqrt{\log m / \log\log m})$ time so that all $\mathit{SUPSs}$ for any
subsequent query interval can be answered in $O(\sqrt{\log m / \log\log m} +
\alpha)$ time, where $\alpha$ is the number of outputs, and
$\sigma_{\mathit{RLE}_{S}}$ is the number of distinct runs of
$\mathit{RLE}_{S}$. Additionaly, we consider a variant of the SUPS problem
where a query interval is also given in a run-length encoded form. For this
variant of the problem, we present two alternative algorithms with faster
queries. The first one answers queries in $O(\sqrt{\log\log m /\log\log\log m}
+ \alpha)$ time and can be built in $O(m \log \sigma_{\mathit{RLE}_{S}} + m
\sqrt{\log m / \log\log m})$ time, and the second one answers queries in
$O(\log \log m + \alpha)$ time and can be built in $O(m \log
\sigma_{\mathit{RLE}_{S}})$ time. Both of these data structures require $O(m)$
space.
</summary>
    <author>
      <name>Kiichi Watanabe</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1903.06290v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.06290v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.06289">
    <id>http://arxiv.org/abs/1903.06289v1</id>
    <updated>2019-03-14T22:45:34Z</updated>
    <published>2019-03-14T22:45:34Z</published>
    <title>The Parameterized Position Heap of a Trie</title>
    <summary>  Let $\Sigma$ and $\Pi$ be disjoint alphabets of respective size $\sigma$ and
$\pi$. Two strings over $\Sigma \cup \Pi$ of equal length are said to
parameterized match (p-match) if there is a bijection $f:\Sigma \cup \Pi
\rightarrow \Sigma \cup \Pi$ such that (1) $f$ is identity on $\Sigma$ and (2)
$f$ maps the characters of one string to those of the other string so that the
two strings become identical. We consider the p-matching problem on a
(reversed) trie $\mathcal{T}$ and a string pattern $P$ such that every path
that p-matches $P$ has to be reported. Let $N$ be the size of the given trie
$\mathcal{T}$. In this paper, we propose the parameterized position heap for
$\mathcal{T}$ that occupies $O(N)$ space and supports p-matching queries in
$O(m \log (\sigma + \pi) + m \pi + \mathit{pocc}))$ time, where $m$ is the
length of a query pattern $P$ and $\mathit{pocc}$ is the number of paths in
$\mathcal{T}$ to report. We also present an algorithm which constructs the
parameterized position heap for a given trie $\mathcal{T}$ in $O(N (\sigma +
\pi))$ time and working space.
</summary>
    <author>
      <name>Noriki Fujisato</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1903.06289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.06289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.09779">
    <id>http://arxiv.org/abs/1705.09779v2</id>
    <updated>2017-07-27T07:22:13Z</updated>
    <published>2017-05-27T07:24:44Z</published>
    <title>Linear-size CDAWG: new repetition-aware indexing and grammar compression</title>
    <summary>  In this paper, we propose a novel approach to combine \emph{compact directed
acyclic word graphs} (CDAWGs) and grammar-based compression. This leads us to
an efficient self-index, called Linear-size CDAWGs (L-CDAWGs), which can be
represented with $O(\tilde e_T \log n)$ bits of space allowing for $O(\log
n)$-time random and $O(1)$-time sequential accesses to edge labels, and $O(m
\log \sigma + occ)$-time pattern matching. Here, $\tilde e_T$ is the number of
all extensions of maximal repeats in $T$, $n$ and $m$ are respectively the
lengths of the text $T$ and a given pattern, $\sigma$ is the alphabet size, and
$occ$ is the number of occurrences of the pattern in $T$. The repetitiveness
measure $\tilde e_T$ is known to be much smaller than the text length $n$ for
highly repetitive text. For constant alphabets, our L-CDAWGs achieve $O(m +
occ)$ pattern matching time with $O(e_T^r \log n)$ bits of space, which
improves the pattern matching time of Belazzougui et al.'s run-length
BWT-CDAWGs by a factor of $\log \log n$, with the same space complexity. Here,
$e_T^r$ is the number of right extensions of maximal repeats in $T$. As a
byproduct, our result gives a way of constructing an SLP of size $O(\tilde
e_T)$ for a given text $T$ in $O(n + \tilde e_T \log \sigma)$ time.
</summary>
    <author>
      <name>Takuya Takagi</name>
    </author>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Yuta Fujishige</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hiroki Arimura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.09779v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09779v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.01720">
    <id>http://arxiv.org/abs/1705.01720v1</id>
    <updated>2017-05-04T07:11:47Z</updated>
    <published>2017-05-04T07:11:47Z</published>
    <title>Near-optimal linear decision trees for k-SUM and related problems</title>
    <summary>  We construct near optimal linear decision trees for a variety of decision
problems in combinatorics and discrete geometry. For example, for any constant
$k$, we construct linear decision trees that solve the $k$-SUM problem on $n$
elements using $O(n \log^2 n)$ linear queries. Moreover, the queries we use are
comparison queries, which compare the sums of two $k$-subsets; when viewed as
linear queries, comparison queries are $2k$-sparse and have only $\{-1,0,1\}$
coefficients. We give similar constructions for sorting sumsets $A+B$ and for
solving the SUBSET-SUM problem, both with optimal number of queries, up to
poly-logarithmic terms.
  Our constructions are based on the notion of "inference dimension", recently
introduced by the authors in the context of active classification with
comparison queries. This can be viewed as another contribution to the fruitful
link between machine learning and discrete geometry, which goes back to the
discovery of the VC dimension.
</summary>
    <author>
      <name>Daniel M. Kane</name>
    </author>
    <author>
      <name>Shachar Lovett</name>
    </author>
    <author>
      <name>Shay Moran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 paged, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1007.5406">
    <id>http://arxiv.org/abs/1007.5406v1</id>
    <updated>2010-07-30T10:14:21Z</updated>
    <published>2010-07-30T10:14:21Z</published>
    <title>Tree structure compression with RePair</title>
    <summary>  In this work we introduce a new linear time compression algorithm, called
"Re-pair for Trees", which compresses ranked ordered trees using linear
straight-line context-free tree grammars. Such grammars generalize
straight-line context-free string grammars and allow basic tree operations,
like traversal along edges, to be executed without prior decompression. Our
algorithm can be considered as a generalization of the "Re-pair" algorithm
developed by N. Jesper Larsson and Alistair Moffat in 2000. The latter
algorithm is a dictionary-based compression algorithm for strings. We also
introduce a succinct coding which is specialized in further compressing the
grammars generated by our algorithm. This is accomplished without loosing the
ability do directly execute queries on this compressed representation of the
input tree. Finally, we compare the grammars and output files generated by a
prototype of the Re-pair for Trees algorithm with those of similar compression
algorithms. The obtained results show that that our algorithm outperforms its
competitors in terms of compression ratio, runtime and memory usage.
</summary>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Sebastian Maneth</name>
    </author>
    <author>
      <name>Roy Mennicke</name>
    </author>
    <link href="http://arxiv.org/abs/1007.5406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.5406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P15 (Primary), 68P30 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.08809">
    <id>http://arxiv.org/abs/1902.08809v2</id>
    <updated>2019-04-16T15:52:03Z</updated>
    <published>2019-02-23T16:24:20Z</published>
    <title>Faster and simpler algorithms for finding large patterns in permutations</title>
    <summary>  Permutation patterns and pattern avoidance have been intensively studied in
combinatorics and computer science, going back at least to the seminal work of
Knuth on stack-sorting (1968). Perhaps the most natural algorithmic question in
this area is deciding whether a given permutation of length $n$ contains a
given pattern of length $k$.
  In this work we give two new algorithms for this well-studied problem, one
whose running time is $n^{0.44k+o(k)}$, and one whose running time is the
better of $O(1.6181^n)$ and $n^{k/2+o(k)}$. These results improve the earlier
best bounds of Ahal and Rabinovich (2000), and Bruner and Lackner (2012), and
are the fastest algorithms for the problem when $k = \Omega(\log n)$. When $k =
o(\log n)$, the parameterized algorithm of Guillemot and Marx (2013) dominates.
  Our second algorithm uses polynomial space and is significantly simpler than
all previous approaches with comparable running times, including an
$n^{k/2+o(k)}$ algorithm proposed by Guillemot and Marx. Our approach can be
summarized as follows: "for every matching of the even-valued entries of the
pattern, try to match all odd-valued entries left-to-right". For the special
case of patterns that are Jordan-permutations, we show an improved,
subexponential running time.
</summary>
    <author>
      <name>L√°szl√≥ Kozma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The second analysis of Algorithm~S was mistaken. The corrected bound
  is 1.618^n instead of 1.414^n</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.08809v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.08809v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.09228">
    <id>http://arxiv.org/abs/1902.09228v1</id>
    <updated>2019-02-25T12:29:11Z</updated>
    <published>2019-02-25T12:29:11Z</published>
    <title>Succinct Data Structures for Families of Interval Graphs</title>
    <summary>  We consider the problem of designing succinct data structures for interval
graphs with $n$ vertices while supporting degree, adjacency, neighborhood and
shortest path queries in optimal time in the $\Theta(\log n)$-bit word RAM
model. The degree query reports the number of incident edges to a given vertex
in constant time, the adjacency query returns true if there is an edge between
two vertices in constant time, the neighborhood query reports the set of all
adjacent vertices in time proportional to the degree of the queried vertex, and
the shortest path query returns a shortest path in time proportional to its
length, thus the running times of these queries are optimal. Towards showing
succinctness, we first show that at least $n\log{n} - 2n\log\log n - O(n)$ bits
are necessary to represent any unlabeled interval graph $G$ with $n$ vertices,
answering an open problem of Yang and Pippenger [Proc. Amer. Math. Soc. 2017].
This is augmented by a data structure of size $n\log{n} +O(n)$ bits while
supporting not only the aforementioned queries optimally but also capable of
executing various combinatorial algorithms (like proper coloring, maximum
independent set etc.) on the input interval graph efficiently. Finally, we
extend our ideas to other variants of interval graphs, for example, proper/unit
interval graphs, k-proper and k-improper interval graphs, and circular-arc
graphs, and design succinct/compact data structures for these graph classes as
well along with supporting queries on them efficiently.
</summary>
    <author>
      <name>H√ºseyin Acan</name>
    </author>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Seungbum Jo</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <link href="http://arxiv.org/abs/1902.09228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.09228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.10812">
    <id>http://arxiv.org/abs/1902.10812v1</id>
    <updated>2019-02-27T22:27:12Z</updated>
    <published>2019-02-27T22:27:12Z</published>
    <title>Padovan heaps</title>
    <summary>  We analyze priority queues of Fibonacci family. The paper is inspired by
Violation heap [1], where A. Elmasry saves one pointer in representation of
Fibonacci heap nodes while achieving the same amortized bounds as Fibonacci
heaps [2] of M. L. Fredman and R. E. Tarjan. Unfortunately author forces the
heaps to be wide, what goes against optimal heap principles. Our goal is to
achieve the same result, but with much narrower heaps. We follow the principle
of superexpensive comparison so we try to remember results of all comparisons
and never compare elements that cannot be minimal. We delay comparisons as long
as possible. Actually I have always want to share superexpensive comparison
principle ideas, discovery of Padovan heaps allowed me to do so. Of course
saving one pointer is not that big goal, but I hope the presented reasoning and
amortized analysis of the resulting heaps is worth a publication.
</summary>
    <author>
      <name>Vladan Majerech</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.10812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.10812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.10995">
    <id>http://arxiv.org/abs/1902.10995v2</id>
    <updated>2019-12-05T08:57:20Z</updated>
    <published>2019-02-28T10:29:35Z</published>
    <title>Fast Concurrent Data Sketches</title>
    <summary>  Data sketches are approximate succinct summaries of long streams. They are
widely used for processing massive amounts of data and answering statistical
queries about it in real-time. Existing libraries producing sketches are very
fast, but do not allow parallelism for creating sketches using multiple threads
or querying them while they are being built. We present a generic approach to
parallelising data sketches efficiently, while bounding the error that such
parallelism introduces. Utilising relaxed semantics and the notion of strong
linearisability we prove our algorithm's correctness and analyse the error it
induces in two specific sketches. Our implementation achieves high scalability
while keeping the error small.
</summary>
    <author>
      <name>Arik Rinberg</name>
    </author>
    <author>
      <name>Alexander Spiegelman</name>
    </author>
    <author>
      <name>Edward Bortnikov</name>
    </author>
    <author>
      <name>Eshcar Hillel</name>
    </author>
    <author>
      <name>Idit Keidar</name>
    </author>
    <author>
      <name>Lee Rhodes</name>
    </author>
    <author>
      <name>Hadar Serviansky</name>
    </author>
    <link href="http://arxiv.org/abs/1902.10995v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.10995v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.01387">
    <id>http://arxiv.org/abs/1903.01387v1</id>
    <updated>2019-03-04T17:30:51Z</updated>
    <published>2019-03-04T17:30:51Z</published>
    <title>A Simple Solution to the Level-Ancestor Problem</title>
    <summary>  A Level Ancestory query LA($u$, $d$) asks for the the ancestor of the node
$u$ at a depth $d$. We present a simple solution, which pre-processes the tree
in $O(n)$ time with $O(n)$ extra space, and answers the queries in $O(\log\
{n})$ time. Though other optimal algorithms exist, this is a simple enough
solution that could be taught and implemented easily.
</summary>
    <author>
      <name>Gaurav Menghani</name>
    </author>
    <author>
      <name>Dhruv Matani</name>
    </author>
    <link href="http://arxiv.org/abs/1903.01387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.01909">
    <id>http://arxiv.org/abs/1903.01909v1</id>
    <updated>2019-03-05T15:50:07Z</updated>
    <published>2019-03-05T15:50:07Z</published>
    <title>Lempel-Ziv-like Parsing in Small Space</title>
    <summary>  Lempel-Ziv (LZ77 or, briefly, LZ) is one of the most effective and
widely-used compressors for repetitive texts. However, the existing efficient
methods computing the exact LZ parsing have to use linear or close to linear
space to index the input text during the construction of the parsing, which is
prohibitive for long inputs. An alternative is Relative Lempel-Ziv (RLZ), which
indexes only a fixed reference sequence, whose size can be controlled. Deriving
the reference sequence by sampling the text yields reasonable compression
ratios for RLZ, but performance is not always competitive with that of LZ and
depends heavily on the similarity of the reference to the text. In this paper
we introduce ReLZ, a technique that uses RLZ as a preprocessor to approximate
the LZ parsing using little memory. RLZ is first used to produce a sequence of
phrases, and these are regarded as metasymbols that are input to LZ for a
second-level parsing on a (most often) drastically shorter sequence. This
parsing is finally translated into one on the original sequence.
  We analyze the new scheme and prove that, like LZ, it achieves the $k$th
order empirical entropy compression $n H_k + o(n\log\sigma)$ with $k =
o(\log_\sigma n)$, where $n$ is the input length and $\sigma$ is the alphabet
size. In fact, we prove this entropy bound not only for ReLZ but for a wide
class of LZ-like encodings. Then, we establish a lower bound on ReLZ
approximation ratio showing that the number of phrases in it can be
$\Omega(\log n)$ times larger than the number of phrases in LZ. Our experiments
show that ReLZ is orders of magnitude faster than other alternatives to compute
the (exact or approximate) LZ parsing, at the reasonable price of an
approximation factor below $2.0$ in practice, and sometimes below $1.05$, to
the size of LZ.
</summary>
    <author>
      <name>Daniel Valenzuela</name>
    </author>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.01909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.01465">
    <id>http://arxiv.org/abs/1903.01465v1</id>
    <updated>2019-03-04T18:34:01Z</updated>
    <published>2019-03-04T18:34:01Z</published>
    <title>Lightweight merging of compressed indices based on BWT variants</title>
    <summary>  In this paper we propose a flexible and lightweight technique for merging
compressed indices based on variants of Burrows-Wheeler transform (BWT), thus
addressing the need for algorithms that compute compressed indices over large
collections using a limited amount of working memory. Merge procedures make it
possible to use an incremental strategy for building large indices based on
merging indices for progressively larger subcollections.
  Starting with a known lightweight algorithm for merging BWTs [Holt and
McMillan, Bionformatics 2014], we show how to modify it in order to merge, or
compute from scratch, also the Longest Common Prefix (LCP) array. We then
expand our technique for merging compressed tries and circular/permuterm
compressed indices, two compressed data structures for which there were
hitherto no known merging algorithms.
</summary>
    <author>
      <name>Lavinia Egidi</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages. A preliminary version appeared in Proc. SPIRE 2017,
  Springer Verlag LNCS 10508. arXiv admin note: text overlap with
  arXiv:1609.04618</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.01465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.02533">
    <id>http://arxiv.org/abs/1903.02533v1</id>
    <updated>2019-03-06T18:13:33Z</updated>
    <published>2019-03-06T18:13:33Z</published>
    <title>Entropy Trees and Range-Minimum Queries In Optimal Average-Case Space</title>
    <summary>  The range-minimum query (RMQ) problem is a fundamental data structuring task
with numerous applications. Despite the fact that succinct solutions with
worst-case optimal $2n+o(n)$ bits of space and constant query time are known,
it has been unknown whether such a data structure can be made adaptive to the
reduced entropy of random inputs (Davoodi et al. 2014). We construct a succinct
data structure with the optimal $1.736n+o(n)$ bits of space on average for
random RMQ instances, settling this open problem.
  Our solution relies on a compressed data structure for binary trees that is
of independent interest. It can store a (static) binary search tree generated
by random insertions in asymptotically optimal expected space and supports many
queries in constant time. Using an instance-optimal encoding of subtrees, we
furthermore obtain a "hyper-succinct" data structure for binary trees that
improves upon the ultra-succinct representation of Jansson, Sadakane and Sung
(2012).
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <link href="http://arxiv.org/abs/1903.02533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.02533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.03003">
    <id>http://arxiv.org/abs/1903.03003v4</id>
    <updated>2020-02-18T14:31:53Z</updated>
    <published>2019-03-07T15:46:20Z</published>
    <title>Fast Exact Dynamic Time Warping on Run-Length Encoded Time Series</title>
    <summary>  Dynamic Time Warping (DTW) is a well-known similarity measure for time
series. The standard dynamic programming approach to compute the DTW distance
of two length-$n$ time series, however, requires $O(n^2)$ time, which is often
too slow for real-world applications. Therefore, many heuristics have been
proposed to speed up the DTW computation. These are often based on lower
bounding techniques, approximating the DTW distance, or considering special
input data such as binary or piecewise constant time series. In this paper, we
present a first exact algorithm to compute the DTW distance of two run-length
encoded time series whose running time only depends on the encoding lengths of
the inputs. The worst-case running time is cubic in the encoding length. In
experiments we show that our algorithm is indeed fast for time series with
short encoding lengths.
</summary>
    <author>
      <name>Vincent Froese</name>
    </author>
    <author>
      <name>Brijnesh Jain</name>
    </author>
    <author>
      <name>Maciej Rymar</name>
    </author>
    <link href="http://arxiv.org/abs/1903.03003v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.03003v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.02645">
    <id>http://arxiv.org/abs/1903.02645v1</id>
    <updated>2019-03-06T22:56:24Z</updated>
    <published>2019-03-06T22:56:24Z</published>
    <title>Encoding 3SUM</title>
    <summary>  We consider the following problem: given three sets of real numbers, output a
word-RAM data structure from which we can efficiently recover the sign of the
sum of any triple of numbers, one in each set. This is similar to a previous
work by some of the authors to encode the order type of a finite set of points.
While this previous work showed that it was possible to achieve slightly
subquadratic space and logarithmic query time, we show here that for the
simpler 3SUM problem, one can achieve an encoding that takes
$\tilde{O}(N^{\frac 32})$ space for inputs sets of size $N$ and allows constant
time queries in the word-RAM.
</summary>
    <author>
      <name>Sergio Cabello</name>
    </author>
    <author>
      <name>Jean Cardinal</name>
    </author>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <author>
      <name>Pat Morin</name>
    </author>
    <author>
      <name>Aur√©lien Ooms</name>
    </author>
    <link href="http://arxiv.org/abs/1903.02645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.02645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.04785">
    <id>http://arxiv.org/abs/1902.04785v1</id>
    <updated>2019-02-13T08:45:51Z</updated>
    <published>2019-02-13T08:45:51Z</published>
    <title>Constructing Antidictionaries in Output-Sensitive Space</title>
    <summary>  A word $x$ that is absent from a word $y$ is called minimal if all its proper
factors occur in $y$. Given a collection of $k$ words $y_1,y_2,\ldots,y_k$ over
an alphabet $\Sigma$, we are asked to compute the set
$\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{k}}$ of minimal absent words of length at
most $\ell$ of word $y=y_1\#y_2\#\ldots\#y_k$, $\#\notin\Sigma$. In data
compression, this corresponds to computing the antidictionary of $k$ documents.
In bioinformatics, it corresponds to computing words that are absent from a
genome of $k$ chromosomes. This computation generally requires $\Omega(n)$
space for $n=|y|$ using any of the plenty available $\mathcal{O}(n)$-time
algorithms. This is because an $\Omega(n)$-sized text index is constructed over
$y$ which can be impractical for large $n$. We do the identical computation
incrementally using output-sensitive space. This goal is reasonable when
$||\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{N}}||=o(n)$, for all $N\in[1,k]$. For
instance, in the human genome, $n \approx 3\times 10^9$ but
$||\mathrm{M}^{12}_{y_{1}\#\ldots\#y_{k}}|| \approx 10^6$. We consider a
constant-sized alphabet for stating our results. We show that all
$\mathrm{M}^{\ell}_{y_{1}},\ldots,\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{k}}$ can
be computed in
$\mathcal{O}(kn+\sum^{k}_{N=1}||\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{N}}||)$
total time using $\mathcal{O}(\mathrm{MaxIn}+\mathrm{MaxOut})$ space, where
$\mathrm{MaxIn}$ is the length of the longest word in $\{y_1,\ldots,y_{k}\}$
and
$\mathrm{MaxOut}=\max\{||\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{N}}||:N\in[1,k]\}$.
Proof-of-concept experimental results are also provided confirming our
theoretical findings and justifying our contribution.
</summary>
    <author>
      <name>Lorraine A. K. Ayad</name>
    </author>
    <author>
      <name>Golnaz Badkobeh</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Alice H√©liou</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version accepted to DCC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.04785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.05224">
    <id>http://arxiv.org/abs/1902.05224v1</id>
    <updated>2019-02-14T05:09:57Z</updated>
    <published>2019-02-14T05:09:57Z</published>
    <title>Conversion from RLBWT to LZ77</title>
    <summary>  Converting a compressed format of a string into another compressed format
without an explicit decompression is one of the central research topics in
string processing. We discuss the problem of converting the run-length
Burrows-Wheeler Transform (RLBWT) of a string to Lempel-Ziv 77 (LZ77) phrases
of the reversed string. The first results with Policriti and Prezza's
conversion algorithm [Algorithmica 2018] were $O(n \log r)$ time and $O(r)$
working space for length of the string $n$, number of runs $r$ in the RLBWT,
and number of LZ77 phrases $z$. Recent results with Kempa's conversion
algorithm [SODA 2019] are $O(n / \log n + r \log^{9} n + z \log^{9} n)$ time
and $O(n / \log_{\sigma} n + r \log^{8} n)$ working space for the alphabet size
$\sigma$ of the RLBWT. In this paper, we present a new conversion algorithm by
improving Policriti and Prezza's conversion algorithm where dynamic data
structures for general purpose are used. We argue that these dynamic data
structures can be replaced and present new data structures for faster
conversion. The time and working space of our conversion algorithm with new
data structures are $O(n \min \{ \log \log n, \sqrt{\frac{\log r}{\log\log r}}
\})$ and $O(r)$, respectively.
</summary>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/1902.05224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.05224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.05166">
    <id>http://arxiv.org/abs/1902.05166v1</id>
    <updated>2019-02-13T23:47:23Z</updated>
    <published>2019-02-13T23:47:23Z</published>
    <title>Space-Efficient Data Structures for Lattices</title>
    <summary>  A lattice is a partially-ordered set in which every pair of elements has a
unique meet (greatest lower bound) and join (least upper bound). We present new
data structures for lattices that are simple, efficient, and nearly optimal in
terms of space complexity.
  Our first data structure can answer partial order queries in constant time
and find the meet or join of two elements in $O(n^{3/4})$ time, where $n$ is
the number of elements in the lattice. It occupies $O(n^{3/2}\log n)$ bits of
space, which is only a $\Theta(\log n)$ factor from the $\Theta(n^{3/2})$-bit
lower bound for storing lattices. The preprocessing time is $O(n^2)$.
  This structure admits a simple space-time tradeoff so that, for any $c \in
[\frac{1}{2}, 1]$, the data structure supports meet and join queries in
$O(n^{1-c/2})$ time, occupies $O(n^{1+c}\log n)$ bits of space, and can be
constructed in $O(n^2 + n^{1+3c/2})$ time.
  Our second data structure uses $O(n^{3/2}\log n)$ bits of space and supports
meet and join in $O(d \frac{\log n}{\log d})$ time, where $d$ is the maximum
degree of any element in the transitive reduction graph of the lattice. This
structure is much faster for lattices with low-degree elements.
  This paper also identifies an error in a long-standing solution to the
problem of representing lattices. We discuss the issue with this previous work.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Bryce Sandlund</name>
    </author>
    <author>
      <name>Corwin Sinnamon</name>
    </author>
    <link href="http://arxiv.org/abs/1902.05166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.05166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.01280">
    <id>http://arxiv.org/abs/1902.01280v1</id>
    <updated>2019-02-04T16:15:38Z</updated>
    <published>2019-02-04T16:15:38Z</published>
    <title>A New Class of Searchable and Provably Highly Compressible String
  Transformations</title>
    <summary>  The Burrows-Wheeler Transform is a string transformation that plays a
fundamental role for the design of self-indexing compressed data structures.
Over the years, researchers have successfully extended this transformation
outside the domains of strings. However, efforts to find non-trivial
alternatives of the original, now 25 years old, Burrows-Wheeler string
transformation have met limited success. In this paper we bring new lymph to
this area by introducing a whole new family of transformations that have all
the myriad virtues of the BWT: they can be computed and inverted in linear
time, they produce provably highly compressible strings, and they support
linear time pattern search directly on the transformed string. This new family
is a special case of a more general class of transformations based on context
adaptive alphabet orderings, a concept introduced here. This more general class
includes also the Alternating BWT, another invertible string transforms
recently introduced in connection with a generalization of Lyndon words.
</summary>
    <author>
      <name>Raffaele Giancarlo</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <link href="http://arxiv.org/abs/1902.01280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.05540">
    <id>http://arxiv.org/abs/1902.05540v1</id>
    <updated>2019-02-14T18:47:03Z</updated>
    <published>2019-02-14T18:47:03Z</published>
    <title>On long words avoiding Zimin patterns</title>
    <summary>  A pattern is encountered in a word if some infix of the word is the image of
the pattern under some non-erasing morphism. A pattern $p$ is unavoidable if,
over every finite alphabet, every sufficiently long word encounters $p$. A
theorem by Zimin and independently by Bean, Ehrenfeucht and McNulty states that
a pattern over $n$ distinct variables is unavoidable if, and only if, $p$
itself is encountered in the $n$-th Zimin pattern. Given an alphabet size $k$,
we study the minimal length $f(n,k)$ such that every word of length $f(n,k)$
encounters the $n$-th Zimin pattern. It is known that $f$ is upper-bounded by a
tower of exponentials. Our main result states that $f(n,k)$ is lower-bounded by
a tower of $n-3$ exponentials, even for $k=2$. To the best of our knowledge,
this improves upon a previously best-known doubly-exponential lower bound. As a
further result, we prove a doubly-exponential upper bound for encountering
Zimin patterns in the abelian sense.
</summary>
    <author>
      <name>Arnaud Carayol</name>
    </author>
    <author>
      <name>Stefan G√∂ller</name>
    </author>
    <link href="http://arxiv.org/abs/1902.05540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.05540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.05651">
    <id>http://arxiv.org/abs/1902.05651v1</id>
    <updated>2019-02-15T00:19:24Z</updated>
    <published>2019-02-15T00:19:24Z</published>
    <title>Finite test sets for morphisms which are square-free on some of Thue's
  square-free ternary words</title>
    <summary>  Let $S$ be one of $\{aba,bcb\}$ and $\{aba, aca\}$, and let $w$ be an
infinite square-free word over $\Sigma=\{a,b,c\}$ with no factor in $S$.
Suppose that $f:\Sigma\rightarrow T^*$ is a non-erasing morphism. Word $f(w)$
is square-free if and only if $f$ is square-free on factors of $w$ of length 8
or less.
</summary>
    <author>
      <name>James D. Currie</name>
    </author>
    <link href="http://arxiv.org/abs/1902.05651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.05651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.06864">
    <id>http://arxiv.org/abs/1902.06864v3</id>
    <updated>2020-01-29T19:20:08Z</updated>
    <published>2019-02-19T02:32:32Z</published>
    <title>A sub-quadratic algorithm for the longest common increasing subsequence
  problem</title>
    <summary>  The Longest Common Increasing Subsequence problem (LCIS) is a natural variant
of the celebrated Longest Common Subsequence (LCS) problem. For LCIS, as well
as for LCS, there is an $O(n^2)$-time algorithm and a SETH-based conditional
lower bound of $O(n^{2-\varepsilon})$. For LCS, there is also the
Masek-Paterson $O(n^2 / \log{n})$-time algorithm, which does not seem to adapt
to LCIS in any obvious way. Hence, a natural question arises: does any
(slightly) sub-quadratic algorithm exist for the Longest Common Increasing
Subsequence problem? We answer this question positively, presenting a $O(n^2 /
\log^a{n})$-time algorithm for $a = \frac{1}{6}-o(1)$. The algorithm is not
based on memorizing small chunks of data (often used for logarithmic speedups,
including the "Four Russians Trick" in LCS), but rather utilizes a new
technique, bounding the number of significant symbol matches between the two
sequences.
</summary>
    <author>
      <name>Lech Duraj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages; STACS 2020 version -- heavily corrected from the previous
  one, might actually be readable</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.06864v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.06864v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.07599">
    <id>http://arxiv.org/abs/1902.07599v1</id>
    <updated>2019-02-20T15:34:06Z</updated>
    <published>2019-02-20T15:34:06Z</published>
    <title>Fast, Small, and Simple Document Listing on Repetitive Text Collections</title>
    <summary>  Document listing on string collections is the task of finding all documents
where a pattern appears. It is regarded as the most fundamental document
retrieval problem, and is useful in various applications. Many of the
fastest-growing string collections are composed of very similar documents, such
as versioned code and document collections, genome repositories, etc. Plain
pattern-matching indexes designed for repetitive text collections achieve
orders-of-magnitude reductions in space. Instead, there are not many analogous
indexes for document retrieval. In this paper we present a simple document
listing index for repetitive string collections of total length $n$ that lists
the $ndoc$ distinct documents where a pattern of length $m$ appears in time
$\mathcal{O}(m+ndoc \cdot \log n)$. We exploit the repetitiveness of the
document array (i.e., the suffix array coarsened to document identifiers) to
grammar-compress it while precomputing the answers to nonterminals, and store
them in grammar-compressed form as well. Our experimental results show that our
index sharply outperforms existing alternatives in the space/time tradeoff map.
</summary>
    <author>
      <name>Dustin Cobas</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1902.07599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.07599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.07353">
    <id>http://arxiv.org/abs/1902.07353v1</id>
    <updated>2019-02-19T23:58:11Z</updated>
    <published>2019-02-19T23:58:11Z</published>
    <title>In oder Aus</title>
    <summary>  Bloom filters are data structures used to determine set membership of
elements, with applications from string matching to networking and security
problems. These structures are favored because of their reduced memory
consumption and fast wallclock and asymptotic time bounds. Generally, Bloom
filters maintain constant membership query time, making them very fast in their
niche. However, they are limited in their lack of a removal operation, as well
as by their probabilistic nature. In this paper, we discuss various iterations
of and alternatives to the generic Bloom filter that have been researched and
implemented to overcome their inherent limitations. Bloom filters, especially
when used in conjunction with other data structures, are still powerful and
efficient data structures; we further discuss their use in industy and research
to optimize resource utilization.
</summary>
    <author>
      <name>Ethan Madison</name>
    </author>
    <author>
      <name>Zachary Zipper</name>
    </author>
    <link href="http://arxiv.org/abs/1902.07353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.07353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.08744">
    <id>http://arxiv.org/abs/1902.08744v2</id>
    <updated>2020-01-21T15:30:50Z</updated>
    <published>2019-02-23T06:10:34Z</published>
    <title>On Greedy Algorithms for Binary de Bruijn Sequences</title>
    <summary>  We propose a general greedy algorithm for binary de Bruijn sequences, called
Generalized Prefer-Opposite (GPO) Algorithm, and its modifications. By
identifying specific feedback functions and initial states, we demonstrate that
most previously-known greedy algorithms that generate binary de Bruijn
sequences are particular cases of our new algorithm.
</summary>
    <author>
      <name>Zuling Chang</name>
    </author>
    <author>
      <name>Martianus Frederic Ezerman</name>
    </author>
    <author>
      <name>Adamas Aqsa Fahreza</name>
    </author>
    <link href="http://arxiv.org/abs/1902.08744v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.08744v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.01088">
    <id>http://arxiv.org/abs/1902.01088v4</id>
    <updated>2019-07-09T07:53:59Z</updated>
    <published>2019-02-04T09:00:36Z</published>
    <title>Regular Languages meet Prefix Sorting</title>
    <summary>  Indexing strings via prefix (or suffix) sorting is, arguably, one of the most
successful algorithmic techniques developed in the last decades. Can indexing
be extended to languages? The main contribution of this paper is to initiate
the study of the sub-class of regular languages accepted by an automaton whose
states can be prefix-sorted. Starting from the recent notion of Wheeler graph
[Gagie et al., TCS 2017]-which extends naturally the concept of prefix sorting
to labeled graphs-we investigate the properties of Wheeler languages, that is,
regular languages admitting an accepting Wheeler finite automaton.
Interestingly, we characterize this family as the natural extension of regular
languages endowed with the co-lexicographic ordering: when sorted, the strings
belonging to a Wheeler language are partitioned into a finite number of
co-lexicographic intervals, each formed by elements from a single Myhill-Nerode
equivalence class. Moreover: (i) We show that every Wheeler NFA (WNFA) with $n$
states admits an equivalent Wheeler DFA (WDFA) with at most $2n-1-|\Sigma|$
states that can be computed in $O(n^3)$ time. This is in sharp contrast with
general NFAs. (ii) We describe a quadratic algorithm to prefix-sort a proper
superset of the WDFAs, a $O(n\log n)$-time online algorithm to sort acyclic
WDFAs, and an optimal linear-time offline algorithm to sort general WDFAs. By
contribution (i), our algorithms can also be used to index any WNFA at the
moderate price of doubling the automaton's size. (iii) We provide a
minimization theorem that characterizes the smallest WDFA recognizing the same
language of any input WDFA. The corresponding constructive algorithm runs in
optimal linear time in the acyclic case, and in $O(n\log n)$ time in the
general case. (iv) We show how to compute the smallest WDFA equivalent to any
acyclic DFA in nearly-optimal time.
</summary>
    <author>
      <name>Jarno Alanko</name>
    </author>
    <author>
      <name>Giovanna D'Agostino</name>
    </author>
    <author>
      <name>Alberto Policriti</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">added minimization theorems; uploaded submitted version; New version
  with new results (W-MH theorem, linear determinization), added author:
  Giovanna D'Agostino</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.01088v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01088v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.02187">
    <id>http://arxiv.org/abs/1902.02187v2</id>
    <updated>2019-09-20T11:09:45Z</updated>
    <published>2019-02-06T14:01:45Z</published>
    <title>Top Tree Compression of Tries</title>
    <summary>  We present a compressed representation of tries based on top tree compression
[ICALP 2013] that works on a standard, comparison-based, pointer machine model
of computation and supports efficient prefix search queries. Namely, we show
how to preprocess a set of strings of total length $n$ over an alphabet of size
$\sigma$ into a compressed data structure of worst-case optimal size
$O(n/\log_\sigma n)$ that given a pattern string $P$ of length $m$ determines
if $P$ is a prefix of one of the strings in time $O(\min(m\log \sigma,m + \log
n))$. We show that this query time is in fact optimal regardless of the size of
the data structure.
  Existing solutions either use $\Omega(n)$ space or rely on word RAM
techniques, such as tabulation, hashing, address arithmetic, or word-level
parallelism, and hence do not work on a pointer machine. Our result is the
first solution on a pointer machine that achieves worst-case $o(n)$ space.
Along the way, we develop several interesting data structures that work on a
pointer machine and are of independent interest. These include an optimal data
structures for random access to a grammar-compressed string and an optimal data
structure for a variant of the level ancestor problem.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract appeared at ISAAC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.02187v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02187v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.02499">
    <id>http://arxiv.org/abs/1902.02499v4</id>
    <updated>2019-03-02T03:02:40Z</updated>
    <published>2019-02-07T07:21:11Z</published>
    <title>A fast algorithm for constructing balanced binary search trees</title>
    <summary>  We suggest a new non-recursive algorithm for constructing a binary search
tree given an array of numbers. The algorithm has $O(N)$ time and $O(1)$ memory
complexity if the given array of $N$ numbers is sorted. The resulting tree is
of minimal height and can be transformed to a complete binary search tree
(retaining minimal height) with $O(\log N)$ time and $O(1)$ memory.
  The algorithm allows simple and effective parallelization.
</summary>
    <author>
      <name>Pavel S. Ruzankin</name>
    </author>
    <link href="http://arxiv.org/abs/1902.02499v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02499v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W01, 68W10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.02889">
    <id>http://arxiv.org/abs/1902.02889v5</id>
    <updated>2019-07-26T19:05:01Z</updated>
    <published>2019-02-07T23:56:50Z</published>
    <title>Space-efficient merging of succinct de Bruijn graphs</title>
    <summary>  We propose a new algorithm for merging succinct representations of de Bruijn
graphs introduced in [Bowe et al. WABI 2012]. Our algorithm is based on the
lightweight BWT merging approach by Holt and McMillan [Bionformatics 2014,
ACM-BCB 2014]. Our algorithm has the same asymptotic cost of the state of the
art tool for the same problem presented by Muggli et al. [bioRxiv 2017,
Bioinformatics 2019], but it uses less than half of its working space. A novel
important feature of our algorithm, not found in any of the existing tools, is
that it can compute the Variable Order succinct representation of the union
graph within the same asymptotic time/space bounds.
</summary>
    <author>
      <name>Lavinia Egidi</name>
    </author>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SPIRE'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.02889v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02889v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.03274">
    <id>http://arxiv.org/abs/1902.03274v1</id>
    <updated>2019-02-08T20:08:03Z</updated>
    <published>2019-02-08T20:08:03Z</published>
    <title>Faster Repetition-Aware Compressed Suffix Trees based on Block Trees</title>
    <summary>  Suffix trees are a fundamental data structure in stringology, but their space
usage, though linear, is an important problem for its applications. We design
and implement a new compressed suffix tree targeted to highly repetitive texts,
such as large genomic collections of the same species. Our suffix tree tree
builds on Block Trees, a recent Lempel-Ziv-bounded data structure that captures
the repetitiveness of its input. We use Block Trees to compress the topology of
the suffix tree, and augment the Block Tree nodes with data that speeds up
suffix tree navigation.
  Our compressed suffix tree is slightly larger than previous repetition-aware
suffix trees based on grammars, but outperforms them in time, often by orders
of magnitude. The component that represents the tree topology achieves a speed
comparable to that of general-purpose compressed trees, while using 2.3--10
times less space, and might be of interest in other scenarios.
</summary>
    <author>
      <name>Manuel C√°ceres</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1902.03274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.4; H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.03568">
    <id>http://arxiv.org/abs/1902.03568v4</id>
    <updated>2019-10-01T11:58:08Z</updated>
    <published>2019-02-10T10:30:46Z</published>
    <title>Balancing Straight-Line Programs</title>
    <summary>  It is shown that a context-free grammar of size $m$ that produces a single
string $w$ (such a grammar is also called a string straight-line program) can
be transformed in linear time into a context-free grammar for $w$ of size
$\mathcal{O}(m)$, whose unique derivation tree has depth $\mathcal{O}(\log
|w|)$. This solves an open problem in the area of grammar-based compression.
Similar results are shown for two formalism for grammar-based tree compression:
top dags and forest straight-line programs. These balancing results are all
deduced from a single meta theorem stating that the depth of an algebraic
circuit over an algebra with a certain finite base property can be reduced to
$\mathcal{O}(\log n)$ with the cost of a constant multiplicative size increase.
Here, $n$ refers to the size of the unfolding (or unravelling) of the circuit.
</summary>
    <author>
      <name>Moses Ganardi</name>
    </author>
    <author>
      <name>Artur Je≈º</name>
    </author>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extended abstract of this paper appears in the Proceedings of FOCS
  2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.03568v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03568v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68W32" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.03534">
    <id>http://arxiv.org/abs/1902.03534v1</id>
    <updated>2019-02-10T04:10:34Z</updated>
    <published>2019-02-10T04:10:34Z</published>
    <title>Set Cover in Sub-linear Time</title>
    <summary>  We study the classic set cover problem from the perspective of sub-linear
algorithms. Given access to a collection of $m$ sets over $n$ elements in the
query model, we show that sub-linear algorithms derived from existing
techniques have almost tight query complexities.
  On one hand, first we show an adaptation of the streaming algorithm presented
in Har-Peled et al. [2016] to the sub-linear query model, that returns an
$\alpha$-approximate cover using $\tilde{O}(m(n/k)^{1/(\alpha-1)} + nk)$
queries to the input, where $k$ denotes the value of a minimum set cover. We
then complement this upper bound by proving that for lower values of $k$, the
required number of queries is $\tilde{\Omega}(m(n/k)^{1/(2\alpha)})$, even for
estimating the optimal cover size. Moreover, we prove that even checking
whether a given collection of sets covers all the elements would require
$\Omega(nk)$ queries. These two lower bounds provide strong evidence that the
upper bound is almost tight for certain values of the parameter $k$.
  On the other hand, we show that this bound is not optimal for larger values
of the parameter $k$, as there exists a $(1+\varepsilon)$-approximation
algorithm with $\tilde{O}(mn/k\varepsilon^2)$ queries. We show that this bound
is essentially tight for sufficiently small constant $\varepsilon$, by
establishing a lower bound of $\tilde{\Omega}(mn/k)$ query complexity.
</summary>
    <author>
      <name>Piotr Indyk</name>
    </author>
    <author>
      <name>Sepideh Mahabadi</name>
    </author>
    <author>
      <name>Ronitt Rubinfeld</name>
    </author>
    <author>
      <name>Ali Vakilian</name>
    </author>
    <author>
      <name>Anak Yodpinyanee</name>
    </author>
    <link href="http://arxiv.org/abs/1902.03534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.03560">
    <id>http://arxiv.org/abs/1902.03560v1</id>
    <updated>2019-02-10T09:43:38Z</updated>
    <published>2019-02-10T09:43:38Z</published>
    <title>On the Complexity of Exact Pattern Matching in Graphs: Determinism and
  Zig-Zag Matching</title>
    <summary>  Exact pattern matching in labeled graphs is the problem of searching paths of
a graph $G=(V,E)$ that spell the same string as the given pattern $P[1..m]$.
This basic problem can be found at the heart of more complex operations on
variation graphs in computational biology, query operations in graph databases,
and analysis of heterogeneous networks, where the nodes of some paths must
match a sequence of labels or types. In our recent work we described a
conditional lower bound stating that the exact pattern matching problem in
labeled graphs cannot be solved in less than quadratic time, namely, $O(|E|^{1
- \epsilon} \, m)$ time or $O(|E| \, m^{1 - \epsilon})$ time for any constant
$\epsilon>0$, unless the Strong Exponential Time Hypothesis (SETH) is false.
The result holds even if node labels and pattern $P$ are drawn from a binary
alphabet, and $G$ is restricted to undirected graphs of maximum degree three or
directed acyclic graphs of maximum sum of indegree and outdegree three. It was
left open what happens on undirected graphs of maximum degree two, i.e., when
the pattern can have a zig-zag match in a (cyclic) bidirectional string. Also,
the reduction created a non-determistic directed acyclic graph, and it was left
open if determinism would make the problem easier. In this work, we show
through the Orthogonal Vectors hypothesis (OV) that the same conditional lower
bound holds even for these restricted cases.
</summary>
    <author>
      <name>Massimo Equi</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Alexandru I. Tomescu</name>
    </author>
    <author>
      <name>Veli M√§kinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Further developments on our previous work: arXiv:1901.05264</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.03560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.1; F.2.2; G.2.2; H.2.3; H.2.8; H.3.3; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.03285">
    <id>http://arxiv.org/abs/1902.03285v1</id>
    <updated>2019-02-08T20:42:07Z</updated>
    <published>2019-02-08T20:42:07Z</published>
    <title>Fast Sequence Segmentation using Log-Linear Models</title>
    <summary>  Sequence segmentation is a well-studied problem, where given a sequence of
elements, an integer K, and some measure of homogeneity, the task is to split
the sequence into K contiguous segments that are maximally homogeneous. A
classic approach to find the optimal solution is by using a dynamic program.
Unfortunately, the execution time of this program is quadratic with respect to
the length of the input sequence. This makes the algorithm slow for a sequence
of non-trivial length. In this paper we study segmentations whose measure of
goodness is based on log-linear models, a rich family that contains many of the
standard distributions. We present a theoretical result allowing us to prune
many suboptimal segmentations. Using this result, we modify the standard
dynamic program for one-dimensional log-linear models, and by doing so reduce
the computational time. We demonstrate empirically, that this approach can
significantly reduce the computational burden of finding the optimal
segmentation.
</summary>
    <author>
      <name>Nikolaj Tatti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10618-012-0301-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10618-012-0301-y" rel="related"/>
    <link href="http://arxiv.org/abs/1902.03285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.04427">
    <id>http://arxiv.org/abs/1902.04427v2</id>
    <updated>2019-05-29T09:59:23Z</updated>
    <published>2019-02-12T15:05:59Z</published>
    <title>Compressed Range Minimum Queries</title>
    <summary>  Given a string $S$ of $n$ integers in $[0,\sigma)$, a range minimum query
RMQ$(i, j)$ asks for the index of the smallest integer in $S[i \dots j]$. It is
well known that the problem can be solved with a succinct data structure of
size $2n + o(n)$ and constant query-time. In this paper we show how to
preprocess $S$ into a compressed representation that allows fast range minimum
queries. This allows for sublinear size data structures with logarithmic query
time. The most natural approach is to use string compression and construct a
data structure for answering range minimum queries directly on the compressed
string. We investigate this approach in the context of grammar compression. We
then consider an alternative approach. Instead of compressing $S$ using string
compression, we compress the Cartesian tree of $S$ using tree compression. We
show that this approach can be exponentially better than the former, is never
worse by more than an $O(\sigma)$ factor (i.e. for constant alphabets it is
never asymptotically worse), and can in fact be worse by an $\Omega(\sigma)$
factor.
</summary>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Seungbum Jo</name>
    </author>
    <author>
      <name>Shay Mozes</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <link href="http://arxiv.org/abs/1902.04427v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04427v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.10453">
    <id>http://arxiv.org/abs/1901.10453v2</id>
    <updated>2019-11-29T13:15:51Z</updated>
    <published>2019-01-29T18:57:00Z</published>
    <title>Simulating the DNA String Graph in Succinct Space</title>
    <summary>  Converting a set of sequencing reads into a lossless compact data structure
that encodes all the relevant biological information is a major challenge. The
classical approaches are to build the string graph or the de Bruijn graph. Each
has advantages over the other depending on the application. Still, the ideal
setting would be to have an index of the reads that is easy to build and can be
adapted to any type of biological analysis. In this paper, we propose a new
data structure we call rBOSS, which gets close to that ideal. Our rBOSS is a de
Bruijn graph in practice, but it simulates any length up to k and can compute
overlaps of size at least m between the labels of the nodes, with k and m being
parameters. If we choose the parameter k equal to the size of the reads, then
we can simulate a complete string graph. As most BWT-based structures, rBOSS is
unidirectional, but it exploits the property of the DNA reverse complements to
simulate bi-directionality with some time-space trade-offs. We implemented a
genome assembler on top of rBOSS to demonstrate its usefulness. Our
experimental results show that using k = 100, rBOSS can assemble 185 MB of
reads in less than 15 minutes and using 110 MB in total. It produces contigs of
mean sizes over 10,000, which is twice the size obtained by using a pure de
Bruijn graph of fixed length k.
</summary>
    <author>
      <name>Diego D√≠az-Dom√≠nguez</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.10453v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10453v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3, E.1, G.2.2" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3; E.1; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.10165">
    <id>http://arxiv.org/abs/1901.10165v2</id>
    <updated>2019-06-09T14:08:49Z</updated>
    <published>2019-01-29T08:33:09Z</published>
    <title>Fully-functional bidirectional Burrows-Wheeler indexes</title>
    <summary>  Given a string $T$ on an alphabet of size $\sigma$, we describe a
bidirectional Burrows-Wheeler index that takes $O(|T|\log{\sigma})$ bits of
space, and that supports the addition \emph{and removal} of one character, on
the left or right side of any substring of $T$, in constant time. Previously
known data structures that used the same space allowed constant-time addition
to any substring of $T$, but they could support removal only from specific
substrings of $T$. We also describe an index that supports bidirectional
addition and removal in $O(\log{\log{|T|}})$ time, and that occupies a number
of words proportional to the number of left and right extensions of the maximal
repeats of $T$. We use such fully-functional indexes to implement
bidirectional, frequency-aware, variable-order de Bruijn graphs in small space,
with no upper bound on their order, and supporting natural criteria for
increasing and decreasing the order during traversal.
</summary>
    <author>
      <name>Fabio Cunial</name>
    </author>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <link href="http://arxiv.org/abs/1901.10165v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10165v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.10045">
    <id>http://arxiv.org/abs/1901.10045v3</id>
    <updated>2019-04-10T08:45:20Z</updated>
    <published>2019-01-29T00:14:32Z</published>
    <title>Online Algorithms for Constructing Linear-size Suffix Trie</title>
    <summary>  The suffix trees are fundamental data structures for various kinds of string
processing. The suffix tree of a string $T$ of length $n$ has $O(n)$ nodes and
edges, and the string label of each edge is encoded by a pair of positions in
$T$. Thus, even after the tree is built, the input text $T$ needs to be kept
stored and random access to $T$ is still needed. The linear-size suffix tries
(LSTs), proposed by Crochemore et al. [Linear-size suffix tries, TCS
638:171-178, 2016], are a `stand-alone' alternative to the suffix trees.
Namely, the LST of a string $T$ of length $n$ occupies $O(n)$ total space, and
supports pattern matching and other tasks in the same efficiency as the suffix
tree without the need to store the input text $T$. Crochemore et al. proposed
an offline algorithm which transforms the suffix tree of $T$ into the LST of
$T$ in $O(n \log \sigma)$ time and $O(n)$ space, where $\sigma$ is the alphabet
size. In this paper, we present two types of online algorithms which `directly'
construct the LST, from right to left, and from left to right, without
constructing the suffix tree as an intermediate structure. Both algorithms
construct the LST incrementally when a new symbol is read, and do not access to
the previously read symbols. The right-to-left construction algorithm works in
$O(n \log \sigma)$ time and $O(n)$ space and the left-to-right construction
algorithm works in $O(n (\log \sigma + \log n / \log \log n))$ time and $O(n)$
space. The main feature of our algorithms is that the input text does not need
to be stored.
</summary>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Takuya Takagi</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.10045v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10045v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.10744">
    <id>http://arxiv.org/abs/1901.10744v3</id>
    <updated>2019-02-13T11:36:43Z</updated>
    <published>2019-01-30T10:17:52Z</published>
    <title>A study for Image compression using Re-Pair algorithm</title>
    <summary>  The compression is an important topic in computer science which allows we to
storage more amount of data on our data storage. There are several techniques
to compress any file. In this manuscript will be described the most important
algorithm to compress images such as JPEG and it will be compared with another
method to retrieve good reason to not use this method on images. So to compress
the text the most encoding technique known is the Huffman Encoding which it
will be explained in exhaustive way. In this manuscript will showed how to
compute a text compression method on images in particular the method and the
reason to choice a determinate image format against the other. The method
studied and analyzed in this manuscript is the Re-Pair algorithm which is
purely for grammatical context to be compress. At the and it will be showed the
good result of this application.
</summary>
    <author>
      <name>Pasquale De Luca</name>
    </author>
    <author>
      <name>Vincenzo Maria Russiello</name>
    </author>
    <author>
      <name>Raffaele Ciro Sannino</name>
    </author>
    <author>
      <name>Lorenzo Valente</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.10744v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10744v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.10722">
    <id>http://arxiv.org/abs/1901.10722v1</id>
    <updated>2019-01-30T09:31:31Z</updated>
    <published>2019-01-30T09:31:31Z</published>
    <title>Faster queries for longest substring palindrome after block edit</title>
    <summary>  Palindromes are important objects in strings which have been extensively
studied from combinatorial, algorithmic, and bioinformatics points of views.
Manacher [J. ACM 1975] proposed a seminal algorithm that computes the longest
substring palindromes (LSPals) of a given string in O(n) time, where n is the
length of the string. In this paper, we consider the problem of finding the
LSPal after the string is edited. We present an algorithm that uses O(n) time
and space for preprocessing, and answers the length of the LSPals in O(\ell +
\log \log n) time, after a substring in T is replaced by a string of arbitrary
length \ell. This outperforms the query algorithm proposed in our previous work
[CPM 2018] that uses O(\ell + \log n) time for each query.
</summary>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1901.10722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.10633">
    <id>http://arxiv.org/abs/1901.10633v2</id>
    <updated>2019-01-31T05:05:45Z</updated>
    <published>2019-01-30T01:26:07Z</published>
    <title>Computing runs on a trie</title>
    <summary>  A maximal repetition, or run, in a string, is a periodically maximal
substring whose smallest period is at most half the length of the substring. In
this paper, we consider runs that correspond to a path on a trie, or in other
words, on a rooted edge-labeled tree where the endpoints of the path must be a
descendant/ancestor of the other. For a trie with $n$ edges, we show that the
number of runs is less than $n$. We also show an $O(n\sqrt{\log n}\log \log n)$
time and $O(n)$ space algorithm for counting and finding the shallower endpoint
of all runs. We further show an $O(n\sqrt{\log n}\log^2\log n)$ time and $O(n)$
space algorithm for finding both endpoints of all runs.
</summary>
    <author>
      <name>Ryo Sugahara</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">corrections and improvements to version submitted to CPM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.10633v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10633v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.11305">
    <id>http://arxiv.org/abs/1901.11305v1</id>
    <updated>2019-01-31T11:15:04Z</updated>
    <published>2019-01-31T11:15:04Z</published>
    <title>Quasi-Linear-Time Algorithm for Longest Common Circular Factor</title>
    <summary>  We introduce the Longest Common Circular Factor (LCCF) problem in which,
given strings $S$ and $T$ of length $n$, we are to compute the longest factor
of $S$ whose cyclic shift occurs as a factor of $T$. It is a new similarity
measure, an extension of the classic Longest Common Factor. We show how to
solve the LCCF problem in $O(n \log^5 n)$ time.
</summary>
    <author>
      <name>Mai Alzamel</name>
    </author>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszy≈Ñski</name>
    </author>
    <author>
      <name>Tomasz Wale≈Ñ</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <link href="http://arxiv.org/abs/1901.11305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.11305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.08833">
    <id>http://arxiv.org/abs/1810.08833v2</id>
    <updated>2019-05-29T05:10:14Z</updated>
    <published>2018-10-20T17:54:37Z</published>
    <title>MinJoin: Efficient Edit Similarity Joins via Local Hash Minima</title>
    <summary>  We study the problem of computing similarity joins under edit distance on a
set of strings. Edit similarity joins is a fundamental problem in databases,
data mining and bioinformatics. It finds important applications in data
cleaning and integration, collaborative filtering, genome sequence assembly,
etc. This problem has attracted significant attention in the past two decades.
However, all previous algorithms either cannot scale well to long strings and
large similarity thresholds, or suffer from imperfect accuracy.
  In this paper we propose a new algorithm for edit similarity joins using a
novel string partition based approach. We show mathematically that with high
probability our algorithm achieves a perfect accuracy, and runs in linear time
plus a data-dependent verification step. Experiments on real world datasets
show that our algorithm significantly outperforms the state-of-the-art
algorithms for edit similarity joins, and achieves perfect accuracy on all the
datasets that we have tested.
</summary>
    <author>
      <name>Haoyu Zhang</name>
    </author>
    <author>
      <name>Qin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to KDD 2019, full version, 22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.08833v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.08833v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.00257">
    <id>http://arxiv.org/abs/1902.00257v1</id>
    <updated>2019-02-01T10:06:52Z</updated>
    <published>2019-02-01T10:06:52Z</published>
    <title>An efficient sorting algorithm - Ultimate Heapsort(UHS)</title>
    <summary>  Motivated by the development of computer theory, the sorting algorithm is
emerging in an endless stream. Inspired by decrease and conquer method, we
propose a brand new sorting algorithmUltimately Heapsort. The algorithm
consists of two parts: building a heap and adjusting a heap. Through the
asymptotic analysis and experimental analysis of the algorithm, the time
complexity of our algorithm can reach O(nlogn) under any condition. Moreover,
its space complexity is only O(1). It can be seen that our algorithm is
superior to all previous algorithms.
</summary>
    <author>
      <name>Feiyang Chen</name>
    </author>
    <author>
      <name>Nan Chen</name>
    </author>
    <author>
      <name>Hanyang Mao</name>
    </author>
    <author>
      <name>Hanlin Hu</name>
    </author>
    <link href="http://arxiv.org/abs/1902.00257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.00257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.00216">
    <id>http://arxiv.org/abs/1902.00216v3</id>
    <updated>2019-09-04T04:37:56Z</updated>
    <published>2019-02-01T08:20:26Z</published>
    <title>An Extension of Linear-size Suffix Tries for Parameterized Strings</title>
    <summary>  In this paper, we propose a new indexing structure for parameterized strings
which we call PLSTs, by generalizing linear-size suffix tries for ordinary
strings. Two parameterized strings are said to match if there is a bijection on
the symbol set that makes the two coincide. PLSTs are applicable to the
parameterized pattern matching problem, which is to decide whether the input
parameterized text has a substring that matches the input parameterized
pattern. The size of PLSTs is linear in the text size, with which our algorithm
solves the parameterized pattern matching problem in linear time in the pattern
size. PLSTs can be seen as a compacted version of parameterized suffix tries
and a combination of linear-size suffix tries and parameterized suffix trees.
We experimentally show that PLSTs are more space efficient than parameterized
suffix trees for highly repetitive strings.
</summary>
    <author>
      <name>Katsuhito Nakashima</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.00216v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.00216v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.04068">
    <id>http://arxiv.org/abs/1901.04068v1</id>
    <updated>2019-01-13T21:28:35Z</updated>
    <published>2019-01-13T21:28:35Z</published>
    <title>Longest Common Subsequence on Weighted Sequences</title>
    <summary>  We consider the general problem of the Longest Common Subsequence (LCS) on
weighted sequences. Weighted sequences are an extension of classical strings,
where in each position every letter of the alphabet may occur with some
probability. In this paper we provide faster algorithms and prove a series of
hardness results for more general variants of the problem. In particular, we
provide an NP-Completeness result on the general variant of the problem instead
of the log-probability version used in earlier papers, already for alphabets of
size 2. Furthermore, we design an EPTAS for bounded alphabets, which is also an
improved, compared to previous results, PTAS for unbounded alphabets. These are
in a sense optimal, since it is known that there is no FPTAS for bounded
alphabets, while we prove that there is no EPTAS for unbounded alphabets.
Finally, we provide a matching conditional (under the Exponential Time
Hypothesis) lower bound for any PTAS. As a side note, we prove that it is
sufficient to work with only one threshold in the general variant of the
problem.
</summary>
    <author>
      <name>Evangelos Kipouridis</name>
    </author>
    <author>
      <name>Kostas Tsichlas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.04068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.04068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.03783">
    <id>http://arxiv.org/abs/1901.03783v1</id>
    <updated>2019-01-12T02:36:21Z</updated>
    <published>2019-01-12T02:36:21Z</published>
    <title>On Huang and Wong's Algorithm for Generalized Binary Split Trees</title>
    <summary>  Huang and Wong [5] proposed a polynomial-time dynamic-programming algorithm
for computing optimal generalized binary split trees. We show that their
algorithm is incorrect. Thus, it remains open whether such trees can be
computed in polynomial time. Spuler [11, 12] proposed modifying Huang and
Wong's algorithm to obtain an algorithm for a different problem: computing
optimal two-way-comparison search trees. We show that the dynamic program
underlying Spuler's algorithm is not valid, in that it does not satisfy the
necessary optimal-substructure property and its proposed recurrence relation is
incorrect. It remains unknown whether the algorithm is guaranteed to compute a
correct overall solution.
</summary>
    <author>
      <name>Marek Chrobak</name>
    </author>
    <author>
      <name>Mordecai Golin</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Neal E. Young</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal version of some results from arXiv:1505.00357</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.03783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.03783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P10, 68P30, 68W25, 94A45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; G.1.6; G.2.2; H.3.1; I.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.04358">
    <id>http://arxiv.org/abs/1901.04358v1</id>
    <updated>2019-01-14T15:08:16Z</updated>
    <published>2019-01-14T15:08:16Z</published>
    <title>Quotient Hash Tables - Efficiently Detecting Duplicates in Streaming
  Data</title>
    <summary>  This article presents the Quotient Hash Table (QHT) a new data structure for
duplicate detection in unbounded streams. QHTs stem from a corrected analysis
of streaming quotient filters (SQFs), resulting in a 33\% reduction in memory
usage for equal performance. We provide a new and thorough analysis of both
algorithms, with results of interest to other existing constructions.
  We also introduce an optimised version of our new data structure dubbed
Queued QHT with Duplicates (QQHTD).
  Finally we discuss the effect of adversarial inputs for hash-based duplicate
filters similar to QHT.
</summary>
    <author>
      <name>R√©mi G√©raud</name>
    </author>
    <author>
      <name>Marius Lombard-Platet</name>
    </author>
    <author>
      <name>David Naccache</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3297280.3297335</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3297280.3297335" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Shorter version was accepted at SIGAPP SAC '19</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.04358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.04358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.04911">
    <id>http://arxiv.org/abs/1901.04911v1</id>
    <updated>2019-01-15T16:24:07Z</updated>
    <published>2019-01-15T16:24:07Z</published>
    <title>Unconstrained Church-Turing thesis cannot possibly be true</title>
    <summary>  The Church-Turing thesis asserts that if a partial strings-to-strings
function is effectively computable then it is computable by a Turing machine.
  In the 1930s, when Church and Turing worked on their versions of the thesis,
there was a robust notion of algorithm. These traditional algorithms are known
also as classical or sequential. In the original thesis, effectively computable
meant computable by an effective classical algorithm. Based on an earlier
axiomatization of classical algorithms, the original thesis was proven in 2008.
  Since the 1930s, the notion of algorithm has changed dramatically. New
species of algorithms have been and are being introduced. We argue that the
generalization of the original thesis, where effectively computable means
computable by an effective algorithm of any species, cannot possibly be true.
</summary>
    <author>
      <name>Yuri Gurevich</name>
    </author>
    <link href="http://arxiv.org/abs/1901.04911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.04911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.05226">
    <id>http://arxiv.org/abs/1901.05226v3</id>
    <updated>2019-01-22T17:00:22Z</updated>
    <published>2019-01-16T10:53:33Z</published>
    <title>Space-Efficient Computation of the LCP Array from the Burrows-Wheeler
  Transform</title>
    <summary>  We show that the Longest Common Prefix Array of a text collection of total
size n on alphabet [1, {\sigma}] can be computed from the Burrows-Wheeler
transformed collection in O(n log {\sigma}) time using o(n log {\sigma}) bits
of working space on top of the input and output. Our result improves (on small
alphabets) and generalizes (to string collections) the previous solution from
Beller et al., which required O(n) bits of extra working space. We also show
how to merge the BWTs of two collections of total size n within the same time
and space bounds. The procedure at the core of our algorithms can be used to
enumerate suffix tree intervals in succinct space from the BWT, which is of
independent interest. An engineered implementation of our first algorithm on
DNA alphabet induces the LCP of a large (16 GiB) collection of short (100
bases) reads at a rate of 2.92 megabases per second using in total 1.5 Bytes
per base in RAM. Our second algorithm merges the BWTs of two short-reads
collections of 8 GiB each at a rate of 1.7 megabases per second and uses 0.625
Bytes per base in RAM. An extension of this algorithm that computes also the
LCP array of the merged collection processes the data at a rate of 1.48
megabases per second and uses 1.625 Bytes per base in RAM.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <link href="http://arxiv.org/abs/1901.05226v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05226v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.05264">
    <id>http://arxiv.org/abs/1901.05264v2</id>
    <updated>2019-02-08T16:33:56Z</updated>
    <published>2019-01-16T12:59:07Z</published>
    <title>On the Complexity of Exact Pattern Matching in Graphs: Binary Strings
  and Bounded Degree</title>
    <summary>  Exact pattern matching in labeled graphs is the problem of searching paths of
a graph $G=(V,E)$ that spell the same string as the pattern $P[1..m]$. This
basic problem can be found at the heart of more complex operations on variation
graphs in computational biology, of query operations in graph databases, and of
analysis operations in heterogeneous networks, where the nodes of some paths
must match a sequence of labels or types. We describe a simple conditional
lower bound that, for any constant $\epsilon>0$, an $O(|E|^{1 - \epsilon} \,
m)$-time or an $O(|E| \, m^{1 - \epsilon})$-time algorithm for exact pattern
matching on graphs, with node labels and patterns drawn from a binary alphabet,
cannot be achieved unless the Strong Exponential Time Hypothesis (SETH) is
false. The result holds even if restricted to undirected graphs of maximum
degree three or directed acyclic graphs of maximum sum of indegree and
outdegree three. Although a conditional lower bound of this kind can be somehow
derived from previous results (Backurs and Indyk, FOCS'16), we give a direct
reduction from SETH for dissemination purposes, as the result might interest
researchers from several areas, such as computational biology, graph database,
and graph mining, as mentioned before. Indeed, as approximate pattern matching
on graphs can be solved in $O(|E|\,m)$ time, exact and approximate matching are
thus equally hard (quadratic time) on graphs under the SETH assumption. In
comparison, the same problems restricted to strings have linear time vs
quadratic time solutions, respectively, where the latter ones have a matching
SETH lower bound on computing the edit distance of two strings (Backurs and
Indyk, STOC'15).
</summary>
    <author>
      <name>Massimo Equi</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Veli M√§kinen</name>
    </author>
    <link href="http://arxiv.org/abs/1901.05264v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05264v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.1; F.2.2; G.2.2; H.2.3; H.2.8; H.3.3; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.06493">
    <id>http://arxiv.org/abs/1901.06493v1</id>
    <updated>2019-01-19T09:53:29Z</updated>
    <published>2019-01-19T09:53:29Z</published>
    <title>Dynamic Partition Bloom Filters: A Bounded False Positive Solution For
  Dynamic Set Membership (Extended Abstract)</title>
    <summary>  Dynamic Bloom filters (DBF) were proposed by Guo et. al. in 2010 to tackle
the situation where the size of the set to be stored compactly is not known in
advance or can change during the course of the application. We propose a novel
competitor to DBF with the following important property that DBF is not able to
achieve: our structure is able to maintain a bound on the false positive rate
for the set membership query across all possible sizes of sets that are stored
in it. The new data structure we propose is a dynamic structure that we call
Dynamic Partition Bloom filter (DPBF). DPBF is based on our novel concept of a
Bloom partition tree which is a tree structure with standard Bloom filters at
the leaves. DPBF is superior to standard Bloom filters because it can
efficiently handle a large number of unions and intersections of sets of
different sizes while controlling the false positive rate. This makes DPBF the
first structure to do so to the best of our knowledge. We provide theoretical
bounds comparing the false positive probability of DPBF to DBF.
</summary>
    <author>
      <name>Sidharth Negi</name>
    </author>
    <author>
      <name>Ameya Dubey</name>
    </author>
    <author>
      <name>Amitabha Bagchi</name>
    </author>
    <author>
      <name>Manish Yadav</name>
    </author>
    <author>
      <name>Nishant Yadav</name>
    </author>
    <author>
      <name>Jeetu Raj</name>
    </author>
    <link href="http://arxiv.org/abs/1901.06493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.06493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.07502">
    <id>http://arxiv.org/abs/1901.07502v1</id>
    <updated>2019-01-22T18:24:32Z</updated>
    <published>2019-01-22T18:24:32Z</published>
    <title>Palindromic Subsequences in Finite Words</title>
    <summary>  In 1999 Lyngs{\o} and Pedersen proposed a conjecture stating that every
binary circular word of length $n$ with equal number of zeros and ones has an
antipalindromic linear subsequence of length at least $\frac{2}{3}n$. No
progress over a trivial $\frac{1}{2}n$ bound has been achieved since then. We
suggest a palindromic counterpart to this conjecture and provide a non-trivial
infinite series of circular words which prove the upper bound of $\frac{2}{3}n$
for both conjectures at the same time. The construction also works for words
over an alphabet of size $k$ and gives rise to a generalization of the
conjecture by Lyngs{\o} and Pedersen. Moreover, we discuss some possible
strengthenings and weakenings of the named conjectures. We also propose two
similar conjectures for linear words and provide some evidences for them.
</summary>
    <author>
      <name>Clemens M√ºllner</name>
    </author>
    <author>
      <name>Andrew Ryzhikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to LATA 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.07502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.07502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.07809">
    <id>http://arxiv.org/abs/1901.07809v1</id>
    <updated>2019-01-23T10:42:25Z</updated>
    <published>2019-01-23T10:42:25Z</published>
    <title>A randomized strategy in the mirror game</title>
    <summary>  Alice and Bob take turns (with Alice playing first) in declaring numbers from
the set $[1,2N]$. If a player declares a number that was previously declared,
that player looses and the other player wins. If all numbers are declared
without repetition, the outcome is a tie. If both players have unbounded memory
and play optimally, then the game will be tied. Garg and Schneider [ITCS 2019]
showed that if Alice has unbounded memory, then Bob can secure a tie with $\log
N$ memory, whereas if Bob has unbounded memory, then Alice needs memory linear
in $N$ in order to secure a tie.
  Garg and Schneider also considered an {\em auxiliary matching} model in which
Alice gets as an additional input a random matching $M$ over the numbers
$[1,2N]$, and storing this input does not count towards the memory used by
Alice. They showed that is this model there is a strategy for Alice that ties
with probability at least $1 - \frac{1}{N}$, and uses only $O(\sqrt{N} (\log
N)^2)$ memory. We show how to modify Alice's strategy so that it uses only
$O((\log N)^3)$ space.
</summary>
    <author>
      <name>Uriel Feige</name>
    </author>
    <link href="http://arxiv.org/abs/1901.07809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.07809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.08544">
    <id>http://arxiv.org/abs/1901.08544v3</id>
    <updated>2020-02-14T19:22:44Z</updated>
    <published>2019-01-24T18:07:59Z</published>
    <title>Learning Space Partitions for Nearest Neighbor Search</title>
    <summary>  Space partitions of $\mathbb{R}^d$ underlie a vast and important class of
fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical
work on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,
Waingarten STOC 2018, FOCS 2018], we develop a new framework for building space
partitions reducing the problem to \emph{balanced graph partitioning} followed
by \emph{supervised classification.} We instantiate this general approach with
the KaHIP graph partitioner [Sanders, Schulz SEA 2013] and neural networks,
respectively, to obtain a new partitioning procedure called Neural
Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for
NNS, our experiments show that the partitions obtained by Neural LSH
consistently outperform partitions found by quantization-based and tree-based
methods.
</summary>
    <author>
      <name>Yihe Dong</name>
    </author>
    <author>
      <name>Piotr Indyk</name>
    </author>
    <author>
      <name>Ilya Razenshteyn</name>
    </author>
    <author>
      <name>Tal Wagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.08544v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.08544v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.10974">
    <id>http://arxiv.org/abs/1812.10974v1</id>
    <updated>2018-12-28T12:24:18Z</updated>
    <published>2018-12-28T12:24:18Z</published>
    <title>A Grammar-based Compressed Representation of 3D Trajectories</title>
    <summary>  Much research has been published about trajectory management on the ground or
at the sea, but compression or indexing of flight trajectories have usually
been less explored. However, air traffic management is a challenge because
airspace is becoming more and more congested, and large flight data collections
must be preserved and exploited for varied purposes. This paper proposes
3DGraCT, a new method for representing these flight trajectories. It extends
the GraCT compact data structure to cope with a third dimension (altitude),
while retaining its space/time complexities. 3DGraCT improves space
requirements of traditional spatio-temporal data structures by two orders of
magnitude, being competitive for the considered types of queries, even leading
the comparison for a particular one.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Adri√°n G√≥mez-Brand√≥n</name>
    </author>
    <author>
      <name>Miguel A. Mart√≠nez-Prieto</name>
    </author>
    <author>
      <name>Jos√© R. Param√°</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-00479-8_9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-00479-8_9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">String Processing and Information Retrieval: 25th International
  Symposium, SPIRE 2018, Lima, Peru, October 9-11, 2018, Proceedings. Springer
  International Publishing. pp 102-116. ISBN: 9783030004781</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.10974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.10974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.11244">
    <id>http://arxiv.org/abs/1812.11244v1</id>
    <updated>2018-12-28T23:09:59Z</updated>
    <published>2018-12-28T23:09:59Z</published>
    <title>Using Compressed Suffix-Arrays for a Compact Representation of
  Temporal-Graphs</title>
    <summary>  Temporal graphs represent binary relationships that change along time. They
can model the dynamism of, for example, social and communication networks.
Temporal graphs are defined as sets of contacts that are edges tagged with the
temporal intervals when they are active. This work explores the use of the
Compressed Suffix Array (CSA), a well-known compact and self-indexed data
structure in the area of text indexing, to represent large temporal graphs. The
new structure, called Temporal Graph CSA (TGCSA), is experimentally compared
with the most competitive compact data structures in the state-of-the-art,
namely, EDGELOG and CET. The experimental results show that TGCSA obtains a
good space-time trade-off. It uses a reasonable space and is efficient for
solving complex temporal queries. Furthermore, TGCSA has wider expressive
capabilities than EDGELOG and CET, because it is able to represent temporal
graphs where contacts on an edge can temporally overlap.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Diego Caro</name>
    </author>
    <author>
      <name>Antonio Fari√±a</name>
    </author>
    <author>
      <name>M. Andrea Rodriguez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2018.07.023</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2018.07.023" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, Information Sciences</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Sciences Volume 465, October 2018, Pages 459-483</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.11244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.11244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.11249">
    <id>http://arxiv.org/abs/1812.11249v1</id>
    <updated>2018-12-28T23:57:14Z</updated>
    <published>2018-12-28T23:57:14Z</published>
    <title>A Compact Representation for Trips over Networks built on self-indexes</title>
    <summary>  Representing the movements of objects (trips) over a network in a compact way
while retaining the capability of exploiting such data effectively is an
important challenge of real applications. We present a new Compact Trip
Representation (CTR) that handles the spatio-temporal data associated with
users' trips over transportation networks. Depending on the network and types
of queries, nodes in the network can represent intersections, stops, or even
street segments.
  CTR represents separately sequences of nodes and the time instants when users
traverse these nodes. The spatial component is handled with a data structure
based on the well-known Compressed Suffix Array (CSA), which provides both a
compact representation and interesting indexing capabilities. The temporal
component is self-indexed with either a Hu-Tucker-shaped Wavelet-tree or a
Wavelet Matrix that solve range-interval queries efficiently. We show how CTR
can solve relevant counting-based spatial, temporal, and spatio-temporal
queries over large sets of trips. Experimental results show the space
requirements (around 50-70% of the space needed by a compact non-indexed
baseline) and query efficiency (most queries are solved in the range of 1-1000
microseconds) of CTR.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Antonio Fari√±a</name>
    </author>
    <author>
      <name>Daniil Galaktionov</name>
    </author>
    <author>
      <name>M. Andrea Rodriguez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.is.2018.06.010</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.is.2018.06.010" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Systems, Volume 78, November 2018, Pages 1-22</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.11249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.11249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.00718">
    <id>http://arxiv.org/abs/1901.00718v1</id>
    <updated>2019-01-03T13:58:15Z</updated>
    <published>2019-01-03T13:58:15Z</published>
    <title>Mergeable Dictionaries With Shifts</title>
    <summary>  We revisit the mergeable dictionaries with shift problem, where the goal is
to maintain a family of sets subject to search, split, merge, make-set, and
shift operations. The search, split, and make-set operations are the usual
well-known textbook operations. The merge operation merges two sets and the
shift operation adds or subtracts an integer from all elements in a set. Note
that unlike the join operation on standard balanced search tree structures,
such as AVL trees or 2-4 trees, the merge operation has no restriction on the
key space of the input sets and supports merging arbitrarily interleaved sets.
This problem is a key component in searching Lempel-Ziv compressed texts, in
the mergeable trees problem, and in the union-split-find problem.
  We present the first solution achieving O(log U) amortized time for all
operations, where {1, 2, ..., U} is the universe of the sets. This bound is
optimal when the size of the universe is polynomially bounded by the sum of the
sizes of the sets. Our solution is simple and based on a novel extension of
biased search trees.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Mikko Berggren Etienne</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <link href="http://arxiv.org/abs/1901.00718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.00718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.01665">
    <id>http://arxiv.org/abs/1901.01665v1</id>
    <updated>2019-01-07T04:48:34Z</updated>
    <published>2019-01-07T04:48:34Z</published>
    <title>Communication cost of consensus for nodes with limited memory</title>
    <summary>  Motivated by applications in blockchains and sensor networks, we consider a
model of $n$ nodes trying to reach consensus on their majority bit. Each node
$i$ is assigned a bit at time zero, and is a finite automaton with $m$ bits of
memory (i.e., $2^m$ states) and a Poisson clock. When the clock of $i$ rings,
$i$ can choose to communicate, and is then matched to a uniformly chosen node
$j$. The nodes $j$ and $i$ may update their states based on the state of the
other node. Previous work has focused on minimizing the time to consensus and
the probability of error, while our goal is minimizing the number of
communications. We show that when $m>3 \log\log\log(n)$, consensus can be
reached at linear communication cost, but this is impossible if
$m&lt;\log\log\log(n)$. We also study a synchronous variant of the model, where
our upper and lower bounds on $m$ for achieving linear communication cost are
$2\log\log\log(n)$ and $\log\log\log(n)$, respectively. A key step is to
distinguish when nodes can become aware of knowing the majority bit and stop
communicating. We show that this is impossible if their memory is too low.
</summary>
    <author>
      <name>Giulia Fanti</name>
    </author>
    <author>
      <name>Nina Holden</name>
    </author>
    <author>
      <name>Yuval Peres</name>
    </author>
    <author>
      <name>Gireeja Ranade</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">62 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.01665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.01665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.01825">
    <id>http://arxiv.org/abs/1901.01825v2</id>
    <updated>2019-01-11T08:26:58Z</updated>
    <published>2019-01-07T14:21:01Z</published>
    <title>Multiple Set Matching and Pre-Filtering with Bloom Multifilters</title>
    <summary>  Bloom filter is a space-efficient probabilistic data structure for checking
elements' membership in a set. Given multiple sets, however, a standard Bloom
filter is not sufficient when looking for the items to which an element or a
set of input elements belong to. In this article, we solve multiple set
matching problem by proposing two efficient Bloom Multifilters called Bloom
Matrix and Bloom Vector. Both of them are space efficient and answer queries
with a set of identifiers for multiple set matching problems. We show that the
space efficiency can be optimized further according to the distribution of
labels among multiple sets: Uniform and Zipf. While both of them are space
efficient, Bloom Vector can efficiently exploit Zipf distribution of data for
further space reduction. Our results also highlight that basic ADD and LOOKUP
operations on Bloom Matrix are faster than on Bloom Vector. However, Bloom
Matrix does not meet the theoretical false positive rate of less than $10^{-2}$
for LOOKUP operations if the represented data or the labels are not uniformly
distributed among the multiple sets. Consequently, we introduce \textit{Bloom
Test} which uses Bloom Matrix as the pre-filter structure to determine which
structure is suitable for improved performance with an arbitrary input dataset.
</summary>
    <author>
      <name>Francesco Concas</name>
    </author>
    <author>
      <name>Pengfei Xu</name>
    </author>
    <author>
      <name>Mohammad A. Hoque</name>
    </author>
    <author>
      <name>Jiaheng Lu</name>
    </author>
    <author>
      <name>Sasu Tarkoma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 11 figures, Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.01825v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.01825v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.01926">
    <id>http://arxiv.org/abs/1901.01926v1</id>
    <updated>2019-01-07T17:14:25Z</updated>
    <published>2019-01-07T17:14:25Z</published>
    <title>An in-place, subquadratic algorithm for permutation inversion</title>
    <summary>  We assume the permutation $\pi$ is given by an $n$-element array in which the
$i$-th element denotes the value $\pi(i)$. Constructing its inverse in-place
(i.e. using $O(\log{n})$ bits of additional memory) can be achieved in linear
time with a simple algorithm. Limiting the numbers that can be stored in our
array to the range $[1...n]$ still allows a straightforward $O(n^2)$ time
solution. The time complexity can be improved using randomization, but this
only improves the expected, not the pessimistic running time. We present a
deterministic algorithm that runs in $O(n^{3/2})$ time.
</summary>
    <author>
      <name>Grzegorz Gu≈õpiel</name>
    </author>
    <link href="http://arxiv.org/abs/1901.01926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.01926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.01944">
    <id>http://arxiv.org/abs/1901.01944v1</id>
    <updated>2019-01-07T17:52:23Z</updated>
    <published>2019-01-07T17:52:23Z</published>
    <title>A Compact Representation of Raster Time Series</title>
    <summary>  The raster model is widely used in Geographic Information Systems to
represent data that vary continuously in space, such as temperatures,
precipitations, elevation, among other spatial attributes. In applications like
weather forecast systems, not just a single raster, but a sequence of rasters
covering the same region at different timestamps, known as a raster time
series, needs to be stored and queried. Compact data structures have proven
successful to provide space-efficient representations of rasters with query
capabilities. Hence, a naive approach to save space is to use such a
representation for each raster in a time series. However, in this paper we show
that it is possible to take advantage of the temporal locality that exists in a
raster time series to reduce the space necessary to store it while keeping
competitive query times for several types of queries.
</summary>
    <author>
      <name>Nataly Cruces</name>
    </author>
    <author>
      <name>Diego Seco</name>
    </author>
    <author>
      <name>Gilberto Guti√©rrez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Data Compression Conference (DCC 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1901.01944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.01944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.03155">
    <id>http://arxiv.org/abs/1901.03155v2</id>
    <updated>2019-01-17T05:22:50Z</updated>
    <published>2019-01-10T13:41:19Z</published>
    <title>Entropy Bounds for Grammar-Based Tree Compressors</title>
    <summary>  The definition of $k^{th}$-order empirical entropy of strings is extended to
node labelled binary trees. A suitable binary encoding of tree straight-line
programs (that have been used for grammar-based tree compression before) is
shown to yield binary tree encodings of size bounded by the $k^{th}$-order
empirical entropy plus some lower order terms. This generalizes recent results
for grammar-based string compression to grammar-based tree compression.
</summary>
    <author>
      <name>Danny Hucke</name>
    </author>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Louisa Seelbach Benkner</name>
    </author>
    <link href="http://arxiv.org/abs/1901.03155v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.03155v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P30" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.03689">
    <id>http://arxiv.org/abs/1901.03689v1</id>
    <updated>2019-01-11T18:50:21Z</updated>
    <published>2019-01-11T18:50:21Z</published>
    <title>Depth First Search in the Semi-streaming Model</title>
    <summary>  Depth first search (DFS) tree is a fundamental data structure for solving
various graph problems. The classical DFS algorithm requires $O(m+n)$ time for
a graph having $n$ vertices and $m$ edges. In the streaming model, an algorithm
is allowed several passes (preferably single) over the input graph having a
restriction on the size of local space used.
  Trivially, a DFS tree can be computed using a single pass using $O(m)$ space.
In the semi-streaming model allowing $O(n)$ space, it can be computed in $O(n)$
passes, where each pass adds one vertex to the DFS tree. However, it remains an
open problem to compute a DFS tree using $o(n)$ passes using $o(m)$ space even
in any relaxed streaming environment.
  We present the first semi-streaming algorithms that compute a DFS tree of an
undirected graph in $o(n)$ passes using $o(m)$ space. We first describe an
extremely simple algorithm that requires at most $\lceil n/k\rceil$ passes
using $O(nk)$ space, where $k$ is any positive integer. We then improve this
algorithm by using more involved techniques to reduce the number of passes to
$\lceil h/k\rceil$ under similar space constraints, where $h$ is the height of
the computed DFS tree. In particular, this algorithm improves the bounds for
the case where the computed DFS tree is shallow (having $o(n)$ height).
Moreover, this algorithm is presented as a framework that allows the
flexibility of using any algorithm to maintain a DFS tree of a stored sparser
subgraph as a black box, which may be of independent interest. Both these
algorithms essentially demonstrate the existence of a trade-off between the
space and number of passes required for computing a DFS tree. Furthermore, we
evaluate these algorithms experimentally which reveals their exceptional
performance in practice. For both random and real graphs, they require merely a
few passes even when allowed just $O(n)$ space.
</summary>
    <author>
      <name>Shahbaz Khan</name>
    </author>
    <author>
      <name>Shashank K. Mehta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 6 Figures, STACS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.03689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.03689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; G.2.2; G.4; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.02570">
    <id>http://arxiv.org/abs/1812.02570v2</id>
    <updated>2019-06-26T18:14:23Z</updated>
    <published>2018-12-06T14:51:25Z</published>
    <title>ALLSAT compressed with wildcards: Partitionings and face-numbers of
  simplicial complexes</title>
    <summary>  Given the facets of a finite simplicial complex, we use wildcards to
enumerate its faces in compressed fashion. Our algorithm, coded in high-level
Mathematica code, compares favorably to the hardwired Mathematica command
BooleanConvert (=exclusive sums of products). As to running time, depending on
the particular problem instance, either method can excel. When our method wins
it may not just beat BooleanConvert but also SatisfiabilityCount (although the
latter is only asked to c o u n t). Independent of running time, our
compression rate is always higher.
</summary>
    <author>
      <name>Marcel Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.02570v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.02570v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.04261">
    <id>http://arxiv.org/abs/1812.04261v1</id>
    <updated>2018-12-11T08:17:52Z</updated>
    <published>2018-12-11T08:17:52Z</published>
    <title>LZRR: LZ77 Parsing with Right Reference</title>
    <summary>  Lossless data compression has been widely studied in computer science. One of
the most widely used lossless data compressions is Lempel-Zip(LZ) 77 parsing,
which achieves a high compression ratio. Bidirectional (a.k.a. macro) parsing
is a lossless data compression and computes a sequence of phrases copied from
another substring (target phrase) on either the left or the right position in
an input string. Gagie et al.(LATIN 2018) recently showed that a large gap
exists between the number of smallest bidirectional phrases of a given string
and that of LZ77 phrases. In addition, finding the smallest bidirectional parse
of a given text is NP-complete. Several variants of bidirectional parsing have
been proposed thus far, but no prior work for bidirectional parsing has
achieved high compression that is smaller than that of LZ77 phrasing for any
string. In this paper, we present the first practical bidirectional parsing
named LZ77 parsing with right reference (LZRR), in which the number of LZRR
phrases is theoretically guaranteed to be smaller than the number of LZ77
phrases. Experimental results using benchmark strings show the number of LZRR
phrases is approximately five percent smaller than that of LZ77 phrases.
</summary>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/1812.04261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.04261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1712.04886">
    <id>http://arxiv.org/abs/1712.04886v8</id>
    <updated>2019-05-20T01:00:47Z</updated>
    <published>2017-12-13T17:56:24Z</published>
    <title>Optimal Construction of Compressed Indexes for Highly Repetitive Texts</title>
    <summary>  We propose algorithms that, given the input string of length $n$ over integer
alphabet of size $\sigma$, construct the Burrows-Wheeler transform (BWT), the
permuted longest-common-prefix (PLCP) array, and the LZ77 parsing in
$O(n/\log_{\sigma}n+r\,{\rm polylog}\,n)$ time and working space, where $r$ is
the number of runs in the BWT of the input. These are the essential components
of many compressed indexes such as compressed suffix tree, FM-index, and
grammar and LZ77-based indexes, but also find numerous applications in sequence
analysis and data compression. The value of $r$ is a common measure of
repetitiveness that is significantly smaller than $n$ if the string is highly
repetitive. Since just accessing every symbol of the string requires
$\Omega(n/\log_{\sigma}n)$ time, the presented algorithms are time and space
optimal for inputs satisfying the assumption $n/r\in\Omega({\rm polylog}\,n)$
on the repetitiveness. For such inputs our result improves upon the currently
fastest general algorithms of Belazzougui (STOC 2014) and Munro et al. (SODA
2017) which run in $O(n)$ time and use $O(n/\log_{\sigma} n)$ working space. We
also show how to use our techniques to obtain optimal solutions on highly
repetitive data for other fundamental string processing problems such as:
Lyndon factorization, construction of run-length compressed suffix arrays, and
some classical "textbook" problems such as computing the longest substring
occurring at least some fixed number of times.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <link href="http://arxiv.org/abs/1712.04886v8" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04886v8" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.05306">
    <id>http://arxiv.org/abs/1812.05306v1</id>
    <updated>2018-12-13T08:04:09Z</updated>
    <published>2018-12-13T08:04:09Z</published>
    <title>Optimal Algorithm for Profiling Dynamic Arrays with Finite Values</title>
    <summary>  How can one quickly answer the most and top popular objects at any time,
given a large log stream in a system of billions of users? It is equivalent to
find the mode and top-frequent elements in a dynamic array corresponding to the
log stream. However, most existing work either restrain the dynamic array
within a sliding window, or do not take advantages of only one element can be
added or removed in a log stream. Therefore, we propose a profiling algorithm,
named S-Profile, which is of $O(1)$ time complexity for every updating of the
dynamic array, and optimal in terms of computational complexity. With the
profiling results, answering the queries on the statistics of dynamic array
becomes trivial and fast. With the experiments of various settings of dynamic
arrays, our accurate S-Profile algorithm outperforms the well-known methods,
showing at least 2X speedup to the heap based approach and 13X or larger
speedup to the balanced tree based approach.
</summary>
    <author>
      <name>Dingcheng Yang</name>
    </author>
    <author>
      <name>Wenjian Yu</name>
    </author>
    <author>
      <name>Junhui Deng</name>
    </author>
    <author>
      <name>Shenghua Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.05306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.05306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.06177">
    <id>http://arxiv.org/abs/1812.06177v5</id>
    <updated>2020-03-03T02:54:28Z</updated>
    <published>2018-12-14T21:40:09Z</published>
    <title>Simple Concurrent Labeling Algorithms for Connected Components</title>
    <summary>  We study a class of simple algorithms for concurrently computing the
connected components of an $n$-vertex, $m$-edge graph. Our algorithms are easy
to implement in either the COMBINING CRCW PRAM or the MPC computing model. For
two related algorithms in this class, we obtain $\Theta(\lg n)$ step and
$\Theta(m \lg n)$ work bounds. For two others, we obtain $O(\lg^2 n)$ step and
$O(m \lg^2 n)$ work bounds, which are tight for one of them. All our algorithms
are simpler than related algorithms in the literature. We also point out some
gaps and errors in the analysis of previous algorithms. Our results show that
even a basic problem like connected components still has secrets to reveal.
</summary>
    <author>
      <name>S. Cliff Liu</name>
    </author>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.06177v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.06177v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.06778">
    <id>http://arxiv.org/abs/1812.06778v3</id>
    <updated>2018-12-24T20:40:40Z</updated>
    <published>2018-12-13T22:40:34Z</published>
    <title>Minuet: A method to solve Sudoku puzzles by hand</title>
    <summary>  This paper presents a systematic method to solve difficult 9 x 9 Sudoku
puzzles by hand. While computer algorithms exist to solve these puzzles, these
algorithms are not good for human's to use because they involve too many steps
and require too much memory. For humans, all one can find in the literature are
individual tricks, which used together in ad hoc ways can be used to solve some
puzzles--but not all. To the author's knowledge, a systematic procedure made up
of well-defined steps that can be carried out by hand and solve all puzzles has
not been devised. This paper proposes one such technique--the "minuet" method.
It is based on a new system of markings and a new way of simplifying the
puzzles that can be easily carried out by hand--or by computer. The author has
solved hundreds of puzzles of the most difficult kind ("evil" in Sudoku's
parlance) and never found one that could not be solved. The average time to
solve one of these puzzles is slightly over 1 hour. It is conjectured that this
method can solve all well-posed 9 x 9 puzzles. The method's distinguishing
feature is a "minuet" strategy that is applied when the puzzle cannot be
further simplified with basic tricks. The strategy consists in concurrently
developing two potential solutions, sometimes alone and sometimes in concert as
if they were dancing a minuet.
</summary>
    <author>
      <name>Carlos F. Daganzo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; Working Paper, University of California, Berkeley</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.06778v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.06778v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.07030">
    <id>http://arxiv.org/abs/1812.07030v1</id>
    <updated>2018-12-17T19:59:44Z</updated>
    <published>2018-12-17T19:59:44Z</published>
    <title>A Fast Combination of AES Encryption and LZ4 Compression Algorithms</title>
    <summary>  From a long time ago, beside encryption of data and making it secure,
compression packing it was also important that could make transmission of data
faster. In the past years need for improvement of encryption and compression
for a fast and easy transmission is more necessary. In this paper, a new method
for combination of LZ4 combination and AES encryption algorithms for a fast and
easy packing, securing and compressing of data is presented. Choose of these
two algorithms was for some special features of them about aim of this paper.
This paper also is introducing a method for Parallelism of compression and
encryption in a special way for improvement of speed and security of data.
</summary>
    <author>
      <name>Saber Malekzadeh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.33644.56960</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.33644.56960" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the 3rd International Conference on applied research in
  Computer Science and Information Technology. in Farsi</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.07030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.07030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.08101">
    <id>http://arxiv.org/abs/1812.08101v1</id>
    <updated>2018-12-19T17:22:32Z</updated>
    <published>2018-12-19T17:22:32Z</published>
    <title>Efficient Representation and Counting of Antipower Factors in Words</title>
    <summary>  A $k$-antipower (for $k \ge 2$) is a concatenation of $k$ pairwise distinct
words of the same length. The study of antipower factors of a word was
initiated by Fici et al. (ICALP 2016) and first algorithms for computing
antipower factors were presented by Badkobeh et al. (Inf. Process. Lett.,
2018). We address two open problems posed by Badkobeh et al. Our main results
are algorithms for counting and reporting factors of a word which are
$k$-antipowers. They work in $\mathcal{O}(nk \log k)$ time and $\mathcal{O}(nk
\log k + C)$ time, respectively, where $C$ is the number of reported factors.
For $k=o(\sqrt{n/\log n})$, this improves the time complexity of
$\mathcal{O}(n^2/k)$ of the solution by Badkobeh et al. Our main algorithmic
tools are runs and gapped repeats. We also present an improved data structure
that checks, for a given factor of a word and an integer $k$, if the factor is
a $k$-antipower.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszy≈Ñski</name>
    </author>
    <author>
      <name>Tomasz Wale≈Ñ</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper accepted to LATA 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.08101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.08101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.09094">
    <id>http://arxiv.org/abs/1812.09094v3</id>
    <updated>2019-11-02T18:17:50Z</updated>
    <published>2018-12-21T13:03:06Z</published>
    <title>A Simple Algorithm for Computing the Document Array</title>
    <summary>  We present a simple algorithm for computing the document array given a string
collection and its suffix array as input. Our algorithm runs in linear time
using constant additional space for strings from constant alphabets.
</summary>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <link href="http://arxiv.org/abs/1812.09094v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.09094v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.10950">
    <id>http://arxiv.org/abs/1812.10950v2</id>
    <updated>2019-02-15T14:16:45Z</updated>
    <published>2018-12-28T10:43:12Z</published>
    <title>Fast Breadth-First Search in Still Less Space</title>
    <summary>  It is shown that a breadth-first search in a directed or undirected graph
with $n$ vertices and $m$ edges can be carried out in $O(n+m)$ time with
$n\log_2 3+O((\log n)^2)$ bits of working memory.
</summary>
    <author>
      <name>Torben Hagerup</name>
    </author>
    <link href="http://arxiv.org/abs/1812.10950v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.10950v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.02457">
    <id>http://arxiv.org/abs/1811.02457v2</id>
    <updated>2019-05-29T12:39:32Z</updated>
    <published>2018-11-06T16:08:32Z</published>
    <title>Tunneling on Wheeler Graphs</title>
    <summary>  The Burrows-Wheeler Transform (BWT) is an important technique both in data
compression and in the design of compact indexing data structures. It has been
generalized from single strings to collections of strings and some classes of
labeled directed graphs, such as tries and de Bruijn graphs. The BWTs of
repetitive datasets are often compressible using run-length compression, but
recently Baier (CPM 2018) described how they could be even further compressed
using an idea he called tunneling. In this paper we show that tunneled BWTs can
still be used for indexing and extend tunneling to the BWTs of Wheeler graphs,
a framework that includes all the generalizations mentioned above.
</summary>
    <author>
      <name>Jarno Alanko</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Louisa Seelbach Benkner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 Pages, 1 figure. This research has received funding from the
  European Union's Horizon 2020 research and innovation programme under the
  Marie Sk{\l}odowska-Curie Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.02457v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.02457v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.02177">
    <id>http://arxiv.org/abs/1811.02177v1</id>
    <updated>2018-11-06T06:05:14Z</updated>
    <published>2018-11-06T06:05:14Z</published>
    <title>The entropy of lies: playing twenty questions with a liar</title>
    <summary>  `Twenty questions' is a guessing game played by two players: Bob thinks of an
integer between $1$ and $n$, and Alice's goal is to recover it using a minimal
number of Yes/No questions. Shannon's entropy has a natural interpretation in
this context. It characterizes the average number of questions used by an
optimal strategy in the distributional variant of the game: let $\mu$ be a
distribution over $[n]$, then the average number of questions used by an
optimal strategy that recovers $x\sim \mu$ is between $H(\mu)$ and $H(\mu)+1$.
We consider an extension of this game where at most $k$ questions can be
answered falsely. We extend the classical result by showing that an optimal
strategy uses roughly $H(\mu) + k H_2(\mu)$ questions, where $H_2(\mu) = \sum_x
\mu(x)\log\log\frac{1}{\mu(x)}$. This also generalizes a result by Rivest et
al. for the uniform distribution. Moreover, we design near optimal strategies
that only use comparison queries of the form `$x \leq c$?' for $c\in[n]$. The
usage of comparison queries lends itself naturally to the context of sorting,
where we derive sorting algorithms in the presence of adversarial noise.
</summary>
    <author>
      <name>Yuval Dagan</name>
    </author>
    <author>
      <name>Yuval Filmus</name>
    </author>
    <author>
      <name>Daniel Kane</name>
    </author>
    <author>
      <name>Shay Moran</name>
    </author>
    <link href="http://arxiv.org/abs/1811.02177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.02177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.02725">
    <id>http://arxiv.org/abs/1811.02725v3</id>
    <updated>2019-02-13T23:40:39Z</updated>
    <published>2018-11-07T02:11:39Z</published>
    <title>Static Data Structure Lower Bounds Imply Rigidity</title>
    <summary>  We show that static data structure lower bounds in the group (linear) model
imply semi-explicit lower bounds on matrix rigidity. In particular, we prove
that an explicit lower bound of $t \geq \omega(\log^2 n)$ on the cell-probe
complexity of linear data structures in the group model, even against
arbitrarily small linear space $(s= (1+\varepsilon)n)$, would already imply a
semi-explicit ($\bf P^{NP}\rm$) construction of rigid matrices with
significantly better parameters than the current state of art (Alon, Panigrahy
and Yekhanin, 2009). Our results further assert that polynomial ($t\geq
n^{\delta}$) data structure lower bounds against near-optimal space, would
imply super-linear circuit lower bounds for log-depth linear circuits (a
four-decade open question). In the succinct space regime $(s=n+o(n))$, we show
that any improvement on current cell-probe lower bounds in the linear model
would also imply new rigidity bounds. Our results rely on a new connection
between the "inner" and "outer" dimensions of a matrix (Paturi and Pudlak,
2006), and on a new reduction from worst-case to average-case rigidity, which
is of independent interest.
</summary>
    <author>
      <name>Zeev Dvir</name>
    </author>
    <author>
      <name>Alexander Golovnev</name>
    </author>
    <author>
      <name>Omri Weinstein</name>
    </author>
    <link href="http://arxiv.org/abs/1811.02725v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.02725v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.04596">
    <id>http://arxiv.org/abs/1811.04596v2</id>
    <updated>2019-02-18T05:29:30Z</updated>
    <published>2018-11-12T08:11:54Z</published>
    <title>MR-RePair: Grammar Compression based on Maximal Repeats</title>
    <summary>  We analyze the grammar generation algorithm of the RePair compression
algorithm and show the relation between a grammar generated by RePair and
maximal repeats. We reveal that RePair replaces step by step the most frequent
pairs within the corresponding most frequent maximal repeats. Then, we design a
novel variant of RePair, called MR-RePair, which substitutes the most frequent
maximal repeats at once instead of substituting the most frequent pairs
consecutively. We implemented MR-RePair and compared the size of the grammar
generated by MR-RePair to that by RePair on several text corpus. Our
experiments show that MR-RePair generates more compact grammars than RePair
does, especially for highly repetitive texts.
</summary>
    <author>
      <name>Isamu Furuya</name>
    </author>
    <author>
      <name>Takuya Takagi</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Takuya Kida</name>
    </author>
    <link href="http://arxiv.org/abs/1811.04596v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04596v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.04300">
    <id>http://arxiv.org/abs/1811.04300v1</id>
    <updated>2018-11-10T20:06:19Z</updated>
    <published>2018-11-10T20:06:19Z</published>
    <title>Efficiently Approximating Edit Distance Between Pseudorandom Strings</title>
    <summary>  We present an algorithm for approximating the edit distance
$\operatorname{ed}(x, y)$ between two strings $x$ and $y$ in time parameterized
by the degree to which one of the strings $x$ satisfies a natural
pseudorandomness property. The pseudorandomness model is asymmetric in that no
requirements are placed on the second string $y$, which may be constructed by
an adversary with full knowledge of $x$.
  We say that $x$ is \emph{$(p, B)$-pseudorandom} if all pairs $a$ and $b$ of
disjoint $B$-letter substrings of $x$ satisfy $\operatorname{ed}(a, b) \ge pB$.
Given parameters $p$ and $B$, our algorithm computes the edit distance between
a $(p, B)$-pseudorandom string $x$ and an arbitrary string $y$ within a factor
of $O(1/p)$ in time $\tilde{O}(nB)$, with high probability.
  Our algorithm is robust in the sense that it can handle a small portion of
$x$ being adversarial (i.e., not satisfying the pseudorandomness property). In
this case, the algorithm incurs an additive approximation error proportional to
the fraction of $x$ which behaves maliciously.
  The asymmetry of our pseudorandomness model has particular appeal for the
case where $x$ is a \emph{source string}, meaning that $\operatorname{ed}(x,
y)$ will be computed for many strings $y$. Suppose that one wishes to achieve
an $O(\alpha)$-approximation for each $\operatorname{ed}(x, y)$ computation,
and that $B$ is the smallest block-size for which the string $x$ is $(1/\alpha,
B)$-pseudorandom. We show that without knowing $B$ beforehand, $x$ may be
preprocessed in time $\tilde{O}(n^{1.5}\sqrt{B})$, so that all future
computations of the form $\operatorname{ed}(x, y)$ may be
$O(\alpha)$-approximated in time $\tilde{O}(nB)$. Furthermore, for the special
case where only a single $\operatorname{ed}(x, y)$ computation will be
performed, we show how to achieve an $O(\alpha)$-approximation in time
$\tilde{O}(n^{4/3}B^{2/3})$.
</summary>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SODA 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.04300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.06933">
    <id>http://arxiv.org/abs/1811.06933v1</id>
    <updated>2018-11-16T17:33:22Z</updated>
    <published>2018-11-16T17:33:22Z</published>
    <title>Efficient Construction of a Complete Index for Pan-Genomics Read
  Alignment</title>
    <summary>  While short read aligners, which predominantly use the FM-index, are able to
easily index one or a few human genomes, they do not scale well to indexing
databases containing thousands of genomes. To understand why, it helps to
examine the main components of the FM-index in more detail, which is a rank
data structure over the Burrows-Wheeler Transform (BWT) of the string that will
allow us to find the interval in the string's suffix array (SA) containing
pointers to starting positions of occurrences of a given pattern; second, a
sample of the SA that --- when used with the rank data structure --- allows us
access the SA. The rank data structure can be kept small even for large genomic
databases, by run-length compressing the BWT, but until recently there was no
means known to keep the SA sample small without greatly slowing down access to
the SA. Now that Gagie et al. (SODA 2018) have defined an SA sample that takes
about the same space as the run-length compressed BWT --- we have the design
for efficient FM-indexes of genomic databases but are faced with the problem of
building them. In 2018 we showed how to build the BWT of large genomic
databases efficiently (WABI 2018) but the problem of building Gagie et al.'s SA
sample efficiently was left open. We compare our approach to state-of-the-art
methods for constructing the SA sample, and demonstrate that it is the fastest
and most space-efficient method on highly repetitive genomic databases. Lastly,
we apply our method for indexing partial and whole human genomes, and show that
it improves over Bowtie with respect to both memory and time.
</summary>
    <author>
      <name>Alan Kuhnle</name>
    </author>
    <author>
      <name>Taher Mun</name>
    </author>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Ben Langmead</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <link href="http://arxiv.org/abs/1811.06933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.10498">
    <id>http://arxiv.org/abs/1811.10498v1</id>
    <updated>2018-11-26T16:45:30Z</updated>
    <published>2018-11-26T16:45:30Z</published>
    <title>An optimized Parallel Failure-less Aho-Corasick algorithm for DNA
  sequence matching</title>
    <summary>  The Aho-Corasick algorithm is multiple patterns searching algorithm running
sequentially in various applications like network intrusion detection and
bioinformatics for finding several input strings within a given large input
string. The parallel version of the Aho-Corasick algorithm is called as
Parallel Failure-less Aho-Corasick algorithm because it doesn't need failure
links like in the original Aho-Corasick algorithm. In this research, we
implemented an application specific parallel failureless Aho-Corasick algorithm
to the general purpose graphics processing unit by applying several cache
optimization techniques for matching DNA sequences. Our parallel Aho-Corasick
algorithm shows better performance than the available parallel Aho-Corasick
algorithm library due to its simplicity and optimized cache memory usage of
graphics processing units for matching DNA sequences.
</summary>
    <author>
      <name>Vajira Thambawita</name>
    </author>
    <author>
      <name>Roshan G. Ragel</name>
    </author>
    <author>
      <name>Dhammike Elkaduwe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICIAFS.2016.7946533</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICIAFS.2016.7946533" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, 4 tables, 5 graphs, 2016 IEEE International
  Conference on Information and Automation for Sustainability (ICIAfS)</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.10498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.10498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.12779">
    <id>http://arxiv.org/abs/1811.12779v6</id>
    <updated>2019-09-04T18:48:37Z</updated>
    <published>2018-11-30T13:16:24Z</published>
    <title>Optimal-Time Dictionary-Compressed Indexes</title>
    <summary>  We describe the first self-indexes able to count and locate pattern
occurrences in optimal time within a space bounded by the size of the most
popular dictionary compressors. To achieve this result we combine several
recent findings, including \emph{string attractors} --- new combinatorial
objects encompassing most known compressibility measures for highly repetitive
texts ---, and grammars based on \emph{locally-consistent parsing}.
  More in detail, let $\gamma$ be the size of the smallest attractor for a text
$T$ of length $n$. The measure $\gamma$ is an (asymptotic) lower bound to the
size of dictionary compressors based on Lempel--Ziv, context-free grammars, and
many others. The smallest known text representations in terms of attractors use
space $O(\gamma\log(n/\gamma))$, and our lightest indexes work within the same
asymptotic space. Let $\epsilon>0$ be a suitably small constant fixed at
construction time, $m$ be the pattern length, and $occ$ be the number of its
text occurrences. Our index counts pattern occurrences in
$O(m+\log^{2+\epsilon}n)$ time, and locates them in $O(m+(occ+1)\log^\epsilon
n)$ time. These times already outperform those of most dictionary-compressed
indexes, while obtaining the least asymptotic space for any index searching
within $O((m+occ)\,\textrm{polylog}\,n)$ time. Further, by increasing the space
to $O(\gamma\log(n/\gamma)\log^\epsilon n)$, we reduce the locating time to the
optimal $O(m+occ)$, and within $O(\gamma\log(n/\gamma)\log n)$ space we can
also count in optimal $O(m)$ time. No dictionary-compressed index had obtained
this time before. All our indexes can be constructed in $O(n)$ space and
$O(n\log n)$ expected time.
  As a byproduct of independent interest...
</summary>
    <author>
      <name>Anders Roy Christiansen</name>
    </author>
    <author>
      <name>Mikko Berggren Ettienne</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1811.12779v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.12779v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.00359">
    <id>http://arxiv.org/abs/1812.00359v2</id>
    <updated>2020-01-01T10:10:44Z</updated>
    <published>2018-12-02T09:11:20Z</published>
    <title>Locally Consistent Parsing for Text Indexing in Small Space</title>
    <summary>  We consider two closely related problems of text indexing in a sub-linear
working space. The first problem is the Sparse Suffix Tree (SST) construction
of a set of suffixes $B$ using only $O(|B|)$ words of space. The second problem
is the Longest Common Extension (LCE) problem, where for some parameter
$1\le\tau\le n$, the goal is to construct a data structure that uses $O(\frac
{n}{\tau})$ words of space and can compute the longest common prefix length of
any pair of suffixes. We show how to use ideas based on the Locally Consistent
Parsing technique, that was introduced by Sahinalp and Vishkin [STOC '94], in
some non-trivial ways in order to improve the known results for the above
problems. We introduce new Las-Vegas and deterministic algorithms for both
problems.
  We introduce the first Las-Vegas SST construction algorithm that takes $O(n)$
time. This is an improvement over the last result of Gawrychowski and Kociumaka
[SODA '17] who obtained $O(n)$ time for Monte-Carlo algorithm, and
$O(n\sqrt{\log |B|})$ time for Las-Vegas algorithm. In addition, we introduce a
randomized Las-Vegas construction for an LCE data structure that can be
constructed in linear time and answers queries in $O(\tau)$ time.
  For the deterministic algorithms, we introduce an SST construction algorithm
that takes $O(n\log \frac{n}{|B|})$ time (for $|B|=\Omega(\log n)$). This is
the first almost linear time, $O(n\cdot poly\log{n})$, deterministic SST
construction algorithm, where all previous algorithms take at least
$\Omega\left(\min\{n|B|,\frac{n^2}{|B|}\}\right)$ time. For the LCE problem, we
introduce a data structure that answers LCE queries in $O(\tau\sqrt{\log^*n})$
time, with $O(n\log\tau)$ construction time (for $\tau=O(\frac{n}{\log n})$).
This data structure improves both query time and construction time upon the
results of Tanimura et al. [CPM '16].
</summary>
    <author>
      <name>Or Birenzwige</name>
    </author>
    <author>
      <name>Shay Golan</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract to appear is SODA 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.00359v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.00359v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.01844">
    <id>http://arxiv.org/abs/1812.01844v1</id>
    <updated>2018-12-05T07:41:53Z</updated>
    <published>2018-12-05T07:41:53Z</published>
    <title>Improving Similarity Search with High-dimensional Locality-sensitive
  Hashing</title>
    <summary>  We propose a new class of data-independent locality-sensitive hashing (LSH)
algorithms based on the fruit fly olfactory circuit. The fundamental difference
of this approach is that, instead of assigning hashes as dense points in a low
dimensional space, hashes are assigned in a high dimensional space, which
enhances their separability. We show theoretically and empirically that this
new family of hash functions is locality-sensitive and preserves rank
similarity for inputs in any `p space. We then analyze different variations on
this strategy and show empirically that they outperform existing LSH methods
for nearest-neighbors search on six benchmark datasets. Finally, we propose a
multi-probe version of our algorithm that achieves higher performance for the
same query time, or conversely, that maintains performance of prior approaches
while taking significantly less indexing time and memory. Overall, our approach
leverages the advantages of separability provided by high-dimensional spaces,
while still remaining computationally efficient
</summary>
    <author>
      <name>Jaiyam Sharma</name>
    </author>
    <author>
      <name>Saket Navlakha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.01844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.01844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.12388">
    <id>http://arxiv.org/abs/1810.12388v1</id>
    <updated>2018-10-29T20:16:28Z</updated>
    <published>2018-10-29T20:16:28Z</published>
    <title>Distinct Sampling on Streaming Data with Near-Duplicates</title>
    <summary>  In this paper we study how to perform distinct sampling in the streaming
model where data contain near-duplicates. The goal of distinct sampling is to
return a distinct element uniformly at random from the universe of elements,
given that all the near-duplicates are treated as the same element. We also
extend the result to the sliding window cases in which we are only interested
in the most recent items. We present algorithms with provable theoretical
guarantees for datasets in the Euclidean space, and also verify their
effectiveness via an extensive set of experiments.
</summary>
    <author>
      <name>Jiecao Chen</name>
    </author>
    <author>
      <name>Qin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to PODS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.12388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.12388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.13187">
    <id>http://arxiv.org/abs/1810.13187v1</id>
    <updated>2018-10-31T09:53:50Z</updated>
    <published>2018-10-31T09:53:50Z</published>
    <title>Non-Empty Bins with Simple Tabulation Hashing</title>
    <summary>  We consider the hashing of a set $X\subseteq U$ with $|X|=m$ using a simple
tabulation hash function $h:U\to [n]=\{0,\dots,n-1\}$ and analyse the number of
non-empty bins, that is, the size of $h(X)$. We show that the expected size of
$h(X)$ matches that with fully random hashing to within low-order terms. We
also provide concentration bounds. The number of non-empty bins is a
fundamental measure in the balls and bins paradigm, and it is critical in
applications such as Bloom filters and Filter hashing. For example, normally
Bloom filters are proportioned for a desired low false-positive probability
assuming fully random hashing (see \url{en.wikipedia.org/wiki/Bloom_filter}).
Our results imply that if we implement the hashing with simple tabulation, we
obtain the same low false-positive probability for any possible input.
</summary>
    <author>
      <name>Anders Aamand</name>
    </author>
    <author>
      <name>Mikkel Thorup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at SODA'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.13187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.13187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.00833">
    <id>http://arxiv.org/abs/1811.00833v1</id>
    <updated>2018-11-02T12:18:23Z</updated>
    <published>2018-11-02T12:18:23Z</published>
    <title>Worst-Case Efficient Sorting with QuickMergesort</title>
    <summary>  The two most prominent solutions for the sorting problem are Quicksort and
Mergesort. While Quicksort is very fast on average, Mergesort additionally
gives worst-case guarantees, but needs extra space for a linear number of
elements. Worst-case efficient in-place sorting, however, remains a challenge:
the standard solution, Heapsort, suffers from a bad cache behavior and is also
not overly fast for in-cache instances.
  In this work we present median-of-medians QuickMergesort (MoMQuickMergesort),
a new variant of QuickMergesort, which combines Quicksort with Mergesort
allowing the latter to be implemented in place. Our new variant applies the
median-of-medians algorithm for selecting pivots in order to circumvent the
quadratic worst case. Indeed, we show that it uses at most $n \log n + 1.6n$
comparisons for $n$ large enough.
  We experimentally confirm the theoretical estimates and show that the new
algorithm outperforms Heapsort by far and is only around 10% slower than
Introsort (std::sort implementation of stdlibc++), which has a rather poor
guarantee for the worst case. We also simulate the worst case, which is only
around 10% slower than the average case. In particular, the new algorithm is a
natural candidate to replace Heapsort as a worst-case stopper in Introsort.
</summary>
    <author>
      <name>Stefan Edelkamp</name>
    </author>
    <author>
      <name>Armin Wei√ü</name>
    </author>
    <link href="http://arxiv.org/abs/1811.00833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.00833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.07422">
    <id>http://arxiv.org/abs/1810.07422v2</id>
    <updated>2019-08-07T14:13:28Z</updated>
    <published>2018-10-17T08:12:01Z</published>
    <title>Fast and Longest Rollercoasters</title>
    <summary>  For $k\geq 3$, a k-rollercoaster is a sequence of numbers whose every maximal
contiguous subsequence, that is increasing or decreasing, has length at least
$k$; $3$-rollercoasters are called simply rollercoasters. Given a sequence of
distinct numbers, we are interested in computing its maximum-length (not
necessarily contiguous) subsequence that is a $k$-rollercoaster. Biedl et al.
[ICALP 2018] have shown that each sequence of $n$ distinct real numbers
contains a rollercoaster of length at least $\lceil n/2\rceil$ for $n>7$, and
that a longest rollercoaster contained in such a sequence can be computed in
$O(n\log n)$-time. They have also shown that every sequence of $n\geq
(k-1)^2+1$ distinct real numbers contains a $k$-rollercoaster of length at
least $\frac{n}{2(k-1)}-\frac{3k}{2}$, and gave an $O(nk\log n)$-time algorithm
computing a longest $k$-rollercoaster in a sequence of length $n$.
  In this paper, we give an $O(nk^2)$-time algorithm computing the length of a
longest $k$-rollercoaster contained in a sequence of $n$ distinct real numbers;
hence, for constant $k$, our algorithm computes the length of a longest
$k$-rollercoaster in optimal linear time. The algorithm can be easily adapted
to output the respective $k$-rollercoaster. In particular, this improves the
results of Biedl et al. [ICALP 2018], by showing that a longest rollercoaster
can be computed in optimal linear time. We also present an algorithm computing
the length of a longest $k$-rollercoaster in $O(n \log^2 n)$-time, that is,
subquadratic even for large values of $k\leq n$. Again, the rollercoaster can
be easily retrieved. Finally, we show an $\Omega(n \log k)$ lower bound for the
number of comparisons in any comparison-based algorithm computing the length of
a longest $k$-rollercoaster.
</summary>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Rados≈Çaw Serafin</name>
    </author>
    <link href="http://arxiv.org/abs/1810.07422v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.07422v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.01472">
    <id>http://arxiv.org/abs/1811.01472v1</id>
    <updated>2018-11-05T01:24:30Z</updated>
    <published>2018-11-05T01:24:30Z</published>
    <title>RePair in Compressed Space and Time</title>
    <summary>  Given a string $T$ of length $N$, the goal of grammar compression is to
construct a small context-free grammar generating only $T$. Among existing
grammar compression methods, RePair (recursive paring) [Larsson and Moffat,
1999] is notable for achieving good compression ratios in practice. Although
the original paper already achieved a time-optimal algorithm to compute the
RePair grammar RePair($T$) in expected $O(N)$ time, the study to reduce its
working space is still active so that it is applicable to large-scale data. In
this paper, we propose the first RePair algorithm working in compressed space,
i.e., potentially $o(N)$ space for highly compressible texts. The key idea is
to give a new way to restructure an arbitrary grammar $S$ for $T$ into
RePair($T$) in compressed space and time. Based on the recompression technique,
we propose an algorithm for RePair($T$) in $O(\min(N, nm \log N))$ space and
expected $O(\min(N, nm \log N) m)$ time or $O(\min(N, nm \log N) \log \log N)$
time, where $n$ is the size of $S$ and $m$ is the number of variables in
RePair($T$). We implemented our algorithm running in $O(\min(N, nm \log N) m)$
time and show it can actually run in compressed space. We also present a new
approach to reduce the peak memory usage of existing RePair algorithms
combining with our algorithms, and show that the new approach outperforms, both
in computation time and space, the most space efficient linear-time RePair
implementation to date.
</summary>
    <author>
      <name>Kensuke Sakai</name>
    </author>
    <author>
      <name>Tatsuya Ohno</name>
    </author>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <link href="http://arxiv.org/abs/1811.01472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.01259">
    <id>http://arxiv.org/abs/1811.01259v1</id>
    <updated>2018-11-03T18:00:29Z</updated>
    <published>2018-11-03T18:00:29Z</published>
    <title>QuickXsort - A Fast Sorting Scheme in Theory and Practice</title>
    <summary>  QuickXsort is a highly efficient in-place sequential sorting scheme that
mixes Hoare's Quicksort algorithm with X, where X can be chosen from a wider
range of other known sorting algorithms, like Heapsort, Insertionsort and
Mergesort. Its major advantage is that QuickXsort can be in-place even if X is
not. In this work we provide general transfer theorems expressing the number of
comparisons of QuickXsort in terms of the number of comparisons of X. More
specifically, if pivots are chosen as medians of (not too fast) growing size
samples, the average number of comparisons of QuickXsort and X differ only by
$o(n)$-terms. For median-of-$k$ pivot selection for some constant $k$, the
difference is a linear term whose coefficient we compute precisely. For
instance, median-of-three QuickMergesort uses at most $n \lg n - 0.8358n +
O(\log n)$ comparisons.
  Furthermore, we examine the possibility of sorting base cases with some other
algorithm using even less comparisons. By doing so the average-case number of
comparisons can be reduced down to $n \lg n- 1.4106n + o(n)$ for a remaining
gap of only $0.0321n$ comparisons to the known lower bound (while using only
$O(\log n)$ additional space and $O(n \log n)$ time overall).
  Implementations of these sorting strategies show that the algorithms
challenge well-established library implementations like Musser's Introsort.
</summary>
    <author>
      <name>Stefan Edelkamp</name>
    </author>
    <author>
      <name>Armin Wei√ü</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <link href="http://arxiv.org/abs/1811.01259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.01248">
    <id>http://arxiv.org/abs/1811.01248v2</id>
    <updated>2019-03-31T11:33:54Z</updated>
    <published>2018-11-03T17:06:18Z</published>
    <title>Compressed Multiple Pattern Matching</title>
    <summary>  Given $d$ strings over the alphabet $\{0,1,\ldots,\sigma{-}1\}$, the
classical Aho--Corasick data structure allows us to find all $occ$ occurrences
of the strings in any text $T$ in $O(|T| + occ)$ time using $O(m\log m)$ bits
of space, where $m$ is the number of edges in the trie containing the strings.
Fix any constant $\varepsilon \in (0, 2)$. We describe a compressed solution
for the problem that, provided $\sigma \le m^\delta$ for a constant $\delta &lt;
1$, works in $O(|T| \frac{1}{\varepsilon} \log\frac{1}{\varepsilon} + occ)$
time, which is $O(|T| + occ)$ since $\varepsilon$ is constant, and occupies
$mH_k + 1.443 m + \varepsilon m + O(d\log\frac{m}{d})$ bits of space, for all
$0 \le k \le \max\{0,\alpha\log_\sigma m - 2\}$ simultaneously, where $\alpha
\in (0,1)$ is an arbitrary constant and $H_k$ is the $k$th-order empirical
entropy of the trie. Hence, we reduce the $3.443m$ term in the space bounds of
previously best succinct solutions to $(1.443 + \varepsilon)m$, thus solving an
open problem posed by Belazzougui. Further, we notice that $L =
\log\binom{\sigma (m+1)}{m} - O(\log(\sigma m))$ is a worst-case space lower
bound for any solution of the problem and, for $d = o(m)$ and constant
$\varepsilon$, our approach allows to achieve $L + \varepsilon m$ bits of
space, which gives an evidence that, for $d = o(m)$, the space of our data
structure is theoretically optimal up to the $\varepsilon m$ additive term and
it is hardly possible to eliminate the term $1.443m$. In addition, we refine
the space analysis of previous works by proposing a more appropriate definition
for $H_k$. We also simplify the construction for practice adapting the fixed
block compression boosting technique, then implement our data structure, and
conduct a number of experiments showing that it is comparable to the state of
the art in terms of time and is superior in space.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Nikita Sivukhin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.01248v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01248v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.01226">
    <id>http://arxiv.org/abs/1811.01226v1</id>
    <updated>2018-11-03T14:21:14Z</updated>
    <published>2018-11-03T14:21:14Z</published>
    <title>Multidimensional segment trees can do range queries and updates in
  logarithmic time</title>
    <summary>  Updating and querying on a range is a classical algorithmic problem with a
multitude of applications. The Segment Tree data structure is particularly
notable in handling the range query and update operations. A Segment Segment
Tree divides the range into disjoint segments and merges them together to
perform range queries and range updates elegantly. Although this data structure
is remarkably potent for 1-dimensional problems, it falls short in higher
dimensions. Lazy Propagation enables the operations to be computed in $O(logn)$
time in single dimension. However, the concept of lazy propagation could not be
translated to higher dimensional cases, which imposes a time complexity of
$O(n^{k-1} \; logn)$ for operations on $k$-dimensional data. In this paper, we
have made an attempt to emulate the idea of lazy propagation differently so
that it can be applied for 2-dimensional cases. Moreover, the proposed
modification is capable of performing any general aggregate function similar to
the original Segment Tree, and can also be extended to even higher dimensions.
Our proposed algorithm manages to perform range queries and updates in
$O(\log^2 n)$ time for a 2-dimensional problem, which becomes $O(\log^d n)$ for
a $d$-dimensional situation.
</summary>
    <author>
      <name>Nabil Ibtehaz</name>
    </author>
    <author>
      <name>M. Kaykobad</name>
    </author>
    <author>
      <name>M. Sohel Rahman</name>
    </author>
    <link href="http://arxiv.org/abs/1811.01226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.01209">
    <id>http://arxiv.org/abs/1811.01209v3</id>
    <updated>2018-12-21T18:00:33Z</updated>
    <published>2018-11-03T13:09:57Z</published>
    <title>Optimal Rank and Select Queries on Dictionary-Compressed Text</title>
    <summary>  We study the problem of supporting queries on a string $S$ of length $n$
within a space bounded by the size $\gamma$ of a string attractor for $S$.
Recent works showed that random access on $S$ can be supported in optimal
$O(\log(n/\gamma)/\log\log n)$ time within $O\left (\gamma\ \rm{polylog}\ n
\right)$ space. In this paper, we extend this result to \emph{rank} and
\emph{select} queries and provide lower bounds matching our upper bounds on
alphabets of polylogarithmic size. Our solutions are given in the form of a
space-time trade-off that is more general than the one previously known for
grammars and that improves existing bounds on LZ77-compressed text by a
$\log\log n$ time-factor in \emph{select} queries. We also provide matching
lower and upper bounds for \emph{partial sum} and \emph{predecessor} queries
within attractor-bounded space, and extend our lower bounds to encompass
navigation of dictionary-compressed tree representations.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">improved select bound with reduction to psum. Added lower bounds on
  trees</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.01209v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01209v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.02078">
    <id>http://arxiv.org/abs/1811.02078v2</id>
    <updated>2019-04-05T02:47:30Z</updated>
    <published>2018-11-05T23:05:04Z</published>
    <title>Optimal Succinct Rank Data Structure via Approximate Nonnegative Tensor
  Decomposition</title>
    <summary>  Given an $n$-bit array $A$, the succinct rank data structure problem asks to
construct a data structure using space $n+r$ bits for $r\ll n$, supporting rank
queries of form $\mathtt{rank}(x)=\sum_{i=0}^{x-1} A[i]$. In this paper, we
design a new succinct rank data structure with $r=n/(\log
n)^{\Omega(t)}+n^{1-c}$ and query time $O(t)$ for some constant $c>0$,
improving the previous best-known by Patrascu [Pat08], which has
$r=n/(\frac{\log n}{t})^{\Omega(t)}+\tilde{O}(n^{3/4})$ bits of redundancy. For
$r>n^{1-c}$, our space-time tradeoff matches the cell-probe lower bound by
Patrascu and Viola [PV10], which asserts that $r$ must be at least $n/(\log
n)^{O(t)}$. Moreover, one can avoid an $n^{1-c}$-bit lookup table when the data
structure is implemented in the cell-probe model, achieving $r=\lceil n/(\log
n)^{\Omega(t)}\rceil$. It matches the lower bound for the full range of
parameters.
  En route to our new data structure design, we establish an interesting
connection between succinct data structures and approximate nonnegative tensor
decomposition. Our connection shows that for specific problems, to construct a
space-efficient data structure, it suffices to approximate a particular tensor
by a sum of (few) nonnegative rank-$1$ tensors. For the rank problem, we
explicitly construct such an approximation, which yields an explicit
construction of the data structure.
</summary>
    <author>
      <name>Huacheng Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary version of this paper will appear in STOC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.02078v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.02078v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.05303">
    <id>http://arxiv.org/abs/1810.05303v1</id>
    <updated>2018-10-12T01:05:11Z</updated>
    <published>2018-10-12T01:05:11Z</published>
    <title>Parallelism in Randomized Incremental Algorithms</title>
    <summary>  In this paper we show that many sequential randomized incremental algorithms
are in fact parallel. We consider algorithms for several problems including
Delaunay triangulation, linear programming, closest pair, smallest enclosing
disk, least-element lists, and strongly connected components.
  We analyze the dependences between iterations in an algorithm, and show that
the dependence structure is shallow with high probability, or that by violating
some dependences the structure is shallow and the work is not increased
significantly. We identify three types of algorithms based on their dependences
and present a framework for analyzing each type. Using the framework gives
work-efficient polylogarithmic-depth parallel algorithms for most of the
problems that we study.
  This paper shows the first incremental Delaunay triangulation algorithm with
optimal work and polylogarithmic depth, which is an open problem for over 30
years. This result is important since most implementations of parallel Delaunay
triangulation use the incremental approach. Our results also improve bounds on
strongly connected components and least-elements lists, and significantly
simplify parallel algorithms for several problems.
</summary>
    <author>
      <name>Guy E. Blelloch</name>
    </author>
    <author>
      <name>Yan Gu</name>
    </author>
    <author>
      <name>Julian Shun</name>
    </author>
    <author>
      <name>Yihan Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1810.05303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.05303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.05313">
    <id>http://arxiv.org/abs/1810.05313v2</id>
    <updated>2018-10-26T01:30:24Z</updated>
    <published>2018-10-12T01:40:57Z</published>
    <title>Xorshift1024*, Xorshift1024+, Xorshift128+ and Xoroshiro128+ Fail
  Statistical Tests for Linearity</title>
    <summary>  L'Ecuyer &amp; Simard's Big Crush statistical test suite has revealed statistical
flaws in many popular random number generators including Marsaglia's Xorshift
generators. Vigna recently proposed some 64-bit variations on the Xorshift
scheme that are further scrambled (i.e., Xorshift1024*, Xorshift1024+,
Xorshift128+, Xoroshiro128+). Unlike their unscrambled counterparts, they pass
Big Crush when interleaving blocks of 32 bits for each 64-bit word (most
significant, least significant, most significant, least significant, etc.). We
report that these scrambled generators systematically fail Big
Crush---specifically the linear-complexity and matrix-rank tests that detect
linearity---when taking the 32 lowest-order bits in reverse order from each
64-bit word.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Melissa E. O'Neill</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cam.2018.10.019</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cam.2018.10.019" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational and Applied Mathematics 350, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.05313v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.05313v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.05753">
    <id>http://arxiv.org/abs/1810.05753v1</id>
    <updated>2018-10-12T23:01:24Z</updated>
    <published>2018-10-12T23:01:24Z</published>
    <title>Relative compression of trajectories</title>
    <summary>  We present RCT, a new compact data structure to represent trajectories of
objects. It is based on a relative compression technique called Relative
Lempel-Ziv (RLZ), which compresses sequences by applying an LZ77 encoding with
respect to an artificial reference. Combined with $O(z)$-sized data structures
on the sequence of phrases that allows to solve trajectory and spatio-temporal
queries efficiently. We plan that RCT improves in compression and time
performance the previous compressed representations in the state of the art.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Adri√°n G√≥mez-Brand√≥n</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Jos√© R. Param√°</name>
    </author>
    <link href="http://arxiv.org/abs/1810.05753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.05753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.07259">
    <id>http://arxiv.org/abs/1810.07259v1</id>
    <updated>2018-10-16T20:27:46Z</updated>
    <published>2018-10-16T20:27:46Z</published>
    <title>Nearly Optimal Space Efficient Algorithm for Depth First Search</title>
    <summary>  We design a space-efficient algorithm for performing depth-first search
traversal(DFS) of a graph in $O(m+n\log^* n)$ time using $O(n)$ bits of space.
While a normal DFS algorithm results in a DFS-tree (in case the graph is
connected), our space bounds do not permit us even to store such a tree.
However, our algorithm correctly outputs all edges of the DFS-tree.
  The previous best algorithm (which used $O(n)$ working space) took $O(m \log
n)$ time (Asano, Izumi, Kiyomi, Konagaya, Ono, Otachi, Schweitzer, Tarui,
Uehara (ISAAC 2014) and Elmasry, Hagerup, Krammer (STACS 2015)). The main open
question left behind in this area was to design faster algorithm for DFS using
$O(n)$ bits of space. Our algorithm answers this open question as it has a
nearly optimal running time (as the DFS takes $O(m+n)$ time even if there is no
space restriction).
</summary>
    <author>
      <name>Jayesh Choudhari</name>
    </author>
    <author>
      <name>Manoj Gupta</name>
    </author>
    <author>
      <name>Shivdutt Sharma</name>
    </author>
    <link href="http://arxiv.org/abs/1810.07259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.07259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.10635">
    <id>http://arxiv.org/abs/1810.10635v1</id>
    <updated>2018-10-24T21:43:32Z</updated>
    <published>2018-10-24T21:43:32Z</published>
    <title>Lower Bounds for Oblivious Data Structures</title>
    <summary>  An oblivious data structure is a data structure where the memory access
patterns reveals no information about the operations performed on it. Such data
structures were introduced by Wang et al. [ACM SIGSAC'14] and are intended for
situations where one wishes to store the data structure at an untrusted server.
One way to obtain an oblivious data structure is simply to run a classic data
structure on an oblivious RAM (ORAM). Until very recently, this resulted in an
overhead of $\omega(\lg n)$ for the most natural setting of parameters.
Moreover, a recent lower bound for ORAMs by Larsen and Nielsen [CRYPTO'18] show
that they always incur an overhead of at least $\Omega(\lg n)$ if used in a
black box manner. To circumvent the $\omega(\lg n)$ overhead, researchers have
instead studied classic data structure problems more directly and have obtained
efficient solutions for many such problems such as stacks, queues, deques,
priority queues and search trees. However, none of these data structures
process operations faster than $\Theta(\lg n)$, leaving open the question of
whether even faster solutions exist. In this paper, we rule out this
possibility by proving $\Omega(\lg n)$ lower bounds for oblivious stacks,
queues, deques, priority queues and search trees.
</summary>
    <author>
      <name>Riko Jacob</name>
    </author>
    <author>
      <name>Kasper Green Larsen</name>
    </author>
    <author>
      <name>Jesper Buus Nielsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at SODA'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.10635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.10635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.11308">
    <id>http://arxiv.org/abs/1810.11308v1</id>
    <updated>2018-10-26T13:17:20Z</updated>
    <published>2018-10-26T13:17:20Z</published>
    <title>Sub-O(log n) Out-of-Order Sliding-Window Aggregation</title>
    <summary>  Sliding-window aggregation summarizes the most recent information in a data
stream. Users specify how that summary is computed, usually as an associative
binary operator because this is the most general known form for which it is
possible to avoid naively scanning every window. For strictly in-order
arrivals, there are algorithms with $O(1)$ time per window change assuming
associative operators. Meanwhile, it is common in practice for streams to have
data arriving slightly out of order, for instance, due to clock drifts or
communication delays. Unfortunately, for out-of-order streams, one has to
resort to latency-prone buffering or pay $O(\log n)$ time per insert or evict,
where $n$ is the window size.
  This paper presents the design, analysis, and implementation of FiBA, a novel
sliding-window aggregation algorithm with an amortized upper bound of $O(\log
d)$ time per insert or evict, where $d$ is the distance of the inserted or
evicted value to the closer end of the window. This means $O(1)$ time for
in-order arrivals and nearly $O(1)$ time for slightly out-of-order arrivals,
with a smooth transition towards $O(\log n)$ as $d$ approaches $n$. We also
prove a matching lower bound on running time, showing optimality. Our algorithm
is as general as the prior state-of-the-art: it requires associativity, but not
invertibility nor commutativity. At the heart of the algorithm is a careful
combination of finger-searching techniques, lazy rebalancing, and
position-aware partial aggregates. We further show how to answer range queries
that aggregate subwindows for window sharing. Finally, our experimental
evaluation shows that FiBA performs well in practice and supports the
theoretical findings.
</summary>
    <author>
      <name>Kanat Tangwongsan</name>
    </author>
    <author>
      <name>Martin Hirzel</name>
    </author>
    <author>
      <name>Scott Schneider</name>
    </author>
    <link href="http://arxiv.org/abs/1810.11308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.11308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.11262">
    <id>http://arxiv.org/abs/1810.11262v1</id>
    <updated>2018-10-26T10:56:24Z</updated>
    <published>2018-10-26T10:56:24Z</published>
    <title>Some comments on the structure of the best known networks sorting 16
  elements</title>
    <summary>  We propose an explanation of the structure of the best known sorting networks
for 16 elements with respect to the complexity and to the depth due to Green
and van Voorhis.
</summary>
    <author>
      <name>Igor S. Sergeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7+7 pages, 6 figures (in English and Russian)</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.11262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.11262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.11863">
    <id>http://arxiv.org/abs/1810.11863v2</id>
    <updated>2019-04-09T23:37:33Z</updated>
    <published>2018-10-28T19:21:33Z</published>
    <title>Near-Linear Time Insertion-Deletion Codes and
  (1+$\varepsilon$)-Approximating Edit Distance via Indexing</title>
    <summary>  We introduce fast-decodable indexing schemes for edit distance which can be
used to speed up edit distance computations to near-linear time if one of the
strings is indexed by an indexing string $I$. In particular, for every length
$n$ and every $\varepsilon >0$, one can in near linear time construct a string
$I \in \Sigma'^n$ with $|\Sigma'| = O_{\varepsilon}(1)$, such that, indexing
any string $S \in \Sigma^n$, symbol-by-symbol, with $I$ results in a string $S'
\in \Sigma''^n$ where $\Sigma'' = \Sigma \times \Sigma'$ for which edit
distance computations are easy, i.e., one can compute a
$(1+\varepsilon)$-approximation of the edit distance between $S'$ and any other
string in $O(n \text{poly}(\log n))$ time.
  Our indexing schemes can be used to improve the decoding complexity of
state-of-the-art error correcting codes for insertions and deletions. In
particular, they lead to near-linear time decoding algorithms for the
insertion-deletion codes of [Haeupler, Shahrasbi; STOC `17] and faster decoding
algorithms for list-decodable insertion-deletion codes of [Haeupler, Shahrasbi,
Sudan; ICALP `18]. Interestingly, the latter codes are a crucial ingredient in
the construction of fast-decodable indexing schemes.
</summary>
    <author>
      <name>Bernhard Haeupler</name>
    </author>
    <author>
      <name>Aviad Rubinstein</name>
    </author>
    <author>
      <name>Amirbehshad Shahrasbi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3313276.3316371</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3313276.3316371" rel="related"/>
    <link href="http://arxiv.org/abs/1810.11863v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.11863v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.12047">
    <id>http://arxiv.org/abs/1810.12047v1</id>
    <updated>2018-10-29T10:37:21Z</updated>
    <published>2018-10-29T10:37:21Z</published>
    <title>Simple and Fast BlockQuicksort using Lomuto's Partitioning Scheme</title>
    <summary>  This paper presents simple variants of the BlockQuicksort algorithm described
by Edelkamp and Weiss (ESA 2016). The simplification is achieved by using
Lomuto's partitioning scheme instead of Hoare's crossing pointer technique to
partition the input. To achieve a robust sorting algorithm that works well on
many different input types, the paper introduces a novel two-pivot variant of
Lomuto's partitioning scheme. A surprisingly simple twist to the generic
two-pivot quicksort approach makes the algorithm robust. The paper provides an
analysis of the theoretical properties of the proposed algorithms and compares
them to their competitors. The analysis shows that Lomuto-based approaches
incur a higher average sorting cost than the Hoare-based approach of
BlockQuicksort. Moreover, the analysis is particularly useful to reason about
pivot choices that suit the two-pivot approach. An extensive experimental study
shows that, despite their worse theoretical behavior, the simpler variants
perform as well as the original version of BlockQuicksort.
</summary>
    <author>
      <name>Martin Aum√ºller</name>
    </author>
    <author>
      <name>Nikolaj Hass</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ALENEX 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.12047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.12047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.12322">
    <id>http://arxiv.org/abs/1810.12322v1</id>
    <updated>2018-10-29T18:02:20Z</updated>
    <published>2018-10-29T18:02:20Z</published>
    <title>Sesquickselect: One and a half pivots for cache-efficient selection</title>
    <summary>  Because of unmatched improvements in CPU performance, memory transfers have
become a bottleneck of program execution. As discovered in recent years, this
also affects sorting in internal memory. Since partitioning around several
pivots reduces overall memory transfers, we have seen renewed interest in
multiway Quicksort. Here, we analyze in how far multiway partitioning helps in
Quickselect.
  We compute the expected number of comparisons and scanned elements
(approximating memory transfers) for a generic class of (non-adaptive) multiway
Quickselect and show that three or more pivots are not helpful, but two pivots
are. Moreover, we consider "adaptive" variants which choose partitioning and
pivot-selection methods in each recursive step from a finite set of
alternatives depending on the current (relative) sought rank. We show that
"Sesquickselect", a new Quickselect variant that uses either one or two pivots,
makes better use of small samples w.r.t. memory transfers than other
Quickselect variants.
</summary>
    <author>
      <name>Conrado Mart√≠nez</name>
    </author>
    <author>
      <name>Markus Nebel</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/1.9781611975505.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/1.9781611975505.6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">appears in ANALCO 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.12322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.12322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.01238">
    <id>http://arxiv.org/abs/1810.01238v1</id>
    <updated>2018-10-02T13:42:21Z</updated>
    <published>2018-10-02T13:42:21Z</published>
    <title>Sketching, Streaming, and Fine-Grained Complexity of (Weighted) LCS</title>
    <summary>  We study sketching and streaming algorithms for the Longest Common
Subsequence problem (LCS) on strings of small alphabet size $|\Sigma|$. For the
problem of deciding whether the LCS of strings $x,y$ has length at least $L$,
we obtain a sketch size and streaming space usage of $\mathcal{O}(L^{|\Sigma| -
1} \log L)$.
  We also prove matching unconditional lower bounds.
  As an application, we study a variant of LCS where each alphabet symbol is
equipped with a weight that is given as input, and the task is to compute a
common subsequence of maximum total weight. Using our sketching algorithm, we
obtain an $\mathcal{O}(\textrm{min}\{nm, n + m^{{\lvert \Sigma
\rvert}}\})$-time algorithm for this problem, on strings $x,y$ of length $n,m$,
with $n \ge m$. We prove optimality of this running time up to lower order
factors, assuming the Strong Exponential Time Hypothesis.
</summary>
    <author>
      <name>Karl Bringmann</name>
    </author>
    <author>
      <name>Bhaskar Ray Chaudhury</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in FSTTCS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.01238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Computer Science" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.01676">
    <id>http://arxiv.org/abs/1810.01676v3</id>
    <updated>2019-07-23T07:48:46Z</updated>
    <published>2018-10-03T10:38:59Z</published>
    <title>Approximating Approximate Pattern Matching</title>
    <summary>  Given a text $T$ of length $n$ and a pattern $P$ of length $m$, the
approximate pattern matching problem asks for computation of a particular
\emph{distance} function between $P$ and every $m$-substring of $T$. We
consider a $(1\pm\varepsilon)$ multiplicative approximation variant of this
problem, for $\ell_p$ distance function. In this paper, we describe two
$(1+\varepsilon)$-approximate algorithms with a runtime of
$\widetilde{O}(\frac{n}{\varepsilon})$ for all (constant) non-negative values
of $p$. For constant $p \ge 1$ we show a deterministic
$(1+\varepsilon)$-approximation algorithm. Previously, such run time was known
only for the case of $\ell_1$ distance, by Gawrychowski and Uzna\'nski [ICALP
2018] and only with a randomized algorithm. For constant $0 \le p \le 1$ we
show a randomized algorithm for the $\ell_p$, thereby providing a smooth
tradeoff between algorithms of Kopelowitz and Porat [FOCS~2015, SOSA~2018] for
Hamming distance (case of $p=0$) and of Gawrychowski and Uzna\'nski for
$\ell_1$ distance.
</summary>
    <author>
      <name>Jan Studen√Ω</name>
    </author>
    <author>
      <name>Przemys≈Çaw Uzna≈Ñski</name>
    </author>
    <link href="http://arxiv.org/abs/1810.01676v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01676v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.01726">
    <id>http://arxiv.org/abs/1810.01726v2</id>
    <updated>2019-03-27T13:35:14Z</updated>
    <published>2018-10-03T13:21:14Z</published>
    <title>Fault Tolerant and Fully Dynamic DFS in Undirected Graphs: Simple Yet
  Efficient</title>
    <summary>  We present an algorithm for a fault tolerant Depth First Search (DFS) Tree in
an undirected graph. This algorithm is drastically simpler than the current
state-of-the-art algorithms for this problem, uses optimal space and optimal
preprocessing time, and still achieves better time complexity. This algorithm
also leads to a better time complexity for maintaining a DFS tree in a fully
dynamic environment.
</summary>
    <author>
      <name>Surender Baswana</name>
    </author>
    <author>
      <name>Shiv Kumar Gupta</name>
    </author>
    <author>
      <name>Ayush Tulsyan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.01726v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01726v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.01785">
    <id>http://arxiv.org/abs/1810.01785v1</id>
    <updated>2018-10-03T15:07:51Z</updated>
    <published>2018-10-03T15:07:51Z</published>
    <title>Weighted dynamic finger in binary search trees</title>
    <summary>  It is shown that the online binary search tree data structure GreedyASS
performs asymptotically as well on a sufficiently long sequence of searches as
any static binary search tree where each search begins from the previous search
(rather than the root). This bound is known to be equivalent to assigning each
item $i$ in the search tree a positive weight $w_i$ and bounding the search
cost of an item in the search sequence $s_1,\ldots,s_m$ by $$O\left(1+ \log
\frac{\displaystyle \sum_{\min(s_{i-1},s_i) \leq x \leq
\max(s_{i-1},s_i)}w_x}{\displaystyle \min(w_{s_i},w_{s_{i-1}})} \right)$$
amortized. This result is the strongest finger-type bound to be proven for
binary search trees. By setting the weights to be equal, one observes that our
bound implies the dynamic finger bound. Compared to the previous proof of the
dynamic finger bound for Splay trees, our result is significantly shorter,
stronger, simpler, and has reasonable constants.
</summary>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An earlier version of this work appeared in the Proceedings of the
  Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.01785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.02270">
    <id>http://arxiv.org/abs/1810.02270v1</id>
    <updated>2018-10-04T15:11:47Z</updated>
    <published>2018-10-04T15:11:47Z</published>
    <title>Compound Binary Search Tree and Algorithms</title>
    <summary>  The Binary Search Tree (BST) is average in computer science which supports a
compact data structure in memory and oneself even conducts a row of quick
algorithms, by which people often apply it in dynamical circumstance. Besides
these edges, it is also with weakness on its own structure specially with poor
performance at worst case. In this paper, we will develop this data structure
into a synthesis to show a series of novel features residing in. Of that, there
are new methods invented for raising the performance and efficiency
nevertheless some existing ones in logarithm or linear time.
</summary>
    <author>
      <name>Yong Tan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages with words into 6300</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.02270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.02270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05C05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.7; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.02099">
    <id>http://arxiv.org/abs/1810.02099v1</id>
    <updated>2018-10-04T08:39:06Z</updated>
    <published>2018-10-04T08:39:06Z</published>
    <title>Longest Property-Preserved Common Factor</title>
    <summary>  In this paper we introduce a new family of string processing problems. We are
given two or more strings and we are asked to compute a factor common to all
strings that preserves a specific property and has maximal length. Here we
consider three fundamental string properties: square-free factors, periodic
factors, and palindromic factors under three different settings, one per
property. In the first setting, we are given a string $x$ and we are asked to
construct a data structure over $x$ answering the following type of on-line
queries: given string $y$, find a longest square-free factor common to $x$ and
$y$. In the second setting, we are given $k$ strings and an integer $1 &lt; k'\leq
k$ and we are asked to find a longest periodic factor common to at least $k'$
strings. In the third setting, we are given two strings and we are asked to
find a longest palindromic factor common to the two strings. We present
linear-time solutions for all settings. We anticipate that our paradigm can be
extended to other string properties or settings.
</summary>
    <author>
      <name>Lorraine A. K Ayad</name>
    </author>
    <author>
      <name>Giulia Bernardini</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Nadia Pisanti</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of SPIRE 2018 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.02099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.02099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.02227">
    <id>http://arxiv.org/abs/1810.02227v1</id>
    <updated>2018-10-04T14:08:57Z</updated>
    <published>2018-10-04T14:08:57Z</published>
    <title>Randen - fast backtracking-resistant random generator with
  AES+Feistel+Reverie</title>
    <summary>  Algorithms that rely on a pseudorandom number generator often lose their
performance guarantees when adversaries can predict the behavior of the
generator. To protect non-cryptographic applications against such attacks, we
propose 'strong' pseudorandom generators characterized by two properties:
computationally indistinguishable from random and backtracking-resistant. Some
existing cryptographically secure generators also meet these criteria, but they
are too slow to be accepted for general-purpose use. We introduce a new
open-sourced generator called 'Randen' and show that it is 'strong' in addition
to outperforming Mersenne Twister, PCG, ChaCha8, ISAAC and Philox in real-world
benchmarks. This is made possible by hardware acceleration. Randen is an
instantiation of Reverie, a recently published robust sponge-like random
generator, with a new permutation built from an improved generalized Feistel
structure with 16 branches. We provide new bounds on active s-boxes for up to
24 rounds of this construction, made possible by a memory-efficient search
algorithm. Replacing existing generators with Randen can protect randomized
algorithms such as reservoir sampling from attack. The permutation may also be
useful for wide-block ciphers and hashing functions.
</summary>
    <author>
      <name>Jan Wassenberg</name>
    </author>
    <author>
      <name>Robert Obryk</name>
    </author>
    <author>
      <name>Jyrki Alakuijala</name>
    </author>
    <author>
      <name>Emmanuel Mogenet</name>
    </author>
    <link href="http://arxiv.org/abs/1810.02227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.02227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A60" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.03551">
    <id>http://arxiv.org/abs/1810.03551v2</id>
    <updated>2018-11-05T11:03:00Z</updated>
    <published>2018-10-08T16:10:17Z</published>
    <title>Approximate Online Pattern Matching in Sub-linear Time</title>
    <summary>  We consider the approximate pattern matching problem under edit distance. In
this problem we are given a pattern $P$ of length $w$ and a text $T$ of length
$n$ over some alphabet $\Sigma$, and a positive integer $k$. The goal is to
find all the positions $j$ in $T$ such that there is a substring of $T$ ending
at $j$ which has edit distance at most $k$ from the pattern $P$. Recall, the
edit distance between two strings is the minimum number of character
insertions, deletions, and substitutions required to transform one string into
the other. For a position $t$ in $\{1,...,n\}$, let $k_t$ be the smallest edit
distance between $P$ and any substring of $T$ ending at $t$. In this paper we
give a constant factor approximation to the sequence $k_1,k_2,...,k_{n}$. We
consider both offline and online settings.
  In the offline setting, where both $P$ and $T$ are available, we present an
algorithm that for all $t$ in $\{1,...,n\}$, computes the value of $k_t$
approximately within a constant factor. The worst case running time of our
algorithm is $O(n w^{3/4})$. As a consequence we break the $O(nw)$-time barrier
for this problem.
  In the online setting, we are given $P$ and then $T$ arrives one symbol at a
time. We design an algorithm that upon arrival of the $t$-th symbol of $T$
computes $k_t$ approximately within $O(1)$-multiplicative factor and
$w^{8/9}$-additive error. Our algorithm takes $O(w^{1-(7/54)})$ amortized time
per symbol arrival and takes $O(w^{1-(1/54)})$ additional space apart from
storing the pattern $P$.
  Both of our algorithms are randomized and produce correct answer with high
probability. To the best of our knowledge this is the first worst-case
sub-linear (in the length of the pattern) time and sub-linear succinct space
algorithm for online approximate pattern matching problem.
</summary>
    <author>
      <name>Diptarka Chakraborty</name>
    </author>
    <author>
      <name>Debarati Das</name>
    </author>
    <author>
      <name>Michal Koucky</name>
    </author>
    <link href="http://arxiv.org/abs/1810.03551v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.03551v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.03374">
    <id>http://arxiv.org/abs/1810.03374v1</id>
    <updated>2018-10-08T11:14:24Z</updated>
    <published>2018-10-08T11:14:24Z</published>
    <title>On the discrepancy of random low degree set systems</title>
    <summary>  Motivated by the celebrated Beck-Fiala conjecture, we consider the random
setting where there are $n$ elements and $m$ sets and each element lies in $t$
randomly chosen sets. In this setting, Ezra and Lovett showed an $O((t \log
t)^{1/2})$ discrepancy bound in the regime when $n \leq m$ and an $O(1)$ bound
when $n \gg m^t$.
  In this paper, we give a tight $O(\sqrt{t})$ bound for the entire range of
$n$ and $m$, under a mild assumption that $t = \Omega (\log \log m)^2$. The
result is based on two steps. First, applying the partial coloring method to
the case when $n = m \log^{O(1)} m$ and using the properties of the random set
system we show that the overall discrepancy incurred is at most $O(\sqrt{t})$.
Second, we reduce the general case to that of $n \leq m \log^{O(1)}m$ using LP
duality and a careful counting argument.
</summary>
    <author>
      <name>Nikhil Bansal</name>
    </author>
    <author>
      <name>Raghu Meka</name>
    </author>
    <link href="http://arxiv.org/abs/1810.03374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.03374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.03664">
    <id>http://arxiv.org/abs/1810.03664v1</id>
    <updated>2018-10-08T19:08:37Z</updated>
    <published>2018-10-08T19:08:37Z</published>
    <title>Approximating Edit Distance Within Constant Factor in Truly
  Sub-Quadratic Time</title>
    <summary>  Edit distance is a measure of similarity of two strings based on the minimum
number of character insertions, deletions, and substitutions required to
transform one string into the other. The edit distance can be computed exactly
using a dynamic programming algorithm that runs in quadratic time. Andoni,
Krauthgamer and Onak (2010) gave a nearly linear time algorithm that
approximates edit distance within approximation factor $\text{poly}(\log n)$.
  In this paper, we provide an algorithm with running time
$\tilde{O}(n^{2-2/7})$ that approximates the edit distance within a constant
factor.
</summary>
    <author>
      <name>Diptarka Chakraborty</name>
    </author>
    <author>
      <name>Debarati Das</name>
    </author>
    <author>
      <name>Elazar Goldenberg</name>
    </author>
    <author>
      <name>Michal Koucky</name>
    </author>
    <author>
      <name>Michael Saks</name>
    </author>
    <link href="http://arxiv.org/abs/1810.03664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.03664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.01759">
    <id>http://arxiv.org/abs/1809.01759v1</id>
    <updated>2018-09-05T22:52:51Z</updated>
    <published>2018-09-05T22:52:51Z</published>
    <title>Multi-finger binary search trees</title>
    <summary>  We study multi-finger binary search trees (BSTs), a far-reaching extension of
the classical BST model, with connections to the well-studied $k$-server
problem. Finger search is a popular technique for speeding up BST operations
when a query sequence has locality of reference. BSTs with multiple fingers can
exploit more general regularities in the input. In this paper we consider the
cost of serving a sequence of queries in an optimal (offline) BST with $k$
fingers, a powerful benchmark against which other algorithms can be measured.
  We show that the $k$-finger optimum can be matched by a standard dynamic BST
(having a single root-finger) with an $O(\log{k})$ factor overhead. This result
is tight for all $k$, improving the $O(k)$ factor implicit in earlier work.
Furthermore, we describe new online BSTs that match this bound up to a
$(\log{k})^{O(1)}$ factor. Previously only the "one-finger" special case was
known to hold for an online BST (Iacono, Langerman, 2016; Cole et al., 2000).
Splay trees, assuming their conjectured optimality (Sleator and Tarjan, 1983),
would have to match our bounds for all $k$.
  Our online algorithms are randomized and combine techniques developed for the
$k$-server problem with a multiplicative-weights scheme for learning tree
metrics. To our knowledge, this is the first time when tools developed for the
$k$-server problem are used in BSTs. As an application of our $k$-finger
results, we show that BSTs can efficiently serve queries that are close to some
recently accessed item. This is a (restricted) form of the unified property
(Iacono, 2001) that was previously not known to hold for any BST algorithm,
online or offline.
</summary>
    <author>
      <name>Parinya Chalermsook</name>
    </author>
    <author>
      <name>Mayank Goswami</name>
    </author>
    <author>
      <name>L√°szl√≥ Kozma</name>
    </author>
    <author>
      <name>Kurt Mehlhorn</name>
    </author>
    <author>
      <name>Thatchaphol Saranurak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at ISAAC 2018. Also extends (and supersedes parts of)
  arXiv:1603.04892, with possible text overlaps</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.03685">
    <id>http://arxiv.org/abs/1809.03685v2</id>
    <updated>2018-09-14T23:41:03Z</updated>
    <published>2018-09-11T05:24:43Z</published>
    <title>Massively Parallel Dynamic Programming on Trees</title>
    <summary>  Dynamic programming is a powerful technique that is, unfortunately, often
inherently sequential. That is, there exists no unified method to parallelize
algorithms that use dynamic programming. In this paper, we attempt to address
this issue in the Massively Parallel Computations (MPC) model which is a
popular abstraction of MapReduce-like paradigms. Our main result is an
algorithmic framework to adapt a large family of dynamic programs defined over
trees.
  We introduce two classes of graph problems that admit dynamic programming
solutions on trees. We refer to them as "(polylog)-expressible" and
"linear-expressible" problems. We show that both classes can be parallelized in
$O(\log n)$ rounds using a sublinear number of machines and a sublinear memory
per machine. To achieve this result, we introduce a series of techniques that
can be plugged together. To illustrate the generality of our framework, we
implement in $O(\log n)$ rounds of MPC, the dynamic programming solution of
graph problems such as minimum bisection, $k$-spanning tree, maximum
independent set, longest path, etc., when the input graph is a tree.
</summary>
    <author>
      <name>MohammadHossein Bateni</name>
    </author>
    <author>
      <name>Soheil Behnezhad</name>
    </author>
    <author>
      <name>Mahsa Derakhshan</name>
    </author>
    <author>
      <name>MohammadTaghi Hajiaghayi</name>
    </author>
    <author>
      <name>Vahab Mirrokni</name>
    </author>
    <link href="http://arxiv.org/abs/1809.03685v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.03685v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.05419">
    <id>http://arxiv.org/abs/1809.05419v1</id>
    <updated>2018-09-14T13:45:27Z</updated>
    <published>2018-09-14T13:45:27Z</published>
    <title>Approximate Query Processing over Static Sets and Sliding Windows</title>
    <summary>  Indexing of static and dynamic sets is fundamental to a large set of
applications such as information retrieval and caching. Denoting the
characteristic vector of the set by B, we consider the problem of encoding sets
and multisets to support approximate versions of the operations rank(i) (i.e.,
computing sum_{j &lt;= i}B[j]) and select(i) (i.e., finding min{p | rank(p) >= i})
queries. We study multiple types of approximations (allowing an error in the
query or the result) and present lower bounds and succinct data structures for
several variants of the problem. We also extend our model to sliding windows,
in which we process a stream of elements and compute suffix sums. This is a
generalization of the window summation problem that allows the user to specify
the window size at query time. Here, we provide an algorithm that supports
updates and queries in constant time while requiring just (1+o(1)) factor more
space than the fixed-window summation algorithms.
</summary>
    <author>
      <name>Ran Ben Basat</name>
    </author>
    <author>
      <name>Seungbum Jo</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <author>
      <name>Shubham Ugare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ISAAC 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.05419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.05419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.05844">
    <id>http://arxiv.org/abs/1809.05844v1</id>
    <updated>2018-09-16T09:43:53Z</updated>
    <published>2018-09-16T09:43:53Z</published>
    <title>Calculation of extended gcd by normalization</title>
    <summary>  We propose a new algorithm solving the extended gcd problem, which provides a
solution minimizing one of the two coordinates. The algorithm relies on
elementary arithmetic properties.
</summary>
    <author>
      <name>Marc Wolf</name>
    </author>
    <author>
      <name>Fran√ßois Wolf</name>
    </author>
    <author>
      <name>Corentin Le Coz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SCIREA Journal of Mathematics. Vol. 3, No. 3, 2018, pp. 118 - 131</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.05844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.05844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11D04" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.07067">
    <id>http://arxiv.org/abs/1809.07067v2</id>
    <updated>2018-09-20T14:01:39Z</updated>
    <published>2018-09-19T08:51:58Z</published>
    <title>Encoding two-dimensional range top-k queries revisited</title>
    <summary>  We consider the problem of encoding two-dimensional arrays, whose elements
come from a total order, for answering top-k queries. The aim is to obtain
encodings that use space close to the information-theoretic lower bound, which
can be constructed efficiently. For $2 \times n$ arrays, we first give upper
and lower bounds on space for answering sorted and unsorted 3-sided top-k
queries. For $m \times n$ arrays, with $m \le n$ and $k \le mn$, we obtain
$(m\lg{{(k+1)n \choose n}}+4nm(m-1)+o(n))$-bit encoding for answering sorted
4-sided top-k queries. This improves the $\min{(O(mn\lg{n}),m^2\lg{{(k+1)n
\choose n}} +m\lg{m}+o(n))}$-bit encoding of Jo et al. [CPM, 2016] when $m =
o(\lg{n})$. This is a consequence of a new encoding that encodes a $2 \times n$
array to support sorted 4-sided top-k queries on it using an additional $4n$
bits, in addition to the encodings to support the top-k queries on individual
rows. This new encoding is a non-trivial generalization of the encoding of Jo
et al. [CPM, 2016] that supports sorted 4-sided top-2 queries on it using an
additional $3n$ bits. We also give almost optimal space encodings for $3$-sided
top-k queries, and show lower bounds on encodings for $3$-sided and $4$-sided
top-k queries.
</summary>
    <author>
      <name>Seungbum Jo</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <link href="http://arxiv.org/abs/1809.07067v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.07067v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.07320">
    <id>http://arxiv.org/abs/1809.07320v3</id>
    <updated>2019-02-01T11:28:28Z</updated>
    <published>2018-09-19T15:58:53Z</published>
    <title>Relaxing Wheeler Graphs for Indexing Reads</title>
    <summary>  As industry standards for average-coverage rates increase, DNA readsets are
becoming more repetitive. The run-length compressed Burrows-Wheeler Transform
(RLBWT) is the basis for several powerful algorithms and data structures
designed to handle repetitive genetic datasets, but applying it directly to
readsets is problematic because end-of-string symbols break up runs and, worse,
the characters at the ends of the reads lack context and are thus scattered
throughout the BWT. In this paper we first propose storing the readset as a
Wheeler graph consisting of a set of paths, to avoid end-of-string symbols at
the cost of storing nodes' in- and out-degrees. We then propose rebuilding the
Wheeler graph as if each read were preceded by some imaginary context. This
requires us to relax the constraint that nodes with in-degree 0 in the graph
should appear first in the ordering showing that it is a Wheeler graph, and can
lead to false-positive pattern matches. Nevertheless, we first describe how to
support fast locating, which allows us to filter out false matches and return
all true matches, in time bounded in terms of the total number of matches. More
importantly, we then also show how to augment the RLBWT for the relaxed Wheeler
graph such that we can tell after what point a backward search will return only
false matches, and quickly return as a witness one true match if a backward
search yields any.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Garance Gourdel</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Jared Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1809.07320v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.07320v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.07661">
    <id>http://arxiv.org/abs/1809.07661v1</id>
    <updated>2018-09-20T14:57:04Z</updated>
    <published>2018-09-20T14:57:04Z</published>
    <title>Small Uncolored and Colored Choice Dictionaries</title>
    <summary>  A choice dictionary can be initialized with a parameter $n\in\mathbb{N}$ and
subsequently maintains an initially empty subset $S$ of $\{1,\ldots,n\}$ under
insertion, deletion, membership queries and an operation $\textit{choice}$ that
returns an arbitrary element of $S$. The choice dictionary is fundamental in
space-efficient computing and has numerous applications. The best previous
choice dictionary can be initialized with $n$ and $t\in\mathbb{N}$ and
subsequently executes all operations in $O(t)$ time and occupies
$n+O(n({t/w})^t+\log n)$ bits on a word RAM with a word length of
$w=\Omega(\log n)$ bits. We describe a new choice dictionary that executes all
operations in constant time and, in addition to the space needed to store the
integer $n$, occupies only $n+1$ bits, which is shown to be optimal if
$w=o(n)$.
  A generalization of the choice dictionary called a colored choice dictionary
is initialized with $c\in\mathbb{N}$ in addition to $n$ and subsequently
maintains a semipartition $(S_0,\ldots,S_{c-1})$ of $\{1,\ldots,n\}$ under the
operations $\textit{setcolor}(j,\ell)$, which moves $\ell$ from its current
subset to $S_j$, $\textit{color}(\ell)$, which returns the unique
$j\in\{0,\ldots,c-1\}$ with $\ell\in S_j$, and $\textit{choice}(j)$, which
returns an arbitrary element of $S_j$. We describe new colored choice
dictionaries that, if initialized with constant $c$, execute
$\textit{setcolor}$, $\textit{color}$ and $\textit{choice}$ in constant time
and occupy $n\log_2\!c+1$ bits plus the space needed to store $n$ if $c$ is a
power of 2, and at most $n\log_2\!c+n^\epsilon$ bits in general, for arbitrary
fixed $\epsilon>0$. We also study the possibility of iterating over the set $S$
or over $S_j$ for given $j\in\{0,\ldots,c-1\}$ and an application of this to
breadth-first search.
</summary>
    <author>
      <name>Torben Hagerup</name>
    </author>
    <link href="http://arxiv.org/abs/1809.07661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.07661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.08411">
    <id>http://arxiv.org/abs/1809.08411v3</id>
    <updated>2019-10-30T15:39:24Z</updated>
    <published>2018-09-22T08:50:24Z</published>
    <title>Adaptive Shivers Sort: An Alternative Sorting Algorithm</title>
    <summary>  We present one stable mergesort algorithm, called \Adaptive Shivers Sort,
that exploits the existence of monotonic runs for sorting efficiently partially
sorted data. We also prove that, although this algorithm is simple to
implement, its computational cost, in number of comparisons performed, is
optimal up to a small additive linear term.
</summary>
    <author>
      <name>Vincent Jug√©</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of the article published in the proceedings of the 31th
  Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.08411v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08411v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.08669">
    <id>http://arxiv.org/abs/1809.08669v2</id>
    <updated>2019-07-08T15:58:52Z</updated>
    <published>2018-09-23T20:13:14Z</published>
    <title>Collapsing Superstring Conjecture</title>
    <summary>  In the Shortest Common Superstring (SCS) problem, one is given a collection
of strings, and needs to find a shortest string containing each of them as a
substring. SCS admits $2\frac{11}{23}$-approximation in polynomial time (Mucha,
SODA'13). While this algorithm and its analysis are technically involved, the
30 years old Greedy Conjecture claims that the trivial and efficient Greedy
Algorithm gives a 2-approximation for SCS.
  We develop a graph-theoretic framework for studying approximation algorithms
for SCS. The framework is reminiscent of the classical 2-approximation for
Traveling Salesman: take two copies of an optimal solution, apply a trivial
edge-collapsing procedure, and get an approximate solution. In this framework,
we observe two surprising properties of SCS solutions, and we conjecture that
they hold for all input instances. The first conjecture, that we call
Collapsing Superstring conjecture, claims that there is an elementary way to
transform any solution repeated twice into the same graph $G$. This conjecture
would give an elementary 2-approximate algorithm for SCS. The second conjecture
claims that not only the resulting graph $G$ is the same for all solutions, but
that $G$ can be computed by an elementary greedy procedure called Greedy
Hierarchical Algorithm.
  While the second conjecture clearly implies the first one, perhaps
surprisingly we prove their equivalence. We support these equivalent
conjectures by giving a proof for the special case where all input strings have
length at most 3. We prove that the standard Greedy Conjecture implies Greedy
Hierarchical Conjecture, while the latter is sufficient for an efficient greedy
2-approximate approximation of SCS. Except for its (conjectured) good
approximation ratio, the Greedy Hierarchical Algorithm provably finds a
3.5-approximation.
</summary>
    <author>
      <name>Alexander Golovnev</name>
    </author>
    <author>
      <name>Alexander S. Kulikov</name>
    </author>
    <author>
      <name>Alexander Logunov</name>
    </author>
    <author>
      <name>Ivan Mihajlin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">visualization available at: http://compsciclub.ru/scs/</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.08669v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08669v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.09330">
    <id>http://arxiv.org/abs/1809.09330v2</id>
    <updated>2019-08-20T19:40:27Z</updated>
    <published>2018-09-25T05:47:44Z</published>
    <title>Improved Parallel Cache-Oblivious Algorithms for Dynamic Programming and
  Linear Algebra</title>
    <summary>  Emerging non-volatile main memory (NVRAM) technologies provide
byte-addressability, low idle power, and improved memory-density, and are
likely to be a key component in the future memory hierarchy. However, a
critical challenge in achieving high performance is in accounting for the
asymmetry that NVRAM writes can be significantly more expensive than NVRAM
reads.
  In this paper, we consider a large class of cache-oblivious algorithms for
dynamic programming (DP) and linear algebra, and try to reduce the writes in
the asymmetric setting while maintaining high parallelism. To achieve that, our
key approach is to show the correspondence between these problems and an
abstraction for their computation, which is referred to as the $k$-d grids.
Then by showing lower bound and new algorithms for computing $k$-d grids, we
show a list of improved cache-oblivious algorithms of many DP recurrences and
in linear algebra in the asymmetric setting, both sequentially and in parallel.
  Surprisingly, even without considering the read-write asymmetry (i.e.,
setting the write cost to be the same as the read cost in the algorithms), the
new algorithms improve the existing cache complexity of many problems. We
believe the reason is that the extra level of abstraction of $k$-d grids helps
us to better understand the complexity and difficulties of these problems. We
believe that the novelty of our framework is of interests and leads to many new
questions for future work.
</summary>
    <author>
      <name>Guy E. Blleloch</name>
    </author>
    <author>
      <name>Yan Gu</name>
    </author>
    <link href="http://arxiv.org/abs/1809.09330v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.09330v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.11580">
    <id>http://arxiv.org/abs/1807.11580v1</id>
    <updated>2018-07-27T01:37:45Z</updated>
    <published>2018-07-27T01:37:45Z</published>
    <title>Enumerating Cryptarithms Using Deterministic Finite Automata</title>
    <summary>  A cryptarithm is a mathematical puzzle where given an arithmetic equation
written with letters rather than numerals, a player must discover an assignment
of numerals on letters that makes the equation hold true. In this paper, we
propose a method to construct a DFA that accepts cryptarithms that admit
(unique) solutions for each base. We implemented the method and constructed a
DFA for bases $k \le 7$. Those DFAs can be used as complete catalogues of
cryptarithms,whose applications include enumeration of and counting the exact
numbers $G_k(n)$ of cryptarithm instances with $n$ digits that admit base-$k$
solutions. Moreover, explicit formulas for $G_2(n)$ and $G_3(n)$ are given.
</summary>
    <author>
      <name>Yuki Nozaki</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Takashi Horiyama</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.00674">
    <id>http://arxiv.org/abs/1808.00674v1</id>
    <updated>2018-08-02T05:53:17Z</updated>
    <published>2018-08-02T05:53:17Z</published>
    <title>Reconstructing Strings from Substrings: Optimal Randomized and
  Average-Case Algorithms</title>
    <summary>  The problem called "String reconstruction from substrings" is a mathematical
model of sequencing by hybridization that plays an important role in DNA
sequencing. In this problem, we are given a blackbox oracle holding an unknown
string ${\mathcal X}$ and are required to obtain (reconstruct) ${\mathcal X}$
through "substring queries" $Q(S)$. $Q(S)$ is given to the oracle with a string
$S$ and the answer of the oracle is Yes if ${\mathcal X}$ includes $S$ as a
substring and No otherwise. Our goal is to minimize the number of queries for
the reconstruction. In this paper, we deal with only binary strings for
${\mathcal X}$ whose length $n$ is given in advance by using a sequence of good
$S$'s. In 1995, Skiena and Sundaram first studied this problem and obtained an
algorithm whose query complexity is $n+O(\log n)$. Its information theoretic
lower bound is $n$, and they posed an obvious open question; if we can remove
the $O(\log n)$ additive term. No progress has been made until now. This paper
gives two partially positive answers to this open question. One is a randomized
algorithm whose query complexity is $n+O(1)$ with high probability and the
other is an average-case algorithm also having a query complexity of $n+O(1)$
on average. The $n$ lower bound is still true for both cases, and hence they
are optimal up to an additive constant.
</summary>
    <author>
      <name>Kazuo Iwama</name>
    </author>
    <author>
      <name>Junichi Teruyama</name>
    </author>
    <author>
      <name>Shuntaro Tsuyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.00963">
    <id>http://arxiv.org/abs/1808.00963v1</id>
    <updated>2018-08-02T16:38:49Z</updated>
    <published>2018-08-02T16:38:49Z</published>
    <title>Scalable String and Suffix Sorting: Algorithms, Techniques, and Tools</title>
    <summary>  This dissertation focuses on two fundamental sorting problems: string sorting
and suffix sorting. The first part considers parallel string sorting on
shared-memory multi-core machines, the second part external memory suffix
sorting using the induced sorting principle, and the third part distributed
external memory suffix sorting with a new distributed algorithmic big data
framework named Thrill.
</summary>
    <author>
      <name>Timo Bingmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5445/IR/1000085031</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5445/IR/1000085031" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">396 pages, dissertation, Karlsruher Instituts f\"ur Technologie
  (2018). arXiv admin note: text overlap with arXiv:1101.3448 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.01071">
    <id>http://arxiv.org/abs/1808.01071v1</id>
    <updated>2018-08-03T02:24:26Z</updated>
    <published>2018-08-03T02:24:26Z</published>
    <title>Right-to-left online construction of parameterized position heaps</title>
    <summary>  Two strings of equal length are said to parameterized match if there is a
bijection that maps the characters of one string to those of the other string,
so that two strings become identical. The parameterized pattern matching
problem is, given two strings $T$ and $P$, to find the occurrences of
substrings in $T$ that parameterized match $P$. Diptarama et al. [Position
Heaps for Parameterized Strings, CPM 2017] proposed an indexing data structure
called parameterized position heaps, and gave a left-to-right online
construction algorithm. In this paper, we present a right-to-left online
construction algorithm for parameterized position heaps. For a text string $T$
of length $n$ over two kinds of alphabets $\Sigma$ and $\Pi$ of respective size
$\sigma$ and $\pi$, our construction algorithm runs in $O(n \log(\sigma +
\pi))$ time with $O(n)$ space. Our right-to-left parameterized position heaps
support pattern matching queries in $O(m \log (\sigma + \pi) + m \pi +
\mathit{pocc}))$ time, where $m$ is the length of a query pattern $P$ and
$\mathit{pocc}$ is the number of occurrences to report. Our construction and
pattern matching algorithms are as efficient as Diptarama et al.'s algorithms.
</summary>
    <author>
      <name>Noriki Fujisato</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.03553">
    <id>http://arxiv.org/abs/1808.03553v1</id>
    <updated>2018-08-10T14:05:28Z</updated>
    <published>2018-08-10T14:05:28Z</published>
    <title>Dynamic all scores matrices for LCS score</title>
    <summary>  The problem of aligning two strings A,B in order to determine their
similarity is fundamental in the field of pattern matching. An important
concept in this domain is the "all scores matrix" that encodes the local
alignment comparison of two strings. Namely, let K denote the all scores matrix
containing the alignment score of every substring of B with A, and let J denote
the all scores matrix containing the alignment score of every suffix of B with
every prefix of A.
  In this paper we consider the problem of maintaining an all scores matrix
where the scoring function is the LCS score, while supporting single character
prepend and append operations to A and N. Our algorithms exploit the sparsity
parameters L=LCS(A,B) and Delta = |B|-L. For the matrix K we propose an
algorithm that supports incremental operations to both ends of A in O(Delta)
time. Whilst for the matrix J we propose an algorithm that supports a single
type of incremental operation, either a prepend operation to A or an append
operation to B, in O(L) time. This structure can also be extended to support
both operations simultaneously in O(L log log L) time.
</summary>
    <author>
      <name>Amir Carmel</name>
    </author>
    <author>
      <name>Dekel Tsur</name>
    </author>
    <author>
      <name>Michal Ziv-Ukelson</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.03307">
    <id>http://arxiv.org/abs/1808.03307v1</id>
    <updated>2018-08-09T19:06:40Z</updated>
    <published>2018-08-09T19:06:40Z</published>
    <title>Longest Increasing Subsequence under Persistent Comparison Errors</title>
    <summary>  We study the problem of computing a longest increasing subsequence in a
sequence $S$ of $n$ distinct elements in the presence of persistent comparison
errors. In this model, every comparison between two elements can return the
wrong result with some fixed (small) probability $ p $, and comparisons cannot
be repeated. Computing the longest increasing subsequence exactly is impossible
in this model, therefore, the objective is to identify a subsequence that (i)
is indeed increasing and (ii) has a length that approximates the length of the
longest increasing subsequence.
  We present asymptotically tight upper and lower bounds on both the
approximation factor and the running time. In particular, we present an
algorithm that computes an $O(\log n)$-approximation in time $O(n\log n)$, with
high probability. This approximation relies on the fact that that we can
approximately sort $n$ elements in $O(n\log n)$ time such that the maximum
dislocation of an element is at most $O(\log n)$. For the lower bounds, we
prove that (i) there is a set of sequences, such that on a sequence picked
randomly from this set every algorithm must return an $\Omega(\log
n)$-approximation with high probability, and (ii) any $O(\log n)$-approximation
algorithm for longest increasing subsequence requires $\Omega(n \log n)$
comparisons, even in the absence of errors.
</summary>
    <author>
      <name>Barbara Geissmann</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.03978">
    <id>http://arxiv.org/abs/1808.03978v2</id>
    <updated>2018-12-05T06:09:03Z</updated>
    <published>2018-08-12T18:16:59Z</published>
    <title>Local Decodability of the Burrows-Wheeler Transform</title>
    <summary>  The Burrows-Wheeler Transform (BWT) is among the most influential discoveries
in text compression and DNA storage. It is a reversible preprocessing step that
rearranges an $n$-letter string into runs of identical characters (by
exploiting context regularities), resulting in highly compressible strings, and
is the basis of the \texttt{bzip} compression program. Alas, the decoding
process of BWT is inherently sequential and requires $\Omega(n)$ time even to
retrieve a \emph{single} character.
  We study the succinct data structure problem of locally decoding short
substrings of a given text under its \emph{compressed} BWT, i.e., with small
additive redundancy $r$ over the \emph{Move-To-Front} (\texttt{bzip})
compression. The celebrated BWT-based FM-index (FOCS '00), as well as other
related literature, yield a trade-off of $r=\tilde{O}(n/\sqrt{t})$ bits, when a
single character is to be decoded in $O(t)$ time. We give a near-quadratic
improvement $r=\tilde{O}(n\lg(t)/t)$. As a by-product, we obtain an
\emph{exponential} (in $t$) improvement on the redundancy of the FM-index for
counting pattern-matches on compressed text. In the interesting regime where
the text compresses to $n^{1-o(1)}$ bits, these results provide an $\exp(t)$
\emph{overall} space reduction. For the local decoding problem of BWT, we also
prove an $\Omega(n/t^2)$ cell-probe lower bound for "symmetric" data
structures.
  We achieve our main result by designing a compressed partial-sums (Rank) data
structure over BWT. The key component is a \emph{locally-decodable}
Move-to-Front (MTF) code: with only $O(1)$ extra bits per block of length
$n^{\Omega(1)}$, the decoding time of a single character can be decreased from
$\Omega(n)$ to $O(\lg n)$. This result is of independent interest in
algorithmic information theory.
</summary>
    <author>
      <name>Sandip Sinha</name>
    </author>
    <author>
      <name>Omri Weinstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The following two technical typos were fixed: (1) On page 2,
  following Theorem 1, the decoding time of a contiguous substring of size
  $\ell$ was corrected from $O(t + \ell)$ to $O(t + \ell \cdot \lg t)$. (2) In
  the statement of Theorem 2, the query time to count occurrences of patterns
  of length $\ell$ was corrected to $O(t \ell)$, independent of the number of
  occurrences</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03978v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03978v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.03658">
    <id>http://arxiv.org/abs/1808.03658v1</id>
    <updated>2018-08-10T18:06:01Z</updated>
    <published>2018-08-10T18:06:01Z</published>
    <title>The effective entropy of next/previous larger/smaller value queries</title>
    <summary>  We study the problem of storing the minimum number of bits required to answer
next/previous larger/smaller value queries on an array $A$ of $n$ numbers,
without storing $A$. We show that these queries can be answered by storing at
most $3.701 n$ bits. Our result improves the result of Jo and Satti [TCS 2016]
that gives an upper bound of $4.088n$ bits for this problem.
</summary>
    <author>
      <name>Dekel Tsur</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03658v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03658v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.05879">
    <id>http://arxiv.org/abs/1808.05879v3</id>
    <updated>2018-12-18T22:27:26Z</updated>
    <published>2018-08-17T14:26:00Z</published>
    <title>Cardinality Estimators do not Preserve Privacy</title>
    <summary>  Cardinality estimators like HyperLogLog are sketching algorithms that
estimate the number of distinct elements in a large multiset. Their use in
privacy-sensitive contexts raises the question of whether they leak private
information. In particular, can they provide any privacy guarantees while
preserving their strong aggregation properties? We formulate an abstract notion
of cardinality estimators, that captures this aggregation requirement: one can
merge sketches without losing precision. We propose an attacker model and a
corresponding privacy definition, strictly weaker than differential privacy: we
assume that the attacker has no prior knowledge of the data. We then show that
if a cardinality estimator satisfies this definition, then it cannot have a
reasonable level of accuracy. We prove similar results for weaker versions of
our definition, and analyze the privacy of existing algorithms, showing that
their average privacy loss is significant, even for multisets with large
cardinalities. We conclude that strong aggregation requirements are
incompatible with any reasonable definition of privacy, and that cardinality
estimators should be considered as sensitive as raw data. We also propose risk
mitigation strategies for their real-world applications.
</summary>
    <author>
      <name>Damien Desfontaines</name>
    </author>
    <author>
      <name>Andreas Lochbihler</name>
    </author>
    <author>
      <name>David Basin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05879v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05879v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.02792">
    <id>http://arxiv.org/abs/1809.02792v2</id>
    <updated>2019-07-04T15:31:22Z</updated>
    <published>2018-09-08T12:15:58Z</published>
    <title>Fully-Functional Suffix Trees and Optimal Text Searching in BWT-runs
  Bounded Space</title>
    <summary>  Indexing highly repetitive texts - such as genomic databases, software
repositories and versioned text collections - has become an important problem
since the turn of the millennium. A relevant compressibility measure for
repetitive texts is r, the number of runs in their Burrows-Wheeler Transforms
(BWTs). One of the earliest indexes for repetitive collections, the Run-Length
FM-index, used O(r) space and was able to efficiently count the number of
occurrences of a pattern of length m in the text (in loglogarithmic time per
pattern symbol, with current techniques). However, it was unable to locate the
positions of those occurrences efficiently within a space bounded in terms of
r. In this paper we close this long-standing problem, showing how to extend the
Run-Length FM-index so that it can locate the occ occurrences efficiently
within O(r) space (in loglogarithmic time each), and reaching optimal time, O(m
+ occ), within O(r log log w ({\sigma} + n/r)) space, for a text of length n
over an alphabet of size {\sigma} on a RAM machine with words of w =
{\Omega}(log n) bits. Within that space, our index can also count in optimal
time, O(m). Multiplying the space by O(w/ log {\sigma}), we support count and
locate in O(dm log({\sigma})/we) and O(dm log({\sigma})/we + occ) time, which
is optimal in the packed setting and had not been obtained before in compressed
space. We also describe a structure using O(r log(n/r)) space that replaces the
text and extracts any text substring of length ` in almost-optimal time
O(log(n/r) + ` log({\sigma})/w). Within that space, we similarly provide direct
access to suffix array, inverse suffix array, and longest common prefix array
cells, and extend these capabilities to full suffix tree functionality,
typically in O(log(n/r)) time per operation.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted version; optimal count and locate in smaller space: O(r log
  log_w(n/r + sigma))</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.02792v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.02792v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.03827">
    <id>http://arxiv.org/abs/1807.03827v1</id>
    <updated>2018-07-10T19:00:45Z</updated>
    <published>2018-07-10T19:00:45Z</published>
    <title>Improved Time and Space Bounds for Dynamic Range Mode</title>
    <summary>  Given an array A of $n$ elements, we wish to support queries for the most
frequent and least frequent element in a subrange $[l, r]$ of $A$. We also wish
to support updates that change a particular element at index $i$ or insert/
delete an element at index $i$. For the range mode problem, our data structure
supports all operations in $O(n^{2/3})$ deterministic time using only $O(n)$
space. This improves two results by Chan et al. \cite{C14}: a linear space data
structure supporting update and query operations in $\tilde{O}(n^{3/4})$ time
and an $O(n^{4/3})$ space data structure supporting update and query operations
in $\tilde{O}(n^{2/3})$ time. For the range least frequent problem, we address
two variations. In the first, we are allowed to answer with an element of $A$
that may not appear in the query range, and in the second, the returned element
must be present in the query range. For the first variation, we develop a data
structure that supports queries in $\tilde{O}(n^{2/3})$ time, updates in
$O(n^{2/3})$ time, and occupies $O(n)$ space. For the second variation, we
develop a Monte Carlo data structure that supports queries in $O(n^{2/3})$
time, updates in $\tilde{O}(n^{2/3})$ time, and occupies $\tilde{O}(n)$ space,
but requires that updates are made independently of the results of previous
queries. The Monte Carlo data structure is also capable of answering
$k$-frequency queries; that is, the problem of finding an element of given
frequency in the specified query range. Previously, no dynamic data structures
were known for least frequent element or $k$-frequency queries.
</summary>
    <author>
      <name>Hicham El-Zein</name>
    </author>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Bryce Sandlund</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.04613">
    <id>http://arxiv.org/abs/1807.04613v1</id>
    <updated>2018-07-12T14:00:35Z</updated>
    <published>2018-07-12T14:00:35Z</published>
    <title>Push-Down Trees: Optimal Self-Adjusting Complete Trees</title>
    <summary>  Since Sleator and Tarjan's seminal work on self-adjusting lists, heaps and
binary search trees, researchers have been fascinated by dynamic datastructures
and the questions related to their performance over time. This paper initiates
the study of another classic datastructure, self-adjusting (binary) Complete
Trees (CTs): trees which do not provide a simple search mechanism but allow to
efficiently access items given a global map. Our problem finds applications,
e.g., in the context of warehouse optimization or self-adjusting communication
networks which can adapt to the demand they serve. We show that self-adjusting
complete trees assume an interesting position between the complexity of
self-adjusting (unordered) lists and binary search trees. In particular, we
observe that in contrast to lists, a simple move-to-front strategy alone is
insufficient to achieve a constant competitive ratio. Rather, and similarly to
binary search trees, an additional (efficient) tree update rule is needed.
Intriguingly, while it is unknown whether the working set is a lower bound for
binary search trees, we show that this holds in our model. So while finding an
update rule is still an open problem for binary search trees, this paper shows
that there exists a simple, random update rule for complete trees. Our main
result is a dynamically optimal (i.e., constant competitive) self-adjusting CT
called Push-Down Tree, on expectation against an oblivious adversary. At the
heart of our approach lies a distributed algorithm called Random-Push: this
algorithm approximates a natural notion of Most Recently Used (MRU) tree
(essentially an approximate working set), by first performing move-to-front,
but then pushing less recently accessed items down the tree using a random
walk.
</summary>
    <author>
      <name>Chen Avin</name>
    </author>
    <author>
      <name>Kaushik Mondal</name>
    </author>
    <author>
      <name>Stefan Schmid</name>
    </author>
    <link href="http://arxiv.org/abs/1807.04613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.04682">
    <id>http://arxiv.org/abs/1807.04682v2</id>
    <updated>2018-07-13T12:28:09Z</updated>
    <published>2018-07-12T15:47:56Z</published>
    <title>Know When to Fold 'Em: Self-Assembly of Shapes by Folding in Oritatami</title>
    <summary>  An oritatami system (OS) is a theoretical model of self-assembly via
co-transcriptional folding. It consists of a growing chain of beads which can
form bonds with each other as they are transcribed. During the transcription
process, the $\delta$ most recently produced beads dynamically fold so as to
maximize the number of bonds formed, self-assemblying into a shape
incrementally. The parameter $\delta$ is called the delay and is related to the
transcription rate in nature.
  This article initiates the study of shape self-assembly using oritatami. A
shape is a connected set of points in the triangular lattice. We first show
that oritatami systems differ fundamentally from tile-assembly systems by
exhibiting a family of infinite shapes that can be tile-assembled but cannot be
folded by any OS. As it is NP-hard in general to determine whether there is an
OS that folds into (self-assembles) a given finite shape, we explore the
folding of upscaled versions of finite shapes. We show that any shape can be
folded from a constant size seed, at any scale n >= 3, by an OS with delay 1.
We also show that any shape can be folded at the smaller scale 2 by an OS with
unbounded delay. This leads us to investigate the influence of delay and to
prove that, for all {\delta} > 2, there are shapes that can be folded (at scale
1) with delay {\delta} but not with delay {\delta}'&lt;{\delta}. These results
serve as a foundation for the study of shape-building in this new model of
self-assembly, and have the potential to provide better understanding of
cotranscriptional folding in biology, as well as improved abilities of
experimentalists to design artificial systems that self-assemble via this
complex dynamical process.
</summary>
    <author>
      <name>Erik D. Demaine</name>
    </author>
    <author>
      <name>Jacob Hendricks</name>
    </author>
    <author>
      <name>Meagan Olsen</name>
    </author>
    <author>
      <name>Matthew J. Patitz</name>
    </author>
    <author>
      <name>Trent A. Rogers</name>
    </author>
    <author>
      <name>Nicolas Schabanel</name>
    </author>
    <author>
      <name>Shinnosuke Seki</name>
    </author>
    <author>
      <name>Hadley Thomas</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Short version published at DNA24, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.04682v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04682v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.05356">
    <id>http://arxiv.org/abs/1807.05356v1</id>
    <updated>2018-07-14T08:33:39Z</updated>
    <published>2018-07-14T08:33:39Z</published>
    <title>A Simple and Space Efficient Segment Tree Implementation</title>
    <summary>  The segment tree is an extremely versatile data structure. In this paper, a
new heap based implementation of segment trees is proposed. In such an
implementation of segment tree, the structural information associated with the
tree nodes can be removed completely. Some primary computational geometry
problems such as stabbing counting queries, measure of union of intervals, and
maximum clique size of Intervals are used to demonstrate the efficiency of the
new heap based segment tree implementation. Each interval in a set $S=\{I_1
,I_2 ,\cdots,I_n\}$ of $n$ intervals can be insert into or delete from the heap
based segment tree in $O(\log n)$ time. All the primary computational geometry
problems can be solved efficiently.
</summary>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Xiaodong Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.06359">
    <id>http://arxiv.org/abs/1807.06359v1</id>
    <updated>2018-07-17T11:44:45Z</updated>
    <published>2018-07-17T11:44:45Z</published>
    <title>Using statistical encoding to achieve tree succinctness never seen
  before</title>
    <summary>  We propose a new succinct representation of labeled trees which represents a
tree T using |T|H_k(T) number of bits (plus some smaller order terms), where
|T|H_k(T) denotes the k-th order (tree label) entropy, as defined by Ferragina
at al. 2005. Our representation employs a new, simple method of partitioning
the tree, which preserves both tree shape and node degrees. Previously, the
only representation that used |T|H_k(T) bits was based on XBWT, a
transformation that linearizes tree labels into a single string, combined with
compression boosting. The proposed representation is much simpler than the one
based on XBWT, which used additional linear space (bounded by 0.01n) hidden in
the "smaller order terms" notion, as an artifact of using zeroth order entropy
coder; our representation uses sublinear additional space (for reasonable
values of k and size of the label alphabet {\sigma}). The proposed
representation can be naturally extended to a succinct data structure for
trees, which uses |T|H_k(T) plus additional O(|T|k log_{\sigma}/ log_{\sigma}
|T| + |T| log log_{\sigma} |T|/ log_{\sigma} |T|) bits and supports all the
usual navigational queries in constant time. At the cost of increasing the
query time to O(log log |T|/ log |T|) we can further reduce the space
redundancy to O(|T| log log |T|/ log_{\sigma} |T|) bits, assuming k &lt;=
log_{\sigma} |T|. This is a major improvement over representation based on
XBWT: even though XBWT-based representation uses |T|H_k(T) bits, the space
needed for structure supporting navigational queries is much larger: (...)
</summary>
    <author>
      <name>Micha≈Ç Ga≈Ñczorz</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.07596">
    <id>http://arxiv.org/abs/1807.07596v1</id>
    <updated>2018-07-19T18:33:20Z</updated>
    <published>2018-07-19T18:33:20Z</published>
    <title>The colored longest common prefix array computed via sequential scans</title>
    <summary>  Due to the increased availability of large datasets of biological sequences,
the tools for sequence comparison are now relying on efficient alignment-free
approaches to a greater extent. Most of the alignment-free approaches require
the computation of statistics of the sequences in the dataset. Such
computations become impractical in internal memory when very large collections
of long sequences are considered. In this paper, we present a new conceptual
data structure, the colored longest common prefix array (cLCP), that allows to
efficiently tackle several problems with an alignment-free approach. In fact,
we show that such a data structure can be computed via sequential scans in
semi-external memory. By using cLCP, we propose an efficient lightweight
strategy to solve the multi-string Average Common Substring (ACS) problem, that
consists in the pairwise comparison of a single string against a collection of
$m$ strings simultaneously, in order to obtain $m$ ACS induced distances.
Experimental results confirm the effectiveness of our approach.
</summary>
    <author>
      <name>F. Garofalo</name>
    </author>
    <author>
      <name>G. Rosone</name>
    </author>
    <author>
      <name>M. Sciortino</name>
    </author>
    <author>
      <name>D. Verzotto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version of the paper that will be included in the SPIRE
  2018 proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.08777">
    <id>http://arxiv.org/abs/1807.08777v3</id>
    <updated>2019-10-31T17:41:30Z</updated>
    <published>2018-07-23T18:30:25Z</published>
    <title>Two Algorithms to Find Primes in Patterns</title>
    <summary>  Let $k\ge 1$ be an integer, and let $P= (f_1(x), \ldots, f_k(x) )$ be $k$
admissible linear polynomials over the integers, or \textit{the pattern}. We
present two algorithms that find all integers $x$ where $\max{ \{f_i(x) \} }
\le n$ and all the $f_i(x)$ are prime.
  Our first algorithm takes at most $O_P(n/(\log\log n)^k)$ arithmetic
operations using $O(k\sqrt{n})$ space.
  Our second algorithm takes slightly more time, $O_P(n/(\log \log n)^{k-1})$
arithmetic operations, but uses only $n^{1/c}$ space for a constant $c>2$. We
prove correctness unconditionally, but the running time relies on two unproven
but reasonable conjectures.
  We are unaware of any previous complexity results for this problem beyond the
use of a prime sieve. We also implemented several parallel versions of our
second algorithm to show it is viable in practice. In particular, we found some
new Cunningham chains of length 15, and we found all quadruplet primes up to
$10^{17}$.
</summary>
    <author>
      <name>Jonathan P. Sorenson</name>
    </author>
    <author>
      <name>Jonathan Webster</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08777v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08777v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11A11, 11Y11, 11Y16, 68Q25" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.10483">
    <id>http://arxiv.org/abs/1807.10483v1</id>
    <updated>2018-07-27T08:16:29Z</updated>
    <published>2018-07-27T08:16:29Z</published>
    <title>Faster Recovery of Approximate Periods over Edit Distance</title>
    <summary>  The approximate period recovery problem asks to compute all
$\textit{approximate word-periods}$ of a given word $S$ of length $n$: all
primitive words $P$ ($|P|=p$) which have a periodic extension at edit distance
smaller than $\tau_p$ from $S$, where $\tau_p = \lfloor
\frac{n}{(3.75+\epsilon)\cdot p} \rfloor$ for some $\epsilon>0$. Here, the set
of periodic extensions of $P$ consists of all finite prefixes of $P^\infty$.
  We improve the time complexity of the fastest known algorithm for this
problem of Amir et al. [Theor. Comput. Sci., 2018] from $O(n^{4/3})$ to $O(n
\log n)$. Our tool is a fast algorithm for Approximate Pattern Matching in
Periodic Text. We consider only verification for the period recovery problem
when the candidate approximate word-period $P$ is explicitly given up to cyclic
rotation; the algorithm of Amir et al. reduces the general problem in $O(n)$
time to a logarithmic number of such more specific instances.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszy≈Ñski</name>
    </author>
    <author>
      <name>Tomasz Wale≈Ñ</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SPIRE 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.11328">
    <id>http://arxiv.org/abs/1807.11328v3</id>
    <updated>2019-02-15T14:01:33Z</updated>
    <published>2018-07-30T13:03:27Z</published>
    <title>Guidesort: Simpler Optimal Deterministic Sorting for the Parallel Disk
  Model</title>
    <summary>  A new algorithm, Guidesort, for sorting in the uniprocessor variant of the
parallel disk model (PDM) of Vitter and Shriver is presented. The algorithm is
deterministic and executes a number of (parallel) I/O operations that comes
within a constant factor $C$ of the optimum. The algorithm and its analysis are
simpler than those proposed in previous work, and the achievable constant
factor $C$ of essentially 3 appears to be smaller than for all other known
deterministic algorithms, at least for plausible parameter values.
</summary>
    <author>
      <name>Torben Hagerup</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11328v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11328v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.05942">
    <id>http://arxiv.org/abs/1806.05942v1</id>
    <updated>2018-06-15T13:14:08Z</updated>
    <published>2018-06-15T13:14:08Z</published>
    <title>Enhanced string factoring from alphabet orderings</title>
    <summary>  In this note we consider the concept of alphabet ordering in the context of
string factoring. We propose a greedy-type algorithm which produces Lyndon
factorizations with small numbers of factors along with a modification for
large numbers of factors. For the technique we introduce the Exponent Parikh
vector. Applications and research directions derived from circ-UMFFs are
discussed.
</summary>
    <author>
      <name>Amanda Clare</name>
    </author>
    <author>
      <name>Jacqueline W. Daykin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.06726">
    <id>http://arxiv.org/abs/1806.06726v4</id>
    <updated>2019-11-12T02:39:25Z</updated>
    <published>2018-06-18T14:22:07Z</published>
    <title>Zip Trees</title>
    <summary>  We introduce the zip tree, a form of randomized binary search tree that
integrates previous ideas into one practical, performant, and
pleasant-to-implement package. A zip tree is a binary search tree in which each
node has a numeric rank and the tree is (max)-heap-ordered with respect to
ranks, with rank ties broken in favor of smaller keys. Zip trees are
essentially treaps (Seidel and Aragon 1996), except that ranks are drawn from a
geometric distribution instead of a uniform distribution, and we allow rank
ties. These changes enable us to use fewer random bits per node. We perform
insertions and deletions by unmerging and merging paths ("unzipping" and
"zipping") rather than by doing rotations, which avoids some pointer changes
and improves efficiency. The methods of zipping and unzipping take inspiration
from previous top-down approaches to insertion and deletion (Stephenson 1980;
Mart\'inez and Roura 1998; Sprugnoli 1980). From a theoretical standpoint, this
work provides two main results. First, zip trees require only $O(\log \log n)$
bits (with high probability) to represent the largest rank in an $n$-node
binary search tree; previous data structures require $O(\log n)$ bits for the
largest rank. Second, zip trees are naturally isomorphic to skip lists (Pugh
1990), and simplify the mapping of (Dean and Jones 2007) between skip lists and
binary search trees.
</summary>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <author>
      <name>Caleb C. Levy</name>
    </author>
    <author>
      <name>Stephen Timmel</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06726v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06726v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.07598">
    <id>http://arxiv.org/abs/1806.07598v1</id>
    <updated>2018-06-20T07:57:10Z</updated>
    <published>2018-06-20T07:57:10Z</published>
    <title>A Faster External Memory Priority Queue with DecreaseKeys</title>
    <summary>  A priority queue is a fundamental data structure that maintains a dynamic set
of (key, priority)-pairs and supports Insert, Delete, ExtractMin and
DecreaseKey operations. In the external memory model, the current best priority
queue supports each operation in amortized $O(\frac{1}{B}\log \frac{N}{B})$
I/Os. If the DecreaseKey operation does not need to be supported, one can
design a more efficient data structure that supports the Insert, Delete and
ExtractMin operations in $O(\frac{1}{B}\log \frac{N}{B}/ \log \frac{M}{B})$
I/Os. A recent result shows that a degradation in performance is inevitable by
proving a lower bound of $\Omega(\frac{1}{B}\log B/\log\log N)$ I/Os for
priority queues with DecreaseKeys. In this paper we tighten the gap between the
lower bound and the upper bound by proposing a new priority queue which
supports the DecreaseKey operation and has an expected amortized I/O complexity
of $O(\frac{1}{B}\log \frac{N}{B}/\log\log N)$. Our result improves the
external memory priority queue with DecreaseKeys for the first time in over a
decade, and also gives the fastest external memory single source shortest path
algorithm.
</summary>
    <author>
      <name>Shunhua Jiang</name>
    </author>
    <author>
      <name>Kasper Green Larsen</name>
    </author>
    <link href="http://arxiv.org/abs/1806.07598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.08692">
    <id>http://arxiv.org/abs/1806.08692v1</id>
    <updated>2018-06-22T14:26:39Z</updated>
    <published>2018-06-22T14:26:39Z</published>
    <title>Improved bounds for multipass pairing heaps and path-balanced binary
  search trees</title>
    <summary>  We revisit multipass pairing heaps and path-balanced binary search trees
(BSTs), two classical algorithms for data structure maintenance. The pairing
heap is a simple and efficient "self-adjusting" heap, introduced in 1986 by
Fredman, Sedgewick, Sleator, and Tarjan. In the multipass variant (one of the
original pairing heap variants described by Fredman et al.) the minimum item is
extracted via repeated pairing rounds in which neighboring siblings are linked.
  Path-balanced BSTs, proposed by Sleator (Subramanian, 1996), are a natural
alternative to Splay trees (Sleator and Tarjan, 1983). In a path-balanced BST,
whenever an item is accessed, the search path leading to that item is
re-arranged into a balanced tree.
  Despite their simplicity, both algorithms turned out to be difficult to
analyse. Fredman et al. showed that operations in multipass pairing heaps take
amortized $O(\log{n} \cdot \log\log{n} / \log\log\log{n})$ time. For searching
in path-balanced BSTs, Balasubramanian and Raman showed in 1995 the same
amortized time bound of $O(\log{n} \cdot \log\log{n} / \log\log\log{n})$, using
a different argument.
  In this paper we show an explicit connection between the two algorithms and
improve the two bounds to $O\left(\log{n} \cdot 2^{\log^{\ast}{n}} \cdot
\log^{\ast}{n}\right)$, respectively $O\left(\log{n} \cdot 2^{\log^{\ast}{n}}
\cdot (\log^{\ast}{n})^2 \right)$, where $\log^{\ast}(\cdot)$ denotes the very
slowly growing iterated logarithm function. These are the first improvements in
more than three, resp. two decades, approaching in both cases the
information-theoretic lower bound of $\Omega(\log{n})$.
</summary>
    <author>
      <name>Dani Dorfman</name>
    </author>
    <author>
      <name>Haim Kaplan</name>
    </author>
    <author>
      <name>L√°szl√≥ Kozma</name>
    </author>
    <author>
      <name>Seth Pettie</name>
    </author>
    <author>
      <name>Uri Zwick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at ESA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.09646">
    <id>http://arxiv.org/abs/1806.09646v1</id>
    <updated>2018-06-25T18:02:05Z</updated>
    <published>2018-06-25T18:02:05Z</published>
    <title>Fast entropy-bounded string dictionary look-up with mismatches</title>
    <summary>  We revisit the fundamental problem of dictionary look-up with mismatches.
Given a set (dictionary) of $d$ strings of length $m$ and an integer $k$, we
must preprocess it into a data structure to answer the following queries: Given
a query string $Q$ of length $m$, find all strings in the dictionary that are
at Hamming distance at most $k$ from $Q$. Chan and Lewenstein (CPM 2015) showed
a data structure for $k = 1$ with optimal query time $O(m/w + occ)$, where $w$
is the size of a machine word and $occ$ is the size of the output. The data
structure occupies $O(w d \log^{1+\varepsilon} d)$ extra bits of space (beyond
the entropy-bounded space required to store the dictionary strings). In this
work we give a solution with similar bounds for a much wider range of values
$k$. Namely, we give a data structure that has $O(m/w + \log^k d + occ)$ query
time and uses $O(w d \log^k d)$ extra bits of space.
</summary>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper accepted to MFCS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.10498">
    <id>http://arxiv.org/abs/1806.10498v1</id>
    <updated>2018-06-27T14:27:02Z</updated>
    <published>2018-06-27T14:27:02Z</published>
    <title>Dynamic Trees with Almost-Optimal Access Cost</title>
    <summary>  An optimal binary search tree for an access sequence on elements is a static
tree that minimizes the total search cost. Constructing perfectly optimal
binary search trees is expensive so the most efficient algorithms construct
almost optimal search trees. There exists a long literature of constructing
almost optimal search trees dynamically, i.e., when the access pattern is not
known in advance. All of these trees, e.g., splay trees and treaps, provide a
multiplicative approximation to the optimal search cost.
  In this paper we show how to maintain an almost optimal weighted binary
search tree under access operations and insertions of new elements where the
approximation is an additive constant. More technically, we maintain a tree in
which the depth of the leaf holding an element $e_i$ does not exceed
$\min(\log(W/w_i),\log n)+O(1)$ where $w_i$ is the number of times $e_i$ was
accessed and $W$ is the total length of the access sequence.
  Our techniques can also be used to encode a sequence of $m$ symbols with a
dynamic alphabetic code in $O(m)$ time so that the encoding length is bounded
by $m(H+O(1))$, where $H$ is the entropy of the sequence. This is the first
efficient algorithm for adaptive alphabetic coding that runs in constant time
per symbol.
</summary>
    <author>
      <name>Mordecai Golin</name>
    </author>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of an ESA'18 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.10261">
    <id>http://arxiv.org/abs/1806.10261v1</id>
    <updated>2018-06-27T00:58:24Z</updated>
    <published>2018-06-27T00:58:24Z</published>
    <title>BDDs Naturally Represent Boolean Functions, and ZDDs Naturally Represent
  Sets of Sets</title>
    <summary>  This paper studies a difference between Binary Decision Diagrams (BDDs) and
Zero-suppressed BDDs (ZDDs) from a conceptual point of view. It is commonly
understood that a BDD is a representation of a Boolean function, whereas a ZDD
is a representation of a set of sets. However, there is a one-to-one
correspondence between Boolean functions and sets of sets, and therefore we
could also regard a BDD as a representation of a set of sets, and similarly for
a ZDD and a Boolean function. The aim of this paper is to give an explanation
why the distinction between BDDs and ZDDs mentioned above is made despite the
existence of the one-to-one correspondence. To achieve this, we first observe
that Boolean functions and sets of sets are equipped with non-isomorphic
functor structures, and show that these functor structures are reflected in the
definitions of BDDs and ZDDs. This result can be stated formally as naturality
of certain maps. To the author's knowledge, this is the first formally stated
theorem that justifies the commonly accepted distinction between BDDs and ZDDs.
In addition, we show that this result extends to sentential decision diagrams
and their zero-suppressed variant.
</summary>
    <author>
      <name>Kensuke Kojima</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.0; E.1; F.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.10176">
    <id>http://arxiv.org/abs/1806.10176v1</id>
    <updated>2018-06-26T19:12:10Z</updated>
    <published>2018-06-26T19:12:10Z</published>
    <title>Practical Access to Dynamic Programming on Tree Decompositions</title>
    <summary>  Parameterized complexity theory has lead to a wide range of algorithmic
breakthroughs within the last decades, but the practicability of these methods
for real-world problems is still not well understood. We investigate the
practicability of one of the fundamental approaches of this field: dynamic
programming on tree decompositions. Indisputably, this is a key technique in
parameterized algorithms and modern algorithm design. Despite the enormous
impact of this approach in theory, it still has very little influence on
practical implementations. The reasons for this phenomenon are manifold. One of
them is the simple fact that such an implementation requires a long chain of
non-trivial tasks (as computing the decomposition, preparing it,...). We
provide an easy way to implement such dynamic programs that only requires the
definition of the update rules. With this interface, dynamic programs for
various problems, such as 3-coloring, can be implemented easily in about 100
lines of structured Java code.
  The theoretical foundation of the success of dynamic programming on tree
decompositions is well understood due to Courcelle's celebrated theorem, which
states that every MSO-definable problem can be efficiently solved if a tree
decomposition of small width is given. We seek to provide practical access to
this theorem as well, by presenting a lightweight model-checker for a small
fragment of MSO. This fragment is powerful enough to describe many natural
problems, and our model-checker turns out to be very competitive against
similar state-of-the-art tools.
</summary>
    <author>
      <name>Max Bannach</name>
    </author>
    <author>
      <name>Sebastian Berndt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ESA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.00112">
    <id>http://arxiv.org/abs/1807.00112v1</id>
    <updated>2018-06-30T02:33:15Z</updated>
    <published>2018-06-30T02:33:15Z</published>
    <title>Approximate Nearest Neighbors in Limited Space</title>
    <summary>  We consider the $(1+\epsilon)$-approximate nearest neighbor search problem:
given a set $X$ of $n$ points in a $d$-dimensional space, build a data
structure that, given any query point $y$, finds a point $x \in X$ whose
distance to $y$ is at most $(1+\epsilon) \min_{x \in X} \|x-y\|$ for an
accuracy parameter $\epsilon \in (0,1)$. Our main result is a data structure
that occupies only $O(\epsilon^{-2} n \log(n) \log(1/\epsilon))$ bits of space,
assuming all point coordinates are integers in the range $\{-n^{O(1)} \ldots
n^{O(1)}\}$, i.e., the coordinates have $O(\log n)$ bits of precision. This
improves over the best previously known space bound of $O(\epsilon^{-2} n
\log(n)^2)$, obtained via the randomized dimensionality reduction method of
Johnson and Lindenstrauss (1984). We also consider the more general problem of
estimating all distances from a collection of query points to all data points
$X$, and provide almost tight upper and lower bounds for the space complexity
of this problem.
</summary>
    <author>
      <name>Piotr Indyk</name>
    </author>
    <author>
      <name>Tal Wagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">COLT 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.01804">
    <id>http://arxiv.org/abs/1807.01804v2</id>
    <updated>2018-11-02T17:37:28Z</updated>
    <published>2018-07-04T22:49:10Z</published>
    <title>Optimal Ball Recycling</title>
    <summary>  Balls-and-bins games have been a wildly successful tool for modeling load
balancing problems. In this paper, we study a new scenario, which we call the
ball recycling game, defined as follows:
  Throw m balls into n bins i.i.d. according to a given probability
distribution p. Then, at each time step, pick a non-empty bin and recycle its
balls: take the balls from the selected bin and re-throw them according to p.
  This balls-and-bins game closely models memory-access heuristics in
databases. The goal is to have a bin-picking method that maximizes the
recycling rate, defined to be the expected number of balls recycled per step in
the stationary distribution. We study two natural strategies for ball
recycling: Fullest Bin, which greedily picks the bin with the maximum number of
balls, and Random Ball, which picks a ball at random and recycles its bin. We
show that for general p, Random Ball is constant-optimal, whereas Fullest Bin
can be pessimal. However, when p = u, the uniform distribution, Fullest Bin is
optimal to within an additive constant.
</summary>
    <author>
      <name>Michael A. Bender</name>
    </author>
    <author>
      <name>Jake Christensen</name>
    </author>
    <author>
      <name>Alex Conway</name>
    </author>
    <author>
      <name>Mart√≠n Farach-Colton</name>
    </author>
    <author>
      <name>Rob Johnson</name>
    </author>
    <author>
      <name>Meng-Tsung Tsai</name>
    </author>
    <link href="http://arxiv.org/abs/1807.01804v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01804v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.01217">
    <id>http://arxiv.org/abs/1806.01217v1</id>
    <updated>2018-06-04T16:59:15Z</updated>
    <published>2018-06-04T16:59:15Z</published>
    <title>Efficient Genomic Interval Queries Using Augmented Range Trees</title>
    <summary>  Efficient large-scale annotation of genomic intervals is essential for
personal genome interpretation in the realm of precision medicine. There are 13
possible relations between two intervals according to Allen's interval algebra.
Conventional interval trees are routinely used to identify the genomic
intervals satisfying a coarse relation with a query interval, but cannot
support efficient query for more refined relations such as all Allen's
relations. We design and implement a novel approach to address this unmet need.
Through rewriting Allen's interval relations, we transform an interval query to
a range query, then adapt and utilize the range trees for querying. We
implement two types of range trees: a basic 2-dimensional range tree (2D-RT)
and an augmented range tree with fractional cascading (RTFC) and compare them
with the conventional interval tree (IT). Theoretical analysis shows that RTFC
can achieve the best time complexity for interval queries regarding all Allen's
relations among the three trees. We also perform comparative experiments on the
efficiency of RTFC, 2D-RT and IT in querying noncoding element annotations in a
large collection of personal genomes. Our experimental results show that 2D-RT
is more efficient than IT for interval queries regarding most of Allen's
relations, RTFC is even more efficient than 2D-RT. The results demonstrate that
RTFC is an efficient data structure for querying large-scale datasets regarding
Allen's relations between genomic intervals, such as those required by
interpreting genome-wide variation in large populations.
</summary>
    <author>
      <name>Chengsheng Mao</name>
    </author>
    <author>
      <name>Alal Eran</name>
    </author>
    <author>
      <name>Yuan Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.00588">
    <id>http://arxiv.org/abs/1806.00588v1</id>
    <updated>2018-06-02T06:18:15Z</updated>
    <published>2018-06-02T06:18:15Z</published>
    <title>Fast Locality Sensitive Hashing for Beam Search on GPU</title>
    <summary>  We present a GPU-based Locality Sensitive Hashing (LSH) algorithm to speed up
beam search for sequence models. We utilize the winner-take-all (WTA) hash,
which is based on relative ranking order of hidden dimensions and thus
resilient to perturbations in numerical values. Our algorithm is designed by
fully considering the underling architecture of CUDA-enabled GPUs
(Algorithm/Architecture Co-design): 1) A parallel Cuckoo hash table is applied
for LSH code lookup (guaranteed O(1) lookup time); 2) Candidate lists are
shared across beams to maximize the parallelism; 3) Top frequent words are
merged into candidate lists to improve performance. Experiments on 4
large-scale neural machine translation models demonstrate that our algorithm
can achieve up to 4x speedup on softmax module, and 2x overall speedup without
hurting BLEU on GPU.
</summary>
    <author>
      <name>Xing Shi</name>
    </author>
    <author>
      <name>Shizhen Xu</name>
    </author>
    <author>
      <name>Kevin Knight</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.01799">
    <id>http://arxiv.org/abs/1806.01799v2</id>
    <updated>2019-04-27T17:00:27Z</updated>
    <published>2018-06-05T16:41:07Z</published>
    <title>Survey and Taxonomy of Lossless Graph Compression and Space-Efficient
  Graph Representations</title>
    <summary>  Various graphs such as web or social networks may contain up to trillions of
edges. Compressing such datasets can accelerate graph processing by reducing
the amount of I/O accesses and the pressure on the memory subsystem. Yet,
selecting a proper compression method is challenging as there exist a plethora
of techniques, algorithms, domains, and approaches in compressing graphs. To
facilitate this, we present a survey and taxonomy on lossless graph compression
that is the first, to the best of our knowledge, to exhaustively analyze this
domain. Moreover, our survey does not only categorize existing schemes, but
also explains key ideas, discusses formal underpinning in selected works, and
describes the space of the existing compression schemes using three dimensions:
areas of research (e.g., compressing web graphs), techniques (e.g., gap
encoding), and features (e.g., whether or not a given scheme targets dynamic
graphs). Our survey can be used as a guide to select the best lossless
compression scheme in a given setting.
</summary>
    <author>
      <name>Maciej Besta</name>
    </author>
    <author>
      <name>Torsten Hoefler</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01799v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01799v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.01804">
    <id>http://arxiv.org/abs/1806.01804v2</id>
    <updated>2018-09-06T20:29:19Z</updated>
    <published>2018-06-05T16:53:02Z</published>
    <title>Tree Path Majority Data Structures</title>
    <summary>  We present the first solution to $\tau$-majorities on tree paths. Given a
tree of $n$ nodes, each with a label from $[1..\sigma]$, and a fixed threshold
$0&lt;\tau&lt;1$, such a query gives two nodes $u$ and $v$ and asks for all the
labels that appear more than $\tau \cdot |P_{uv}|$ times in the path $P_{uv}$
from $u$ to $v$, where $|P_{uv}|$ denotes the number of nodes in $P_{uv}$. Note
that the answer to any query is of size up to $1/\tau$. On a $w$-bit RAM, we
obtain a linear-space data structure with $O((1/\tau)\log^* n \log\log_w
\sigma)$ query time. For any $\kappa > 1$, we can also build a structure that
uses $O(n\log^{[\kappa]} n)$ space, where $\log^{[\kappa]} n$ denotes the
function that applies logarithm $\kappa$ times to $n$, and answers queries in
time $O((1/\tau)\log\log_w \sigma)$. The construction time of both structures
is $O(n\log n)$. We also describe two succinct-space solutions with the same
query time of the linear-space structure. One uses $2nH + 4n + o(n)(H+1)$ bits,
where $H \le \lg\sigma$ is the entropy of the label distribution, and can be
built in $O(n\log n)$ time. The other uses $nH + O(n) + o(nH)$ bits and is
built in $O(n\log n)$ time w.h.p.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01804v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01804v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.02004">
    <id>http://arxiv.org/abs/1806.02004v1</id>
    <updated>2018-06-06T04:39:41Z</updated>
    <published>2018-06-06T04:39:41Z</published>
    <title>Another Proof of Cuckoo hashing with New Variants</title>
    <summary>  We show a new proof for the load of obtained by a Cuckoo Hashing data
structure. Our proof is arguably simpler than previous proofs and allows for
new generalizations. The proof first appeared in Pinkas et. al. \cite{PSWW19}
in the context of a protocol for private set intersection. We present it here
separately to improve its readability.
</summary>
    <author>
      <name>Udi Wieder</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.02718">
    <id>http://arxiv.org/abs/1806.02718v1</id>
    <updated>2018-06-07T15:03:00Z</updated>
    <published>2018-06-07T15:03:00Z</published>
    <title>Alignment-free sequence comparison using absent words</title>
    <summary>  Sequence comparison is a prerequisite to virtually all comparative genomic
analyses. It is often realised by sequence alignment techniques, which are
computationally expensive. This has led to increased research into
alignment-free techniques, which are based on measures referring to the
composition of sequences in terms of their constituent patterns. These
measures, such as $q$-gram distance, are usually computed in time linear with
respect to the length of the sequences. In this paper, we focus on the
complementary idea: how two sequences can be efficiently compared based on
information that does not occur in the sequences. A word is an {\em absent
word} of some sequence if it does not occur in the sequence. An absent word is
{\em minimal} if all its proper factors occur in the sequence. Here we present
the first linear-time and linear-space algorithm to compare two sequences by
considering {\em all} their minimal absent words. In the process, we present
results of combinatorial interest, and also extend the proposed techniques to
compare circular sequences. We also present an algorithm that, given a word $x$
of length $n$, computes the largest integer for which all factors of $x$ of
that length occur in some minimal absent word of $x$ in time and space
$\cO(n)$. Finally, we show that the known asymptotic upper bound on the number
of minimal absent words of a word is tight.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Robert Mercas</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of "Linear-Time Sequence Comparison Using Minimal
  Absent Words &amp; Applications" Proc. LATIN 2016, arxiv:1506.04917</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.02718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.03102">
    <id>http://arxiv.org/abs/1806.03102v1</id>
    <updated>2018-06-08T11:58:40Z</updated>
    <published>2018-06-08T11:58:40Z</published>
    <title>Compressed Communication Complexity of Longest Common Prefixes</title>
    <summary>  We consider the communication complexity of fundamental longest common prefix
(Lcp) problems. In the simplest version, two parties, Alice and Bob, each hold
a string, $A$ and $B$, and we want to determine the length of their longest
common prefix $l=\text{Lcp}(A,B)$ using as few rounds and bits of communication
as possible. We show that if the longest common prefix of $A$ and $B$ is
compressible, then we can significantly reduce the number of rounds compared to
the optimal uncompressed protocol, while achieving the same (or fewer) bits of
communication. Namely, if the longest common prefix has an LZ77 parse of $z$
phrases, only $O(\lg z)$ rounds and $O(\lg \ell)$ total communication is
necessary.
  We extend the result to the natural case when Bob holds a set of strings
$B_1, \ldots, B_k$, and the goal is to find the length of the maximal longest
prefix shared by $A$ and any of $B_1, \ldots, B_k$. Here, we give a protocol
with $O(\log z)$ rounds and $O(\lg z \lg k + \lg \ell)$ total communication.
  We present our result in the public-coin model of computation but by a
standard technique our results generalize to the private-coin model.
Furthermore, if we view the input strings as integers the problems are the
greater-than problem and the predecessor problem.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Mikko Berggreen Ettienne</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <author>
      <name>Eva Rotenberg</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.03611">
    <id>http://arxiv.org/abs/1806.03611v1</id>
    <updated>2018-06-10T08:29:30Z</updated>
    <published>2018-06-10T08:29:30Z</published>
    <title>CuCoTrack: Cuckoo Filter Based Connection Tracking</title>
    <summary>  This paper introduces CuCoTrack, a cuckoo hash based data structure designed
to efficiently implement connection tracking. The proposed scheme exploits the
fact that queries always match one existing connection to compress the 5-tuple
that identifies the connection. This reduces significantly the amount of memory
needed to store the connections and also the memory bandwidth needed for
lookups. CuCoTrack uses a dynamic fingerprint to avoid collisions thus ensuring
that queries are completed in at most two memory accesses and facilitating a
hardware implementation. The proposed scheme has been analyzed theoretically
and validated by simulation. The results show that using 16 bits for the
fingerprint is enough to avoid collisions in practical configurations.
</summary>
    <author>
      <name>Pedro Reviriego</name>
    </author>
    <author>
      <name>Salvatore Pontarelli</name>
    </author>
    <author>
      <name>Gil Levy</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.04277">
    <id>http://arxiv.org/abs/1806.04277v1</id>
    <updated>2018-06-12T00:22:04Z</updated>
    <published>2018-06-12T00:22:04Z</published>
    <title>Indexed Dynamic Programming to boost Edit Distance and LCSS Computation</title>
    <summary>  There are efficient dynamic programming solutions to the computation of the
Edit Distance from $S\in[1..\sigma]^n$ to $T\in[1..\sigma]^m$, for many natural
subsets of edit operations, typically in time within $O(nm)$ in the worst-case
over strings of respective lengths $n$ and $m$ (which is likely to be optimal),
and in time within $O(n{+}m)$ in some special cases (e.g. disjoint alphabets).
We describe how indexing the strings (in linear time), and using such an index
to refine the recurrence formulas underlying the dynamic programs, yield faster
algorithms in a variety of models, on a continuum of classes of instances of
intermediate difficulty between the worst and the best case, thus refining the
analysis beyond the worst case analysis. As a side result, we describe similar
properties for the computation of the Longest Common Sub Sequence $LCSS(S,T)$
between $S$ and $T$, since it is a particular case of Edit Distance, and we
discuss the application of similar algorithmic and analysis techniques for
other dynamic programming solutions. More formally, we propose a parameterized
analysis of the computational complexity of the Edit Distance for various set
of operators and of the Longest Common Sub Sequence in function of the area of
the dynamic program matrix relevant to the computation.
</summary>
    <author>
      <name>J√©r√©my Barbay</name>
    </author>
    <author>
      <name>Andr√©s Olivares</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.04890">
    <id>http://arxiv.org/abs/1806.04890v1</id>
    <updated>2018-06-13T08:30:00Z</updated>
    <published>2018-06-13T08:30:00Z</published>
    <title>$O(n \log n)$-time text compression by LZ-style longest first
  substitution</title>
    <summary>  Mauer et al. [A Lempel-Ziv-style Compression Method for Repetitive Texts, PSC
2017] proposed a hybrid text compression method called LZ-LFS which has both
features of Lempel-Ziv 77 factorization and longest first substitution. They
showed that LZ-LFS can achieve better compression ratio for repetitive texts,
compared to some state-of-the-art compression algorithms. The drawback of Mauer
et al.'s method is that their LZ-LFS compression algorithm takes $O(n^2)$ time
on an input string of length $n$. In this paper, we show a faster LZ-LFS
compression algorithm that works in $O(n \log n)$ time. We also propose a
simpler version of LZ-LFS that can be computed in $O(n)$ time.
</summary>
    <author>
      <name>Akihiro Nishi</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.08612">
    <id>http://arxiv.org/abs/1805.08612v3</id>
    <updated>2019-07-07T21:03:01Z</updated>
    <published>2018-05-22T14:27:38Z</published>
    <title>On the Worst-Case Complexity of TimSort</title>
    <summary>  TimSort is an intriguing sorting algorithm designed in 2002 for Python, whose
worst-case complexity was announced, but not proved until our recent preprint.
In fact, there are two slightly different versions of TimSort that are
currently implemented in Python and in Java respectively. We propose a
pedagogical and insightful proof that the Python version runs in
$\mathcal{O}(n\log n)$. The approach we use in the analysis also applies to the
Java version, although not without very involved technical details. As a
byproduct of our study, we uncover a bug in the Java implementation that can
cause the sorting method to fail during the execution. We also give a proof
that Python's TimSort running time is in $\mathcal{O}(n + n\log \rho)$, where
$\rho$ is the number of runs (i.e. maximal monotonic sequences), which is quite
a natural parameter here and part of the explanation for the good behavior of
TimSort on partially sorted inputs.
</summary>
    <author>
      <name>Nicolas Auger</name>
    </author>
    <author>
      <name>Vincent Jug√©</name>
    </author>
    <author>
      <name>Cyril Nicaud</name>
    </author>
    <author>
      <name>Carine Pivoteau</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08612v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08612v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.08602">
    <id>http://arxiv.org/abs/1805.08602v1</id>
    <updated>2018-05-19T14:23:51Z</updated>
    <published>2018-05-19T14:23:51Z</published>
    <title>Orthogonal Point Location and Rectangle Stabbing Queries in 3-d</title>
    <summary>  In this work, we present a collection of new results on two fundamental
problems in geometric data structures: orthogonal point location and rectangle
stabbing.
  -We give the first linear-space data structure that supports 3-d point
location queries on $n$ disjoint axis-aligned boxes with optimal $O\left( \log
n\right)$ query time in the (arithmetic) pointer machine model. This improves
the previous $O\left( \log^{3/2} n \right)$ bound of Rahul [SODA 2015]. We
similarly obtain the first linear-space data structure in the I/O model with
optimal query cost, and also the first linear-space data structure in the word
RAM model with sub-logarithmic query time.
  -We give the first linear-space data structure that supports 3-d $4$-sided
and $5$-sided rectangle stabbing queries in optimal $O(\log_wn+k)$ time in the
word RAM model. We similarly obtain the first optimal data structure for the
closely related problem of 2-d top-$k$ rectangle stabbing in the word RAM
model, and also improved results for 3-d 6-sided rectangle stabbing.
  For point location, our solution is simpler than previous methods, and is
based on an interesting variant of the van Emde Boas recursion, applied in a
round-robin fashion over the dimensions, combined with bit-packing techniques.
For rectangle stabbing, our solution is a variant of Alstrup, Brodal, and
Rauhe's grid-based recursive technique (FOCS 2000), combined with a number of
new ideas.
</summary>
    <author>
      <name>Timothy M. Chan</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Saladi Rahul</name>
    </author>
    <author>
      <name>Konstantinos Tsakalidis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of the ICALP'18 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.08816">
    <id>http://arxiv.org/abs/1805.08816v1</id>
    <updated>2018-05-22T18:58:54Z</updated>
    <published>2018-05-22T18:58:54Z</published>
    <title>copMEM: Finding maximal exact matches via sampling both genomes</title>
    <summary>  Genome-to-genome comparisons require designating anchor points, which are
given by Maximum Exact Matches (MEMs) between their sequences. For large
genomes this is a challenging problem and the performance of existing
solutions, even in parallel regimes, is not quite satisfactory. We present a
new algorithm, copMEM, that allows to sparsely sample both input genomes, with
sampling steps being coprime. Despite being a single-threaded implementation,
copMEM computes all MEMs of minimum length 100 between the human and mouse
genomes in less than 2 minutes, using less than 10 GB of RAM memory.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Wojciech Bieniecki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The source code of copMEM is freely available at
  https://github.com/wbieniec/copmem. Contact: wbieniec@kis.p.lodz.pl,
  wbieniec@kis.p.lodz.pl</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.09423">
    <id>http://arxiv.org/abs/1805.09423v1</id>
    <updated>2018-05-23T21:00:47Z</updated>
    <published>2018-05-23T21:00:47Z</published>
    <title>Optimal Hashing in External Memory</title>
    <summary>  Hash tables are a ubiquitous class of dictionary data structures. However,
standard hash table implementations do not translate well into the external
memory model, because they do not incorporate locality for insertions.
  Iacono and Patracsu established an update/query tradeoff curve for external
hash tables: a hash table that performs insertions in $O(\lambda/B)$ amortized
IOs requires $\Omega(\log_\lambda N)$ expected IOs for queries, where $N$ is
the number of items that can be stored in the data structure, $B$ is the size
of a memory transfer, $M$ is the size of memory, and $\lambda$ is a tuning
parameter.
  They provide a hashing data structure that meets this curve for $\lambda$
that is $\Omega(\log\log M + \log_M N)$. Their data structure, which we call an
\defn{IP hash table}, is complicated and, to the best of our knowledge, has not
been implemented.
  In this paper, we present a new and much simpler optimal external memory hash
table, the \defn{Bundle of Arrays Hash Table} (BOA). BOAs are based on
size-tiered LSMs, a well-studied data structure, and are almost as easy to
implement. The BOA is optimal for a narrower range of $\lambda$. However, the
simplicity of BOAs allows them to be readily modified to achieve the following
results:
  \begin{itemize}
  \item A new external memory data structure, the \defn{Bundle of Trees Hash
Table} (BOT), that matches the performance of the IP hash table, while
retaining some of the simplicity of the BOAs.
  \item The \defn{cache-oblivious Bundle of Trees Hash Table} (COBOT), the
first cache-oblivious hash table. This data structure matches the optimality of
BOTs and IP hash tables over the same range of $\lambda$. \end{itemize}
</summary>
    <author>
      <name>Alex Conway</name>
    </author>
    <author>
      <name>Martin Farach-Colton</name>
    </author>
    <author>
      <name>Philip Shilane</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.10070">
    <id>http://arxiv.org/abs/1805.10070v1</id>
    <updated>2018-05-25T10:24:12Z</updated>
    <published>2018-05-25T10:24:12Z</published>
    <title>Strong link between BWT and XBW via Aho-Corasick automaton and
  applications to Run-Length Encoding</title>
    <summary>  The boom of genomic sequencing makes compression of set of sequences
inescapable. This underlies the need for multi-string indexing data structures
that helps compressing the data. The most prominent example of such data
structures is the Burrows-Wheeler Transform (BWT), a reversible permutation of
a text that improves its compressibility. A similar data structure, the
eXtended Burrows-Wheeler Transform (XBW), is able to index a tree labelled with
alphabet symbols. A link between a multi-string BWT and the Aho-Corasick
automaton has already been found and led to a way to build a XBW from a
multi-string BWT. We exhibit a stronger link between a multi-string BWT and a
XBW by using the order of the concatenation in the multi-string. This bijective
link has several applications: first, it allows to build one data structure
from the other; second, it enables one to compute an ordering of the input
strings that optimises a Run-Length measure (i.e., the compressibility) of the
BWT or of the XBW.
</summary>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Eric Rivals</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.10042">
    <id>http://arxiv.org/abs/1805.10042v1</id>
    <updated>2018-05-25T08:54:26Z</updated>
    <published>2018-05-25T08:54:26Z</published>
    <title>Algorithms for Anti-Powers in Strings</title>
    <summary>  A string $S[1,n]$ is a power (or tandem repeat) of order $k$ and period $n/k$
if it can decomposed into $k$ consecutive equal-length blocks of letters.
Powers and periods are fundamental to string processing, and algorithms for
their efficient computation have wide application and are heavily studied.
Recently, Fici et al. (Proc. ICALP 2016) defined an {\em anti-power} of order
$k$ to be a string composed of $k$ pairwise-distinct blocks of the same length
($n/k$, called {\em anti-period}). Anti-powers are a natural converse to
powers, and are objects of combinatorial interest in their own right. In this
paper we initiate the algorithmic study of anti-powers. Given a string $S$, we
describe an optimal algorithm for locating all substrings of $S$ that are
anti-powers of a specified order. The optimality of the algorithm follows form
a combinatorial lemma that provides a lower bound on the number of distinct
anti-powers of a given order: we prove that a string of length $n$ can contain
$\Theta(n^2/k)$ distinct anti-powers of order $k$.
</summary>
    <author>
      <name>Golnaz Badkobeh</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ipl.2018.05.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ipl.2018.05.003" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Informnation Processing Letters Volume 137, September
  2018, Pages 57-60</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.10042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.09924">
    <id>http://arxiv.org/abs/1805.09924v2</id>
    <updated>2018-07-01T07:39:26Z</updated>
    <published>2018-05-24T22:14:27Z</published>
    <title>Longest Unbordered Factor in Quasilinear Time</title>
    <summary>  A border u of a word w is a proper factor of w occurring both as a prefix and
as a suffix. The maximal unbordered factor of w is the longest factor of w
which does not have a border. Here an O(n log n)-time with high probability (or
O(n log n log^2 log n)-time deterministic) algorithm to compute the Longest
Unbordered Factor Array of w for general alphabets is presented, where n is the
length of w. This array specifies the length of the maximal unbordered factor
starting at each position of w. This is a major improvement on the running time
of the currently best worst-case algorithm working in O(n^{1.5} ) time for
integer alphabets [Gawrychowski et al., 2015].
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Ritu Kundu</name>
    </author>
    <author>
      <name>Manal Mohamed</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09924v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09924v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.11255">
    <id>http://arxiv.org/abs/1805.11255v1</id>
    <updated>2018-05-29T05:53:08Z</updated>
    <published>2018-05-29T05:53:08Z</published>
    <title>Succinct data structure for dynamic trees with faster queries</title>
    <summary>  Navarro and Sadakane [TALG 2014] gave a dynamic succinct data structure for
storing an ordinal tree. The structure supports tree queries in either $O(\log
n/\log\log n)$ or $O(\log n)$ time, and insertion or deletion of a single node
in $O(\log n)$ time. In this paper we improve the result of Navarro and
Sadakane by reducing the time complexities of some queries (e.g.\ degree and
level\_ancestor) from $O(\log n)$ to $O(\log n/\log\log n)$.
</summary>
    <author>
      <name>Dekel Tsur</name>
    </author>
    <link href="http://arxiv.org/abs/1805.11255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.11864">
    <id>http://arxiv.org/abs/1805.11864v1</id>
    <updated>2018-05-30T08:56:47Z</updated>
    <published>2018-05-30T08:56:47Z</published>
    <title>Space-Efficient DFS and Applications: Simpler, Leaner, Faster</title>
    <summary>  The problem of space-efficient depth-first search (DFS) is reconsidered. A
particularly simple and fast algorithm is presented that, on a directed or
undirected input graph $G=(V,E)$ with $n$ vertices and $m$ edges, carries out a
DFS in $O(n+m)$ time with $n+\sum_{v\in V_{\ge 3}}\lceil{\log_2(d_v-1)}\rceil
  +O(\log n)\le n+m+O(\log n)$ bits of working memory, where $d_v$ is the
(total) degree of $v$, for each $v\in V$, and $V_{\ge 3}=\{v\in V\mid d_v\ge
3\}$. A slightly more complicated variant of the algorithm works in the same
time with at most $n+({4/5})m+O(\log n)$ bits. It is also shown that a DFS can
be carried out in a graph with $n$ vertices and $m$ edges in $O(n+m\log^*\! n)$
time with $O(n)$ bits or in $O(n+m)$ time with either $O(n\log\log(4+{m/n}))$
bits or, for arbitrary integer $k\ge 1$, $O(n\log^{(k)}\! n)$ bits. These
results among them subsume or improve most earlier results on space-efficient
DFS. Some of the new time and space bounds are shown to extend to applications
of DFS such as the computation of cut vertices, bridges, biconnected components
and 2-edge-connected components in undirected graphs.
</summary>
    <author>
      <name>Torben Hagerup</name>
    </author>
    <link href="http://arxiv.org/abs/1805.11864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.00198">
    <id>http://arxiv.org/abs/1806.00198v3</id>
    <updated>2018-08-06T07:50:30Z</updated>
    <published>2018-06-01T05:22:55Z</published>
    <title>Block Palindromes: A New Generalization of Palindromes</title>
    <summary>  We study a new generalization of palindromes and gapped palindromes called
block palindromes. A block palindrome is a string that becomes a palindrome
when identical substrings are replaced with a distinct character. We
investigate several properties of block palindromes and in particular, study
substrings of a string which are block palindromes. In so doing, we introduce
the notion of a \emph{maximal block palindrome}, which leads to a compact
representation of all block palindromes that occur in a string. We also propose
an algorithm which enumerates all maximal block palindromes that appear in a
given string $T$ in $O(|T| + \|\mathit{MBP}(T)\|)$ time, where
$\|\mathit{MBP}(T)\|$ is the output size, which is optimal unless all the
maximal block palindromes can be represented in a more compact way.
</summary>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00198v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00198v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.03834">
    <id>http://arxiv.org/abs/1805.03834v2</id>
    <updated>2018-06-15T06:47:25Z</updated>
    <published>2018-05-10T06:05:26Z</published>
    <title>Haplotype-aware graph indexes</title>
    <summary>  The variation graph toolkit (VG) represents genetic variation as a graph.
Each path in the graph is a potential haplotype, though most paths are unlikely
recombinations of true haplotypes. We augment the VG model with haplotype
information to identify which paths are more likely to be correct. For this
purpose, we develop a scalable implementation of the graph extension of the
positional Burrows--Wheeler transform. We demonstrate the scalability of the
new implementation by indexing the 1000 Genomes Project haplotypes. We also
develop an algorithm for simplifying variation graphs for k-mer indexing
without losing any k-mers in the haplotypes.
</summary>
    <author>
      <name>Jouni Sir√©n</name>
    </author>
    <author>
      <name>Erik Garrison</name>
    </author>
    <author>
      <name>Adam M. Novak</name>
    </author>
    <author>
      <name>Benedict Paten</name>
    </author>
    <author>
      <name>Richard Durbin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to WABI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.03834v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03834v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.04154">
    <id>http://arxiv.org/abs/1805.04154v1</id>
    <updated>2018-05-10T20:00:42Z</updated>
    <published>2018-05-10T20:00:42Z</published>
    <title>Nearly-Optimal Mergesorts: Fast, Practical Sorting Methods That
  Optimally Adapt to Existing Runs</title>
    <summary>  We present two stable mergesort variants, "peeksort" and "powersort", that
exploit existing runs and find nearly-optimal merging orders with practically
negligible overhead. Previous methods either require substantial effort for
determining the merging order (Takaoka 2009; Barbay &amp; Navarro 2013) or do not
have a constant-factor optimal worst-case guarantee (Peters 2001; Auger, Nicaud
&amp; Pivoteau 2015; Buss &amp; Knop 2018). We demonstrate that our methods are
competitive in terms of running time with state-of-the-art implementations of
stable sorting methods.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/lipics.esa.2018.63</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/lipics.esa.2018.63" rel="related"/>
    <link href="http://arxiv.org/abs/1805.04154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.04272">
    <id>http://arxiv.org/abs/1805.04272v2</id>
    <updated>2018-08-15T16:24:39Z</updated>
    <published>2018-05-11T08:28:55Z</published>
    <title>An $O(N)$ Sorting Algorithm: Machine Learning Sort</title>
    <summary>  We propose an $O(N\cdot M)$ sorting algorithm by Machine Learning method,
which shows a huge potential sorting big data. This sorting algorithm can be
applied to parallel sorting and is suitable for GPU or TPU acceleration.
Furthermore, we discuss the application of this algorithm to sparse hash table.
</summary>
    <author>
      <name>Hanqing Zhao</name>
    </author>
    <author>
      <name>Yuehan Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.04272v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04272v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.04151">
    <id>http://arxiv.org/abs/1805.04151v1</id>
    <updated>2018-05-10T19:53:55Z</updated>
    <published>2018-05-10T19:53:55Z</published>
    <title>Beating Fredman-Koml√≥s for perfect $k$-hashing</title>
    <summary>  We say a subset $C \subseteq \{1,2,\dots,k\}^n$ is a $k$-hash code (also
called $k$-separated) if for every subset of $k$ codewords from $C$, there
exists a coordinate where all these codewords have distinct values.
Understanding the largest possible rate (in bits), defined as $(\log_2 |C|)/n$,
of a $k$-hash code is a classical problem. It arises in two equivalent
contexts: (i) the smallest size possible for a perfect hash family that maps a
universe of $N$ elements into $\{1,2,\dots,k\}$, and (ii) the zero-error
capacity for decoding with lists of size less than $k$ for a certain
combinatorial channel.
  A general upper bound of $k!/k^{k-1}$ on the rate of a $k$-hash code (in the
limit of large $n$) was obtained by Fredman and Koml\'{o}s in 1984 for any $k
\geq 4$. While better bounds have been obtained for $k=4$, their original bound
has remained the best known for each $k \ge 5$. In this work, we obtain the
first improvement to the Fredman-Koml\'{o}s bound for every $k \ge 5$. While we
get explicit (numerical) bounds for $k=5,6$, for larger $k$ we only show that
the FK bound can be improved by a positive, but unspecified, amount. Under a
conjecture on the optimum value of a certain polynomial optimization problem
over the simplex, our methods allow an effective bound to be computed for every
$k$.
</summary>
    <author>
      <name>Venkatesan Guruswami</name>
    </author>
    <author>
      <name>Andrii Riazanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.04151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.05228">
    <id>http://arxiv.org/abs/1805.05228v1</id>
    <updated>2018-05-14T15:26:30Z</updated>
    <published>2018-05-14T15:26:30Z</published>
    <title>Assembling Omnitigs using Hidden-Order de Bruijn Graphs</title>
    <summary>  De novo DNA assembly is a fundamental task in Bioinformatics, and finding
Eulerian paths on de Bruijn graphs is one of the dominant approaches to it. In
most of the cases, there may be no one order for the de Bruijn graph that works
well for assembling all of the reads. For this reason, some de Bruijn-based
assemblers try assembling on several graphs of increasing order, in turn.
Boucher et al. (2015) went further and gave a representation making it possible
to navigate in the graph and change order on the fly, up to a maximum $K$, but
they can use up to $\lg K$ extra bits per edge because they use an LCP array.
In this paper, we replace the LCP array by a succinct representation of that
array's Cartesian tree, which takes only 2 extra bits per edge and still lets
us support interesting navigation operations efficiently. These operations are
not enough to let us easily extract unitigs and only unitigs from the graph but
they do let us extract a set of safe strings that contains all unitigs. Suppose
we are navigating in a variable-order de Bruijn graph representation, following
these rules: if there are no outgoing edges then we reduce the order, hoping
one appears; if there is exactly one outgoing edge then we take it (increasing
the current order, up to $K$); if there are two or more outgoing edges then we
stop. Then we traverse a (variable-order) path such that we cross edges only
when we have no choice or, equivalently, we generate a string appending
characters only when we have no choice. It follows that the strings we extract
are safe. Our experiments show we extract a set of strings more informative
than the unitigs, while using a reasonable amount of memory.
</summary>
    <author>
      <name>Diego D√≠az-Dom√≠nguez</name>
    </author>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Veli M√§kinen</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <link href="http://arxiv.org/abs/1805.05228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.05787">
    <id>http://arxiv.org/abs/1805.05787v3</id>
    <updated>2018-07-11T11:27:04Z</updated>
    <published>2018-05-15T14:14:20Z</published>
    <title>Parallel Working-Set Search Structures</title>
    <summary>  In this paper we present two versions of a parallel working-set map on p
processors that supports searches, insertions and deletions. In both versions,
the total work of all operations when the map has size at least p is bounded by
the working-set bound, i.e., the cost of an item depends on how recently it was
accessed (for some linearization): accessing an item in the map with recency r
takes O(1+log r) work. In the simpler version each map operation has O((log
p)^2+log n) span (where n is the maximum size of the map). In the pipelined
version each map operation on an item with recency r has O((log p)^2+log r)
span. (Operations in parallel may have overlapping span; span is additive only
for operations in sequence.)
  Both data structures are designed to be used by a dynamic multithreading
parallel program that at each step executes a unit-time instruction or makes a
data structure call. To achieve the stated bounds, the pipelined data structure
requires a weak-priority scheduler, which supports a limited form of 2-level
prioritization. At the end we explain how the results translate to practical
implementations using work-stealing schedulers.
  To the best of our knowledge, this is the first parallel implementation of a
self-adjusting search structure where the cost of an operation adapts to the
access sequence. A corollary of the working-set bound is that it achieves work
static optimality: the total work is bounded by the access costs in an optimal
static search tree.
</summary>
    <author>
      <name>Kunal Agrawal</name>
    </author>
    <author>
      <name>Seth Gilbert</name>
    </author>
    <author>
      <name>Wei Quan Lim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3210377.3210390</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3210377.3210390" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Authors' version of a paper accepted to SPAA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.05787v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05787v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.05592">
    <id>http://arxiv.org/abs/1805.05592v2</id>
    <updated>2018-07-11T10:08:57Z</updated>
    <published>2018-05-15T07:02:20Z</published>
    <title>Parallel Write-Efficient Algorithms and Data Structures for
  Computational Geometry</title>
    <summary>  In this paper, we design parallel write-efficient geometric algorithms that
perform asymptotically fewer writes than standard algorithms for the same
problem. This is motivated by emerging non-volatile memory technologies with
read performance being close to that of random access memory but writes being
significantly more expensive in terms of energy and latency. We design
algorithms for planar Delaunay triangulation, $k$-d trees, and static and
dynamic augmented trees. Our algorithms are designed in the recently introduced
Asymmetric Nested-Parallel Model, which captures the parallel setting in which
there is a small symmetric memory where reads and writes are unit cost as well
as a large asymmetric memory where writes are $\omega$ times more expensive
than reads. In designing these algorithms, we introduce several techniques for
obtaining write-efficiency, including DAG tracing, prefix doubling,
reconstruction-based rebalancing and $\alpha$-labeling, which we believe will
be useful for designing other parallel write-efficient algorithms.
</summary>
    <author>
      <name>Guy E. Blelloch</name>
    </author>
    <author>
      <name>Yan Gu</name>
    </author>
    <author>
      <name>Yihan Sun</name>
    </author>
    <author>
      <name>Julian Shun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3210377.3210380</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3210377.3210380" rel="related"/>
    <link href="http://arxiv.org/abs/1805.05592v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05592v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.06177">
    <id>http://arxiv.org/abs/1805.06177v1</id>
    <updated>2018-05-16T07:56:49Z</updated>
    <published>2018-05-16T07:56:49Z</published>
    <title>On Computing Average Common Substring Over Run Length Encoded Sequences</title>
    <summary>  The Average Common Substring (ACS) is a popular alignment-free distance
measure for phylogeny reconstruction. The ACS can be computed in O(n) space and
time, where n=x+y is the input size. The compressed string matching is the
study of string matching problems with the following twist: the input data is
in a compressed format and the underling task must be performed with little or
no decompression. In this paper, we revisit the ACS problem under this paradigm
where the input sequences are given in their run-length encoded format. We
present an algorithm to compute ACS(X,Y) in O(Nlog N) time using O(N) space,
where N is the total length of sequences after run-length encoding.
</summary>
    <author>
      <name>Sahar Hooshmand</name>
    </author>
    <author>
      <name>Neda Tavakoli</name>
    </author>
    <author>
      <name>Paniz Abedin</name>
    </author>
    <author>
      <name>Sharma V. Thankachan</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.06821">
    <id>http://arxiv.org/abs/1805.06821v1</id>
    <updated>2018-05-17T15:28:49Z</updated>
    <published>2018-05-17T15:28:49Z</published>
    <title>External memory BWT and LCP computation for sequence collections with
  applications</title>
    <summary>  We propose an external memory algorithm for the computation of the BWT and
LCP array for a collection of sequences. Our algorithm takes the amount of
available memory as an input parameter, and tries to make the best use of it by
splitting the input collection into subcollections sufficiently small that it
can compute their BWT in RAM using an optimal linear time algorithm. Next, it
merges the partial BWTs in external memory and in the process it also computes
the LCP values. We prove that our algorithm performs O(n AveLcp) sequential
I/Os, where n is the total length of the collection, and AveLcp is the average
Longest Common Prefix of the collection. This bound is an improvement over the
known algorithms for the same task. The experimental results show that our
algorithm outperforms the current best algorithm for collections of sequences
with different lengths and for collections with relatively small average
Longest Common Prefix.
  In the second part of the paper, we show that our algorithm can be modified
to output two additional arrays that, used with the BWT and LCP arrays, provide
simple, scan based, external memory algorithms for three well known problems in
bioinformatics: the computation of maximal repeats, the all pairs suffix-prefix
overlaps, and the construction of succinct de Bruijn graphs. To our knowledge,
there are no other known external memory algorithms for these problems.
</summary>
    <author>
      <name>Lavinia Egidi</name>
    </author>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Guilherme P. Telles</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.06869">
    <id>http://arxiv.org/abs/1805.06869v2</id>
    <updated>2018-10-26T12:01:18Z</updated>
    <published>2018-05-17T17:16:07Z</published>
    <title>Revisiting the tree edit distance and its backtracing: A tutorial</title>
    <summary>  Almost 30 years ago, Zhang and Shasha (1989) published a seminal paper
describing an efficient dynamic programming algorithm computing the tree edit
distance, that is, the minimum number of node deletions, insertions, and
replacements that are necessary to transform one tree into another. Since then,
the tree edit distance has been widely applied, for example in biology and
intelligent tutoring systems. However, the original paper of Zhang and Shasha
can be challenging to read for newcomers and it does not describe how to
efficiently infer the optimal edit script. In this contribution, we provide a
comprehensive tutorial to the tree edit distance algorithm of Zhang and Shasha.
We further prove metric properties of the tree edit distance, and describe
efficient algorithms to infer the cheapest edit script, as well as a summary of
all cheapest edit scripts between two trees.
</summary>
    <author>
      <name>Benjamin Paa√üen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supplementary material for the ICML 2018 paper: Tree Edit Distance
  Learning via Adaptive Symbol Embeddings</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06869v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06869v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.08285">
    <id>http://arxiv.org/abs/1804.08285v1</id>
    <updated>2018-04-23T08:44:22Z</updated>
    <published>2018-04-23T08:44:22Z</published>
    <title>Succinct Oblivious RAM</title>
    <summary>  Reducing the database space overhead is critical in big-data processing. In
this paper, we revisit oblivious RAM (ORAM) using big-data standard for the
database space overhead.
  ORAM is a cryptographic primitive that enables users to perform arbitrary
database accesses without revealing the access pattern to the server. It is
particularly important today since cloud services become increasingly common
making it necessary to protect users' private information from database access
pattern analyses. Previous ORAM studies focused mostly on reducing the access
overhead. Consequently, the access overhead of the state-of-the-art ORAM
constructions is almost at practical levels in certain application scenarios
such as secure processors. On the other hand, most existing ORAM constructions
require $(1+\Theta(1))n$ (say, $10n$) bits of server space where $n$ is the
database size. Though such space complexity is often considered to be
"optimal", overhead such as $10 \times$ is prohibitive for big-data
applications in practice.
  We propose ORAM constructions that take only $(1+o(1))n$ bits of server space
while maintaining state-of-the-art performance in terms of the access overhead
and the user space. We also give non-asymptotic analyses and simulation results
which indicate that the proposed ORAM constructions are practically effective.
</summary>
    <author>
      <name>Taku Onodera</name>
    </author>
    <author>
      <name>Tetsuo Shibuya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages. A preliminary version of this paper appeared in STACS'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.08285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.08547">
    <id>http://arxiv.org/abs/1804.08547v1</id>
    <updated>2018-04-23T16:38:10Z</updated>
    <published>2018-04-23T16:38:10Z</published>
    <title>Entropy bounds for grammar compression</title>
    <summary>  In grammar compression we represent a string as a context free grammar. This
model is popular both in theoretical and practical applications due to its
simplicity, good compression rate and suitability for processing of the
compressed representations. In practice, achieving compression requires
encoding such grammar as a binary string, there are a few commonly used. We
bound the size of such encodings for several compression methods, along with
well-known \RePair algorithm. For \RePair we prove that its standard encoding,
which is a combination of entropy coding and special encoding of a grammar,
achieves $1.5|S|H_k(S)$. We also show that by stopping after some iteration we
can achieve $|S|H_k(S)$. The latter is particularly important, as it explains
the phenomenon observed in practice, that introducing too many nonterminals
causes the bit-size to grow. We generalize our approach to other compressions
methods like \Greedy or wide class of irreducible grammars, and other bit
encodings (including naive, which uses fixed-length codes). Our approach not
only proves the bounds but also partially explains why \Greedy and \RePair are
much better in practice than the other grammar based methods. At last, we show
that for a wide family of dictionary compression methods (including grammar
compressors) $\Omega\left(nk \log \sigma/\log_\sigma n\right)$ bits of
redundancy are required. This shows a separation between context-based/BWT
methods and dictionary compression algorithms, as for the former there exists
methods where redundancy does not depend on $n$, but only on $k$~and~$\sigma$.
</summary>
    <author>
      <name>Micha≈Ç Ga≈Ñczorz</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.10186">
    <id>http://arxiv.org/abs/1804.10186v1</id>
    <updated>2018-04-26T17:36:57Z</updated>
    <published>2018-04-26T17:36:57Z</published>
    <title>Edit Distance between Unrooted Trees in Cubic Time</title>
    <summary>  Edit distance between trees is a natural generalization of the classical edit
distance between strings, in which the allowed elementary operations are
contraction, uncontraction and relabeling of an edge. Demaine et al. [ACM
Trans. on Algorithms, 6(1), 2009] showed how to compute the edit distance
between rooted trees on $n$ nodes in $\mathcal{O}(n^{3})$ time. However,
generalizing their method to unrooted trees seems quite problematic, and the
most efficient known solution remains to be the previous $\mathcal{O}(n^{3}\log
n)$ time algorithm by Klein [ESA 1998]. Given the lack of progress on improving
this complexity, it might appear that unrooted trees are simply more difficult
than rooted trees. We show that this is, in fact, not the case, and edit
distance between unrooted trees on $n$ nodes can be computed in
$\mathcal{O}(n^{3})$ time. A significantly faster solution is unlikely to
exist, as Bringmann et al. [SODA 2018] proved that the complexity of computing
the edit distance between rooted trees cannot be decreased to
$\mathcal{O}(n^{3-\epsilon})$ unless some popular conjecture fails, and the
lower bound easily extends to unrooted trees. We also show that for two
unrooted trees of size $m$ and $n$, where $m\le n$, our algorithm can be
modified to run in $\mathcal{O}(nm^2(1+\log\frac nm))$. This, again, matches
the complexity achieved by Demaine et al. for rooted trees, who also showed
that this is optimal if we restrict ourselves to the so-called decomposition
algorithms.
</summary>
    <author>
      <name>Bart≈Çomiej Dudek</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.10062">
    <id>http://arxiv.org/abs/1804.10062v1</id>
    <updated>2018-04-26T13:50:38Z</updated>
    <published>2018-04-26T13:50:38Z</published>
    <title>QuickMergesort: Practically Efficient Constant-Factor Optimal Sorting</title>
    <summary>  We consider the fundamental problem of internally sorting a sequence of $n$
elements. In its best theoretical setting QuickMergesort, a combination
Quicksort with Mergesort with a Median-of-$\sqrt{n}$ pivot selection, requires
at most $n \log n - 1.3999n + o(n)$ element comparisons on the average. The
questions addressed in this paper is how to make this algorithm practical. As
refined pivot selection usually adds much overhead, we show that the
Median-of-3 pivot selection of QuickMergesort leads to at most $n \log n -
0{.}75n + o(n)$ element comparisons on average, while running fast on
elementary data. The experiments show that QuickMergesort outperforms
state-of-the-art library implementations, including C++'s Introsort and Java's
Dual-Pivot Quicksort. Further trade-offs between a low running time and a low
number of comparisons are studied. Moreover, we describe a practically
efficient version with $n \log n + O(n)$ comparisons in the worst case.
</summary>
    <author>
      <name>Stefan Edelkamp</name>
    </author>
    <author>
      <name>Armin Wei√ü</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.09907">
    <id>http://arxiv.org/abs/1804.09907v2</id>
    <updated>2018-05-05T01:44:58Z</updated>
    <published>2018-04-26T06:34:20Z</published>
    <title>On Estimating Edit Distance: Alignment, Dimension Reduction, and
  Embeddings</title>
    <summary>  Edit distance is a fundamental measure of distance between strings and has
been widely studied in computer science. While the problem of estimating edit
distance has been studied extensively, the equally important question of
actually producing an alignment (i.e., the sequence of edits) has received far
less attention. Somewhat surprisingly, we show that any algorithm to estimate
edit distance can be used in a black-box fashion to produce an approximate
alignment of strings, with modest loss in approximation factor and small loss
in run time. Plugging in the result of Andoni, Krauthgamer, and Onak, we obtain
an alignment that is a $(\log n)^{O(1/\varepsilon^2)}$ approximation in time
$\tilde{O}(n^{1 + \varepsilon})$.
  Closely related to the study of approximation algorithms is the study of
metric embeddings for edit distance. We show that min-hash techniques can be
useful in designing edit distance embeddings through three results: (1) An
embedding from Ulam distance (edit distance over permutations) to Hamming space
that matches the best known distortion of $O(\log n)$ and also implicitly
encodes a sequence of edits between the strings; (2) In the case where the edit
distance between the input strings is known to have an upper bound $K$, we show
that embeddings of edit distance into Hamming space with distortion $f(n)$ can
be modified in a black-box fashion to give distortion
$O(f(\operatorname{poly}(K)))$ for a class of periodic-free strings; (3) A
randomized dimension-reduction map with contraction $c$ and asymptotically
optimal expected distortion $O(c)$, improving on the previous $\tilde{O}(c^{1 +
2 / \log \log \log n})$ distortion result of Batu, Ergun, and Sahinalp.
</summary>
    <author>
      <name>Moses Charikar</name>
    </author>
    <author>
      <name>Ofir Geri</name>
    </author>
    <author>
      <name>Michael P. Kim</name>
    </author>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <link href="http://arxiv.org/abs/1804.09907v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.09907v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.00060">
    <id>http://arxiv.org/abs/1805.00060v1</id>
    <updated>2018-04-30T19:00:22Z</updated>
    <published>2018-04-30T19:00:22Z</published>
    <title>On improving the approximation ratio of the r-shortest common
  superstring problem</title>
    <summary>  The Shortest Common Superstring problem (SCS) consists, for a set of strings
S = {s_1,...,s_n}, in finding a minimum length string that contains all s_i,
1&lt;= i &lt;= n, as substrings. While a 2+11/30 approximation ratio algorithm has
recently been published, the general objective is now to break the conceptual
lower bound barrier of 2. This paper is a step ahead in this direction. Here we
focus on a particular instance of the SCS problem, meaning the r-SCS problem,
which requires all input strings to be of the same length, r. Golonev et al.
proved an approximation ratio which is better than the general one for r&lt;= 6.
Here we extend their approach and improve their approximation ratio, which is
now better than the general one for r&lt;= 7, and less than or equal to 2 up to r
= 6.
</summary>
    <author>
      <name>Tristan Braquelaire</name>
    </author>
    <author>
      <name>Marie Gasparoux</name>
    </author>
    <author>
      <name>Mathieu Raffinot</name>
    </author>
    <author>
      <name>Raluca Uricaru</name>
    </author>
    <link href="http://arxiv.org/abs/1805.00060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.01876">
    <id>http://arxiv.org/abs/1805.01876v3</id>
    <updated>2018-05-10T07:19:19Z</updated>
    <published>2018-05-04T17:44:48Z</published>
    <title>Detecting Mutations by eBWT</title>
    <summary>  In this paper we develop a theory describing how the extended Burrows-Wheeler
Transform (eBWT) of a collection of DNA fragments tends to cluster together the
copies of nucleotides sequenced from a genome G. Our theory accurately predicts
how many copies of any nucleotide are expected inside each such cluster, and
how an elegant and precise LCP array based procedure can locate these clusters
in the eBWT. Our findings are very general and can be applied to a wide range
of different problems. In this paper, we consider the case of alignment-free
and reference-free SNPs discovery in multiple collections of reads. We note
that, in accordance with our theoretical results, SNPs are clustered in the
eBWT of the reads collection, and we develop a tool finding SNPs with a simple
scan of the eBWT and LCP arrays. Preliminary results show that our method
requires much less coverage than state-of-the-art tools while drastically
improving precision and sensitivity.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Nadia Pisanti</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">simplified Proposition 4; extended Thm 2 to ambiguous clusters</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.01876v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01876v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.02200">
    <id>http://arxiv.org/abs/1805.02200v2</id>
    <updated>2019-02-16T03:49:13Z</updated>
    <published>2018-05-06T12:31:28Z</published>
    <title>Wormhole: A Fast Ordered Index for In-memory Data Management</title>
    <summary>  In-memory data management systems, such as key-value stores, have become an
essential infrastructure in today's big-data processing and cloud computing.
They rely on efficient index structures to access data. While unordered
indexes, such as hash tables, can perform point search with O(1) time, they
cannot be used in many scenarios where range queries must be supported. Many
ordered indexes, such as B+ tree and skip list, have a O(log N) lookup cost,
where N is number of keys in an index. For an ordered index hosting billions of
keys, it may take more than 30 key-comparisons in a lookup, which is an order
of magnitude more expensive than that on a hash table. With availability of
large memory and fast network in today's data centers, this O(log N) time is
taking a heavy toll on applications that rely on ordered indexes.
  In this paper we introduce a new ordered index structure, named Wormhole,
that takes O(log L) worst-case time for looking up a key with a length of L.
The low cost is achieved by simultaneously leveraging strengths of three
indexing structures, namely hash table, prefix tree, and B+ tree, to
orchestrate a single fast ordered index. Wormhole's range operations can be
performed by a linear scan of a list after an initial lookup. This improvement
of access efficiency does not come at a price of compromised space efficiency.
Instead, Wormhole's index space is comparable to those of B+ tree and skip
list. Experiment results show that Wormhole outperforms skip list, B+ tree,
ART, and Masstree by up to 8.4x, 4.9x, 4.3x, and 6.6x in terms of key lookup
throughput, respectively.
</summary>
    <author>
      <name>Xingbo Wu</name>
    </author>
    <author>
      <name>Fan Ni</name>
    </author>
    <author>
      <name>Song Jiang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1810479.1810540</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1810479.1810540" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages; 18 figures; 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.02200v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02200v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.03158">
    <id>http://arxiv.org/abs/1805.03158v1</id>
    <updated>2018-05-08T16:48:01Z</updated>
    <published>2018-05-08T16:48:01Z</published>
    <title>Round-Hashing for Data Storage: Distributed Servers and External-Memory
  Tables</title>
    <summary>  This paper proposes round-hashing, which is suitable for data storage on
distributed servers and for implementing external-memory tables in which each
lookup retrieves at most a single block of external memory, using a stash. For
data storage, round-hashing is like consistent hashing as it avoids a full
rehashing of the keys when new servers are added. Experiments show that the
speed to serve requests is tenfold or more than the state of the art. In
distributed data storage, this guarantees better throughput for serving
requests and, moreover, greatly reduces decision times for which data should
move to new servers as rescanning data is much faster.
</summary>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Luca Versari</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.03574">
    <id>http://arxiv.org/abs/1805.03574v2</id>
    <updated>2019-01-08T07:56:43Z</updated>
    <published>2018-05-09T15:04:25Z</published>
    <title>Minimum Segmentation for Pan-genomic Founder Reconstruction in Linear
  Time</title>
    <summary>  Given a threshold $L$ and a set $\mathcal{R} = \{R_1, \ldots, R_m\}$ of $m$
haplotype sequences, each having length $n$, the minimum segmentation problem
for founder reconstruction is to partition the sequences into disjoint segments
$\mathcal{R}[i_1{+}1,i_2], \mathcal{R}[i_2{+}1, i_3], \ldots,
\mathcal{R}[i_{r-1}{+}1, i_r]$, where $0 = i_1 &lt; \cdots &lt; i_r = n$ and
$\mathcal{R}[i_{j-1}{+}1, i_j]$ is the set $\{R_1[i_{j-1}{+}1, i_j], \ldots,
R_m[i_{j-1}{+}1, i_j]\}$, such that the length of each segment, $i_j -
i_{j-1}$, is at least $L$ and $K = \max_j\{ |\mathcal{R}[i_{j-1}{+}1, i_j]| \}$
is minimized. The distinct substrings in the segments $\mathcal{R}[i_{j-1}{+}1,
i_j]$ represent founder blocks that can be concatenated to form $K$ founder
sequences representing the original $\mathcal{R}$ such that crossovers happen
only at segment boundaries. We give an optimal $O(mn)$ time algorithm to solve
the problem, improving over earlier $O(mn^2)$. This improvement enables to
exploit the algorithm on a pan-genomic setting of haplotypes being complete
human chromosomes, with a goal of finding a representative set of references
that can be indexed for read alignment and variant calling.
</summary>
    <author>
      <name>Tuukka Norri</name>
    </author>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Veli M√§kinen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.WABI.2018.15</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.WABI.2018.15" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proc. WABI 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.03574v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03574v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.04260">
    <id>http://arxiv.org/abs/1804.04260v1</id>
    <updated>2018-04-12T00:04:05Z</updated>
    <published>2018-04-12T00:04:05Z</published>
    <title>Graph Pattern Matching Preserving Label-Repetition Constraints</title>
    <summary>  Graph pattern matching is a routine process for a wide variety of
applications such as social network analysis. It is typically defined in terms
of subgraph isomorphism which is NP-Complete. To lower its complexity, many
extensions of graph simulation have been proposed which focus on some
topological constraints of pattern graphs that can be preserved in
polynomial-time over data graphs. We discuss in this paper the satisfaction of
a new topological constraint, called Label-Repetition constraint. To the best
of our knowledge, existing polynomial approaches fail to preserve this
constraint, and moreover, one can adopt only subgraph isomorphism for this end
which is cost-prohibitive. We present first a necessary and sufficient
condition that a data subgraph must satisfy to preserve the Label-Repetition
constraints of the pattern graph. Furthermore, we define matching based on a
notion of triple simulation, an extension of graph simulation by considering
the new topological constraint. We show that with this extension, graph pattern
matching can be performed in polynomial-time, by providing such an algorithm.
Our algorithm is sub-quadratic in the size of data graphs only, and quartic in
general. We show that our results can be combined with orthogonal approaches
for more expressive graph pattern matching.
</summary>
    <author>
      <name>Houari Mahfoud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.04239">
    <id>http://arxiv.org/abs/1804.04239v1</id>
    <updated>2018-04-11T21:51:23Z</updated>
    <published>2018-04-11T21:51:23Z</published>
    <title>Graph Sketching Against Adaptive Adversaries Applied to the Minimum
  Degree Algorithm</title>
    <summary>  Motivated by the study of matrix elimination orderings in combinatorial
scientific computing, we utilize graph sketching and local sampling to give a
data structure that provides access to approximate fill degrees of a matrix
undergoing elimination in $O(\text{polylog}(n))$ time per elimination and
query. We then study the problem of using this data structure in the minimum
degree algorithm, which is a widely-used heuristic for producing elimination
orderings for sparse matrices by repeatedly eliminating the vertex with
(approximate) minimum fill degree. This leads to a nearly-linear time algorithm
for generating approximate greedy minimum degree orderings. Despite extensive
studies of algorithms for elimination orderings in combinatorial scientific
computing, our result is the first rigorous incorporation of randomized tools
in this setting, as well as the first nearly-linear time algorithm for
producing elimination orderings with provable approximation guarantees.
  While our sketching data structure readily works in the oblivious adversary
model, by repeatedly querying and greedily updating itself, it enters the
adaptive adversarial model where the underlying sketches become prone to
failure due to dependency issues with their internal randomness. We show how to
use an additional sampling procedure to circumvent this problem and to create
an independent access sequence. Our technique for decorrelating the interleaved
queries and updates to this randomized data structure may be of independent
interest.
</summary>
    <author>
      <name>Matthew Fahrbach</name>
    </author>
    <author>
      <name>Gary L. Miller</name>
    </author>
    <author>
      <name>Richard Peng</name>
    </author>
    <author>
      <name>Saurabh Sawlani</name>
    </author>
    <author>
      <name>Junxing Wang</name>
    </author>
    <author>
      <name>Shen Chen Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">58 pages, 3 figures. This is a substantially revised version of
  arXiv:1711.08446 with an emphasis on the underlying theoretical problems</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.04720">
    <id>http://arxiv.org/abs/1804.04720v1</id>
    <updated>2018-04-12T20:35:06Z</updated>
    <published>2018-04-12T20:35:06Z</published>
    <title>Fast Prefix Search in Little Space, with Applications</title>
    <summary>  It has been shown in the indexing literature that there is an essential
difference between prefix/range searches on the one hand, and predecessor/rank
searches on the other hand, in that the former provably allows faster query
resolution. Traditionally, prefix search is solved by data structures that are
also dictionaries---they actually contain the strings in $S$. For very large
collections stored in slow-access memory, we propose much more compact data
structures that support \emph{weak} prefix searches---they return the ranks of
matching strings provided that \emph{some} string in $S$ starts with the given
prefix. In fact, we show that our most space-efficient data structure is
asymptotically space-optimal. Previously, data structures such as String
B-trees (and more complicated cache-oblivious string data structures) have
implicitly supported weak prefix queries, but they all have query time that
grows logarithmically with the size of the string collection. In contrast, our
data structures are simple, naturally cache-efficient, and have query time that
depends only on the length of the prefix, all the way down to constant query
time for strings that fit in one machine word. We give several applications of
weak prefix searches, including exact prefix counting and approximate counting
of tuples matching conjunctive prefix conditions.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Paolo Boldi</name>
    </author>
    <author>
      <name>Rasmus Pagh</name>
    </author>
    <author>
      <name>Sebastiano Vigna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 18th Annual European Symposium on Algorithms (ESA),
  Liverpool (UK), September 6-8, 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.04777">
    <id>http://arxiv.org/abs/1804.04777v2</id>
    <updated>2019-01-07T01:28:14Z</updated>
    <published>2018-04-13T02:57:19Z</published>
    <title>Optimizing Bloom Filter: Challenges, Solutions, and Comparisons</title>
    <summary>  Bloom filter (BF) has been widely used to support membership query, i.e., to
judge whether a given element x is a member of a given set S or not. Recent
years have seen a flourish design explosion of BF due to its characteristic of
space-efficiency and the functionality of constant-time membership query. The
existing reviews or surveys mainly focus on the applications of BF, but fall
short in covering the current trends, thereby lacking intrinsic understanding
of their design philosophy. To this end, this survey provides an overview of BF
and its variants, with an emphasis on the optimization techniques. Basically,
we survey the existing variants from two dimensions, i.e., performance and
generalization. To improve the performance, dozens of variants devote
themselves to reducing the false positives and implementation costs. Besides,
tens of variants generalize the BF framework in more scenarios by diversifying
the input sets and enriching the output functionalities. To summarize the
existing efforts, we conduct an in-depth study of the existing literature on BF
optimization, covering more than 60 variants. We unearth the design philosophy
of these variants and elaborate how the employed optimization techniques
improve BF. Furthermore, comprehensive analysis and qualitative comparison are
conducted from the perspectives of BF components. Lastly, we highlight the
future trends of designing BFs. This is, to the best of our knowledge, the
first survey that accomplishes such goals.
</summary>
    <author>
      <name>Lailong Luo</name>
    </author>
    <author>
      <name>Deke Guo</name>
    </author>
    <author>
      <name>Richard T. B. Ma</name>
    </author>
    <author>
      <name>Ori Rottenstreich</name>
    </author>
    <author>
      <name>Xueshan Luo</name>
    </author>
    <link href="http://arxiv.org/abs/1804.04777v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04777v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.05776">
    <id>http://arxiv.org/abs/1804.05776v4</id>
    <updated>2018-07-17T04:06:55Z</updated>
    <published>2018-04-16T16:24:13Z</published>
    <title>Deterministic Document Exchange Protocols, and Almost Optimal Binary
  Codes for Edit Errors</title>
    <summary>  We study two basic problems regarding edit error, i.e. document exchange and
error correcting codes for edit errors (insdel codes). For message length $n$
and edit error upper bound $k$, it is known that in both problems the optimal
sketch size or the optimal number of redundant bits is $\Theta(k \log
\frac{n}{k})$. However, known constructions are far from achieving these
bounds.
  We significantly improve previous results on both problems. For document
exchange, we give an efficient deterministic protocol with sketch size
$O(k\log^2 \frac{n}{k})$. This significantly improves the previous best known
deterministic protocol, which has sketch size $O(k^2 + k \log^2 n)$
(Belazzougui15). For binary insdel codes, we obtain the following results:
  1. An explicit binary insdel code which encodes an $n$-bit message $x$
against $k$ errors with redundancy $O(k \log^2 \frac{n}{k})$. In particular
this implies an explicit family of binary insdel codes that can correct
$\varepsilon$ fraction of insertions and deletions with rate $1-O(\varepsilon
\log^2 (\frac{1}{\varepsilon}))=1-\widetilde{O}(\varepsilon)$.
  2. An explicit binary insdel code which encodes an $n$-bit message $x$
against $k$ errors with redundancy $O(k \log n)$. This is the first explicit
construction of binary insdel codes that has optimal redundancy for a wide
range of error parameters $k$, and this brings our understanding of binary
insdel codes much closer to that of standard binary error correcting codes.
  In obtaining our results we introduce the notion of \emph{$\varepsilon$-self
matching hash functions} and \emph{$\varepsilon$-synchronization hash
functions}. We believe our techniques can have further applications in the
literature.
</summary>
    <author>
      <name>Kuan Cheng</name>
    </author>
    <author>
      <name>Zhengzhong Jin</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Ke Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.05776v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05776v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.05420">
    <id>http://arxiv.org/abs/1804.05420v1</id>
    <updated>2018-04-15T20:15:15Z</updated>
    <published>2018-04-15T20:15:15Z</published>
    <title>A Weighted Generalization of the Graham-Diaconis Inequality for Ranked
  List Similarity</title>
    <summary>  The Graham-Diaconis inequality shows the equivalence between two well-known
methods of measuring the similarity of two given ranked lists of items:
Spearman's footrule and Kendall's tau. The original inequality assumes
unweighted items in input lists. In this paper, we first define versions of
these methods for weighted items. We then prove a generalization of the
inequality for the weighted versions.
</summary>
    <author>
      <name>Ali Dasdan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.05420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.05097">
    <id>http://arxiv.org/abs/1804.05097v1</id>
    <updated>2018-04-12T00:23:21Z</updated>
    <published>2018-04-12T00:23:21Z</published>
    <title>Design and Implementation of Dynamic Memory Management in a Reversible
  Object-Oriented Programming Language</title>
    <summary>  The reversible object-oriented programming language (ROOPL) was presented in
late 2016 and proved that object-oriented programming paradigms works in the
reversible setting. The language featured simple statically scoped objects
which made non-trivial programs tedious, if not impossible to write using the
limited tools provided. We introduce an extension to ROOPL in form the new
language ROOPL++, featuring dynamic memory management and fixed-sized arrays
for increased language expressiveness. The language is a superset of ROOPL and
has formally been defined by its language semantics, type system and
computational universality. Considerations for reversible memory manager
layouts are discussed and ultimately lead to the selection of the Buddy Memory
layout. Translations of the extensions added in ROOPL++ to the reversible
assembly language PISA are presented to provide garbage-free computations. The
dynamic memory management extension successfully increases the expressiveness
of ROOPL and as a result, shows that non-trivial reversible data structures,
such as binary trees and doubly-linked lists, are feasible and do not
contradict the reversible computing paradigm.
</summary>
    <author>
      <name>Martin Holm Cservenka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master's Thesis, 231 pages, 63 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.05097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68N15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.2; D.3.3; D.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.05956">
    <id>http://arxiv.org/abs/1804.05956v2</id>
    <updated>2018-07-22T15:13:39Z</updated>
    <published>2018-04-16T21:54:54Z</published>
    <title>k-Maximum Subarrays for Small k: Divide-and-Conquer made simpler</title>
    <summary>  Given an array A of n real numbers, the maximum subarray problem is to find a
contiguous subarray which has the largest sum. The k-maximum subarrays problem
is to find k such subarrays with the largest sums. For the 1-maximum subarray
the well known divide-and-conquer algorithm, presented in most textbooks,
although suboptimal, is easy to implement and can be made optimal with a simple
change that speeds up the combine phase. On the other hand, the only known
divide-and-conquer algorithm for k > 1, that is efficient for small values of
k, is difficult to implement, due to the intricacies of the combine phase. In
this paper we give a divide- and-conquer solution for the k-maximum subarray
problem that simplifies the combine phase considerably while preserving the
overall running time.
  In the process of designing the combine phase of the algorithm we provide a
simple, sublinear, O($k^{1/2} log^3 k$) time algorithm, for finding the k
largest sums of X + Y, where X and Y are sorted arrays of size n and $k &lt;=
n^2$. The k largest sums are implicitly represented, and can be enumerated with
an additional O(k) time. To our knowledge, this is the first sublinear time
algorithm for this well studied problem.
  Unlike previous solutions, that are fairly complicated and sometimes
difficult to implement, ours rely on simple operations such as merging sorted
arrays, binary search, and selecting the $k^{th}$ smallest number in an array.
We have implemented our algorithms and report excellent performance on test
data.
</summary>
    <author>
      <name>Hemant Malik</name>
    </author>
    <author>
      <name>Ovidiu Daescu</name>
    </author>
    <link href="http://arxiv.org/abs/1804.05956v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05956v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.06809">
    <id>http://arxiv.org/abs/1804.06809v1</id>
    <updated>2018-04-18T16:45:46Z</updated>
    <published>2018-04-18T16:45:46Z</published>
    <title>On Abelian Longest Common Factor with and without RLE</title>
    <summary>  We consider the Abelian longest common factor problem in two scenarios: when
input strings are uncompressed and are of size $n$, and when the input strings
are run-length encoded and their compressed representations have size at most
$m$. The alphabet size is denoted by $\sigma$. For the uncompressed problem, we
show an $o(n^2)$-time and $\Oh(n)$-space algorithm in the case of
$\sigma=\Oh(1)$, making a non-trivial use of tabulation. For the RLE-compressed
problem, we show two algorithms: one working in $\Oh(m^2\sigma^2 \log^3 m)$
time and $\Oh(m (\sigma^2+\log^2 m))$ space, which employs line sweep, and one
that works in $\Oh(m^3)$ time and $\Oh(m)$ space that applies in a careful way
a sliding-window-based approach. The latter improves upon the previously known
$\Oh(nm^2)$-time and $\Oh(m^4)$-time algorithms that were recently developed by
Sugimoto et al.\ (IWOCA 2017) and Grabowski (SPIRE 2017), respectively.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to a journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.07575">
    <id>http://arxiv.org/abs/1804.07575v1</id>
    <updated>2018-04-20T12:33:42Z</updated>
    <published>2018-04-20T12:33:42Z</published>
    <title>Optimal Sorting with Persistent Comparison Errors</title>
    <summary>  We consider the problem of sorting $n$ elements in the case of
\emph{persistent} comparison errors. In this model (Braverman and Mossel,
SODA'08), each comparison between two elements can be wrong with some fixed
(small) probability $p$, and \emph{comparisons cannot be repeated}. Sorting
perfectly in this model is impossible, and the objective is to minimize the
\emph{dislocation} of each element in the output sequence, that is, the
difference between its true rank and its position. Existing lower bounds for
this problem show that no algorithm can guarantee, with high probability,
\emph{maximum dislocation} and \emph{total dislocation} better than
$\Omega(\log n)$ and $\Omega(n)$, respectively, regardless of its running time.
  In this paper, we present the first \emph{$O(n\log n)$-time} sorting
algorithm that guarantees both \emph{$O(\log n)$ maximum dislocation} and
\emph{$O(n)$ total dislocation} with high probability. Besides improving over
the previous state-of-the art algorithms -- the best known algorithm had
running time $\tilde{O}(n^{3/2})$ -- our result indicates that comparison
errors do not make the problem computationally more difficult: a sequence with
the best possible dislocation can be obtained in $O(n\log n)$ time and, even
without comparison errors, $\Omega(n\log n)$ time is necessary to guarantee
such dislocation bounds.
  In order to achieve this optimal result, we solve two sub-problems, and the
respective methods have their own merits for further application. One is how to
locate a position in which to insert an element in an almost-sorted sequence
having $O(\log n)$ maximum dislocation in such a way that the dislocation of
the resulting sequence will still be $O(\log n)$. The other is how to
simultaneously insert $m$ elements into an almost sorted sequence of $m$
different elements, such that the resulting sequence of $2m$ elements remains
almost sorted.
</summary>
    <author>
      <name>Barbara Geissmann</name>
    </author>
    <author>
      <name>Stefano Leucci</name>
    </author>
    <author>
      <name>Chih-Hung Liu</name>
    </author>
    <author>
      <name>Paolo Penna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.07575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.11245">
    <id>http://arxiv.org/abs/1803.11245v4</id>
    <updated>2018-11-16T16:35:53Z</updated>
    <published>2018-03-29T20:36:11Z</published>
    <title>Prefix-Free Parsing for Building Big BWTs</title>
    <summary>  High-throughput sequencing technologies have led to explosive growth of
genomic databases; one of which will soon reach hundreds of terabytes. For many
applications we want to build and store indexes of these databases but
constructing such indexes is a challenge. Fortunately, many of these genomic
databases are highly-repetitive---a characteristic that can be exploited to
ease the computation of the Burrows-Wheeler Transform (BWT), which underlies
many popular indexes. In this paper, we introduce a preprocessing algorithm,
referred to as {\em prefix-free parsing}, that takes a text $T$ as input, and
in one-pass generates a dictionary $D$ and a parse $P$ of $T$ with the property
that the BWT of $T$ can be constructed from $D$ and $P$ using workspace
proportional to their total size and $O (|T|)$-time. Our experiments show that
$D$ and $P$ are significantly smaller than $T$ in practice, and thus, can fit
in a reasonable internal memory even when $T$ is very large. In particular, we
show that with prefix-free parsing we can build an 131-megabyte run-length
compressed FM-index (restricted to support only counting and not locating) for
1000 copies of human chromosome 19 in 2 hours using 21 gigabytes of memory,
suggesting that we can build a 6.73 gigabyte index for 1000 complete
human-genome haplotypes in approximately 102 hours using about 1 terabyte of
memory.
</summary>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Alan Kuhnle</name>
    </author>
    <author>
      <name>Ben Langmead</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Taher Mun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version appeared at WABI '18; full version submitted to a
  journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.11245v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.11245v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.11427">
    <id>http://arxiv.org/abs/1803.11427v1</id>
    <updated>2018-03-30T11:59:08Z</updated>
    <published>2018-03-30T11:59:08Z</published>
    <title>On the Diameter of Tree Associahedra</title>
    <summary>  We consider a natural notion of search trees on graphs, which we show is
ubiquitous in various areas of discrete mathematics and computer science.
Search trees on graphs can be modified by local operations called rotations,
which generalize rotations in binary search trees. The rotation graph of search
trees on a graph $G$ is the skeleton of a polytope called the graph
associahedron of $G$.
  We consider the case where the graph $G$ is a tree. We construct a family of
trees $G$ on $n$ vertices and pairs of search trees on $G$ such that the
minimum number of rotations required to transform one search tree into the
other is $\Omega (n\log n)$. This implies that the worst-case diameter of tree
associahedra is $\Theta (n\log n)$, which answers a question from Thibault
Manneville and Vincent Pilaud. The proof relies on a notion of projection of a
search tree which may be of independent interest.
</summary>
    <author>
      <name>Jean Cardinal</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <author>
      <name>Pablo P√©rez-Lantero</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.11427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.11427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.01937">
    <id>http://arxiv.org/abs/1804.01937v1</id>
    <updated>2018-04-05T16:21:33Z</updated>
    <published>2018-04-05T16:21:33Z</published>
    <title>On Undetected Redundancy in the Burrows-Wheeler Transform</title>
    <summary>  The Burrows-Wheeler-Transform (BWT) is an invertible permutation of a text
known to be highly compressible but also useful for sequence analysis, what
makes the BWT highly attractive for lossless data compression. In this paper,
we present a new technique to reduce the size of a BWT using its combinatorial
properties, while keeping it invertible. The technique can be applied to any
BWT-based compressor, and, as experiments show, is able to reduce the encoding
size by 8-16 % on average and up to 33-57 % in the best cases (depending on the
BWT-compressor used), making BWT-based compressors competitive or even superior
to today's best lossless compressors.
</summary>
    <author>
      <name>Uwe Baier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, accepted for Combinatorial Pattern Matching 2018 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.01642">
    <id>http://arxiv.org/abs/1804.01642v2</id>
    <updated>2019-01-04T15:44:25Z</updated>
    <published>2018-04-05T01:13:27Z</published>
    <title>Optimal streaming and tracking distinct elements with high probability</title>
    <summary>  The distinct elements problem is one of the fundamental problems in streaming
algorithms --- given a stream of integers in the range $\{1,\ldots,n\}$, we
wish to provide a $(1+\varepsilon)$ approximation to the number of distinct
elements in the input. After a long line of research an optimal solution for
this problem with constant probability of success, using
$\mathcal{O}(\frac{1}{\varepsilon^2}+\log n)$ bits of space, was given by Kane,
Nelson and Woodruff in 2010.
  The standard approach used in order to achieve low failure probability
$\delta$ is to take the median of $\log \delta^{-1}$ parallel repetitions of
the original algorithm. We show that such a multiplicative space blow-up is
unnecessary: we provide an optimal algorithm using $\mathcal{O}(\frac{\log
\delta^{-1}}{\varepsilon^2} + \log n)$ bits of space --- matching known lower
bounds for this problem. That is, the $\log\delta^{-1}$ factor does not
multiply the $\log n$ term. This settles completely the space complexity of the
distinct elements problem with respect to all standard parameters.
  We consider also the \emph{strong tracking} (or \emph{continuous monitoring})
variant of the distinct elements problem, where we want an algorithm which
provides an approximation of the number of distinct elements seen so far, at
all times of the stream. We show that this variant can be solved using
$\mathcal{O}(\frac{\log \log n + \log \delta^{-1}}{\varepsilon^2} + \log n)$
bits of space, which we show to be optimal.
</summary>
    <author>
      <name>Jaros≈Çaw B≈Çasiok</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version of this paper appeard in SODA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01642v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01642v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.02112">
    <id>http://arxiv.org/abs/1804.02112v1</id>
    <updated>2018-04-06T02:10:25Z</updated>
    <published>2018-04-06T02:10:25Z</published>
    <title>Red-Black Trees with Constant Update Time</title>
    <summary>  We show how a few modifications to the red-black trees allow for $O(1)$
worst-case update time (once the position of the inserted or deleted element is
known). The resulting structure is based on relaxing some of the properties of
the red-black trees while guaranteeing that the height remains logarithmic with
respect to the number of nodes. Compared to the other search trees with
constant update time, our tree is the first to provide a tailored deletion
procedure without using the global rebuilding technique. In addition, our
structure is very simple to implement and allows for a simpler proof of
correctness than those alternative trees.
</summary>
    <author>
      <name>Amr Elmasry</name>
    </author>
    <author>
      <name>Mostafa Kahla</name>
    </author>
    <author>
      <name>Fady Ahdy</name>
    </author>
    <author>
      <name>Mahmoud Hashem</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.02906">
    <id>http://arxiv.org/abs/1804.02906v2</id>
    <updated>2019-01-29T11:54:20Z</updated>
    <published>2018-04-09T10:46:48Z</published>
    <title>From Regular Expression Matching to Parsing</title>
    <summary>  Given a regular expression $R$ and a string $Q$, the regular expression
parsing problem is to determine if $Q$ matches $R$ and if so, determine how it
matches, e.g., by a mapping of the characters of $Q$ to the characters in $R$.
Regular expression parsing makes finding matches of a regular expression even
more useful by allowing us to directly extract subpatterns of the match, e.g.,
for extracting IP-addresses from internet traffic analysis or extracting
subparts of genomes from genetic data bases. We present a new general
techniques for efficiently converting a large class of algorithms that
determine if a string $Q$ matches regular expression $R$ into algorithms that
can construct a corresponding mapping. As a consequence, we obtain the first
efficient linear space solutions for regular expression parsing.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02906v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02906v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.03604">
    <id>http://arxiv.org/abs/1804.03604v3</id>
    <updated>2019-09-25T23:19:16Z</updated>
    <published>2018-04-10T15:54:34Z</published>
    <title>Optimal Document Exchange and New Codes for Insertions and Deletions</title>
    <summary>  We give the first communication-optimal document exchange protocol. For any
$n$ and $k &lt; n$ our randomized scheme takes any $n$-bit file $F$ and computes a
$\Theta(k \log \frac{n}{k})$-bit summary from which one can reconstruct $F$,
with high probability, given a related file $F'$ with edit distance $ED(F,F')
\leq k$.
  The size of our summary is information-theoretically order optimal for all
values of $k$, giving a randomized solution to a longstanding open question of
[Orlitsky; FOCS'91]. It also is the first non-trivial solution for the
interesting setting where a small constant fraction of symbols have been
edited, producing an optimal summary of size $O(H(\delta)n)$ for $k=\delta n$.
This concludes a long series of better-and-better protocols which produce
larger summaries for sub-linear values of $k$ and sub-polynomial failure
probabilities. In particular, the recent break-through of [Belazzougui, Zhang;
FOCS'16] assumes that $k &lt; n^\epsilon$, produces a summary of size $O(k\log^2 k
+ k\log n)$, and succeeds with probability $1-(k \log n)^{-O(1)}$.
  We also give an efficient derandomized document exchange protocol with
summary size $O(k \log^2 \frac{n}{k})$. This improves, for any $k$, over a
deterministic document exchange protocol by Belazzougui with summary size
$O(k^2 + k \log^2 n)$. Our deterministic document exchange directly provides
new efficient systematic error correcting codes for insertions and deletions.
These (binary) codes correct any $\delta$ fraction of adversarial
insertions/deletions while having a rate of $1 - O(\delta \log^2
\frac{1}{\delta})$ and improve over the codes of Guruswami and Li and Haeupler,
Shahrasbi and Vitercik which have rate $1 - \Theta\left(\sqrt{\delta}
\log^{O(1)} \frac{1}{\epsilon}\right)$.
</summary>
    <author>
      <name>Bernhard Haeupler</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03604v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03604v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.03256">
    <id>http://arxiv.org/abs/1804.03256v1</id>
    <updated>2018-04-09T22:08:13Z</updated>
    <published>2018-04-09T22:08:13Z</published>
    <title>Restructuring expression dags for efficient parallelization</title>
    <summary>  In the field of robust geometric computation it is often necessary to make
exact decisions based on inexact floating-point arithmetic. One common approach
is to store the computation history in an arithmetic expression dag and to
re-evaluate the expression with increasing precision until an exact decision
can be made. We show that exact-decisions number types based on expression dags
can be evaluated faster in practice through parallelization on multiple cores.
We compare the impact of several restructuring methods for the expression dag
on its running time in a parallel environment.
</summary>
    <author>
      <name>Martin Wilhelm</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.04263">
    <id>http://arxiv.org/abs/1804.04263v2</id>
    <updated>2018-04-13T17:12:59Z</updated>
    <published>2018-04-12T00:15:17Z</published>
    <title>Dualities in Tree Representations</title>
    <summary>  A characterization of the tree $T^*$ such that
$\mathrm{BP}(T^*)=\overleftrightarrow{\mathrm{DFUDS}(T)}$, the reversal of
$\mathrm{DFUDS}(T)$ is given. An immediate consequence is a rigorous
characterization of the tree $\hat{T}$ such that
$\mathrm{BP}(\hat{T})=\mathrm{DFUDS}(T)$. In summary, $\mathrm{BP}$ and
$\mathrm{DFUDS}$ are unified within an encompassing framework, which might have
the potential to imply future simplifications with regard to queries in
$\mathrm{BP}$ and/or $\mathrm{DFUDS}$. Immediate benefits displayed here are to
identify so far unnoted commonalities in most recent work on the Range Minimum
Query problem, and to provide improvements for the Minimum Length Interval
Query problem.
</summary>
    <author>
      <name>Rayan Chikhi</name>
    </author>
    <author>
      <name>Alexander Sch√∂nhuth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CPM 2018, extended version</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04263v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04263v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.04178">
    <id>http://arxiv.org/abs/1804.04178v2</id>
    <updated>2018-04-25T20:43:21Z</updated>
    <published>2018-04-11T19:20:04Z</published>
    <title>Approximating Edit Distance in Truly Subquadratic Time: Quantum and
  MapReduce</title>
    <summary>  The edit distance between two strings is defined as the smallest number of
insertions, deletions, and substitutions that need to be made to transform one
of the strings to another one. Approximating edit distance in subquadratic time
is "one of the biggest unsolved problems in the field of combinatorial pattern
matching". Our main result is a quantum constant approximation algorithm for
computing the edit distance in truly subquadratic time. More precisely, we give
an $O(n^{1.858})$ quantum algorithm that approximates the edit distance within
a factor of $7$. We further extend this result to an $O(n^{1.781})$ quantum
algorithm that approximates the edit distance within a larger constant factor.
  Our solutions are based on a framework for approximating edit distance in
parallel settings. This framework requires as black box an algorithm that
computes the distances of several smaller strings all at once. For a quantum
algorithm, we reduce the black box to \textit{metric estimation} and provide
efficient algorithms for approximating it. We further show that this framework
enables us to approximate edit distance in distributed settings. To this end,
we provide a MapReduce algorithm to approximate edit distance within a factor
of $3$, with sublinearly many machines and sublinear memory. Also, our
algorithm runs in a logarithmic number of rounds.
</summary>
    <author>
      <name>Mahdi Boroujeni</name>
    </author>
    <author>
      <name>Soheil Ehsani</name>
    </author>
    <author>
      <name>Mohammad Ghodsi</name>
    </author>
    <author>
      <name>MohammadTaghi HajiAghayi</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary version of this paper was presented at SODA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04178v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04178v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q12" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.01723">
    <id>http://arxiv.org/abs/1803.01723v1</id>
    <updated>2018-03-05T15:22:58Z</updated>
    <published>2018-03-05T15:22:58Z</published>
    <title>Optimal Substring-Equality Queries with Applications to Sparse Text
  Indexing</title>
    <summary>  We consider the problem of encoding a string of length $n$ from an alphabet
$[0,\sigma-1]$ so that access and substring-equality queries (that is,
determining the equality of any two substrings) can be answered efficiently. A
clear lower bound on the size of any prefix-free encoding of this kind is
$n\log\sigma + \Theta(\log (n\sigma))$ bits. We describe a new encoding
matching this lower bound when $\sigma\leq n^{O(1)}$ while supporting queries
in optimal $O(1)$-time in the cell-probe model, and show how to extend the
result to the word-RAM model using $\Theta(\log^2n)$ bits of additional space.
Using our new encoding, we obtain the first optimal-space algorithms for
several string-processing problems in the word-RAM model with rewritable input.
In particular, we describe the first in-place algorithm computing the LCP array
in $O(n\log n)$ expected time and the first in-place Monte Carlo solutions to
the sparse suffix sorting, sparse LCP array construction, and suffix selection
problems. Our algorithms are also the first running in sublinear time for small
enough sets of input suffixes. Combining these solutions, we obtain the first
optimal-space and sublinear-time algorithm for building the sparse suffix tree.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1803.01723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1802.10347">
    <id>http://arxiv.org/abs/1802.10347v2</id>
    <updated>2019-11-04T16:02:38Z</updated>
    <published>2018-02-28T10:33:41Z</published>
    <title>Decompressing Lempel-Ziv Compressed Text</title>
    <summary>  We consider the problem of decompressing the Lempel--Ziv 77 representation of
a string $S$ of length $n$ using a working space as close as possible to the
size $z$ of the input. The folklore solution for the problem runs in $O(n)$
time but requires random access to the whole decompressed text. Another
folklore solution is to convert LZ77 into a grammar of size $O(z\log(n/z))$ and
then stream $S$ in linear time. In this paper, we show that $O(n)$ time and
$O(z)$ working space can be achieved for constant-size alphabets. On general
alphabets of size $\sigma$, we describe (i) a trade-off achieving
$O(n\log^\delta \sigma)$ time and $O(z\log^{1-\delta}\sigma)$ space for any
$0\leq \delta\leq 1$, and (ii) a solution achieving $O(n)$ time and
$O(z\log\log (n/z))$ space. The latter solution, in particular, dominates both
folklore algorithms for the problem. Our solutions can, more generally, extract
any specified subsequence of $S$ with little overheads on top of the linear
running time and working space. As an immediate corollary, we show that our
techniques yield improved results for pattern matching problems on
LZ77-compressed text.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Mikko Berggren Ettienne</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1802.10347v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10347v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.01695">
    <id>http://arxiv.org/abs/1803.01695v2</id>
    <updated>2018-04-17T11:53:34Z</updated>
    <published>2018-03-05T14:50:27Z</published>
    <title>String Attractors: Verification and Optimization</title>
    <summary>  String attractors [STOC 2018] are combinatorial objects recently introduced
to unify all known dictionary compression techniques in a single theory. A set
$\Gamma\subseteq [1..n]$ is a $k$-attractor for a string $S\in[1..\sigma]^n$ if
and only if every distinct substring of $S$ of length at most $k$ has an
occurrence straddling at least one of the positions in $\Gamma$. Finding the
smallest $k$-attractor is NP-hard for $k\geq3$, but polylogarithmic
approximations can be found using reductions from dictionary compressors. It is
easy to reduce the $k$-attractor problem to a set-cover instance where string's
positions are interpreted as sets of substrings. The main result of this paper
is a much more powerful reduction based on the truncated suffix tree. Our new
characterization of the problem leads to more efficient algorithms for string
attractors: we show how to check the validity and minimality of a $k$-attractor
in near-optimal time and how to quickly compute exact and approximate
solutions. For example, we prove that a minimum $3$-attractor can be found in
optimal $O(n)$ time when $\sigma\in O(\sqrt[3+\epsilon]{\log n})$ for any
constant $\epsilon>0$, and $2.45$-approximation can be computed in $O(n)$ time
on general alphabets. To conclude, we introduce and study the complexity of the
closely-related sharp-$k$-attractor problem: to find the smallest set of
positions capturing all distinct substrings of length exactly $k$. We show that
the problem is in P for $k=1,2$ and is NP-complete for constant $k\geq 3$.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Alberto Policriti</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Eva Rotenberg</name>
    </author>
    <link href="http://arxiv.org/abs/1803.01695v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01695v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1709.05314">
    <id>http://arxiv.org/abs/1709.05314v2</id>
    <updated>2017-09-19T07:44:08Z</updated>
    <published>2017-09-15T17:09:54Z</published>
    <title>String Attractors</title>
    <summary>  Let $S$ be a string of length $n$. In this paper we introduce the notion of
\emph{string attractor}: a subset of the string's positions $[1,n]$ such that
every distinct substring of $S$ has an occurrence crossing one of the
attractor's elements. We first show that the minimum attractor's size yields
upper-bounds to the string's repetitiveness as measured by its linguistic
complexity and by the length of its longest repeated substring. We then prove
that all known compressors for repetitive strings induce a string attractor
whose size is bounded by their associated repetitiveness measure, and can
therefore be considered as approximations of the smallest one. Using further
reductions, we derive the approximation ratios of these compressors with
respect to the smallest attractor and solve several open problems related to
the asymptotic relations between repetitiveness measures (in particular,
between the the sizes of the Lempel-Ziv factorization, the run-length
Burrows-Wheeler transform, the smallest grammar, and the smallest macro
scheme). These reductions directly provide approximation algorithms for the
smallest string attractor. We then apply string attractors to solve efficiently
a fundamental problem in the field of compressed computation: we present a
universal compressed data structure for text extraction that improves existing
strategies simultaneously for \emph{all} known dictionary compressors and that,
by recent lower bounds, almost matches the optimal running time within the
resulting space. To conclude, we consider generalizations of string attractors
to labeled graphs, show that the attractor problem is NP-complete on trees, and
provide a logarithmic approximation computable in polynomial time.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1709.05314v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05314v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1710.10964">
    <id>http://arxiv.org/abs/1710.10964v4</id>
    <updated>2019-05-28T15:16:16Z</updated>
    <published>2017-10-30T14:24:35Z</published>
    <title>At the Roots of Dictionary Compression: String Attractors</title>
    <summary>  A well-known fact in the field of lossless text compression is that
high-order entropy is a weak model when the input contains long repetitions.
Motivated by this, decades of research have generated myriads of so-called
dictionary compressors: algorithms able to reduce the text's size by exploiting
its repetitiveness. Lempel-Ziv 77 is one of the most successful and well-known
tools of this kind, followed by straight-line programs, run-length
Burrows-Wheeler transform, macro schemes, collage systems, and the compact
directed acyclic word graph. In this paper, we show that these techniques are
different solutions to the same, elegant, combinatorial problem: to find a
small set of positions capturing all text's substrings. We call such a set a
string attractor. We first show reductions between dictionary compressors and
string attractors. This gives the approximation ratios of dictionary
compressors with respect to the smallest string attractor and uncovers new
relations between the output sizes of different compressors. We show that the
$k$-attractor problem: deciding whether a text has a size-$t$ set of positions
capturing substrings of length at most $k$, is NP-complete for $k\geq 3$. We
provide several approximation techniques for the smallest $k$-attractor, show
that the problem is APX-complete for constant $k$, and give strong
inapproximability results. To conclude, we provide matching lower and upper
bounds for the random access problem on string attractors. The upper bound is
proved by showing a data structure supporting queries in optimal time. Our data
structure is universal: by our reductions to string attractors, it supports
random access on any dictionary-compression scheme. In particular, it matches
the lower bound also on LZ77, straight-line programs, collage systems, and
macro schemes, and therefore closes (at once) the random access problem for all
these compressors.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3188745.3188814</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3188745.3188814" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of 50th Annual ACM SIGACT Symposium on the Theory of
  Computing (STOC'18)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10964v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10964v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.08617">
    <id>http://arxiv.org/abs/1803.08617v3</id>
    <updated>2019-05-15T22:35:09Z</updated>
    <published>2018-03-23T00:22:27Z</published>
    <title>Multiversion Concurrency with Bounded Delay and Precise Garbage
  Collection</title>
    <summary>  In this paper we are interested in bounding the number of instructions taken
to process transactions. The main result is a multiversion transactional system
that supports constant delay (extra instructions beyond running in isolation)
for all read-only transactions, delay equal to the number of processes for
writing transactions that are not concurrent with other writers, and
lock-freedom for concurrent writers. The system supports precise garbage
collection in that versions are identified for collection as soon as the last
transaction releases them. As far as we know these are first results that bound
delays for multiple readers and even a single writer. The approach is
particularly useful in situations where read-transactions dominate write
transactions, or where write transactions come in as streams or batches and can
be processed by a single writer (possibly in parallel).
  The approach is based on using functional data structures to support multiple
versions, and an efficient solution to the Version Maintenance (VM) problem for
acquiring, updating and releasing versions. Our solution to the VM problem is
precise, safe and wait-free (PSWF).
  We experimentally validate our approach by applying it to balanced tree data
structures for maintaining ordered maps. We test the transactional system using
multiple algorithms for the VM problem, including our PSWF VM algorithm, and
implementations with weaker guarantees based on epochs, hazard pointers, and
read-copy-update. To evaluate the functional data structure for concurrency and
multi-versioning, we implement batched updates for functional tree structures
and compare the performance with state-of-the-art concurrent data structures
for balanced trees. The experiments indicate our approach works well in
practice over a broad set of criteria.
</summary>
    <author>
      <name>Naama Ben-David</name>
    </author>
    <author>
      <name>Guy E. Blelloch</name>
    </author>
    <author>
      <name>Yihan Sun</name>
    </author>
    <author>
      <name>Yuanhao Wei</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3323165.3323185</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3323165.3323185" rel="related"/>
    <link href="http://arxiv.org/abs/1803.08617v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08617v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.08621">
    <id>http://arxiv.org/abs/1803.08621v2</id>
    <updated>2018-08-06T20:56:08Z</updated>
    <published>2018-03-23T00:48:06Z</published>
    <title>Parallel Range, Segment and Rectangle Queries with Augmented Maps</title>
    <summary>  The range, segment and rectangle query problems are fundamental problems in
computational geometry, and have extensive applications in many domains.
Despite the significant theoretical work on these problems, efficient
implementations can be complicated. We know of very few practical
implementations of the algorithms in parallel, and most implementations do not
have tight theoretical bounds. We focus on simple and efficient parallel
algorithms and implementations for these queries, which have tight worst-case
bound in theory and good parallel performance in practice. We propose to use a
simple framework (the augmented map) to model the problem. Based on the
augmented map interface, we develop both multi-level tree structures and
sweepline algorithms supporting range, segment and rectangle queries in two
dimensions. For the sweepline algorithms, we propose a parallel paradigm and
show corresponding cost bounds. All of our data structures are work-efficient
to build in theory and achieve a low parallel depth. The query time is almost
linear to the output size.
  We have implemented all the data structures described in the paper using a
parallel augmented map library. Based on the library each data structure only
requires about 100 lines of C++ code. We test their performance on large data
sets (up to $10^8$ elements) and a machine with 72-cores (144 hyperthreads).
The parallel construction achieves 32-68x speedup. Speedup numbers on queries
are up to 126-fold. Our sequential implementation outperforms the CGAL library
by at least 2x in both construction and queries. Our sequential implementation
can be slightly slower than the R-tree in the Boost library in some cases
(0.6-2.5x), but has significantly better query performance (1.6-1400x) than
Boost.
</summary>
    <author>
      <name>Yihan Sun</name>
    </author>
    <author>
      <name>Guy E. Blelloch</name>
    </author>
    <link href="http://arxiv.org/abs/1803.08621v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08621v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.09517">
    <id>http://arxiv.org/abs/1803.09517v2</id>
    <updated>2019-10-25T19:03:40Z</updated>
    <published>2018-03-26T11:34:13Z</published>
    <title>On the Approximation Ratio of Ordered Parsings</title>
    <summary>  Shannon's entropy is a clear lower bound for statistical compression. The
situation is not so well understood for dictionary-based compression. A
plausible lower bound is $b$, the least number of phrases of a general
bidirectional parse of a text, where phrases can be copied from anywhere else
in the text. Since computing $b$ is NP-complete, a popular gold standard is
$z$, the number of phrases in the Lempel-Ziv parse of the text, which is the
optimal one when phrases can be copied only from the left. While $z$ can be
computed in linear time with a greedy algorithm, almost nothing has been known
for decades about its approximation ratio with respect to $b$. In this paper we
prove that $z=O(b\log(n/b))$, where $n$ is the text length. We also show that
the bound is tight as a function of $n$, by exhibiting a text family where $z =
\Omega(b\log n)$. Our upper bound is obtained by building a run-length
context-free grammar based on a locally consistent parsing of the text. Our
lower bound is obtained by relating $b$ with $r$, the number of equal-letter
runs in the Burrows-Wheeler transform of the text. We proceed by observing that
Lempel-Ziv is just one particular case of greedy parses, meaning that the
optimal value of $z$ is obtained by scanning the text and maximizing the phrase
length at each step, and of ordered parses, meaning that there is an increasing
order between phrases and their sources. As a new example of ordered greedy
parses, we introduce {\em lexicographical} parses, where phrases can only be
copied from lexicographically smaller text locations. We prove that the size
$v$ of the optimal lexicographical parse is also obtained greedily in $O(n)$
time, that $v=O(b\log(n/b))$, and that there exists a text family where $v =
\Omega(b\log n)$.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Carlos Ochoa</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1803.09517v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09517v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.09675">
    <id>http://arxiv.org/abs/1803.09675v2</id>
    <updated>2018-04-27T14:15:12Z</updated>
    <published>2018-03-26T15:43:50Z</published>
    <title>Extra Space during Initialization of Succinct Data Structures and
  Dynamical Initializable Arrays</title>
    <summary>  Many succinct data structures on the word RAM require precomputed tables to
start operating. Usually, the tables can be constructed in sublinear time. In
this time, most of a data structure is not initialized, i.e., there is plenty
of unused space allocated for the data structure. We present a general
framework to store temporarily extra buffers between the real data so that the
data can be processed immediately, stored first in the buffers, and then moved
into the real data structure after finishing the tables. As an application, we
apply our framework to Dodis, Patrascu, and Thorup's data structure (STOC 2010)
that emulates c-ary memory and to Farzan and Munro's succinct encoding of
arbitrary graphs (TCS 2013). We also use our framework to present an in-place
dynamical initializable array.
</summary>
    <author>
      <name>Frank Kammer</name>
    </author>
    <author>
      <name>Andrej Sajenko</name>
    </author>
    <link href="http://arxiv.org/abs/1803.09675v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09675v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.09520">
    <id>http://arxiv.org/abs/1803.09520v3</id>
    <updated>2018-09-06T08:08:17Z</updated>
    <published>2018-03-26T11:39:19Z</published>
    <title>Universal Compressed Text Indexing</title>
    <summary>  The rise of repetitive datasets has lately generated a lot of interest in
compressed self-indexes based on dictionary compression, a rich and
heterogeneous family that exploits text repetitions in different ways. For each
such compression scheme, several different indexing solutions have been
proposed in the last two decades. To date, the fastest indexes for repetitive
texts are based on the run-length compressed Burrows-Wheeler transform and on
the Compact Directed Acyclic Word Graph. The most space-efficient indexes, on
the other hand, are based on the Lempel-Ziv parsing and on grammar compression.
Indexes for more universal schemes such as collage systems and macro schemes
have not yet been proposed. Very recently, Kempa and Prezza [STOC 2018] showed
that all dictionary compressors can be interpreted as approximation algorithms
for the smallest string attractor, that is, a set of text positions capturing
all distinct substrings. Starting from this observation, in this paper we
develop the first universal compressed self-index, that is, the first indexing
data structure based on string attractors, which can therefore be built on top
of any dictionary-compressed text representation. Let $\gamma$ be the size of a
string attractor for a text of length $n$. Our index takes
$O(\gamma\log(n/\gamma))$ words of space and supports locating the $occ$
occurrences of any pattern of length $m$ in $O(m\log n + occ\log^{\epsilon}n)$
time, for any constant $\epsilon>0$. This is, in particular, the first index
for general macro schemes and collage systems. Our result shows that the
relation between indexing and compression is much deeper than what was
previously thought: the simple property standing at the core of all dictionary
compressors is sufficient to support fast indexed queries.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed with reviewer's comments</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.09520v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09520v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.04282">
    <id>http://arxiv.org/abs/1803.04282v4</id>
    <updated>2019-01-29T10:22:12Z</updated>
    <published>2018-03-12T14:29:54Z</published>
    <title>Linear-Time In-Place DFS and BFS on the Word RAM</title>
    <summary>  We present an in-place depth first search (DFS) and an in-place breadth first
search (BFS) that runs on a word RAM in linear time such that, if the adjacency
arrays of the input graph are given in a sorted order, the input is restored
after running the algorithm. To obtain our results we use properties of the
representation used to store the given graph and show several linear-time
in-place graph transformations from one representation into another.
</summary>
    <author>
      <name>Frank Kammer</name>
    </author>
    <author>
      <name>Andrej Sajenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Short version of this paper is accepted to CIAC2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.04282v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04282v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.05948">
    <id>http://arxiv.org/abs/1803.05948v2</id>
    <updated>2018-05-17T17:34:25Z</updated>
    <published>2018-03-15T18:58:15Z</published>
    <title>Average Cost of QuickXsort with Pivot Sampling</title>
    <summary>  QuickXsort is a strategy to combine Quicksort with another sorting method X,
so that the result has essentially the same comparison cost as X in isolation,
but sorts in place even when X requires a linear-size buffer. We solve the
recurrence for QuickXsort precisely up to the linear term including the
optimization to choose pivots from a sample of k elements. This allows to
immediately obtain overall average costs using only the average costs of
sorting method X (as if run in isolation). We thereby extend and greatly
simplify the analysis of QuickHeapsort and QuickMergesort with practically
efficient pivot selection, and give the first tight upper bounds including the
linear term for such methods.
</summary>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/lipics.aofa.2018.36</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/lipics.aofa.2018.36" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">updated to final version accepted for AofA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.05948v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05948v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.07199">
    <id>http://arxiv.org/abs/1803.07199v2</id>
    <updated>2018-04-13T16:11:40Z</updated>
    <published>2018-03-20T00:04:27Z</published>
    <title>Twelve Simple Algorithms to Compute Fibonacci Numbers</title>
    <summary>  The Fibonacci numbers are a sequence of integers in which every number after
the first two, 0 and 1, is the sum of the two preceding numbers. These numbers
are well known and algorithms to compute them are so easy that they are often
used in introductory algorithms courses. In this paper, we present twelve of
these well-known algorithms and some of their properties. These algorithms,
though very simple, illustrate multiple concepts from the algorithms field, so
we highlight them. We also present the results of a small-scale experimental
comparison of their runtimes on a personal laptop. Finally, we provide a list
of homework questions for the students. We hope that this paper can serve as a
useful resource for the students learning the basics of algorithms.
</summary>
    <author>
      <name>Ali Dasdan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 29 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.07199v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07199v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.01362">
    <id>http://arxiv.org/abs/1803.01362v1</id>
    <updated>2018-03-04T14:46:32Z</updated>
    <published>2018-03-04T14:46:32Z</published>
    <title>Two-Dimensional Block Trees</title>
    <summary>  The Block Tree (BT) is a novel compact data structure designed to compress
sequence collections. It obtains compression ratios close to Lempel-Ziv and
supports efficient direct access to any substring. The BT divides the text
recursively into fixed-size blocks and those appearing earlier are represented
with pointers. On repetitive collections, a few blocks can represent all the
others, and thus the BT reduces the size by orders of magnitude. In this paper
we extend the BT to two dimensions, to exploit repetitiveness in collections of
images, graphs, and maps. This two-dimensional Block Tree divides the image
regularly into subimages and replaces some of them by pointers to other
occurrences thereof. We develop a specific variant aimed at compressing the
adjacency matrices of Web graphs, obtaining space reductions of up to 50\%
compared with the $k^2$-tree, which is the best alternative supporting direct
and reverse navigation in the graph.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Adri√°n G√≥mez-Brand√≥n</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.01362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.03033">
    <id>http://arxiv.org/abs/2004.03033v2</id>
    <updated>2020-04-15T17:26:17Z</updated>
    <published>2020-04-06T22:54:58Z</published>
    <title>SOPanG 2: online searching over a pan-genome without false positives</title>
    <summary>  Motivation: The pan-genome can be stored as elastic-degenerate (ED) string, a
recently introduced compact representation of multiple overlapping sequences.
However, a search over the ED string does not indicate which individuals (if
any) match the entire query.
  Results: We augment the ED string with sources (individuals' indexes) and
propose an extension of the SOPanG (Shift-Or for Pan-Genome) tool to report
only true positive matches, omitting those not occurring in any of the
haplotypes. The additional stage for checking the matches yields a penalty of
less than 3.5% relative speed in practice, which means that SOPanG 2 is able to
report pattern matches in a pan-genome, mapping them onto individuals, at the
single-thread throughput of above 430 MB/s on real data.
  Availability and implementation: SOPanG 2 can be downloaded here:
github.com/MrAlexSee/sopang
</summary>
    <author>
      <name>Aleksander Cis≈Çak</name>
    </author>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <link href="http://arxiv.org/abs/2004.03033v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.03033v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.03206">
    <id>http://arxiv.org/abs/2004.03206v1</id>
    <updated>2020-04-07T08:48:41Z</updated>
    <published>2020-04-07T08:48:41Z</published>
    <title>Zipping Segment Trees</title>
    <summary>  Stabbing queries in sets of intervals are usually answered using segment
trees. A dynamic variant of segment trees has been presented by van Kreveld and
Overmars, which uses red-black trees to do rebalancing operations. This paper
presents zipping segment trees - dynamic segment trees based on zip trees,
which were recently introduced by Tarjan et al. To facilitate zipping segment
trees, we show how to uphold certain segment tree properties during the
operations of a zip tree. We present an in-depth experimental evaluation and
comparison of dynamic segment trees based on red-black trees, weight-balanced
trees and several variants of the novel zipping segment trees. Our results
indicate that zipping segment trees perform better than rotation-based
alternatives.
</summary>
    <author>
      <name>Lukas Barth</name>
    </author>
    <author>
      <name>Dorothea Wagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at SEA 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.03206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.03206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.02781">
    <id>http://arxiv.org/abs/2004.02781v1</id>
    <updated>2020-04-06T16:16:26Z</updated>
    <published>2020-04-06T16:16:26Z</published>
    <title>Indexing Highly Repetitive String Collections</title>
    <summary>  Two decades ago, a breakthrough in indexing string collections made it
possible to represent them within their compressed space while at the same time
offering indexed search functionalities. As this new technology permeated
through applications like bioinformatics, the string collections experienced a
growth that outperforms Moore's Law and challenges our ability of handling them
even in compressed form. It turns out, fortunately, that many of these rapidly
growing string collections are highly repetitive, so that their information
content is orders of magnitude lower than their plain size. The statistical
compression methods used for classical collections, however, are blind to this
repetitiveness, and therefore a new set of techniques has been developed in
order to properly exploit it. The resulting indexes form a new generation of
data structures able to handle the huge repetitive string collections that we
are facing. In this survey we cover the algorithmic developments that have led
to these data structures. We describe the distinct compression paradigms that
have been used to exploit repetitiveness, the fundamental algorithmic ideas
that form the base of all the existing indexes, and the various structures that
have been proposed, comparing them both in theoretical and practical aspects.
We conclude with the current challenges in this fascinating field.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/2004.02781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.02781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.01493">
    <id>http://arxiv.org/abs/2004.01493v3</id>
    <updated>2020-05-15T08:32:48Z</updated>
    <published>2020-04-03T12:12:01Z</published>
    <title>Enumeration of LCP values, LCP intervals and Maximal repeats in BWT-runs
  Bounded Space</title>
    <summary>  Lcp-values, lcp-intervals, and maximal repeats are powerful tools in various
string processing tasks and have a wide variety of applications. Although many
researchers have focused on developing enumeration algorithms for them, those
algorithms are inefficient in that the space usage is proportional to the
length of the input string. Recently, the run-length-encoded Burrows-Wheeler
transform (RLBWT) has attracted increased attention in string processing, and
various algorithms on the RLBWT have been developed. Developing enumeration
algorithms for lcp-intervals, lcp-values, and maximal repeats on the RLBWT,
however, remains a challenge. In this paper, we present the first such
enumeration algorithms with space usage not proportional to the string length.
The complexities of our enumeration algorithms are $O(n \log \log (n/r))$ time
and $O(r)$ words of working space for string length $n$ and RLBWT size $r$.
</summary>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/2004.01493v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.01493v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.01032">
    <id>http://arxiv.org/abs/2004.01032v1</id>
    <updated>2020-04-01T14:00:54Z</updated>
    <published>2020-04-01T14:00:54Z</published>
    <title>Grammar-Compressed Indexes with Logarithmic Search Time</title>
    <summary>  Let a text $T[1..n]$ be the only string generated by a context-free grammar
with $g$ (terminal and nonterminal) symbols, and of size $G$ (measured as the
sum of the lengths of the right-hand sides of the rules). Such a grammar,
called a grammar-compressed representation of $T$, can be encoded using
essentially $G\lg g$ bits. We introduce the first grammar-compressed index that
uses $O(G\lg n)$ bits and can find the $occ$ occurrences of patterns $P[1..m]$
in time $O((m^2+occ)\lg G)$. We implement the index and demonstrate its
practicality in comparison with the state of the art, on highly repetitive text
collections.
</summary>
    <author>
      <name>Francisco Claude</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Alejandro Pacheco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1110.4493</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.01032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.01032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.01120">
    <id>http://arxiv.org/abs/2004.01120v3</id>
    <updated>2020-04-11T08:45:01Z</updated>
    <published>2020-04-02T16:43:21Z</published>
    <title>On Locating Paths in Compressed Cardinal Trees</title>
    <summary>  A compressed index is a data structure representing a text within compressed
space and supporting fast count and locate queries: count/return all positions
where a pattern occurs. The first compressed indexes operate within a space
bounded by the text's entropy. Entropy, however, is insensitive to long
repetitions. For this reason, in recent years more powerful compressed indexes
have emerged; these are based on the Lempel-Ziv factorization, the run-length
BWT, context-free grammars and, more recently, string attractors. Labeled trees
add a whole new dimension to the problem: one needs not only to compress the
labels, but also the tree's topology. On this side, less is known. Jacobson
showed how to represent the topology of a tree with n nodes in 2n+o(n) bits of
space (succinct) while also supporting constant-time navigation queries.
Ferragina et al. presented the first entropy-compressed labeled tree
representation (the XBWT) able to count, but not locate, paths labeled with a
given pattern. Grammars and the Lempel-Ziv factorization have been extended to
trees, but those representations do not support indexing queries. In this
paper, we show for the first time how to support the powerful locate queries on
compressed trees. We start by proposing suitable generalizations of run-length
BWT, high-order entropy, and string attractors to cardinal trees (tries). We
show that the number r $\leq$ n of XBWT-runs upper-bounds the size of the
smallest tree attractor and lower-bounds the trie's high-order worst-case
entropy H. We finally present the first trie index able to locate in pre-order
nodes reached by a path labeled with a given pattern. Our index locates path
occurrences in constant time each and takes 2n + o(n) + O(r log n) $\leq$ 2n +
o(n) + O(H log n) bits of space: the reporting time is optimal and the locate
machinery fits within compressed space on top of the succinct topology.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Improved toehold lemma running time; added more detailed proofs that
  take care of all border cases in the locate strategy</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.01120v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.01120v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.01156">
    <id>http://arxiv.org/abs/2004.01156v1</id>
    <updated>2020-04-02T17:26:50Z</updated>
    <published>2020-04-02T17:26:50Z</published>
    <title>No Repetition: Fast Streaming with Highly Concentrated Hashing</title>
    <summary>  To get estimators that work within a certain error bound with high
probability, a common strategy is to design one that works with constant
probability, and then boost the probability using independent repetitions.
Important examples of this approach are small space algorithms for estimating
the number of distinct elements in a stream, or estimating the set similarity
between large sets. Using standard strongly universal hashing to process each
element, we get a sketch based estimator where the probability of a too large
error is, say, 1/4. By performing $r$ independent repetitions and taking the
median of the estimators, the error probability falls exponentially in $r$.
However, running $r$ independent experiments increases the processing time by a
factor $r$.
  Here we make the point that if we have a hash function with strong
concentration bounds, then we get the same high probability bounds without any
need for repetitions. Instead of $r$ independent sketches, we have a single
sketch that is $r$ times bigger, so the total space is the same. However, we
only apply a single hash function, so we save a factor $r$ in time, and the
overall algorithms just get simpler.
  Fast practical hash functions with strong concentration bounds were recently
proposed by Aamand em et al. (to appear in STOC 2020). Using their hashing
schemes, the algorithms thus become very fast and practical, suitable for
online processing of high volume data streams.
</summary>
    <author>
      <name>Anders Aamand</name>
    </author>
    <author>
      <name>Debarati Das</name>
    </author>
    <author>
      <name>Evangelos Kipouridis</name>
    </author>
    <author>
      <name>Jakob B. T. Knudsen</name>
    </author>
    <author>
      <name>Peter M. R. Rasmussen</name>
    </author>
    <author>
      <name>Mikkel Thorup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.01156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.01156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.13589">
    <id>http://arxiv.org/abs/2003.13589v1</id>
    <updated>2020-03-30T16:07:07Z</updated>
    <published>2020-03-30T16:07:07Z</published>
    <title>A Faster Subquadratic Algorithm for the Longest Common Increasing
  Subsequence Problem</title>
    <summary>  The Longest Common Increasing Subsequence (LCIS) is a variant of the
classical Longest Common Subsequence (LCS), in which we additionally require
the common subsequence to be strictly increasing. While the well-known "Four
Russians" technique can be used to find LCS in subquadratic time, it does not
seem applicable to LCIS. Recently, Duraj [STACS 2020] used a completely
different method based on the combinatorial properties of LCIS to design an
$\mathcal{O}(n^2(\log\log n)^2/\log^{1/6}n)$ time algorithm. We show that an
approach based on exploiting tabulation can be used to construct an
asymptotically faster $\mathcal{O}(n^2 \log\log n/\sqrt{\log n})$ time
algorithm. As our solution avoids using the specific combinatorial properties
of LCIS, it can be also adapted for the Longest Common Weakly Increasing
Subsequence (LCWIS).
</summary>
    <author>
      <name>Anadi Agrawal</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <link href="http://arxiv.org/abs/2003.13589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.13589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.11604">
    <id>http://arxiv.org/abs/2003.11604v1</id>
    <updated>2020-03-25T20:04:20Z</updated>
    <published>2020-03-25T20:04:20Z</published>
    <title>Further Results on Colored Range Searching</title>
    <summary>  We present a number of new results about range searching for colored (or
"categorical") data:
  1. For a set of $n$ colored points in three dimensions, we describe
randomized data structures with $O(n\mathop{\rm polylog}n)$ space that can
report the distinct colors in any query orthogonal range (axis-aligned box) in
$O(k\mathop{\rm polyloglog} n)$ expected time, where $k$ is the number of
distinct colors in the range, assuming that coordinates are in
$\{1,\ldots,n\}$. Previous data structures require $O(\frac{\log n}{\log\log n}
+ k)$ query time. Our result also implies improvements in higher constant
dimensions.
  2. Our data structures can be adapted to halfspace ranges in three dimensions
(or circular ranges in two dimensions), achieving $O(k\log n)$ expected query
time. Previous data structures require $O(k\log^2n)$ query time.
  3. For a set of $n$ colored points in two dimensions, we describe a data
structure with $O(n\mathop{\rm polylog}n)$ space that can answer colored
"type-2" range counting queries: report the number of occurrences of every
distinct color in a query orthogonal range. The query time is $O(\frac{\log
n}{\log\log n} + k\log\log n)$, where $k$ is the number of distinct colors in
the range. Naively performing $k$ uncolored range counting queries would
require $O(k\frac{\log n}{\log\log n})$ time.
  Our data structures are designed using a variety of techniques, including
colored variants of randomized incremental construction (which may be of
independent interest), colored variants of shallow cuttings, and bit-packing
tricks.
</summary>
    <author>
      <name>Timothy M. Chan</name>
    </author>
    <author>
      <name>Qizheng He</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">full version of a SoCG'20 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.11604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.11835">
    <id>http://arxiv.org/abs/2003.11835v1</id>
    <updated>2020-03-26T11:09:39Z</updated>
    <published>2020-03-26T11:09:39Z</published>
    <title>Succinct Dynamic Ordered Sets with Random Access</title>
    <summary>  The representation of a dynamic ordered set of $n$ integer keys drawn from a
universe of size $m$ is a fundamental data structuring problem. Many solutions
to this problem achieve optimal time but take polynomial space, therefore
preserving time optimality in the \emph{compressed} space regime is the problem
we address in this work. For a polynomial universe $m = n^{\Theta(1)}$, we give
a solution that takes $\textsf{EF}(n,m) + o(n)$ bits, where $\textsf{EF}(n,m)
\leq n\lceil \log_2(m/n)\rceil + 2n$ is the cost in bits of the
\emph{Elias-Fano} representation of the set, and supports random access to the
$i$-th smallest element in $O(\log n/ \log\log n)$ time, updates and
predecessor search in $O(\log\log n)$ time. These time bounds are optimal.
</summary>
    <author>
      <name>Giulio Ermanno Pibiri</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <link href="http://arxiv.org/abs/2003.11835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.12590">
    <id>http://arxiv.org/abs/2004.12590v1</id>
    <updated>2020-04-27T05:52:58Z</updated>
    <published>2020-04-27T05:52:58Z</published>
    <title>In-Place Bijective Burrows-Wheeler Transforms</title>
    <summary>  One of the most well-known variants of the Burrows-Wheeler transform (BWT)
[Burrows and Wheeler, 1994] is the bijective BWT (BBWT) [Gil and Scott, arXiv
2012], which applies the extended BWT (EBWT) [Mantaci et al., TCS 2007] to the
multiset of Lyndon factors of a given text. Since the EBWT is invertible, the
BBWT is a bijective transform in the sense that the inverse image of the EBWT
restores this multiset of Lyndon factors such that the original text can be
obtained by sorting these factors in non-increasing order. In this paper, we
present algorithms constructing or inverting the BBWT in-place using quadratic
time. We also present conversions from the BBWT to the BWT, or vice versa,
either (a) in-place using quadratic time, or (b) in the run-length compressed
setting using $O(n \lg r / \lg \lg r)$ time with $O(r \lg n)$ bits of words,
where $r$ is the sum of character runs in the BWT and the BBWT.
</summary>
    <author>
      <name>Dominik K√∂ppl</name>
    </author>
    <author>
      <name>Daiki Hashimoto</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.CPM.2020.23</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.CPM.2020.23" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In proceedings of CPM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.12590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.08350">
    <id>http://arxiv.org/abs/2004.08350v1</id>
    <updated>2020-04-17T17:13:13Z</updated>
    <published>2020-04-17T17:13:13Z</published>
    <title>Faster Approximate Pattern Matching: A Unified Approach</title>
    <summary>  Approximate pattern matching is a natural and well-studied problem on
strings: Given a text $T$, a pattern $P$, and a threshold $k$, find (the
starting positions of) all substrings of $T$ that are at distance at most $k$
from $P$. We consider the two most fundamental string metrics: the Hamming
distance and the edit distance. Under the Hamming distance, we search for
substrings of $T$ that have at most $k$ mismatches with $P$, while under the
edit distance, we search for substrings of $T$ that can be transformed to $P$
with at most $k$ edits.
  Exact occurrences of $P$ in $T$ have a very simple structure: If we assume
for simplicity that $|T| \le 3|P|/2$ and trim $T$ so that $P$ occurs both as a
prefix and as a suffix of $T$, then both $P$ and $T$ are periodic with a common
period. However, an analogous characterization for the structure of occurrences
with up to $k$ mismatches was proved only recently by Bringmann et al.
[SODA'19]: Either there are $O(k^2)$ $k$-mismatch occurrences of $P$ in $T$, or
both $P$ and $T$ are at Hamming distance $O(k)$ from strings with a common
period $O(m/k)$. We tighten this characterization by showing that there are
$O(k)$ $k$-mismatch occurrences in the case when the pattern is not
(approximately) periodic, and we lift it to the edit distance setting, where we
tightly bound the number of $k$-edit occurrences by $O(k^2)$ in the
non-periodic case. Our proofs are constructive and let us obtain a unified
framework for approximate pattern matching for both considered distances. We
showcase the generality of our framework with results for the fully-compressed
setting (where $T$ and $P$ are given as a straight-line program) and for the
dynamic setting (where we extend a data structure of Gawrychowski et al.
[SODA'18]).
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Philip Wellnitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">74 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.08350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.08350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.09051">
    <id>http://arxiv.org/abs/2004.09051v1</id>
    <updated>2020-04-20T04:53:27Z</updated>
    <published>2020-04-20T04:53:27Z</published>
    <title>Black-White Array: A New Data Structure for Dynamic Data Sets</title>
    <summary>  A new array based data structure named black-white array (BWA) is introduced
as an effective and efficient alternative to the list or tree based data
structures for dynamic data set. It consists of two sub-arrays, one white and
one black of half of the size of the white. Both of them are conceptually
partitioned into segments of different ranks with the sizes grow in geometric
sequence. The layout of BWA allows easy calculation of the meta-data about the
segments, which are used extensively in the algorithms for the basic operations
of the dynamic sets. The insertion of a sequence of unordered numbers into BWA
takes amortized time logarithmic to the length of the sequence. It is also
proven that when the searched or deleted value is present in the BWA, the
asymptotic amortized cost for the operations is O(log(n)); otherwise, the time
will fall somewhere between O(log(n)) and O(log^2(n)). It is shown that the
state variable total, which records the number of values in the BWA captures
the dynamics of state transition of BWA. This fact is exploited to produce
concise, easy- to-understand, and efficient coding for the operations. As it
uses arrays as the underlying structure for dynamic set, a BWA need neither the
space to store the pointers referencing other data nodes nor the time to chase
the pointers as with any linked data structures. A C++ implementation of the
BWA is completed. The performance data were gathered and plotted, which
confirmed the theoretic analysis. The testing results showed that the amortized
time for the insert, search, and delete operations is all just between 105.949
and 5720.49 nanoseconds for BWAs of sizes ranging from 210 to 229 under various
conditions.
</summary>
    <author>
      <name>Z. George Mou</name>
    </author>
    <link href="http://arxiv.org/abs/2004.09051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.09051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.05345">
    <id>http://arxiv.org/abs/2004.05345v1</id>
    <updated>2020-04-11T09:24:51Z</updated>
    <published>2020-04-11T09:24:51Z</published>
    <title>Locality-Sensitive Hashing Scheme based on Longest Circular Co-Substring</title>
    <summary>  Locality-Sensitive Hashing (LSH) is one of the most popular methods for
$c$-Approximate Nearest Neighbor Search ($c$-ANNS) in high-dimensional spaces.
In this paper, we propose a novel LSH scheme based on the Longest Circular
Co-Substring (LCCS) search framework (LCCS-LSH) with a theoretical guarantee.
We introduce a novel concept of LCCS and a new data structure named Circular
Shift Array (CSA) for $k$-LCCS search. The insight of LCCS search framework is
that close data objects will have a longer LCCS than the far-apart ones with
high probability. LCCS-LSH is \emph{LSH-family-independent}, and it supports
$c$-ANNS with different kinds of distance metrics. We also introduce a
multi-probe version of LCCS-LSH and conduct extensive experiments over five
real-life datasets. The experimental results demonstrate that LCCS-LSH
outperforms state-of-the-art LSH schemes.
</summary>
    <author>
      <name>Yifan Lei</name>
    </author>
    <author>
      <name>Qiang Huang</name>
    </author>
    <author>
      <name>Mohan Kankanhalli</name>
    </author>
    <author>
      <name>Anthony K. H. Tung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.05345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.05345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.05738">
    <id>http://arxiv.org/abs/2004.05738v1</id>
    <updated>2020-04-13T01:20:51Z</updated>
    <published>2020-04-13T01:20:51Z</published>
    <title>Lower Bound for Succinct Range Minimum Query</title>
    <summary>  Given an integer array $A[1..n]$, the Range Minimum Query problem (RMQ) asks
to preprocess $A$ into a data structure, supporting RMQ queries: given $a,b\in
[1,n]$, return the index $i\in[a,b]$ that minimizes $A[i]$, i.e.,
$\mathrm{argmin}_{i\in[a,b]} A[i]$. This problem has a classic solution using
$O(n)$ space and $O(1)$ query time by Gabow, Bentley, Tarjan (STOC, 1984) and
Harel, Tarjan (SICOMP, 1984). The best known data structure by Fischer, Heun
(SICOMP, 2011) and Navarro, Sadakane (TALG, 2014) uses $2n+n/(\frac{\log
n}{t})^t+\tilde{O}(n^{3/4})$ bits and answers queries in $O(t)$ time, assuming
the word-size is $w=\Theta(\log n)$. In particular, it uses
$2n+n/\mathrm{poly}\log n$ bits of space as long as the query time is a
constant.
  In this paper, we prove the first lower bound for this problem, showing that
$2n+n/\mathrm{poly}\log n$ space is necessary for constant query time. In
general, we show that if the data structure has query time $O(t)$, then it must
use at least $2n+n/(\log n)^{\tilde{O}(t^2)}$ space, in the cell-probe model
with word-size $w=\Theta(\log n)$.
</summary>
    <author>
      <name>Mingmou Liu</name>
    </author>
    <author>
      <name>Huacheng Yu</name>
    </author>
    <link href="http://arxiv.org/abs/2004.05738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.05738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68P30, 68Q17" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.4; F.1.3; F.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.05309">
    <id>http://arxiv.org/abs/2004.05309v2</id>
    <updated>2020-04-27T06:14:12Z</updated>
    <published>2020-04-11T05:27:31Z</published>
    <title>Grammar-compressed Self-index with Lyndon Words</title>
    <summary>  We introduce a new class of straight-line programs (SLPs), named the Lyndon
SLP, inspired by the Lyndon trees (Barcelo, 1990). Based on this SLP, we
propose a self-index data structure of $O(g)$ words of space that can be built
from a string $T$ in $O(n \lg n)$ expected time, retrieving the starting
positions of all occurrences of a pattern $P$ of length $m$ in $O(m + \lg m \lg
n + occ \lg g)$ time, where $n$ is the length of $T$, $g$ is the size of the
Lyndon SLP for $T$, and $occ$ is the number of occurrences of $P$ in $T$.
</summary>
    <author>
      <name>Kazuya Tsuruta</name>
    </author>
    <author>
      <name>Dominik K√∂ppl</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2004.05309v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.05309v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.06474">
    <id>http://arxiv.org/abs/2004.06474v1</id>
    <updated>2020-04-09T20:00:12Z</updated>
    <published>2020-04-09T20:00:12Z</published>
    <title>Two halves of a meaningful text are statistically different</title>
    <summary>  Which statistical features distinguish a meaningful text (possibly written in
an unknown system) from a meaningless set of symbols? Here we answer this
question by comparing features of the first half of a text to its second half.
This comparison can uncover hidden effects, because the halves have the same
values of many parameters (style, genre {\it etc}). We found that the first
half has more different words and more rare words than the second half. Also,
words in the first half are distributed less homogeneously over the text in the
sense of of the difference between the frequency and the inverse spatial
period. These differences hold for the significant majority of several hundred
relatively short texts we studied. The statistical significance is confirmed
via the Wilcoxon test. Differences disappear after random permutation of words
that destroys the linear structure of the text. The differences reveal a
temporal asymmetry in meaningful texts, which is confirmed by showing that
texts are much better compressible in their natural way (i.e. along the
narrative) than in the word-inverted form. We conjecture that these results
connect the semantic organization of a text (defined by the flow of its
narrative) to its statistical features.
</summary>
    <author>
      <name>Weibing Deng</name>
    </author>
    <author>
      <name>R. Xie</name>
    </author>
    <author>
      <name>S. Deng</name>
    </author>
    <author>
      <name>Armen E. Allahverdyan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages and 14 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.06474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.06474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.04858">
    <id>http://arxiv.org/abs/2004.04858v1</id>
    <updated>2020-04-09T23:51:23Z</updated>
    <published>2020-04-09T23:51:23Z</published>
    <title>Pattern Discovery in Colored Strings</title>
    <summary>  We consider the problem of identifying patterns of interest in colored
strings. A colored string is a string in which each position is colored with
one of a finite set of colors. Our task is to find substrings that always occur
followed by the same color at the same distance. The problem is motivated by
applications in embedded systems verification, in particular, assertion mining.
The goal there is to automatically infer properties of the embedded system from
the analysis of its simulation traces. We show that the number of interesting
patterns is upper-bounded by $\mathcal{O}(n^2)$ where $n$ is the length of the
string. We introduce a baseline algorithm with $\mathcal{O}(n^2)$ running time
which identifies all interesting patterns for all colors in the string
satisfying certain minimality conditions. When one is interested in patterns
related to only one color, we provide an algorithm that identifies patterns in
$\mathcal{O}(n^2\log n)$ time, but is faster than the first algorithm in
practice, both on simulated and on real-world patterns.
</summary>
    <author>
      <name>Zsuzsanna Lipt√°k</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 4 figures, 3 tables, accepted at SEA 2020 (18th Symposium
  on Experimental Algorithms, Catania, Italy, June 16-18, 2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.04858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.04858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.04344">
    <id>http://arxiv.org/abs/2004.04344v1</id>
    <updated>2020-04-09T03:12:03Z</updated>
    <published>2020-04-09T03:12:03Z</published>
    <title>A Pedagogically Sound yet Efficient Deletion algorithm for Red-Black
  Trees: The Parity-Seeking Delete Algorithm</title>
    <summary>  Red-black (RB) trees are one of the most efficient variants of balanced
binary search trees. However, they have always been blamed for being too
complicated, hard to explain, and not suitable for pedagogical purposes.
Sedgewick (2008) proposed left-leaning red-black (LLRB) trees in which red
links are restricted to left children, and proposed recursive concise insert
and delete algorithms. However, the top-down deletion algorithm of LLRB is
still very complicated and highly inefficient. In this paper, we first consider
2-3 red-black trees in which both children cannot be red. We propose a
parity-seeking delete algorithm with the basic idea of making the deficient
subtree on a par with its sibling: either by fixing the deficient subtree or by
making the sibling deficient, as well, ascending deficiency to the parent node.
This is the first pedagogically sound algorithm for the delete operation in
red-black trees. Then, we amend our algorithm and propose a parity-seeking
delete algorithm for classical RB trees. Our experiments show that, despite
having more rotations, 2-3 RB trees are almost as efficient as RB trees and
twice faster than LLRB trees. Besides, RB trees with the proposed
parity-seeking delete algorithm have the same number of rotations and almost
identical running time as the classic delete algorithm. While being extremely
efficient, the proposed parity-seeking delete algorithm is easily
understandable and suitable for pedagogical purposes.
</summary>
    <author>
      <name>Kamaledin Ghiasi-Shirazi</name>
    </author>
    <author>
      <name>Taraneh Ghandi</name>
    </author>
    <author>
      <name>Ali Taghizadeh</name>
    </author>
    <author>
      <name>Ali Rahimi-Baigi</name>
    </author>
    <link href="http://arxiv.org/abs/2004.04344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.04344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.04586">
    <id>http://arxiv.org/abs/2004.04586v1</id>
    <updated>2020-04-09T15:14:15Z</updated>
    <published>2020-04-09T15:14:15Z</published>
    <title>Storing Set Families More Compactly with Top ZDDs</title>
    <summary>  Zero-suppressed Binary Decision Diagrams (ZDDs) are data structures for
representing set families in a compressed form. With ZDDs, many valuable
operations on set families can be done in time polynomial in ZDD size. In some
cases, however, the size of ZDDs for representing large set families becomes
too huge to store them in the main memory. This paper proposes top ZDD, a novel
representation of ZDDs which uses less space than existing ones. The top ZDD is
an extension of top tree, which compresses trees, to compress directed acyclic
graphs by sharing identical subgraphs. We prove that navigational operations on
ZDDs can be done in time poly-logarithmicin ZDD size, and show that there exist
set families for which the size of the top ZDD is exponentially smaller than
that of the ZDD. We also show experimentally that our top ZDDs have smaller
size than ZDDs for real data.
</summary>
    <author>
      <name>Kotaro Matsuda</name>
    </author>
    <author>
      <name>Shuhei Denzumi</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages; Accepted for SEA2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.04586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.04586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.07678">
    <id>http://arxiv.org/abs/2005.07678v1</id>
    <updated>2020-05-15T17:48:44Z</updated>
    <published>2020-05-15T17:48:44Z</published>
    <title>Edit Distance in Near-Linear Time: it's a Constant Factor</title>
    <summary>  We present an algorithm for approximating the edit distance between two
strings of length $n$ in time $n^{1+\epsilon}$, for any $\epsilon>0$, up to a
constant factor. Our result completes the research direction set forth in the
recent breakthrough paper [Chakraborty-Das-Goldenberg-Koucky-Saks, FOCS'18],
which showed the first constant-factor approximation algorithm with a
(strongly) sub-quadratic running time. Several recent results have shown
near-linear complexity under different restrictions on the inputs (eg, when the
edit distance is close to maximal, or when one of the inputs is pseudo-random).
In contrast, our algorithm obtains a constant-factor approximation in
near-linear running time for any input strings.
</summary>
    <author>
      <name>Alexandr Andoni</name>
    </author>
    <author>
      <name>Negev Shekel Nosatzki</name>
    </author>
    <link href="http://arxiv.org/abs/2005.07678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.07678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.07644">
    <id>http://arxiv.org/abs/2005.07644v1</id>
    <updated>2020-05-15T16:58:51Z</updated>
    <published>2020-05-15T16:58:51Z</published>
    <title>Breadth-First Rank/Select in Succinct Trees and Distance Oracles for
  Interval Graphs</title>
    <summary>  We present the first succinct data structure for ordinal trees that supports
the mapping between the preorder (i.e., depth-first) ranks and level-order
(breadth-first) ranks of nodes in constant time. It also provides constant-time
support for all the operations provided by different approaches in previous
work, as well as new operations that allow us to retrieve the last internal
node before or the first internal node after a given node in a level-order
traversal. This new representation gives us the functionality needed to design
the first succinct distance oracles for interval graphs, proper interval graphs
and $k$-proper/$k$-improper interval graphs.
</summary>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <author>
      <name>Kaiyu Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2005.07644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.07644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.06213">
    <id>http://arxiv.org/abs/2005.06213v1</id>
    <updated>2020-05-13T09:07:43Z</updated>
    <published>2020-05-13T09:07:43Z</published>
    <title>Efficient and Effective Query Auto-Completion</title>
    <summary>  Query Auto-Completion (QAC) is an ubiquitous feature of modern textual search
systems, suggesting possible ways of completing the query being typed by the
user. Efficiency is crucial to make the system have a real-time responsiveness
when operating in the million-scale search space. Prior work has extensively
advocated the use of a trie data structure for fast prefix-search operations in
compact space. However, searching by prefix has little discovery power in that
only completions that are prefixed by the query are returned. This may impact
negatively the effectiveness of the QAC system, with a consequent monetary loss
for real applications like Web Search Engines and eCommerce. In this work we
describe the implementation that empowers a new QAC system at eBay, and discuss
its efficiency/effectiveness in relation to other approaches at the
state-of-the-art. The solution is based on the combination of an inverted index
with succinct data structures, a much less explored direction in the
literature. This system is replacing the previous implementation based on
Apache SOLR that was not always able to meet the required
service-level-agreement.
</summary>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Giulio Ermanno Pibiri</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in SIGIR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.06213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.06213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.06329">
    <id>http://arxiv.org/abs/2005.06329v1</id>
    <updated>2020-05-13T13:58:13Z</updated>
    <published>2020-05-13T13:58:13Z</published>
    <title>k-Approximate Quasiperiodicity under Hamming and Edit Distance</title>
    <summary>  Quasiperiodicity in strings was introduced almost 30 years ago as an
extension of string periodicity. The basic notions of quasiperiodicity are
cover and seed. A cover of a text $T$ is a string whose occurrences in $T$
cover all positions of $T$. A seed of text $T$ is a cover of a superstring of
$T$. In various applications exact quasiperiodicity is still not sufficient due
to the presence of errors. We consider approximate notions of quasiperiodicity,
for which we allow approximate occurrences in $T$ with a small Hamming,
Levenshtein or weighted edit distance.
  In previous work Sip et al. (2002) and Christodoulakis et al. (2005) showed
that computing approximate covers and seeds, respectively, under weighted edit
distance is NP-hard. They, therefore, considered restricted approximate covers
and seeds which need to be factors of the original string $T$ and presented
polynomial-time algorithms for computing them. Further algorithms, considering
approximate occurrences with Hamming distance bounded by $k$, were given in
several contributions by Guth et al. They also studied relaxed approximate
quasiperiods that do not need to cover all positions of $T$.
  In case of large data the exponents in polynomial time complexity play a
crucial role. We present more efficient algorithms for computing restricted
approximate covers and seeds. In particular, we improve upon the complexities
of many of the aforementioned algorithms, also for relaxed quasiperiods. Our
solutions are especially efficient if the number (or total cost) of allowed
errors is bounded. We also show NP-hardness of computing non-restricted
approximate covers and seeds under Hamming distance.
  Approximate covers were studied in three recent contributions at CPM over the
last three years. However, these works consider a different definition of an
approximate cover of $T$.
</summary>
    <author>
      <name>Aleksander Kƒôdzierski</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to CPM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.06329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.06329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.05681">
    <id>http://arxiv.org/abs/2005.05681v1</id>
    <updated>2020-05-12T10:49:46Z</updated>
    <published>2020-05-12T10:49:46Z</published>
    <title>Counting Distinct Patterns in Internal Dictionary Matching</title>
    <summary>  We consider the problem of preprocessing a text $T$ of length $n$ and a
dictionary $\mathcal{D}$ in order to be able to efficiently answer queries
$CountDistinct(i,j)$, that is, given $i$ and $j$ return the number of patterns
from $\mathcal{D}$ that occur in the fragment $T[i \mathinner{.\,.} j]$. The
dictionary is internal in the sense that each pattern in $\mathcal{D}$ is given
as a fragment of $T$. This way, the dictionary takes space proportional to the
number of patterns $d=|\mathcal{D}|$ rather than their total length, which
could be $\Theta(n\cdot d)$. An $\tilde{\mathcal{O}}(n+d)$-size data structure
that answers $CountDistinct(i,j)$ queries $\mathcal{O}(\log n)$-approximately
in $\tilde{\mathcal{O}}(1)$ time was recently proposed in a work that
introduced internal dictionary matching [ISAAC 2019]. Here we present an
$\tilde{\mathcal{O}}(n+d)$-size data structure that answers
$CountDistinct(i,j)$ queries $2$-approximately in $\tilde{\mathcal{O}}(1)$
time. Using range queries, for any $m$, we give an
$\tilde{\mathcal{O}}(\min(nd/m,n^2/m^2)+d)$-size data structure that answers
$CountDistinct(i,j)$ queries exactly in $\tilde{\mathcal{O}}(m)$ time. We also
consider the special case when the dictionary consists of all square factors of
the string. We design an $\mathcal{O}(n \log^2 n)$-size data structure that
allows us to count distinct squares in a text fragment $T[i \mathinner{.\,.}
j]$ in $\mathcal{O}(\log n)$ time.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Manal Mohamed</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszy≈Ñski</name>
    </author>
    <author>
      <name>Tomasz Wale≈Ñ</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CPM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.05681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.00681">
    <id>http://arxiv.org/abs/2005.00681v1</id>
    <updated>2020-05-02T02:24:03Z</updated>
    <published>2020-05-02T02:24:03Z</published>
    <title>Pointer-Machine Algorithms for Fully-Online Construction of Suffix Trees
  and DAWGs on Multiple Strings</title>
    <summary>  We deal with the problem of maintaining the suffix tree indexing structure
for a fully-online collection of multiple strings, where a new character can be
prepended to any string in the collection at any time. The only previously
known algorithm for the problem, recently proposed by Takagi et al.
[Algorithmica 82(5): 1346-1377 (2020)], runs in $O(N \log \sigma)$ time and
$O(N)$ space on the word RAM model, where $N$ denotes the total length of the
strings and $\sigma$ denotes the alphabet size. Their algorithm makes heavy use
of the nearest marked ancestor (NMA) data structure on semi-dynamic trees, that
can answer queries and supports insertion of nodes in $O(1)$ amortized time on
the word RAM model. In this paper, we present a simpler fully-online
right-to-left algorithm that builds the suffix tree for a given string
collection in $O(N (\log \sigma + \log d))$ time and $O(N)$ space, where $d$ is
the maximum number of in-coming Weiner links to a node of the suffix tree. We
note that $d$ is bounded by the height of the suffix tree, which is further
bounded by the length of the longest string in the collection. The advantage of
this new algorithm is that it works on the pointer machine model, namely, it
does not use the complicated NMA data structures that involve table look-ups.
As a byproduct, we also obtain a pointer-machine algorithm for building the
directed acyclic word graph (DAWG) for a fully-online left-to-right collection
of multiple strings, which runs in $O(N (\log \sigma + \log d))$ time and
$O(N)$ space again without the aid of the NMA data structures.
</summary>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <link href="http://arxiv.org/abs/2005.00681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.00681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.01371">
    <id>http://arxiv.org/abs/2005.01371v1</id>
    <updated>2020-05-04T10:34:07Z</updated>
    <published>2020-05-04T10:34:07Z</published>
    <title>Palindromic Length of Words with Many Periodic Palindromes</title>
    <summary>  The palindromic length $\text{PL}(v)$ of a finite word $v$ is the minimal
number of palindromes whose concatenation is equal to $v$. In 2013, Frid,
Puzynina, and Zamboni conjectured that: If $w$ is an infinite word and $k$ is
an integer such that $\text{PL}(u)\leq k$ for every factor $u$ of $w$ then $w$
is ultimately periodic.
  Suppose that $w$ is an infinite word and $k$ is an integer such
$\text{PL}(u)\leq k$ for every factor $u$ of $w$. Let $\Omega(w,k)$ be the set
of all factors $u$ of $w$ that have more than $\sqrt[k]{k^{-1}\vert u\vert}$
palindromic prefixes. We show that $\Omega(w,k)$ is an infinite set and we show
that for each positive integer $j$ there are palindromes $a,b$ and a word $u\in
\Omega(w,k)$ such that $(ab)^j$ is a factor of $u$ and $b$ is nonempty. Note
that $(ab)^j$ is a periodic word and $(ab)^ia$ is a palindrome for each $i\leq
j$. These results justify the following question: What is the palindromic
length of a concatenation of a suffix of $b$ and a periodic word $(ab)^j$ with
"many" periodic palindromes?
  It is known that $\lvert\text{PL}(uv)-\text{PL}(u)\rvert\leq \text{PL}(v)$,
where $u$ and $v$ are nonempty words. The main result of our article shows that
if $a,b$ are palindromes, $b$ is nonempty, $u$ is a nonempty suffix of $b$,
$\vert ab\vert$ is the minimal period of $aba$, and $j$ is a positive integer
with $j\geq3\text{PL}(u)$ then $\text{PL}(u(ab)^j)-\text{PL}(u)\geq 0$.
</summary>
    <author>
      <name>Josef Rukavicka</name>
    </author>
    <link href="http://arxiv.org/abs/2005.01371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.01371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.02725">
    <id>http://arxiv.org/abs/2005.02725v1</id>
    <updated>2020-05-06T10:52:18Z</updated>
    <published>2020-05-06T10:52:18Z</published>
    <title>Incremental Multiple Longest Common Sub-Sequences</title>
    <summary>  We consider the problem of updating the information about multiple longest
common sub-sequences. This kind of sub-sequences is used to highlight
information that is shared across several information sequences, therefore it
is extensively used namely in bioinformatics and computational genomics. In
this paper we propose a way to maintain this information when the underlying
sequences are subject to modifications, namely when letters are added and
removed from the extremes of the sequence. Experimentally our data structure
obtains significant improvements over the state of the art.
</summary>
    <author>
      <name>Lu√≠s M. S. Russo</name>
    </author>
    <author>
      <name>Alexandre P. Francisco</name>
    </author>
    <author>
      <name>Tatiana Rocher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The work reported in this article was supported by national funds
  through Funda\c{c}\~ao para a Ci\^encia e Tecnologia (FCT) through projects
  NGPHYLO PTDC/CCI-BIO/29676/2017 and UID/CEC/50021/2019. Funded in part by
  European Union Horizon 2020 research and innovation programme under the Marie
  Sk{\l}odowska-Curie Actions grant agreement No 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.02725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.02725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.13389">
    <id>http://arxiv.org/abs/2004.13389v1</id>
    <updated>2020-04-28T09:40:13Z</updated>
    <published>2020-04-28T09:40:13Z</published>
    <title>Approximating longest common substring with $k$ mismatches: Theory and
  practice</title>
    <summary>  In the problem of the longest common substring with $k$ mismatches we are
given two strings $X, Y$ and must find the maximal length $\ell$ such that
there is a length-$\ell$ substring of $X$ and a length-$\ell$ substring of $Y$
that differ in at most $k$ positions. The length $\ell$ can be used as a robust
measure of similarity between $X, Y$. In this work, we develop new
approximation algorithms for computing $\ell$ that are significantly more
efficient that previously known solutions from the theoretical point of view.
Our approach is simple and practical, which we confirm via an experimental
evaluation, and is probably close to optimal as we demonstrate via a
conditional lower bound.
</summary>
    <author>
      <name>Garance Gourdel</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <link href="http://arxiv.org/abs/2004.13389v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.13389v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.12881">
    <id>http://arxiv.org/abs/2004.12881v1</id>
    <updated>2020-04-27T15:41:49Z</updated>
    <published>2020-04-27T15:41:49Z</published>
    <title>The Streaming k-Mismatch Problem: Tradeoffs between Space and Total Time</title>
    <summary>  We revisit the $k$-mismatch problem in the streaming model on a pattern of
length $m$ and a streaming text of length $n$, both over a size-$\sigma$
alphabet. The current state-of-the-art algorithm for the streaming $k$-mismatch
problem, by Clifford et al. [SODA 2019], uses $\tilde O(k)$ space and $\tilde
O\big(\sqrt k\big)$ worst-case time per character. The space complexity is
known to be (unconditionally) optimal, and the worst-case time per character
matches a conditional lower bound. However, there is a gap between the total
time cost of the algorithm, which is $\tilde O(n\sqrt k)$, and the fastest
known offline algorithm, which costs $\tilde O\big(n + \min\big(\frac{nk}{\sqrt
m},\sigma n\big)\big)$ time. Moreover, it is not known whether improvements
over the $\tilde O(n\sqrt k)$ total time are possible when using more than
$O(k)$ space.
  We address these gaps by designing a randomized streaming algorithm for the
$k$-mismatch problem that, given an integer parameter $k\le s \le m$, uses
$\tilde O(s)$ space and costs $\tilde O\big(n+\min\big(\frac
{nk^2}m,\frac{nk}{\sqrt s},\frac{\sigma nm}s\big)\big)$ total time. For $s=m$,
the total runtime becomes $\tilde O\big(n + \min\big(\frac{nk}{\sqrt m},\sigma
n\big)\big)$, which matches the time cost of the fastest offline algorithm.
Moreover, the worst-case time cost per character is still $\tilde O\big(\sqrt
k\big)$.
</summary>
    <author>
      <name>Shay Golan</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract to appear in CPM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.12881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.09281">
    <id>http://arxiv.org/abs/2005.09281v1</id>
    <updated>2020-05-19T08:28:03Z</updated>
    <published>2020-05-19T08:28:03Z</published>
    <title>On Weighted Prefix Normal Words</title>
    <summary>  A prefix normal word is a binary word whose prefixes contain at least as many
1s as any of its factors of the same length. Introduced by Fici and Lipt\'ak in
2011 the notion of prefix normality is so far only defined for words over the
binary alphabet. In this work we investigate possible generalisations for
finite words over arbitrary finite alphabets, namely weighted and subset prefix
normality. We prove that weighted prefix normality is more expressive than both
binary and subset prefix normality and investigate the existence of a weighted
prefix normal form. While subset prefix normality directly inherits most
properties from the binary case, weighted prefix normality comes with several
new peculiarities that did not already occur in the binary case. We
characterise these issues and solve further questions regarding the weighted
prefix normality and weighted prefix normal form.
</summary>
    <author>
      <name>Yannik Eikmeier</name>
    </author>
    <author>
      <name>Pamela Fleischmann</name>
    </author>
    <author>
      <name>Dirk Nowotka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.09281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.08950">
    <id>http://arxiv.org/abs/2005.08950v1</id>
    <updated>2020-05-16T19:22:05Z</updated>
    <published>2020-05-16T19:22:05Z</published>
    <title>Quantum string comparison method</title>
    <summary>  We propose a quantum string comparison method whose main building blocks are
a specially designed oracle construction followed by Grover's search algorithm.
The purpose of the oracle is to compare all alphabets of the string in
parallel. This requires a unique input state preparation, which when combined
with some ancillas will result in a deterministic binary success and failure
compare outcome.
</summary>
    <author>
      <name>Vikram Menon</name>
    </author>
    <author>
      <name>Ayan Chattopadhyay</name>
    </author>
    <link href="http://arxiv.org/abs/2005.08950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.09169">
    <id>http://arxiv.org/abs/2005.09169v1</id>
    <updated>2020-05-19T02:21:18Z</updated>
    <published>2020-05-19T02:21:18Z</published>
    <title>A reduction of the dynamic time warping distance to the longest
  increasing subsequence length</title>
    <summary>  The similarity between a pair of time series, i.e., sequences of indexed
values in time order, is often estimated by the dynamic time warping (DTW)
distance, instead of any in the well-studied family of measurements including
the longest common subsequence (LCS) length and the edit distance. Although it
may seem as if the DTW and LCS(-like) measurements are essentially different,
we reveal that the DTW distance can be represented by the longest increasing
subsequence (LIS) length of a sequence of integers, which is the LCS length
between the integer sequence and itself sorted. To demonstrate that techniques
developed under LCS(-like) measurements are directly applicable to analysis of
time series via our reduction of DTW to LIS, we present time-efficient
algorithms for DTW-related problems utilizing the semi-local sequence
comparison technique developed for LCS-related problems.
</summary>
    <author>
      <name>Yoshifumi Sakai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <link href="http://arxiv.org/abs/2005.09169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.09342">
    <id>http://arxiv.org/abs/2005.09342v1</id>
    <updated>2020-05-19T10:11:33Z</updated>
    <published>2020-05-19T10:11:33Z</published>
    <title>Linear Time Construction of Indexable Founder Block Graphs</title>
    <summary>  We introduce a compact pangenome representation based on an optimal
segmentation concept that aims to reconstruct founder sequences from a multiple
sequence alignment (MSA). Such founder sequences have the feature that each row
of the MSA is a recombination of the founders. Several linear time dynamic
programming algorithms have been previously devised to optimize segmentations
that induce founder blocks that then can be concatenated into a set of founder
sequences. All possible concatenation orders can be expressed as a founder
block graph. We observe a key property of such graphs: if the node labels
(founder segments) do not repeat in the paths of the graph, such graphs can be
indexed for efficient string matching. We call such graphs segment repeat-free
founder block graphs.
  We give a linear time algorithm to construct a segment repeat-free founder
block graph given an MSA. The algorithm combines techniques from the founder
segmentation algorithms (Cazaux et al. SPIRE 2019) and fully-functional
bidirectional Burrows-Wheeler index (Belazzougui and Cunial, CPM 2019). We
derive a succinct index structure to support queries of arbitrary length in the
paths of the graph.
  Experiments on an MSA of SAR-CoV-2 strains are reported. An MSA of size
$410\times 29811$ is compacted in one minute into a segment repeat-free founder
block graph of 3900 nodes and 4440 edges. The maximum length and total length
of node labels is 12 and 34968, respectively. The index on the graph takes only
$3\%$ of the size of the MSA.
</summary>
    <author>
      <name>Veli M√§kinen</name>
    </author>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Massimo Equi</name>
    </author>
    <author>
      <name>Tuukka Norri</name>
    </author>
    <author>
      <name>Alexandru I. Tomescu</name>
    </author>
    <link href="http://arxiv.org/abs/2005.09342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2.2; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.09524">
    <id>http://arxiv.org/abs/2005.09524v1</id>
    <updated>2020-05-19T15:34:18Z</updated>
    <published>2020-05-19T15:34:18Z</published>
    <title>On repetitiveness measures of Thue-Morse words</title>
    <summary>  We show that the size $\gamma(t_n)$ of the smallest string attractor of the
$n$th Thue-Morse word $t_n$ is 4 for any $n\geq 4$, disproving the conjecture
by Mantaci et al. [ICTCS 2019] that it is $n$. We also show that $\delta(t_n) =
\frac{10}{3+2^{4-n}}$ for $n \geq 3$, where $\delta(w)$ is the maximum over all
$k = 1,\ldots,|w|$, the number of distinct substrings of length $k$ in $w$
divided by $k$, which is a measure of repetitiveness recently studied by
Kociumaka et al. [LATIN 2020]. Furthermore, we show that the number $z(t_n)$ of
factors in the self-referencing Lempel-Ziv factorization of $t_n$ is exactly
$2n$.
</summary>
    <author>
      <name>Kanaru Kutsukake</name>
    </author>
    <author>
      <name>Takuya Matsumoto</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2005.09524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.08190">
    <id>http://arxiv.org/abs/2005.08190v1</id>
    <updated>2020-05-17T08:14:43Z</updated>
    <published>2020-05-17T08:14:43Z</published>
    <title>Towards Efficient Interactive Computation of Dynamic Time Warping
  Distance</title>
    <summary>  The dynamic time warping (DTW) is a widely-used method that allows us to
efficiently compare two time series that can vary in speed. Given two strings
$A$ and $B$ of respective lengths $m$ and $n$, there is a fundamental dynamic
programming algorithm that computes the DTW distance for $A$ and $B$ together
with an optimal alignment in $\Theta(mn)$ time and space. In this paper, we
tackle the problem of interactive computation of the DTW distance for dynamic
strings, denoted $\mathrm{D^2TW}$, where character-wise edit operation
(insertion, deletion, substitution) can be performed at an arbitrary position
of the strings. Let $M$ and $N$ be the sizes of the run-length encoding (RLE)
of $A$ and $B$, respectively. We present an algorithm for $\mathrm{D^2TW}$ that
occupies $\Theta(mN+nM)$ space and uses $O(m+n+\#_{\mathrm{chg}}) \subseteq
O(mN + nM)$ time to update a compact differential representation $\mathit{DS}$
of the DP table per edit operation, where $\#_{\mathrm{chg}}$ denotes the
number of cells in $\mathit{DS}$ whose values change after the edit operation.
Our method is at least as efficient as the algorithm recently proposed by
Froese et al. running in $\Theta(mN + nM)$ time, and is faster when
$\#_{\mathrm{chg}}$ is smaller than $O(mN + nM)$ which, as our preliminary
experiments suggest, is likely to be the case in the majority of instances. In
addition, our result leads to interactive LCS/weighted edit distance
computation running in $O(m+n+\#_{\mathrm{chg}}) \subseteq O(mN + nM)$ time per
update using $\Theta(mN + nM)$ space. This improves on Hyyr\"o et al.'s
interactive algorithm that occupies $\Theta(mn)$ space and uses $O(mn)$ time
per update in the worst case.
</summary>
    <author>
      <name>Akihiro Nishi</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2005.08190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.14335">
    <id>http://arxiv.org/abs/2005.14335v1</id>
    <updated>2020-05-28T22:44:01Z</updated>
    <published>2020-05-28T22:44:01Z</published>
    <title>Classical and Quantum Algorithms for Constructing Text from Dictionary
  Problem</title>
    <summary>  We study algorithms for solving the problem of constructing a text (long
string) from a dictionary (sequence of small strings). The problem has an
application in bioinformatics and has a connection with the Sequence assembly
method for reconstructing a long DNA sequence from small fragments. The problem
is constructing a string $t$ of length $n$ from strings $s^1,\dots, s^m$ with
possible intersections. We provide a classical algorithm with running time
$O\left(n+L +m(\log n)^2\right)=\tilde{O}(n+L)$ where $L$ is the sum of lengths
of $s^1,\dots,s^m$. We provide a quantum algorithm with running time $O\left(n
+\log n\cdot(\log m+\log\log n)\cdot \sqrt{m\cdot L}\right)=\tilde{O}\left(n
+\sqrt{m\cdot L}\right)$. Additionally, we show that the lower bound for the
classical algorithm is $\Omega(n+L)$. Thus, our classical algorithm is optimal
up to a log factor, and our quantum algorithm shows speed-up comparing to any
classical algorithm in a case of non-constant length of strings in the
dictionary.
</summary>
    <author>
      <name>Kamil Khadiev</name>
    </author>
    <author>
      <name>Vladislav Remidovskii</name>
    </author>
    <link href="http://arxiv.org/abs/2005.14335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.14335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.00216">
    <id>http://arxiv.org/abs/2006.00216v1</id>
    <updated>2020-05-30T08:09:44Z</updated>
    <published>2020-05-30T08:09:44Z</published>
    <title>Longest Square Subsequence Problem Revisited</title>
    <summary>  The longest square subsequence (LSS) problem consists of computing a longest
subsequence of a given string $S$ that is a square, i.e., a longest subsequence
of form $XX$ appearing in $S$. It is known that an LSS of a string $S$ of
length $n$ can be computed using $O(n^2)$ time [Kosowski 2004], or with
(model-dependent) polylogarithmic speed-ups using $O(n^2 (\log \log n)^2 /
\log^2 n)$ time [Tiskin 2013]. We present the first algorithm for LSS whose
running time depends on other parameters, i.e., we show that an LSS of $S$ can
be computed in $O(r \min\{n, M\}\log \frac{n}{r} + M \log n)$ time with $O(M)$
space, where $r$ is the length of an LSS of $S$ and $M$ is the number of
matching points on $S$.
</summary>
    <author>
      <name>Takafumi Inoue</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <link href="http://arxiv.org/abs/2006.00216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.00216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.00605">
    <id>http://arxiv.org/abs/2006.00605v1</id>
    <updated>2020-05-31T20:49:52Z</updated>
    <published>2020-05-31T20:49:52Z</published>
    <title>A Fast Algorithm for Online k-servers Problem on Trees</title>
    <summary>  We consider online algorithms for the $k$-servers problem on trees. There is
an $k$-competitive algorithm for this problem, and it is the best competitive
ratio. M. Chrobak and L. Larmore suggested it. At the same time, the existing
implementation has $O(n)$ time complexity, where $n$ is a number of nodes in a
tree. We suggest a new time-efficient implementation of the algorithm. It has
$O(n)$ time complexity for preprocessing and $O\left(k(\log n)^2\right)$ for
processing a query.
</summary>
    <author>
      <name>Kamil Khadiev</name>
    </author>
    <author>
      <name>Maxim Yagafarov</name>
    </author>
    <link href="http://arxiv.org/abs/2006.00605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.00605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.12648">
    <id>http://arxiv.org/abs/2005.12648v1</id>
    <updated>2020-05-26T12:08:04Z</updated>
    <published>2020-05-26T12:08:04Z</published>
    <title>On the improvement of the in-place merge algorithm parallelization</title>
    <summary>  In this paper, we present several improvements in the parallelization of the
in-place merge algorithm, which merges two contiguous sorted arrays into one
with an O(T) space complexity (where T is the number of threads). The approach
divides the two arrays into as many pairs of partitions as there are threads
available; such that each thread can later merge a pair of partitions
independently of the others. We extend the existing method by proposing a new
algorithm to find the median of two partitions. Additionally, we provide a new
strategy to divide the input arrays where we minimize the data movement, but at
the cost of making this stage sequential. Finally, we provide the so-called
linear shifting algorithm that swaps two partitions in-place with contiguous
data access. We emphasize that our approach is straightforward to implement and
that it can also be used for external (out of place) merging. The results
demonstrate that it provides a significant speedup compared to sequential
executions, when the size of the arrays is greater than a thousand elements.
</summary>
    <author>
      <name>Berenger Bramas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria, ICube, CAMUS</arxiv:affiliation>
    </author>
    <author>
      <name>Quentin Bramas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ICube, UNISTRA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2005.12648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.12648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.11718">
    <id>http://arxiv.org/abs/2005.11718v2</id>
    <updated>2020-05-27T07:13:27Z</updated>
    <published>2020-05-24T11:12:02Z</published>
    <title>An inequality for the number of periods in a word</title>
    <summary>  We prove an inequality for the number of periods in a word x in terms of the
length of x and its initial critical exponent. Next, we characterize all
periods of the length-n prefix of a characteristic Sturmian word in terms of
the lazy Ostrowski representation of n, and use this result to show that our
inequality is tight for infinitely many words x. We propose two related
measures of periodicity for infinite words. Finally, we also consider special
cases where x is overlap-free or squarefree.
</summary>
    <author>
      <name>Daniel Gabric</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2005.11718v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.11718v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.10917">
    <id>http://arxiv.org/abs/2005.10917v1</id>
    <updated>2020-05-21T21:42:30Z</updated>
    <published>2020-05-21T21:42:30Z</published>
    <title>Succinct Trit-array Trie for Scalable Trajectory Similarity Search</title>
    <summary>  Massive datasets of spatial trajectories representing the mobility of a
diversity of moving objects are ubiquitous in research and industry. Similarity
search of a large collection of trajectories is indispensable for turning these
datasets into knowledge. Current methods for similarity search of trajectories
are inefficient in terms of search time and memory when applied to massive
datasets. In this paper, we address this problem by presenting a scalable
similarity search for Fr\'echet distance on trajectories, which we call
trajectory-indexing succinct trit-array trie (tSTAT). tSTAT achieves time and
memory efficiency by leveraging locality sensitive hashing (LSH) for Fr\'echet
distance and a trie data structure. We also present two novel techniques of
node reduction and a space-efficient representation for tries, which enable to
dramatically enhance a memory efficiency of tries. We experimentally test tSTAT
on its ability to retrieve similar trajectories for a query from large
collections of trajectories and show that tSTAT performs superiorly with
respect to search time and memory efficiency.
</summary>
    <author>
      <name>Shunsuke Kanda</name>
    </author>
    <author>
      <name>Koh Takeuchi</name>
    </author>
    <author>
      <name>Keisuke Fujii</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/2005.10917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.10917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.11188">
    <id>http://arxiv.org/abs/2005.11188v1</id>
    <updated>2020-05-22T13:45:23Z</updated>
    <published>2020-05-22T13:45:23Z</published>
    <title>Still Simpler Static Level Ancestors</title>
    <summary>  A level-ancestor or LA query about a rooted tree $T$ takes as arguments a
node $v$ in $T$, of depth $d_v$, say, and an integer $d$ with $0\le d\le d_v$
and returns the ancestor of $v$ in $T$ of depth $d$. The static LA problem is
to process a given rooted tree $T$ so as to support efficient subsequent
processing of LA queries about $T$. All previous efficient solutions to the
static LA problem work by reducing a given instance of the problem to a smaller
instance of the same or a related problem, solved with a less efficient data
structure, and a collection of small micro-instances for which a different
solution is provided. We indicate the first efficient solution to the static LA
problem that works directly, without resorting to reductions or
micro-instances.
</summary>
    <author>
      <name>Torben Hagerup</name>
    </author>
    <link href="http://arxiv.org/abs/2005.11188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.11188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.10668">
    <id>http://arxiv.org/abs/2005.10668v1</id>
    <updated>2020-05-20T16:14:00Z</updated>
    <published>2020-05-20T16:14:00Z</published>
    <title>Primitive Sets of Words</title>
    <summary>  Given a (finite or infinite) subset $X$ of the free monoid $A^*$ over a
finite alphabet $A$, the rank of $X$ is the minimal cardinality of a set $F$
such that $X \subseteq F^*$. We say that a submonoid $M$ generated by $k$
elements of $A^*$ is {\em $k$-maximal} if there does not exist another
submonoid generated by at most $k$ words containing $M$. We call a set $X
\subseteq A^*$ {\em primitive} if it is the basis of a $|X|$-maximal submonoid.
This definition encompasses the notion of primitive word -- in fact, $\{w\}$ is
a primitive set if and only if $w$ is a primitive word. By definition, for any
set $X$, there exists a primitive set $Y$ such that $X \subseteq Y^*$. We
therefore call $Y$ a {\em primitive root} of $X$. As a main result, we prove
that if a set has rank $2$, then it has a unique primitive root. To obtain this
result, we prove that the intersection of two $2$-maximal submonoids is either
the empty word or a submonoid generated by one single primitive word. For a
single word $w$, we say that the set $\{x,y\}$ is a {\em bi-root} of $w$ if $w$
can be written as a concatenation of copies of $x$ and $y$ and $\{x,y\}$ is a
primitive set. We prove that every primitive word $w$ has at most one bi-root
$\{x,y\}$ such that $|x|+|y|&lt;\sqrt{|w|}$. That is, the bi-root of a word is
unique provided the word is sufficiently long with respect to the size (sum of
lengths) of the root. Our results are also compared to previous approaches that
investigate pseudo-repetitions, where a morphic involutive function $\theta$ is
defined on $A^*$. In this setting, the notions of $\theta$-power,
$\theta$-primitive and $\theta$-root are defined, and it is shown that any word
has a unique $\theta$-primitive root. This result can be obtained with our
approach by showing that a word $w$ is $\theta$-primitive if and only if $\{w,
\theta(w)\}$ is a primitive set.
</summary>
    <author>
      <name>Giuseppa Castiglione</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Antonio Restivo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted. arXiv admin note: substantial text overlap with
  arXiv:1810.02182</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.10668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.10668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.10800">
    <id>http://arxiv.org/abs/2005.10800v1</id>
    <updated>2020-05-21T17:29:40Z</updated>
    <published>2020-05-21T17:29:40Z</published>
    <title>New Approximation Algorithms for Maximum Asymmetric Traveling Salesman
  and Shortest Superstring</title>
    <summary>  In the maximum asymmetric traveling salesman problem (Max ATSP) we are given
a complete directed graph with nonnegative weights on the edges and we wish to
compute a traveling salesman tour of maximum weight. In this paper we give a
fast combinatorial $\frac{7}{10}$-approximation algorithm for Max ATSP. It is
based on techniques of {\em eliminating} and {\em diluting} problematic
subgraphs with the aid of {\it half-edges} and a method of edge coloring. (A
{\it half-edge} of edge $(u,v)$ is informally speaking "either a head or a tail
of $(u,v)$".) A novel technique of {\em diluting} a problematic subgraph $S$
consists in a seeming reduction of its weight, which allows its better
handling.
  The current best approximation algorithms for Max ATSP, achieving the
approximation guarantee of $\frac 23$, are due to Kaplan, Lewenstein, Shafrir,
Sviridenko (2003) and Elbassioni, Paluch, van Zuylen (2012). Using a result by
Mucha, which states that an $\alpha$-approximation algorithm for Max ATSP
implies a $(2+\frac{11(1-\alpha)}{9-2\alpha})$-approximation algorithm for the
shortest superstring problem (SSP), we obtain also a $(2 \frac{33}{76} \approx
2,434)$-approximation algorithm for SSP, beating the previously best known
(having an approximation factor equal to $2 \frac{11}{23} \approx 2,4782$.)
</summary>
    <author>
      <name>Katarzyna Paluch</name>
    </author>
    <link href="http://arxiv.org/abs/2005.10800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.10800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.10095">
    <id>http://arxiv.org/abs/2005.10095v1</id>
    <updated>2020-05-20T14:56:59Z</updated>
    <published>2020-05-20T14:56:59Z</published>
    <title>The K-Centre Problem for Necklaces</title>
    <summary>  In graph theory, the objective of the k-centre problem is to find a set of
$k$ vertices for which the largest distance of any vertex to its closest vertex
in the $k$-set is minimised. In this paper, we introduce the $k$-centre problem
for sets of necklaces, i.e. the equivalence classes of words under the cyclic
shift. This can be seen as the k-centre problem on the complete weighted graph
where every necklace is represented by a vertex, and each edge has a weight
given by the overlap distance between any pair of necklaces. Similar to the
graph case, the goal is to choose $k$ necklaces such that the distance from any
word in the language and its nearest centre is minimised. However, in a case of
k-centre problem for languages the size of associated graph maybe exponential
in relation to the description of the language, i.e., the length of the words l
and the size of the alphabet q. We derive several approximation algorithms for
the $k$-centre problem on necklaces, with logarithmic approximation factor in
the context of l and k, and within a constant factor for a more restricted
case.
</summary>
    <author>
      <name>Duncan Adamson</name>
    </author>
    <author>
      <name>Argyrios Deligkas</name>
    </author>
    <author>
      <name>Vladimir V. Gusev</name>
    </author>
    <author>
      <name>Igor Potapov</name>
    </author>
    <link href="http://arxiv.org/abs/2005.10095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.10095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.01825">
    <id>http://arxiv.org/abs/2006.01825v1</id>
    <updated>2020-06-02T17:56:42Z</updated>
    <published>2020-06-02T17:56:42Z</published>
    <title>Efficient tree-structured categorical retrieval</title>
    <summary>  We study a document retrieval problem in the new framework where $D$ text
documents are organized in a {\em category tree} with a pre-defined number $h$
of categories. This situation occurs e.g. with taxomonic trees in biology or
subject classification systems for scientific literature. Given a string
pattern $p$ and a category (level in the category tree), we wish to efficiently
retrieve the $t$ \emph{categorical units} containing this pattern and belonging
to the category. We propose several efficient solutions for this problem. One
of them uses $n(\log\sigma(1+o(1))+\log D+O(h)) + O(\Delta)$ bits of space and
$O(|p|+t)$ query time, where $n$ is the total length of the documents, $\sigma$
the size of the alphabet used in the documents and $\Delta$ is the total number
of nodes in the category tree. Another solution uses
$n(\log\sigma(1+o(1))+O(\log D))+O(\Delta)+O(D\log n)$ bits of space and
$O(|p|+t\log D)$ query time. We finally propose other solutions which are more
space-efficient at the expense of a slight increase in query time.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Gregory Kucherov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper accepted for presentation at the 31st Annual
  Symposium on Combinatorial Pattern Matching (CPM 2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.01825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.01825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.02408">
    <id>http://arxiv.org/abs/2006.02408v1</id>
    <updated>2020-06-03T17:33:31Z</updated>
    <published>2020-06-03T17:33:31Z</published>
    <title>Dynamic Longest Common Substring in Polylogarithmic Time</title>
    <summary>  The longest common substring problem consists in finding a longest string
that appears as a (contiguous) substring of two input strings. We consider the
dynamic variant of this problem, in which we are to maintain two dynamic
strings $S$ and $T$, each of length at most $n$, that undergo substitutions of
letters, in order to be able to return a longest common substring after each
substitution. Recently, Amir et al. [ESA 2019] presented a solution for this
problem that needs only $\tilde{\mathcal{O}}(n^{2/3})$ time per update. This
brought the challenge of determining whether there exists a faster solution
with polylogarithmic update time, or (as is the case for other dynamic
problems), we should expect a polynomial (conditional) lower bound. We answer
this question by designing a significantly faster algorithm that processes each
substitution in amortized $\log^{\mathcal{O}(1)} n$ time with high probability.
Our solution relies on exploiting the local consistency of the parsing of a
collection of dynamic strings due to Gawrychowski et al. [SODA 2018], and on
maintaining two dynamic trees with labeled bicolored leaves, so that after each
update we can report a pair of nodes, one from each tree, of maximum combined
weight, which have at least one common leaf-descendant of each color. We
complement this with a lower bound of $\Omega(\log n/ \log\log n)$ for the
update time of any polynomial-size data structure that maintains the LCS of two
dynamic strings, and the same lower bound for the update time of any data
structure of size $\tilde{\mathcal{O}}(n)$ that maintains the LCS of a static
and a dynamic string. Both lower bounds hold even allowing amortization and
randomization.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Karol Pokorski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper that is to appear in the ICALP 2020
  proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.02408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.02134">
    <id>http://arxiv.org/abs/2006.02134v1</id>
    <updated>2020-06-03T10:02:51Z</updated>
    <published>2020-06-03T10:02:51Z</published>
    <title>Computing Palindromic Trees for a Sliding Window and Its Applications</title>
    <summary>  The palindromic tree (a.k.a. eertree) for a string $S$ of length $n$ is a
tree-like data structure that represents the set of all distinct palindromic
substrings of $S$, using $O(n)$ space [Rubinchik and Shur, 2018]. It is known
that, when $S$ is over an alphabet of size $\sigma$ and is given in an online
manner, then the palindromic tree of $S$ can be constructed in $O(n\log\sigma)$
time with $O(n)$ space. In this paper, we consider the sliding window version
of the problem: For a fixed window length $d$, we propose two algorithms to
maintain the palindromic tree of size $O(d)$ for every sliding window
$S[i..i+d-1]$ over $S$, one running in $O(n\log\sigma')$ time with $O(d)$ space
where $\sigma' \leq d$ is the maximum number of distinct characters in the
windows, and the other running in $O(n + d\sigma)$ time with $d\sigma + O(d)$
space. We also present applications of our algorithms for computing minimal
unique palindromic substrings (MUPS) and for computing minimal absent
palindromic words (MAPW) for a sliding window.
</summary>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Kiichi Watanabe</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2006.02134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.02219">
    <id>http://arxiv.org/abs/2006.02219v1</id>
    <updated>2020-06-03T12:30:53Z</updated>
    <published>2020-06-03T12:30:53Z</published>
    <title>LCP-Aware Parallel String Sorting</title>
    <summary>  When lexicographically sorting strings, it is not always necessary to inspect
all symbols. For example, the lexicographical rank of "europar" amongst the
strings "eureka", "eurasia", and "excells" only depends on its so called
relevant prefix "euro". The distinguishing prefix size $D$ of a set of strings
is the number of symbols that actually need to be inspected to establish the
lexicographical ordering of all strings. Efficient string sorters should be
$D$-aware, i.e. their complexity should depend on $D$ rather than on the total
number $N$ of all symbols in all strings. While there are many $D$-aware
sorters in the sequential setting, there appear to be no such results in the
PRAM model. We propose a framework yielding a $D$-aware modification of any
existing PRAM string sorter. The derived algorithms are work-optimal with
respect to their original counterpart: If the original algorithm requires
$O(w(N))$ work, the derived one requires $O(w(D))$ work. The execution time
increases only by a small factor that is logarithmic in the length of the
longest relevant prefix. Our framework universally works for deterministic and
randomized algorithms in all variations of the PRAM model, such that future
improvements in ($D$-unaware) parallel string sorting will directly result in
improvements in $D$-aware parallel string sorting.
</summary>
    <author>
      <name>Jonas Ellert</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Nodari Sitchinava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Euro-Par 2020 and to be published by Springer as part of
  the conference proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.02219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.15575">
    <id>http://arxiv.org/abs/2006.15575v2</id>
    <updated>2020-09-30T12:00:50Z</updated>
    <published>2020-06-28T11:25:40Z</published>
    <title>Random Access in Persistent Strings</title>
    <summary>  We consider compact representations of collections of similar strings that
support random access queries. The collection of strings is given by a rooted
tree where edges are labeled by an edit operation (inserting, deleting, or
replacing a character) and a node represents the string obtained by applying
the sequence of edit operations on the path from the root to the node. The goal
is to compactly represent the entire collection while supporting fast random
access to any part of a string in the collection. This problem captures natural
scenarios such as representing the past history of a edited document or
representing highly-repetitive collections. Given a tree with $n$ nodes, we
show how to represent the corresponding collection in $O(n)$ space and optimal
$O(\log n/ \log \log n)$ query time. This improves the previous time-space
trade-offs for the problem. To obtain our results, we introduce new techniques
and ideas, including a reduction to a new geometric line segment selection
together with an efficient solution.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract at ISAAC 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.15575v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.15575v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.13576">
    <id>http://arxiv.org/abs/2006.13576v2</id>
    <updated>2020-07-22T09:08:09Z</updated>
    <published>2020-06-24T09:33:08Z</published>
    <title>Lyndon Words, the Three Squares Lemma, and Primitive Squares</title>
    <summary>  We revisit the so-called "Three Squares Lemma" by Crochemore and Rytter
[Algorithmica 1995] and, using arguments based on Lyndon words, derive a more
general variant which considers three overlapping squares which do not
necessarily share a common prefix. We also give an improved upper bound of
$n\log_2 n$ on the maximum number of (occurrences of) primitively rooted
squares in a string of length $n$, also using arguments based on Lyndon words.
To the best of our knowledge, the only known upper bound was $n \log_\phi n
\approx 1.441n\log_2 n$, where $\phi$ is the golden ratio, reported by Fraenkel
and Simpson [TCS 1999] obtained via the Three Squares Lemma.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <link href="http://arxiv.org/abs/2006.13576v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.13576v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.14029">
    <id>http://arxiv.org/abs/2006.14029v1</id>
    <updated>2020-06-24T20:26:33Z</updated>
    <published>2020-06-24T20:26:33Z</published>
    <title>Small Longest Tandem Scattered Subsequences</title>
    <summary>  We consider the problem of identifying tandem scattered subsequences within a
string. Our algorithm identifies a longest subsequence which occurs twice
without overlap in a string. This algorithm is based on the Hunt-Szymanski
algorithm, therefore its performance improves if the string is not self
similar. This occurs naturally on strings over large alphabets. Our algorithm
relies on new results for data structures that support dynamic longest
increasing sub-sequences. In the process we also obtain improved algorithms for
the decremental string comparison problem.
</summary>
    <author>
      <name>Lu√≠s M. S. Russo</name>
    </author>
    <author>
      <name>Alexandre P. Francisco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The work reported in this article was supported by national funds
  through Funda\c{c}\~ao para a Ci\^encia e Tecnologia (FCT) with reference
  UIDB/50021/2020 and through project NGPHYLO PTDC/CCI-BIO/29676/2017. Funded
  in part by European Union's Horizon 2020 research and innovation programme
  under the Marie Sk{\l}odowska-Curie Actions grant agreement No 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.14029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.14029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.11687">
    <id>http://arxiv.org/abs/2006.11687v1</id>
    <updated>2020-06-21T01:29:47Z</updated>
    <published>2020-06-21T01:29:47Z</published>
    <title>PFP Data Structures</title>
    <summary>  Prefix-free parsing (PFP) was introduced by Boucher et al. (2019) as a
preprocessing step to ease the computation of Burrows-Wheeler Transforms (BWTs)
of genomic databases. Given a string $S$, it produces a dictionary $D$ and a
parse $P$ of overlapping phrases such that $\mathrm{BWT} (S)$ can be computed
from $D$ and $P$ in time and workspace bounded in terms of their combined size
$|\mathrm{PFP} (S)|$. In practice $D$ and $P$ are significantly smaller than
$S$ and computing $\mathrm{BWT} (S)$ from them is more efficient than computing
it from $S$ directly, at least when $S$ consists of genomes from individuals of
the same species. In this paper, we consider $\mathrm{PFP} (S)$ as a {\em data
structure} and show how it can be augmented to support the following queries
quickly, still in $O (|\mathrm{PFP} (S)|)$ space: longest common extension
(LCE), suffix array (SA), longest common prefix (LCP) and BWT. Lastly, we
provide experimental evidence that the PFP data structure can be efficiently
constructed for very large repetitive datasets: it takes one hour and 54 GB
peak memory for $1000$ variants of human chromosome 19, initially occupying
roughly 56 GB.
</summary>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Ond≈ôej Cvacho</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Jan Holub</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <link href="http://arxiv.org/abs/2006.11687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.11687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.11978">
    <id>http://arxiv.org/abs/2006.11978v1</id>
    <updated>2020-06-22T02:54:20Z</updated>
    <published>2020-06-22T02:54:20Z</published>
    <title>Fast Preprocessing for Optimal Orthogonal Range Reporting and Range
  Successor with Applications to Text Indexing</title>
    <summary>  Under the word RAM model, we design three data structures that can be
constructed in $O(n\sqrt{\lg n})$ time over $n$ points in an $n \times n$ grid.
The first data structure is an $O(n\lg^{\epsilon} n)$-word structure supporting
orthogonal range reporting in $O(\lg\lg n+k)$ time, where $k$ denotes output
size and $\epsilon$ is an arbitrarily small constant. The second is an
$O(n\lg\lg n)$-word structure supporting orthogonal range successor in
$O(\lg\lg n)$ time, while the third is an $O(n\lg^{\epsilon} n)$-word structure
supporting sorted range reporting in $O(\lg\lg n+k)$ time. The query times of
these data structures are optimal when the space costs must be within $O(n\
polylog\ n)$ words. Their exact space bounds match those of the best known
results achieving the same query times, and the $O(n\sqrt{\lg n})$ construction
time beats the previous bounds on preprocessing. Previously, among 2d range
search structures, only the orthogonal range counting structure of Chan and
P\v{a}tra\c{s}cu (SODA 2010) and the linear space, $O(\lg^{\epsilon} n)$ query
time structure for orthogonal range successor by Belazzougui and Puglisi (SODA
2016) can be built in the same $O(n\sqrt{\lg n})$ time. Hence our work is the
first that achieve the same preprocessing time for optimal orthogonal range
reporting and range successor. We also apply our results to improve the
construction time of text indexes.
</summary>
    <author>
      <name>Younan Gao</name>
    </author>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/2006.11978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.11978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.10152">
    <id>http://arxiv.org/abs/2006.10152v1</id>
    <updated>2020-06-17T20:58:05Z</updated>
    <published>2020-06-17T20:58:05Z</published>
    <title>Extremal overlap-free and extremal $Œ≤$-free binary words</title>
    <summary>  An overlap-free (or $\beta$-free) word $w$ over a fixed alphabet $\Sigma$ is
extremal if every word obtained from $w$ by inserting a single letter from
$\Sigma$ at any position contains an overlap (or a factor of exponent at least
$\beta$, respectively). We find all lengths which admit an extremal
overlap-free binary word. For every extended real number $\beta$ such that
$2^+\leq\beta\leq 8/3$, we show that there are arbitrarily long extremal
$\beta$-free binary words.
</summary>
    <author>
      <name>Lucas Mol</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2006.10152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.10152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.04177">
    <id>http://arxiv.org/abs/2006.04177v1</id>
    <updated>2020-06-07T15:10:48Z</updated>
    <published>2020-06-07T15:10:48Z</published>
    <title>Sumsets of Wythoff Sequences, Fibonacci Representation, and Beyond</title>
    <summary>  Let $\alpha = (1+\sqrt{5})/2$ and define the lower and upper Wythoff
sequences by $a_i = \lfloor i \alpha \rfloor$, $b_i = \lfloor i \alpha^2
\rfloor$ for $i \geq 1$. In a recent interesting paper, Kawsumarng et al.
proved a number of results about numbers representable as sums of the form $a_i
+ a_j$, $b_i + b_j$, $a_i + b_j$, and so forth. In this paper I show how to
derive all of their results, using one simple idea and existing free software
called Walnut. The key idea is that for each of their sumsets, there is a
relatively small automaton accepting the Fibonacci representation of the
numbers represented. I also show how the automaton approach can easily prove
other results.
</summary>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2006.04177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.04177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.05104">
    <id>http://arxiv.org/abs/2006.05104v2</id>
    <updated>2020-07-16T09:44:44Z</updated>
    <published>2020-06-09T08:21:39Z</published>
    <title>Optimal-Time Queries on BWT-runs Compressed Indexes</title>
    <summary>  Although a significant number of compressed indexes for highly repetitive
strings have been proposed thus far, developing compressed indexes that support
faster queries remains a challenge. Run-length Burrows-Wheeler transform
(RLBWT) is a lossless data compression by a reversible permutation of an input
string and run-length encoding, and it has become a popular research topic in
string processing. R-index[Gagie et al., ACM'20] is an efficient compressed
index on RLBWT whose space usage depends not on string length but the number of
runs in an RLBWT, and it supports locate queries in an optimal time with
$\omega(r)$ words for the number $r$ of runs in the RLBWT of an input string.
Following this line of research, we present the first compressed index on
RLBWT, which we call \emph{r-index-f}, that supports various queries including
locate, count, extract queries, decompression and prefix search in the optimal
time with smaller working space of $O(r)$ words for small alphabets in this
paper. We present efficient data structures for computing two important
functions of LF and $\phi^{-1}$ in constant time with $O(r)$ words of space,
which is a bit step forward in computation time from the previous best result
of $O(\log \log n)$ time for string length $n$ and $O(r)$ words of space.
Finally, We present algorithms for computing queries on RLBWT by leveraging
those two data structures in optimal time with $O(r)$ words of space.
</summary>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/2006.05104v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05104v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.05871">
    <id>http://arxiv.org/abs/2006.05871v1</id>
    <updated>2020-06-10T14:55:34Z</updated>
    <published>2020-06-10T14:55:34Z</published>
    <title>Tailoring r-index for metagenomics</title>
    <summary>  A basic problem in metagenomics is to assign a sequenced read to the correct
species in the reference collection. In typical applications in genomic
epidemiology and viral metagenomics the reference collection consists of set of
species with each species represented by its highly similar strains. It has
been recently shown that accurate read assignment can be achieved with $k$-mer
hashing-based pseudoalignment: A read is assigned to species A if each of its
$k$-mer hits to reference collection is located only on strains of A. We study
the underlying primitives required in pseudoalignment and related tasks. We
propose three space-efficient solutions building upon the document listing with
frequencies problem. All the solutions use an $r$-index (Gagie et al., SODA
2018) as an underlying index structure for the text obtained as concatenation
of the set of species, as well as for each species. Given $t$ species whose
concatenation length is $n$, and whose Burrows-Wheeler transform contains $r$
runs, our first solution, based on a grammar-compressed document array with
precomputed queries at non terminal symbols, reports the frequencies for the
${\tt ndoc}$ distinct documents in which the pattern of length $m$ occurs in
${\cal O}(m + \log(n){\tt ndoc}) $ time. Our second solution is also based on a
grammar-compressed document array, but enhanced with bitvectors and reports the
frequencies in ${\cal O}(m + ((t/w)\log n + \log(n/r)){\tt ndoc})$ time, over a
machine with wordsize $w$. Our third solution, based on the interleaved LCP
array, answers the same query in ${\cal O}(m + \log(n/r){\tt ndoc})$. We
implemented our solutions and tested them on real-world and synthetic datasets.
The results show that all the solutions are fast on highly-repetitive data, and
the size overhead introduced by the indexes are comparable with the size of the
$r$-index.
</summary>
    <author>
      <name>Dustin Cobas</name>
    </author>
    <author>
      <name>Veli M√§kinen</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.05871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.03198">
    <id>http://arxiv.org/abs/2006.03198v2</id>
    <updated>2020-09-27T01:12:06Z</updated>
    <published>2020-06-05T01:55:22Z</published>
    <title>Efficient Semi-External Depth-First Search</title>
    <summary>  Computing Depth-First Search (DFS) results, i.e. depth-first order or
DFS-Tree, on the semi-external environment becomes a hot topic, because the
scales of the graphs grow rapidly which can hardly be hold in the main memory,
in the big data era. Existing semi-external DFS algorithms assume the main
memory could, at least, hold a spanning tree T of a graph G, and gradually
restructure T into a DFS-Tree, which is non-trivial. In this paper, we present
a comprehensive study of semi-external DFS problem, including the first
theoretical analysis of the main challenge of this problem, as far as we know.
Besides, we introduce a new semi-external DFS algorithm with an efficient edge
pruning principle, named EP-DFS. Unlike the traditional algorithms, we not only
focus on addressing such complex problem efficiently with less I/Os, but also
focus on that with simpler CPU calculation (Implementation-friendly) and less
random I/O access (key-to-efficiency). The former is based on our efficient
pruning principle; the latter is addressed by a lightweight index N+-index,
which is a compressed storage for a subset of the edges for G. The extensive
experimental evaluation on both synthetic and real graphs confirms that our
EP-DFS algorithm outperforms the existing techniques.
</summary>
    <author>
      <name>Xiaolong Wan</name>
    </author>
    <author>
      <name>Hongzhi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2006.03198v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.03198v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.09192">
    <id>http://arxiv.org/abs/2007.09192v1</id>
    <updated>2020-07-17T19:13:09Z</updated>
    <published>2020-07-17T19:13:09Z</published>
    <title>The Edit Distance to $k$-Subsequence Universality</title>
    <summary>  A word $u$ is a subsequence of another word $w$ if $u$ can be obtained from
$w$ by deleting some of its letters. The word $w$ with alph$(w)=\Sigma$ is
called $k$-subsequence universal if the set of subsequences of length $k$ of
$w$ contains all possible words of length $k$ over $\Sigma$. We propose a
series of efficient algorithms computing the minimal number of edit operations
(insertion, deletion, substitution) one needs to apply to a given word in order
to reach the set of $k$-subsequence universal words.
</summary>
    <author>
      <name>Pamela Fleischmann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kiel University, Computer Science Department, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Maria Kosche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">G√∂ttingen University, Computer Science Department, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Tore Ko√ü</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">G√∂ttingen University, Computer Science Department, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Florin Manea</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">G√∂ttingen University, Computer Science Department, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Stefan Siemer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">G√∂ttingen University, Computer Science Department, Germany</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2007.09192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.08357">
    <id>http://arxiv.org/abs/2007.08357v1</id>
    <updated>2020-07-16T14:34:14Z</updated>
    <published>2020-07-16T14:34:14Z</published>
    <title>Substring Complexity in Sublinear Space</title>
    <summary>  Shannon's entropy is a definitive lower bound for statistical compression.
Unfortunately, no such clear measure exists for the compressibility of
repetitive strings. Thus, ad-hoc measures are employed to estimate the
repetitiveness of strings, e.g., the size $z$ of the Lempel-Ziv parse or the
number $r$ of equal-letter runs of the Burrows-Wheeler transform. A more recent
one is the size $\gamma$ of a smallest string attractor. Unfortunately, Kempa
and Prezza [STOC 2018] showed that computing $\gamma$ is NP-hard. Kociumaka et
al. [LATIN 2020] considered a new measure that is based on the function $S_T$
counting the cardinalities of the sets of substrings of each length of $T$,
also known as the substring complexity. This new measure is defined as $\delta=
\sup\{S_T(k)/k, k\geq 1\}$ and lower bounds all the measures previously
considered. In particular, $\delta\leq \gamma$ always holds and $\delta$ can be
computed in $\mathcal{O}(n)$ time using $\Omega(n)$ working space. Kociumaka et
al. showed that if $\delta$ is given, one can construct an $\mathcal{O}(\delta
\log \frac{n}{\delta})$-sized representation of $T$ supporting efficient direct
access and efficient pattern matching queries on $T$. Given that for highly
compressible strings, $\delta$ is significantly smaller than $n$, it is natural
to pose the following question: Can we compute $\delta$ efficiently using
sublinear working space?
  It is straightforward to show that any algorithm computing $\delta$ using
$\mathcal{O}(b)$ space requires $\Omega(n^{2-o(1)}/b)$ time through a reduction
from the element distinctness problem [Yao, SIAM J. Comput. 1994]. We present
the following results: an $\mathcal{O}(n^3/b^2)$-time and
$\mathcal{O}(b)$-space algorithm to compute $\delta$, for any $b\in[1,n]$; and
an $\tilde{\mathcal{O}}(n^2/b)$-time and $\mathcal{O}(b)$-space algorithm to
compute $\delta$, for any $b\in[n^{2/3},n]$.
</summary>
    <author>
      <name>Giulia Bernardini</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <link href="http://arxiv.org/abs/2007.08357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.08357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.08188">
    <id>http://arxiv.org/abs/2007.08188v1</id>
    <updated>2020-07-16T08:56:26Z</updated>
    <published>2020-07-16T08:56:26Z</published>
    <title>The Simplest Binary Word with Only Three Squares</title>
    <summary>  We re-examine previous constructions of infinite binary words containing few
distinct squares with the goal of finding the "simplest", in a certain sense.
We exhibit several new constructions. Rather than using tedious case-based
arguments to prove that the constructions have the desired property, we rely
instead on theorem-proving software for their correctness.
</summary>
    <author>
      <name>Daniel Gabric</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2007.08188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.08188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.07718">
    <id>http://arxiv.org/abs/2007.07718v1</id>
    <updated>2020-07-15T14:45:14Z</updated>
    <published>2020-07-15T14:45:14Z</published>
    <title>On Indexing and Compressing Finite Automata</title>
    <summary>  An index for a finite automaton is a powerful data structure that supports
locating paths labeled with a query pattern, thus solving pattern matching on
the underlying regular language. In this paper, we solve the long-standing
problem of indexing arbitrary finite automata. Our solution consists in finding
a partial co-lexicographic order of the states and proving, as in the total
order case, that states reached by a given string form one interval on the
partial order, thus enabling indexing. We provide a lower bound stating that
such an interval requires $O(p)$ words to be represented, $p$ being the order's
width (i.e. the size of its largest antichain). Indeed, we show that $p$
determines the complexity of several fundamental problems on finite automata:
(i) Letting $\sigma$ be the alphabet size, we provide an encoding for NFAs
using $\lceil\log \sigma\rceil + 2\lceil\log p\rceil + 2$ bits per transition
and a smaller encoding for DFAs using $\lceil\log \sigma\rceil + \lceil\log
p\rceil + 2$ bits per transition. This is achieved by generalizing the
Burrows-Wheeler transform to arbitrary automata. (ii) We show that indexed
pattern matching can be solved in $\tilde O(m\cdot p^2)$ query time on NFAs.
(iii) We provide a polynomial-time algorithm to index DFAs, while matching the
optimal value for $ p $. On the other hand, we prove that the problem is
NP-hard on NFAs. (iv) We show that, in the worst case, the classic powerset
construction algorithm for NFA determinization generates an equivalent DFA of
size $2^p(n-p+1)-1$, where $n$ is the number of NFA's states.
</summary>
    <author>
      <name>Nicola Cotumaccio</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/2007.07718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.07718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.06604">
    <id>http://arxiv.org/abs/2007.06604v1</id>
    <updated>2020-07-13T18:11:19Z</updated>
    <published>2020-07-13T18:11:19Z</published>
    <title>Update Query Time Trade-off for dynamic Suffix Arrays</title>
    <summary>  The Suffix Array SA(S) of a string S[1 ... n] is an array containing all the
suffixes of S sorted by lexicographic order. The suffix array is one of the
most well known indexing data structures, and it functions as a key tool in
many string algorithms. In this paper, we present a data structure for
maintaining the Suffix Array of a dynamic string. For every $0 \leq \varepsilon
\leq 1$, our data structure reports SA[i] in $\tilde{O}(n^{\varepsilon})$ time
and handles text modification in $\tilde{O}(n^{1-\varepsilon})$ time.
Additionally, our data structure enables the same query time for reporting
iSA[i], with iSA being the Inverse Suffix Array of S[1 ... n]. Our data
structure can be used to construct sub-linear dynamic variants of static
strings algorithms or data structures that are based on the Suffix Array and
the Inverse Suffix Array.
</summary>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Itai Boneh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.06604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.06604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.06167">
    <id>http://arxiv.org/abs/2007.06167v1</id>
    <updated>2020-07-13T03:20:18Z</updated>
    <published>2020-07-13T03:20:18Z</published>
    <title>Local Editing in LZ-End Compressed Data</title>
    <summary>  This paper presents an algorithm for the modification of data compressed
using LZ-End, a derivate of LZ77, without prior decompression. The performance
of the algorithm and the impact of the modifications on the compression ratio
is evaluated. Finally, we discuss the importance of this work as a first step
towards local editing in Lempel-Ziv compressed data.
</summary>
    <author>
      <name>Daniel Roodt</name>
    </author>
    <author>
      <name>Ulrich Speidel</name>
    </author>
    <author>
      <name>Vimal Kumar</name>
    </author>
    <author>
      <name>Ryan K. L. Ko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 Figure, 2 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.06167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.06167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.04128">
    <id>http://arxiv.org/abs/2007.04128v2</id>
    <updated>2020-09-29T09:18:53Z</updated>
    <published>2020-07-08T13:55:10Z</published>
    <title>String Indexing for Top-$k$ Close Consecutive Occurrences</title>
    <summary>  The classic string indexing problem is to preprocess a string $S$ into a
compact data structure that supports efficient subsequent pattern matching
queries, that is, given a pattern string $P$, report all occurrences of $P$
within $S$. In this paper, we study a basic and natural extension of string
indexing called the string indexing for top-$k$ close consecutive occurrences
problem (SITCCO). Here, a consecutive occurrence is a pair $(i,j)$, $i &lt; j$,
such that $P$ occurs at positions $i$ and $j$ in $S$ and there is no occurrence
of $P$ between $i$ and $j$, and their distance is defined as $j-i$. Given a
pattern $P$ and a parameter $k$, the goal is to report the top-$k$ consecutive
occurrences of $P$ in $S$ of minimal distance. The challenge is to compactly
represent $S$ while supporting queries in time close to length of $P$ and $k$.
We give two time-space trade-offs for the problem. Let $n$ be the length of
$S$, $m$ the length of $P$, and $\epsilon\in(0,1]$. Our first result achieves
$O(n\log n)$ space and optimal query time of $O(m+k)$, and our second result
achieves linear space and query time $O(m+k^{1+\epsilon})$. Along the way, we
develop several techniques of independent interest, including a new translation
of the problem into a line segment intersection problem and a new recursive
clustering technique for trees.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <author>
      <name>Max Rish√∏j Pedersen</name>
    </author>
    <author>
      <name>Eva Rotenberg</name>
    </author>
    <author>
      <name>Teresa Anna Steiner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed typos, minor changes</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.04128v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.04128v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.03040">
    <id>http://arxiv.org/abs/2007.03040v1</id>
    <updated>2020-07-06T19:58:58Z</updated>
    <published>2020-07-06T19:58:58Z</published>
    <title>Near-Linear Time Edit Distance for Indel Channels</title>
    <summary>  We consider the following model for sampling pairs of strings: $s_1$ is a
uniformly random bitstring of length $n$, and $s_2$ is the bitstring arrived at
by applying substitutions, insertions, and deletions to each bit of $s_1$ with
some probability. We show that the edit distance between $s_1$ and $s_2$ can be
computed in $O(n \ln n)$ time with high probability, as long as each bit of
$s_1$ has a mutation applied to it with probability at most a small constant.
The algorithm is simple and only uses the textbook dynamic programming
algorithm as a primitive, first computing an approximate alignment between the
two strings, and then running the dynamic programming algorithm restricted to
entries close to the approximate alignment. The analysis of our algorithm
provides theoretical justification for alignment heuristics used in practice
such as BLAST, FASTA, and MAFFT, which also start by computing approximate
alignments quickly and then find the best alignment near the approximate
alignment. Our main technical contribution is a partitioning of alignments such
that the number of the subsets in the partition is not too large and every
alignment in one subset is worse than an alignment considered by our algorithm
with high probability. Similar techniques may be of interest in the
average-case analysis of other problems commonly solved via dynamic
programming.
</summary>
    <author>
      <name>Arun Ganesh</name>
    </author>
    <author>
      <name>Aaron Sy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in WABI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.03040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.03040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.15999">
    <id>http://arxiv.org/abs/2006.15999v1</id>
    <updated>2020-06-29T12:42:39Z</updated>
    <published>2020-06-29T12:42:39Z</published>
    <title>The Number of Repetitions in 2D-Strings</title>
    <summary>  The notions of periodicity and repetitions in strings, and hence these of
runs and squares, naturally extend to two-dimensional strings. We consider two
types of repetitions in 2D-strings: 2D-runs and quartics (quartics are a
2D-version of squares in standard strings). Amir et al. introduced 2D-runs,
showed that there are $O(n^3)$ of them in an $n \times n$ 2D-string and
presented a simple construction giving a lower bound of $\Omega(n^2)$ for their
number (TCS 2020). We make a significant step towards closing the gap between
these bounds by showing that the number of 2D-runs in an $n \times n$ 2D-string
is $O(n^2 \log^2 n)$. In particular, our bound implies that the $O(n^2\log n +
\textsf{output})$ run-time of the algorithm of Amir et al. for computing
2D-runs is also $O(n^2 \log^2 n)$. We expect this result to allow for
exploiting 2D-runs algorithmically in the area of 2D pattern matching.
  A quartic is a 2D-string composed of $2 \times 2$ identical blocks
(2D-strings) that was introduced by Apostolico and Brimkov (TCS 2000), where by
quartics they meant only primitively rooted quartics, i.e. built of a primitive
block. Here our notion of quartics is more general and analogous to that of
squares in 1D-strings. Apostolico and Brimkov showed that there are $O(n^2
\log^2 n)$ occurrences of primitively rooted quartics in an $n \times n$
2D-string and that this bound is attainable. Consequently the number of
distinct primitively rooted quartics is $O(n^2 \log^2 n)$. Here, we prove that
the number of distinct general quartics is also $O(n^2 \log^2 n)$. This extends
the rich combinatorial study of the number of distinct squares in a 1D-string,
that was initiated by Fraenkel and Simpson (J. Comb. Theory A 1998), to two
dimensions.
  Finally, we show some algorithmic applications of 2D-runs. (Abstract
shortened due to arXiv requirements.)
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Wale≈Ñ</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the ESA 2020 proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.15999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.15999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.16137">
    <id>http://arxiv.org/abs/2006.16137v1</id>
    <updated>2020-06-29T15:56:54Z</updated>
    <published>2020-06-29T15:56:54Z</published>
    <title>Pattern Masking for Dictionary Matching</title>
    <summary>  In the Pattern Masking for Dictionary Matching (PMDM) problem, we are given a
dictionary $\mathcal{D}$ of $d$ strings, each of length $\ell$, a query string
$q$ of length $\ell$, and a positive integer $z$, and we are asked to compute a
smallest set $K\subseteq\{1,\ldots,\ell\}$, so that if $q[i]$, for all $i\in
K$, is replaced by a wildcard, then $q$ matches at least $z$ strings from
$\mathcal{D}$. The PMDM problem lies at the heart of two important applications
featured in large-scale real-world systems: record linkage of databases that
contain sensitive information, and query term dropping. In both applications,
solving PMDM allows for providing data utility guarantees as opposed to
existing approaches.
  We first show, through a reduction from the well-known $k$-Clique problem,
that a decision version of the PMDM problem is NP-complete, even for strings
over a binary alphabet. We present a data structure for PMDM that answers
queries over $\mathcal{D}$ in time
$\mathcal{O}(2^{\ell/2}(2^{\ell/2}+\tau)\ell)$ and requires space
$\mathcal{O}(2^{\ell}d^2/\tau^2+2^{\ell/2}d)$, for any parameter
$\tau\in[1,d]$. We also approach the problem from a more practical perspective.
We show an $\mathcal{O}((d\ell)^{k/3}+d\ell)$-time and
$\mathcal{O}(d\ell)$-space algorithm for PMDM if $k=|K|=\mathcal{O}(1)$. We
generalize our exact algorithm to mask multiple query strings simultaneously.
We complement our results by showing a two-way polynomial-time reduction
between PMDM and the Minimum Union problem [Chlamt\'{a}\v{c} et al., SODA
2017]. This gives a polynomial-time
$\mathcal{O}(d^{1/4+\epsilon})$-approximation algorithm for PMDM, which is
tight under plausible complexity conjectures.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Huiping Chen</name>
    </author>
    <author>
      <name>Peter Christen</name>
    </author>
    <author>
      <name>Grigorios Loukides</name>
    </author>
    <author>
      <name>Nadia Pisanti</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <link href="http://arxiv.org/abs/2006.16137v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.16137v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.12097">
    <id>http://arxiv.org/abs/2007.12097v2</id>
    <updated>2020-07-24T14:07:49Z</updated>
    <published>2020-07-23T16:00:54Z</published>
    <title>A New Upper Bound for Separating Words</title>
    <summary>  We prove that for any distinct $x,y \in \{0,1\}^n$, there is a deterministic
finite automaton with $\widetilde{O}(n^{1/3})$ states that accepts $x$ but not
$y$. This improves Robson's 1989 upper bound of $\widetilde{O}(n^{2/5})$.
</summary>
    <author>
      <name>Zachary Chase</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.12097v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.12097v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.12762">
    <id>http://arxiv.org/abs/2007.12762v2</id>
    <updated>2020-11-15T03:05:29Z</updated>
    <published>2020-07-24T20:33:23Z</published>
    <title>Sublinear-Time Algorithms for Computing &amp; Embedding Gap Edit Distance</title>
    <summary>  In this paper, we design new sublinear-time algorithms for solving the gap
edit distance problem and for embedding edit distance to Hamming distance. For
the gap edit distance problem, we give an $\tilde{O}(\frac{n}{k}+k^2)$-time
greedy algorithm that distinguishes between length-$n$ input strings with edit
distance at most $k$ and those with edit distance exceeding $(3k+5)k$. This is
an improvement and a simplification upon the result of Goldenberg, Krauthgamer,
and Saha [FOCS 2019], where the $k$ vs $\Theta(k^2)$ gap edit distance problem
is solved in $\tilde{O}(\frac{n}{k}+k^3)$ time. We further generalize our
result to solve the $k$ vs $k'$ gap edit distance problem in time
$\tilde{O}(\frac{nk}{k'}+k^2+ \frac{k^2}{k'}\sqrt{nk})$, strictly improving
upon the previously known bound $\tilde{O}(\frac{nk}{k'}+k^3)$. Finally, we
show that if the input strings do not have long highly periodic substrings,
then already the $k$ vs $(1+\epsilon)k$ gap edit distance problem can be solved
in sublinear time. Specifically, if the strings contain no substring of length
$\ell$ with period at most $2k$, then the running time we achieve is
$\tilde{O}(\frac{n}{\epsilon^2 k}+k^2\ell)$.
  We further give the first sublinear-time probabilistic embedding of edit
distance to Hamming distance. For any parameter $p$, our
$\tilde{O}(\frac{n}{p})$-time procedure yields an embedding with distortion
$O(kp)$, where $k$ is the edit distance of the original strings. Specifically,
the Hamming distance of the resultant strings is between $\frac{k-p+1}{p+1}$
and $O(k^2)$ with good probability. This generalizes the linear-time embedding
of Chakraborty, Goldenberg, and Kouck\'y [STOC 2016], where the resultant
Hamming distance is between $\frac k2$ and $O(k^2)$. Our algorithm is based on
a random walk over samples, which we believe will find other applications in
sublinear-time algorithms.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Barna Saha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/FOCS46700.2020.00112</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/FOCS46700.2020.00112" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">FOCS 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.12762v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.12762v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.13356">
    <id>http://arxiv.org/abs/2007.13356v2</id>
    <updated>2020-08-15T23:56:39Z</updated>
    <published>2020-07-27T08:17:57Z</published>
    <title>Optimal construction of a layer-ordered heap</title>
    <summary>  The layer-ordered heap (LOH) is a simple, recently proposed data structure
used in optimal selection on $X+Y$, thealgorithm with the best known runtime
for selection on $X_1+X_2+\cdots+X_m$, and the fastest method in practice for
computing the most abundant isotope peaks in a chemical compound. Here, we
introduce a few algorithms for constructing LOHs, analyze their complexity, and
demonstrate that one algorithm is optimal for building a LOH of any rank
$\alpha$. These results are shown to correspond with empirical experiments of
runtimes when applying the LOH construction algorithms to a common task in
machine learning.
</summary>
    <author>
      <name>Jake Pennington</name>
    </author>
    <author>
      <name>Patrick Kreitzberg</name>
    </author>
    <author>
      <name>Kyle Lucke</name>
    </author>
    <author>
      <name>Oliver Serang</name>
    </author>
    <link href="http://arxiv.org/abs/2007.13356v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.13356v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.13241">
    <id>http://arxiv.org/abs/2007.13241v1</id>
    <updated>2020-07-26T23:18:19Z</updated>
    <published>2020-07-26T23:18:19Z</published>
    <title>Beyond the Worst-Case Analysis of Algorithms (Introduction)</title>
    <summary>  One of the primary goals of the mathematical analysis of algorithms is to
provide guidance about which algorithm is the "best" for solving a given
computational problem. Worst-case analysis summarizes the performance profile
of an algorithm by its worst performance on any input of a given size,
implicitly advocating for the algorithm with the best-possible worst-case
performance. Strong worst-case guarantees are the holy grail of algorithm
design, providing an application-agnostic certification of an algorithm's
robustly good performance. However, for many fundamental problems and
performance measures, such guarantees are impossible and a more nuanced
analysis approach is called for. This chapter surveys several alternatives to
worst-case analysis that are discussed in detail later in the book.
</summary>
    <author>
      <name>Tim Roughgarden</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Chapter 1 of the book Beyond the Worst-Case Analysis of Algorithms,
  edited by Tim Roughgarden and published by Cambridge University Press (2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.13241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.13241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.10095">
    <id>http://arxiv.org/abs/2007.10095v1</id>
    <updated>2020-07-20T13:39:57Z</updated>
    <published>2020-07-20T13:39:57Z</published>
    <title>A Big Data Approach for Sequences Indexing on the Cloud via Burrows
  Wheeler Transform</title>
    <summary>  Indexing sequence data is important in the context of Precision Medicine,
where large amounts of ``omics'' data have to be daily collected and analyzed
in order to categorize patients and identify the most effective therapies. Here
we propose an algorithm for the computation of Burrows Wheeler transform
relying on Big Data technologies, i.e., Apache Spark and Hadoop. Our approach
is the first that distributes the index computation and not only the input
dataset, allowing to fully benefit of the available cloud resources.
</summary>
    <author>
      <name>Mario Randazzo</name>
    </author>
    <author>
      <name>Simona E. Rombo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at HELPLINE@ECAI2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.10095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.10095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.09442">
    <id>http://arxiv.org/abs/2009.09442v1</id>
    <updated>2020-09-20T14:46:48Z</updated>
    <published>2020-09-20T14:46:48Z</published>
    <title>TADOC: Text Analytics Directly on Compression</title>
    <summary>  This article provides a comprehensive description of Text Analytics Directly
on Compression (TADOC), which enables direct document analytics on compressed
textual data. The article explains the concept of TADOC and the challenges to
its effective realizations. Additionally, a series of guidelines and technical
solutions that effectively address those challenges, including the adoption of
a hierarchical compression method and a set of novel algorithms and data
structure designs, are presented. Experiments on six data analytics tasks of
various complexities show that TADOC can save 90.8% storage space and 87.9%
memory usage, while halving data processing times.
</summary>
    <author>
      <name>Feng Zhang</name>
    </author>
    <author>
      <name>Jidong Zhai</name>
    </author>
    <author>
      <name>Xipeng Shen</name>
    </author>
    <author>
      <name>Dalin Wang</name>
    </author>
    <author>
      <name>Zheng Chen</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <author>
      <name>Wenguang Chen</name>
    </author>
    <author>
      <name>Xiaoyong Du</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00778-020-00636-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00778-020-00636-3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 18 figures, VLDB Journal (2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.09442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.2; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.08588">
    <id>http://arxiv.org/abs/2009.08588v1</id>
    <updated>2020-09-18T02:08:40Z</updated>
    <published>2020-09-18T02:08:40Z</published>
    <title>Longest Common Subsequence in Sublinear Space</title>
    <summary>  We present the first $\mathrm{o}(n)$-space polynomial-time algorithm for
computing the length of a longest common subsequence. Given two strings of
length $n$, the algorithm runs in $\mathrm{O}(n^{3})$ time with
$\mathrm{O}\left(\frac{n \log^{1.5} n}{2^{\sqrt{\log n}}}\right)$ bits of
space.
</summary>
    <author>
      <name>Masashi Kiyomi</name>
    </author>
    <author>
      <name>Takashi Horiyama</name>
    </author>
    <author>
      <name>Yota Otachi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.08588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.08588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.04821">
    <id>http://arxiv.org/abs/2009.04821v1</id>
    <updated>2020-09-10T12:46:07Z</updated>
    <published>2020-09-10T12:46:07Z</published>
    <title>Pushdown and Lempel-Ziv Depth</title>
    <summary>  This paper expands upon existing and introduces new formulations of Bennett's
logical depth. In previously published work by Jordon and Moser, notions of
finite-state-depth and pushdown-depth were examined and compared. These were
based on finite-state transducers and information lossless pushdown compressors
respectively. Unfortunately a full separation between the two notions was not
established. This paper introduces a new formulation of pushdown-depth based on
restricting how fast a pushdown compressor's stack can grow. This improved
formulation allows us to do a full comparison by demonstrating the existence of
sequences with high finite-state-depth and low pushdown-depth, and vice-versa.
A new notion based on the Lempel-Ziv `78 algorithm is also introduced. Its
difference from finite-state-depth is shown by demonstrating the existence of a
Lempel-Ziv deep sequence that is not finite-state deep and vice versa.
Lempel-Ziv-depth's difference from pushdown-depth is shown by building
sequences that have a pushdown-depth of roughly $1/2$ but low Lempel-Ziv depth,
and a sequence with high Lempel-Ziv depth but low pushdown-depth. Properties of
all three notions are also discussed and proved.
</summary>
    <author>
      <name>Liam Jordon</name>
    </author>
    <author>
      <name>Philippe Moser</name>
    </author>
    <link href="http://arxiv.org/abs/2009.04821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.04821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.04827">
    <id>http://arxiv.org/abs/2009.04827v1</id>
    <updated>2020-09-10T12:56:00Z</updated>
    <published>2020-09-10T12:56:00Z</published>
    <title>A Normal Sequence Compressed by PPM$^*$ but not by Lempel-Ziv 78</title>
    <summary>  In this paper we compare the difference in performance of two of the
Prediction by Partial Matching (PPM) family of compressors (PPM$^*$ and the
original Bounded PPM algorithm) and the Lempel-Ziv 78 (LZ) algorithm. We
construct an infinite binary sequence whose worst-case compression ratio for
PPM$^*$ is $0$, while Bounded PPM's and LZ's best-case compression ratios are
at least $1/2$ and $1$ respectively. This sequence is an enumeration of all
binary strings in order of length, i.e. all strings of length $1$ followed by
all strings of length $2$ and so on. It is therefore normal, and is built using
repetitions of de Bruijn strings of increasing order
</summary>
    <author>
      <name>Liam Jordon</name>
    </author>
    <author>
      <name>Philippe Moser</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-67731-2_28</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-67731-2_28" rel="related"/>
    <link href="http://arxiv.org/abs/2009.04827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.04827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.03352">
    <id>http://arxiv.org/abs/2009.03352v1</id>
    <updated>2020-09-07T18:12:58Z</updated>
    <published>2020-09-07T18:12:58Z</published>
    <title>A Fast Randomized Algorithm for Finding the Maximal Common Subsequences</title>
    <summary>  Finding the common subsequences of $L$ multiple strings has many applications
in the area of bioinformatics, computational linguistics, and information
retrieval. A well-known result states that finding a Longest Common Subsequence
(LCS) for $L$ strings is NP-hard, e.g., the computational complexity is
exponential in $L$. In this paper, we develop a randomized algorithm, referred
to as {\em Random-MCS}, for finding a random instance of Maximal Common
Subsequence ($MCS$) of multiple strings. A common subsequence is {\em maximal}
if inserting any character into the subsequence no longer yields a common
subsequence. A special case of MCS is LCS where the length is the longest. We
show the complexity of our algorithm is linear in $L$, and therefore is
suitable for large $L$. Furthermore, we study the occurrence probability for a
single instance of MCS and demonstrate via both theoretical and experimental
studies that the longest subsequence from multiple runs of {\em Random-MCS}
often yields a solution to $LCS$.
</summary>
    <author>
      <name>Jin Cao</name>
    </author>
    <author>
      <name>Dewei Zhong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.03352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.03352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.03675">
    <id>http://arxiv.org/abs/2009.03675v1</id>
    <updated>2020-09-05T19:09:41Z</updated>
    <published>2020-09-05T19:09:41Z</published>
    <title>Space efficient merging of de Bruijn graphs and Wheeler graphs</title>
    <summary>  The merging of succinct data structures is a well established technique for
the space efficient construction of large succinct indexes. In the first part
of the paper we propose a new algorithm for merging succinct representations of
de Bruijn graphs. Our algorithm has the same asymptotic cost of the state of
the art algorithm for the same problem but it uses less than half of its
working space. A novel important feature of our algorithm, not found in any of
the existing tools, is that it can compute the Variable Order succinct
representation of the union graph within the same asymptotic time/space bounds.
In the second part of the paper we consider the more general problem of merging
succinct representations of Wheeler graphs, a recently introduced graph family
which includes as special cases de Bruijn graphs and many other known succinct
indexes based on the BWT or one of its variants. We show that Wheeler graphs
merging is in general a much more difficult problem, and we provide a space
efficient algorithm for the slightly simplified problem of determining whether
the union graph has an ordering that satisfies the Wheeler conditions.
</summary>
    <author>
      <name>Lavinia Egidi</name>
    </author>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 10 figures. arXiv admin note: text overlap with
  arXiv:1902.02889</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.03675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.03675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.02934">
    <id>http://arxiv.org/abs/2009.02934v1</id>
    <updated>2020-09-07T08:09:32Z</updated>
    <published>2020-09-07T08:09:32Z</published>
    <title>On prefix palindromic length of automatic words</title>
    <summary>  The prefix palindromic length $\mathrm{PPL}_{\mathbf{u}}(n)$ of an infinite
word $\mathbf{u}$ is the minimal number of concatenated palindromes needed to
express the prefix of length $n$ of $\mathbf{u}$. Since 2013, it is still
unknown if $\mathrm{PPL}_{\mathbf{u}}(n)$ is unbounded for every aperiodic
infinite word $\mathbf{u}$, even though this has been proven for almost all
aperiodic words. At the same time, the only well-known nontrivial infinite word
for which the function $\mathrm{PPL}_{\mathbf{u}}(n)$ has been precisely
computed is the Thue-Morse word $\mathbf{t}$. This word is $2$-automatic and,
predictably, its function $\mathrm{PPL}_{\mathbf{t}}(n)$ is $2$-regular, but is
this the case for all automatic words?
  In this paper, we prove that this function is $k$-regular for every
$k$-automatic word containing only a finite number of palindromes. For two such
words, namely the paperfolding word and the Rudin-Shapiro word, we derive a
formula for this function. Our computational experiments suggest that generally
this is not true: for the period-doubling word, the prefix palindromic length
does not look $2$-regular, and for the Fibonacci word, it does not look
Fibonacci-regular. If proven, these results would give rare (if not first)
examples of a natural function of an automatic word which is not regular.
</summary>
    <author>
      <name>Anna E. Frid</name>
    </author>
    <author>
      <name>Enzo Laborde</name>
    </author>
    <author>
      <name>Jarkko Peltom√§ki</name>
    </author>
    <link href="http://arxiv.org/abs/2009.02934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.02934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.02233">
    <id>http://arxiv.org/abs/2009.02233v1</id>
    <updated>2020-09-04T15:05:49Z</updated>
    <published>2020-09-04T15:05:49Z</published>
    <title>Access-Adaptive Priority Search Tree</title>
    <summary>  In this paper we show that the priority search tree of McCreight, which was
originally developed to satisfy a class of spatial search queries on
2-dimensional points, can be adapted to the problem of dynamically maintaining
a set of keys so that the query complexity adapts to the distribution of
queried keys. Presently, the best-known example of such a data structure is the
splay tree, which dynamically reconfigures itself during each query so that
frequently accessed keys move to the top of the tree and thus can be retrieved
with fewer queries than keys that are lower in the tree. However, while the
splay tree is conjectured to offer optimal adaptive amortized query complexity,
it may require O(n) for individual queries. We show that an access-adaptive
priority search tree (AAPST) can provide competitive adaptive query performance
while ensuring O(log n) worst-case query performance, thus potentially making
it more suitable for certain interactive (e.g.,online and real-time)
applications for which the response time must be bounded.
</summary>
    <author>
      <name>Haley Massa</name>
    </author>
    <author>
      <name>Jeffrey Uhlmann</name>
    </author>
    <link href="http://arxiv.org/abs/2009.02233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.02233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.01353">
    <id>http://arxiv.org/abs/2009.01353v1</id>
    <updated>2020-09-02T21:29:30Z</updated>
    <published>2020-09-02T21:29:30Z</published>
    <title>Zuckerli: A New Compressed Representation for Graphs</title>
    <summary>  Zuckerli is a scalable compression system meant for large real-world graphs.
Graphs are notoriously challenging structures to store efficiently due to their
linked nature, which makes it hard to separate them into smaller, compact
components. Therefore, effective compression is crucial when dealing with large
graphs, which can have billions of nodes and edges. Furthermore, a good
compression system should give the user fast and reasonably flexible access to
parts of the compressed data without requiring full decompression, which may be
unfeasible on their system. Zuckerli improves multiple aspects of WebGraph, the
current state-of-the-art in compressing real-world graphs, by using advanced
compression techniques and novel heuristic graph algorithms. It can produce
both a compressed representation for storage and one which allows fast direct
access to the adjacency lists of the compressed graph without decompressing the
entire graph. We validate the effectiveness of Zuckerli on real-world graphs
with up to a billion nodes and 90 billion edges, conducting an extensive
experimental evaluation of both compression density and decompression
performance. We show that Zuckerli-compressed graphs are 10% to 29% smaller,
and more than 20% in most cases, with a resource usage for decompression
comparable to that of WebGraph.
</summary>
    <author>
      <name>Luca Versari</name>
    </author>
    <author>
      <name>Iulia M. Comsa</name>
    </author>
    <author>
      <name>Alessio Conte</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <link href="http://arxiv.org/abs/2009.01353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.01353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2008.13209">
    <id>http://arxiv.org/abs/2008.13209v2</id>
    <updated>2020-11-26T06:01:46Z</updated>
    <published>2020-08-30T16:23:28Z</published>
    <title>Tight Bound for the Number of Distinct Palindromes in a Tree</title>
    <summary>  For an undirected tree with $n$ edges labelled by single letters, we consider
its substrings, which are labels of the simple paths between pairs of nodes. We
prove that there are $O(n^{1.5})$ different palindromic substrings. This solves
an open problem of Brlek, Lafreni\`ere, and Proven\c{c}al (DLT 2015), who gave
a matching lower-bound construction. Hence, we settle the tight bound of
$\Theta(n^{1.5})$ for the maximum palindromic complexity of trees. For standard
strings, i.e., for paths, the palindromic complexity is $n+1$. We also propose
$O(n^{1.5} \log{n})$-time algorithm for reporting all distinct palindromes in
an undirected tree with $n$ edges.
</summary>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Wale≈Ñ</name>
    </author>
    <link href="http://arxiv.org/abs/2008.13209v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.13209v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.08840">
    <id>http://arxiv.org/abs/2010.08840v1</id>
    <updated>2020-10-17T18:24:08Z</updated>
    <published>2020-10-17T18:24:08Z</published>
    <title>Lazy Search Trees</title>
    <summary>  We introduce the lazy search tree data structure. The lazy search tree is a
comparison-based data structure on the pointer machine that supports
order-based operations such as rank, select, membership, predecessor,
successor, minimum, and maximum while providing dynamic operations insert,
delete, change-key, split, and merge. We analyze the performance of our data
structure based on a partition of current elements into a set of gaps
$\{\Delta_i\}$ based on rank. A query falls into a particular gap and splits
the gap into two new gaps at a rank $r$ associated with the query operation. If
we define $B = \sum_i |\Delta_i| \log_2(n/|\Delta_i|)$, our performance over a
sequence of $n$ insertions and $q$ distinct queries is $O(B + \min(n \log \log
n, n \log q))$. We show $B$ is a lower bound.
  Effectively, we reduce the insertion time of binary search trees from
$\Theta(\log n)$ to $O(\min(\log(n/|\Delta_i|) + \log \log |\Delta_i|, \; \log
q))$, where $\Delta_i$ is the gap in which the inserted element falls. Over a
sequence of $n$ insertions and $q$ queries, a time bound of $O(n \log q + q
\log n)$ holds; better bounds are possible when queries are non-uniformly
distributed. As an extreme case of non-uniformity, if all queries are for the
minimum element, the lazy search tree performs as a priority queue with $O(\log
\log n)$ time insert and decrease-key operations. The same data structure
supports queries for any rank, interpolating between binary search trees and
efficient priority queues.
  Lazy search trees can be implemented to operate mostly on arrays, requiring
only $O(\min(q, n))$ pointers. Via direct reduction, our data structure also
supports the efficient access theorems of the splay tree, providing a powerful
data structure for non-uniform element access, both when the number of accesses
is small and large.
</summary>
    <author>
      <name>Bryce Sandlund</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in FOCS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.08840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.09014">
    <id>http://arxiv.org/abs/2010.09014v1</id>
    <updated>2020-10-18T16:16:18Z</updated>
    <published>2020-10-18T16:16:18Z</published>
    <title>Solving Shisen-Sho boards</title>
    <summary>  We give a simple proof of that determining solvability of Shisen-Sho boards
is NP-complete. Furthermore, we show that under realistic assumptions, one can
compute in logarithmic time if two tiles form a playable pair.
  We combine an implementation of the algoritm to test playability of pairs
with my earlier algorithm to solve Mahjong Solitaire boards with peeking, to
obtain an algorithm to solve Shisen-Sho boards. We sample several Shisen-Sho
and Mahjong Solitaire layouts for solvability for Shisen-Sho and Mahjong
Solitaire.
</summary>
    <author>
      <name>Michiel de Bondt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.09014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.07960">
    <id>http://arxiv.org/abs/2010.07960v1</id>
    <updated>2020-10-15T18:05:52Z</updated>
    <published>2020-10-15T18:05:52Z</published>
    <title>Efficient constructions of the Prefer-same and Prefer-opposite de Bruijn
  sequences</title>
    <summary>  The greedy Prefer-same de Bruijn sequence construction was first presented by
Eldert et al.[AIEE Transactions 77 (1958)]. As a greedy algorithm, it has one
major downside: it requires an exponential amount of space to store the length
$2^n$ de Bruijn sequence. Though de Bruijn sequences have been heavily studied
over the last 60 years, finding an efficient construction for the Prefer-same
de Bruijn sequence has remained a tantalizing open problem. In this paper, we
unveil the underlying structure of the Prefer-same de Bruijn sequence and solve
the open problem by presenting an efficient algorithm to construct it using
$O(n)$ time per bit and only $O(n)$ space. Following a similar approach, we
also present an efficient algorithm to construct the Prefer-opposite de Bruijn
sequence.
</summary>
    <author>
      <name>Evan Sala</name>
    </author>
    <author>
      <name>Joe Sawada</name>
    </author>
    <author>
      <name>Abbas Alhakim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.07960v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07960v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.07076">
    <id>http://arxiv.org/abs/2010.07076v1</id>
    <updated>2020-10-14T13:25:51Z</updated>
    <published>2020-10-14T13:25:51Z</published>
    <title>Contextual Pattern Matching</title>
    <summary>  The research on indexing repetitive string collections has focused on the
same search problems used for regular string collections, though they can make
little sense in this scenario. For example, the basic pattern matching query
"list all the positions where pattern $P$ appears" can produce huge outputs
when $P$ appears in an area shared by many documents. All those occurrences are
essentially the same.
  In this paper we propose a new query that can be more appropriate in these
collections, which we call {\em contextual pattern matching}. The basic query
of this type gives, in addition to $P$, a context length $\ell$, and asks to
report the occurrences of all {\em distinct} strings $XPY$, with
$|X|=|Y|=\ell$.
  While this query is easily solved in optimal time and linear space, we focus
on using space related to the repetitiveness of the text collection and present
the first solution of this kind. Letting $\ovr$ be the maximum of the number of
runs in the BWT of the text $T[1..n]$ and of its reverse, our structure uses
$O(\ovr\log(n/\ovr))$ space and finds the $c$ contextual occurrences $XPY$ of
$(P,\ell)$ in time $O(|P| + c \log n)$. We give other space/time tradeoffs as
well, for compressed and uncompressed indexes.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Improvements and corrections over my SPIRE 2020 paper with the same
  title</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.07076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.05805">
    <id>http://arxiv.org/abs/2010.05805v3</id>
    <updated>2021-02-11T14:54:45Z</updated>
    <published>2020-10-12T16:01:42Z</published>
    <title>New Sublinear Algorithms and Lower Bounds for LIS Estimation</title>
    <summary>  Estimating the length of the longest increasing subsequence (LIS) in an array
is a problem of fundamental importance. Despite the significance of the LIS
estimation problem and the amount of attention it has received, there are
important aspects of the problem that are not yet fully understood. There are
no better lower bounds for LIS estimation than the obvious bounds implied by
testing monotonicity (for adaptive or nonadaptive algorithms). In this paper,
we give the first nontrivial lower bound on the complexity of LIS estimation,
and also provide novel algorithms that complement our lower bound.
  Specifically, for every constant $\epsilon \in (0,1)$, every nonadaptive
algorithm that outputs an estimate of the length of the LIS in an array of
length $n$ to within an additive error of $\epsilon \cdot n$ has to make
$\log^{\Omega(\log (1/\epsilon))} n)$ queries. Next, we design nonadaptive LIS
estimation algorithms whose complexity decreases as the the number of distinct
values, $r$, in the array decreases. We first present a simple algorithm that
makes $\tilde{O}(r/\epsilon^3)$ queries and approximates the LIS length with an
additive error bounded by $\epsilon n$. We then use it to construct a
nonadaptive algorithm with query complexity $\tilde{O}(\sqrt{r} \cdot
\text{poly}(1/\lambda))$ that, for an array with LIS length at least $\lambda
n$, outputs a multiplicative $\Omega(\lambda)$-approximation to the LIS length.
  Finally, we describe a nonadaptive erasure-resilient tester for sortedness,
with query complexity $O(\log n)$. Our result implies that nonadaptive tolerant
testing is strictly harder than nonadaptive erasure-resilient testing for the
natural property of monotonicity.
</summary>
    <author>
      <name>Ilan Newman</name>
    </author>
    <author>
      <name>Nithin Varma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.05805v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.05805v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.05005">
    <id>http://arxiv.org/abs/2010.05005v2</id>
    <updated>2020-11-13T07:10:25Z</updated>
    <published>2020-10-10T14:01:21Z</published>
    <title>Decode efficient prefix codes</title>
    <summary>  Data compression is used in a wide variety of tasks, including compression of
databases, large learning models, videos, images, etc. The cost of
decompressing (decoding) data can be prohibitive for certain real-time
applications. In many scenarios, it is acceptable to sacrifice (to some extent)
on compression in the interest of fast decoding. In this work, we introduce and
study a novel problem of finding a prefix tree having the best decode time
under the constraint that the code length does not exceed a certain threshold
for a natural class of memory access cost functions that use blocking (also
referred to as lookup tables), i.e., these decoding schemes access multiple
prefix tree entries in a single access, using associative memory table
look-ups. We present (i) an exact algorithm for this problem that is polynomial
in the number of characters and the codelength; (ii) a strongly polynomial
pseudo approximation algorithm that achieves the best decode time by relaxing
the codelength constraint by a small factor; and (iii) a more efficient version
of the pseudo approximation algorithm that achieves near optimal decode time by
relaxing the codelength constraint by a small factor. All our algorithms are
based on dynamic programming and capitalize on an interesting structure of the
optimal solution. To the best of our knowledge, there is no prior work that
gives any provable theoretical guarantees for minimizing decode time along with
the code length. We also demonstrate the performance benefits of our algorithm
on different types of real-world data sets, namely (i) a deep learning model
(Mobilenet-V2); (ii) image and (iii) text data. We also implement and evaluate
the performance of our algorithms on the GPU.
</summary>
    <author>
      <name>Shashwat Banchhor</name>
    </author>
    <author>
      <name>Rishikesh Gajjala</name>
    </author>
    <author>
      <name>Yogish Sabharwal</name>
    </author>
    <author>
      <name>Sandeep Sen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.05005v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.05005v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.04752">
    <id>http://arxiv.org/abs/2010.04752v1</id>
    <updated>2020-10-09T18:20:14Z</updated>
    <published>2020-10-09T18:20:14Z</published>
    <title>A Tale of Two Trees: New Analysis for AVL Tree and Binary Heap</title>
    <summary>  In this paper, we provide new insights and analysis for the two elementary
tree-based data structures - the AVL tree and binary heap. We presented two
simple properties that gives a more direct way of relating the size of an AVL
tree and the Fibonacci recurrence to establish the AVL tree's logarithmic
height. We then give a potential function-based analysis of the bottom-up heap
construction to get a simpler and tight bound for its worst-case running-time.
</summary>
    <author>
      <name>Russel L. Villacarlos</name>
    </author>
    <author>
      <name>Jaime M. Samaniego</name>
    </author>
    <author>
      <name>Arian J. Jacildo</name>
    </author>
    <author>
      <name>Maria Art Antonette D. Clari√±o</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.04752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.04752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.11789">
    <id>http://arxiv.org/abs/2009.11789v1</id>
    <updated>2020-09-24T16:33:22Z</updated>
    <published>2020-09-24T16:33:22Z</published>
    <title>A Case for Partitioned Bloom Filters</title>
    <summary>  In a partitioned Bloom Filter the $m$ bit vector is split into $k$ disjoint
$m/k$ sized parts, one per hash function. Contrary to hardware designs, where
they prevail, software implementations mostly adopt standard Bloom filters,
considering partitioned filters slightly worse, due to the slightly larger
false positive rate (FPR). In this paper, by performing an in-depth analysis,
first we show that the FPR advantage of standard Bloom filters is smaller than
thought; more importantly, by studying the per-element FPR, we show that
standard Bloom filters have weak spots in the domain: elements which will be
tested as false positives much more frequently than expected. This is relevant
in scenarios where an element is tested against many filters, e.g., in packet
forwarding. Moreover, standard Bloom filters are prone to exhibit extremely
weak spots if naive double hashing is used, something occurring in several,
even mainstream, libraries. Partitioned Bloom filters exhibit a uniform
distribution of the FPR over the domain and are robust to the naive use of
double hashing, having no weak spots. Finally, by surveying several usages
other than testing set membership, we point out the many advantages of having
disjoint parts: they can be individually sampled, extracted, added or retired,
leading to superior designs for, e.g., SIMD usage, size reduction, test of set
disjointness, or duplicate detection in streams. Partitioned Bloom filters are
better, and should replace the standard form, both in general purpose libraries
and as the base for novel designs.
</summary>
    <author>
      <name>Paulo S√©rgio Almeida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.11789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.11559">
    <id>http://arxiv.org/abs/2009.11559v1</id>
    <updated>2020-09-24T09:13:17Z</updated>
    <published>2020-09-24T09:13:17Z</published>
    <title>Dynamic Similarity Search on Integer Sketches</title>
    <summary>  Similarity-preserving hashing is a core technique for fast similarity
searches, and it randomly maps data points in a metric space to strings of
discrete symbols (i.e., sketches) in the Hamming space. While traditional
hashing techniques produce binary sketches, recent ones produce integer
sketches for preserving various similarity measures. However, most similarity
search methods are designed for binary sketches and inefficient for integer
sketches. Moreover, most methods are either inapplicable or inefficient for
dynamic datasets, although modern real-world datasets are updated over time. We
propose dynamic filter trie (DyFT), a dynamic similarity search method for both
binary and integer sketches. An extensive experimental analysis using large
real-world datasets shows that DyFT performs superiorly with respect to
scalability, time performance, and memory efficiency. For example, on a huge
dataset of 216 million data points, DyFT performs a similarity search 6,000
times faster than a state-of-the-art method while reducing to one-thirteenth in
memory.
</summary>
    <author>
      <name>Shunsuke Kanda</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE ICDM 2020 as a full paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.11559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.10045">
    <id>http://arxiv.org/abs/2009.10045v1</id>
    <updated>2020-09-21T17:36:38Z</updated>
    <published>2020-09-21T17:36:38Z</published>
    <title>Space/time-efficient RDF stores based on circular suffix sorting</title>
    <summary>  In recent years, RDF has gained popularity as a format for the standardized
publication and exchange of information in the Web of Data. In this paper we
introduce RDFCSA, a data structure that is able to self-index an RDF dataset in
small space and supports efficient querying. RDFCSA regards the triples of the
RDF store as short circular strings and applies suffix sorting on those
strings, so that triple-pattern queries reduce to prefix searching on the
string set. The RDF store is then represented compactly using a Compressed
Suffix Array (CSA), a proved technology in text indexing that efficiently
supports prefix searches. Our experimental evaluation shows that RDFCSA is able
to answer triple-pattern queries in a few microseconds per result while using
less than 60% of the space required by the raw original data. We also support
join queries, which provide the basis for full SPARQL query support. Even
though smaller-space solutions exist, as well as faster ones, RDFCSA is shown
to provide an excellent space/time tradeoff, with fast and consistent query
times within much less space than alternatives that compete in time.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Ana Cerdeira-Pena</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Antonio Fari√±a</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE TKDE for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.10045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.10045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.10870">
    <id>http://arxiv.org/abs/2011.10870v1</id>
    <updated>2020-11-21T21:32:05Z</updated>
    <published>2020-11-21T21:32:05Z</published>
    <title>Erd√∂s-Szekeres Partitioning Problem</title>
    <summary>  In this note, we present a substantial improvement on the computational
complexity of the Erd\"{o}s-Szekeres partitioning problem and review recent
works on dynamic \textsf{LIS}.
</summary>
    <author>
      <name>Michael Mitzenmacher</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <link href="http://arxiv.org/abs/2011.10870v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10870v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.11772">
    <id>http://arxiv.org/abs/2011.11772v1</id>
    <updated>2020-11-23T22:30:32Z</updated>
    <published>2020-11-23T22:30:32Z</published>
    <title>Selectable Heaps and Optimal Lazy Search Trees</title>
    <summary>  We show the $O(\log n)$ time extract minimum function of efficient priority
queues can be generalized to the extraction of the $k$ smallest elements in
$O(k \log(n/k))$ time, where we define $\log(x)$ as $\max(\log_2(x), 1)$. We
first show heap-ordered tree selection (Kaplan et al., SOSA '19) can be applied
on the heap-ordered trees of the classic Fibonacci heap to support the
extraction in $O(k \log(n/k))$ amortized time. We then show selection is
possible in a priority queue with optimal worst-case guarantees by applying
heap-ordered tree selection on Brodal queues (SODA '96), supporting the
operation in $O(k \log(n/k))$ worst-case time. Via a reduction from the
multiple selection problem, $\Omega(k \log(n/k))$ time is necessary if
insertion is supported in $o(\log n)$ time.
  We then apply the result to lazy search trees (Sandlund &amp; Wild, FOCS '20),
creating a new interval data structure based on selectable heaps. This gives
optimal $O(B+n)$ time lazy search tree performance, lowering insertion
complexity into a gap $\Delta_i$ to $O(\log(n/|\Delta_i|))$ time. An $O(1)$
time merge operation is also made possible when used as a priority queue, among
other situations. If Brodal queues are used, runtimes of the lazy search tree
can be made worst-case in the general case of two-sided gaps. The presented
data structure makes fundamental use of soft heaps (Chazelle, J. ACM '00),
biased search trees, and efficient priority queues, approaching the
theoretically-best data structure for ordered data.
</summary>
    <author>
      <name>Bryce Sandlund</name>
    </author>
    <author>
      <name>Lingyi Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2011.11772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.11772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.10874">
    <id>http://arxiv.org/abs/2011.10874v2</id>
    <updated>2021-03-09T20:37:39Z</updated>
    <published>2020-11-21T21:39:37Z</published>
    <title>Improved Dynamic Algorithms for Longest Increasing Subsequence</title>
    <summary>  We study dynamic algorithms for the longest increasing subsequence
(\textsf{LIS}) problem. A dynamic \textsf{LIS} algorithm maintains a sequence
subject to operations of the following form arriving one by one: (i) insert an
element, (ii) delete an element, or (iii) substitute an element for another.
After performing each operation, the algorithm must report the length of the
longest increasing subsequence of the current sequence.
  Our main contribution is the first exact dynamic \textsf{LIS} algorithm with
sublinear update time. More precisely, we present a randomized algorithm that
performs each operation in time $\tilde O(n^{2/3})$ and after each update,
reports the answer to the \textsf{LIS} problem correctly with high probability.
We use several novel techniques and observations for this algorithm that may
find their applications in future work.
  In the second part of the paper, we study approximate dynamic \textsf{LIS}
algorithms, which are allowed to underestimate the solution size within a
bounded multiplicative factor. In this setting, we give a deterministic
algorithm with update time $O(n^{o(1)})$ and approximation factor $1-o(1)$.
This result substantially improves upon the previous work of Mitzenmacher and
Seddighin (STOC'20) that presents an $\Omega(\epsilon
^{O(1/\epsilon)})$-approximation algorithm with update time $\tilde
O(n^\epsilon)$ for any constant $\epsilon > 0$.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <link href="http://arxiv.org/abs/2011.10874v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10874v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.09761">
    <id>http://arxiv.org/abs/2011.09761v2</id>
    <updated>2020-12-03T12:13:19Z</updated>
    <published>2020-11-19T10:39:58Z</published>
    <title>Fully Dynamic Approximation of LIS in Polylogarithmic Time</title>
    <summary>  We revisit the problem of maintaining the longest increasing subsequence
(LIS) of an array under (i) inserting an element, and (ii) deleting an element
of an array. In a recent breakthrough, Mitzenmacher and Seddighin [STOC 2020]
designed an algorithm that maintains an
$\mathcal{O}((1/\epsilon)^{\mathcal{O}(1/\epsilon)})$-approximation of LIS
under both operations with worst-case update time $\mathcal{\tilde
O}(n^{\epsilon})$, for any constant $\epsilon>0$. We exponentially improve on
their result by designing an algorithm that maintains an
$(1+\epsilon)$-approximation of LIS under both operations with worst-case
update time $\mathcal{\tilde O}(\epsilon^{-5})$. Instead of working with the
grid packing technique introduced by Mitzenmacher and Seddighin, we take a
different approach building on a new tool that might be of independent
interest: LIS sparsification.
  A particularly interesting consequence of our result is an improved solution
for the so-called Erd\H{o}s-Szekeres partitioning, in which we seek a partition
of a given permutation of $\{1,2,\ldots,n\}$ into $\mathcal{O}(\sqrt{n})$
monotone subsequences. This problem has been repeatedly stated as one of the
natural examples in which we see a large gap between the decision-tree
complexity and algorithmic complexity. The result of Mitzenmacher and Seddighin
implies an $\mathcal{O}(n^{1+\epsilon})$ time solution for this problem, for
any $\epsilon>0$. Our algorithm (in fact, its simpler decremental version)
further improves this to $\mathcal{\tilde O}(n)$.
</summary>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Wojciech Janczewski</name>
    </author>
    <link href="http://arxiv.org/abs/2011.09761v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.09761v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.10008">
    <id>http://arxiv.org/abs/2011.10008v2</id>
    <updated>2020-12-13T10:01:03Z</updated>
    <published>2020-11-19T18:07:53Z</published>
    <title>Subpath Queries on Compressed Graphs: a Survey</title>
    <summary>  Text indexing is a classical algorithmic problem that has been studied for
over four decades: given a text $T$, pre-process it off-line so that, later, we
can quickly count and locate the occurrences of any string (the query pattern)
in $T$ in time proportional to the query's length. The earliest optimal-time
solution to the problem, the suffix tree, dates back to 1973 and requires up to
two orders of magnitude more space than the plain text just to be stored. In
the year 2000, two breakthrough works showed that efficient queries can be
achieved without this space overhead: a fast index be stored in a space
proportional to the text's entropy. These contributions had an enormous impact
in bioinformatics: nowadays, virtually any DNA aligner employs compressed
indexes. Recent trends considered more powerful compression schemes (dictionary
compressors) and generalizations of the problem to labeled graphs: after all,
texts can be viewed as labeled directed paths. In turn, since finite state
automata can be considered as a particular case of labeled graphs, these
findings created a bridge between the fields of compressed indexing and regular
language theory, ultimately allowing to index regular languages and promising
to shed new light on problems such as regular expression matching. This survey
is a gentle introduction to the main landmarks of the fascinating journey that
took us from suffix trees to today's compressed indexes for labeled graphs and
regular languages.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed some typos and references to Boyer-Moore-Galil's and
  Apostolico-Giancarlo's algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.10008v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10008v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.08119">
    <id>http://arxiv.org/abs/2011.08119v1</id>
    <updated>2020-11-16T17:31:51Z</updated>
    <published>2020-11-16T17:31:51Z</published>
    <title>The Longest Run Subsequence Problem: Further Complexity Results</title>
    <summary>  Longest Run Subsequence is a problem introduced recently in the context of
the scaffolding phase of genome assembly (Schrinner et al.,WABI 2020). The
problem asks for a maximum length subsequence of a given string that contains
at most one run for each symbol (a run is a maximum substring of consecutive
identical symbols). The problem has been shown to be NP-hard and to be
fixed-parameter tractable when the parameter is the size of the alphabet on
which the input string is defined. In this paper we further investigate the
complexity of the problem and we show that it is fixed-parameter tractable when
it is parameterized by the number of runs in a solution, a smaller parameter.
Moreover, we investigate the kernelization complexity of Longest Run
Subsequence and we prove that it does not admit a polynomial kernel when
parameterized by the size of the alphabet or by the number of runs. Finally, we
consider the restriction of Longest Run Subsequence when each symbol has at
most two occurrences in the input string and we show that it is APX-hard.
</summary>
    <author>
      <name>Riccardo Dondi</name>
    </author>
    <author>
      <name>Florian Sikora</name>
    </author>
    <link href="http://arxiv.org/abs/2011.08119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.08119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.07999">
    <id>http://arxiv.org/abs/2011.07999v1</id>
    <updated>2020-11-13T03:16:02Z</updated>
    <published>2020-11-13T03:16:02Z</published>
    <title>A grammar compressor for collections of reads with applications to the
  construction of the BWT</title>
    <summary>  We describe a grammar for DNA sequencing reads from which we can compute the
BWT directly. Our motivation is to perform in succinct space genomic analyses
that require complex string queries not yet supported by repetition-based
self-indexes. Our approach is to store the set of reads as a grammar, but when
required, compute its BWT to carry out the analysis by using self-indexes. Our
experiments in real data showed that the space reduction we achieve with our
compressor is competitive with LZ-based methods and better than entropy-based
approaches. Compared to other popular grammars, in this kind of data, we
achieve, on average, 12\% of extra compression and require less working space
and time.
</summary>
    <author>
      <name>Diego D√≠az-Dom√≠nguez</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/2011.07999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.07143">
    <id>http://arxiv.org/abs/2011.07143v1</id>
    <updated>2020-11-13T21:57:18Z</updated>
    <published>2020-11-13T21:57:18Z</published>
    <title>Substring Query Complexity of String Reconstruction</title>
    <summary>  Suppose an oracle knows a string $S$ that is unknown to us and we want to
determine. The oracle can answer queries of the form "Is $s$ a substring of
$S$?". The \emph{Substring Query Complexity} of a string $S$, denoted
$\chi(S)$, is the minimum number of adaptive substring queries that are needed
to exactly reconstruct (or learn) $S$. It has been introduced in 1995 by Skiena
and Sundaram, who showed that $\chi(S) \geq \sigma n/4 -O(n)$ in the worst
case, where $\sigma$ is the size of the alphabet of $S$ and $n$ its length, and
gave an algorithm that spends $(\sigma-1)n+O(\sigma \sqrt{n})$ queries to
reconstruct $S$. We show that for any binary string $S$, $\chi(S)$ is
asymptotically equal to the Kolmogorov complexity of $S$ and therefore lower
bounds any other measure of compressibility. However, since this result does
not yield an efficient algorithm for the reconstruction, we present new
algorithms to compute a set of substring queries whose size grows as a function
of other known measures of complexity, e.g., the number {\sf rle} of runs in
$S$, the size $g$ of the smallest grammar producing (only) $S$ or the size
$z_{no}$ of the non-overlapping LZ77 factorization of $S$. We first show that
any string of length $n$ over an integer alphabet of size $\sigma$ with {\sf
rle} runs can be reconstructed with $q=O({\sf rle} (\sigma + \log \frac{n}{{\sf
rle}}))$ substring queries in linear time and space. We then present an
algorithm that spends $q \in O(\sigma g\log n) \subseteq O(\sigma z_{no}\log
(n/z_{no})\log n)$ substring queries and runs in $O(n(\log n + \log \sigma)+
q)$ time using linear space. This algorithm actually reconstructs the suffix
tree of the string using a dynamic approach based on the centroid
decomposition.
</summary>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <link href="http://arxiv.org/abs/2011.07143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.05610">
    <id>http://arxiv.org/abs/2011.05610v2</id>
    <updated>2021-02-11T06:56:52Z</updated>
    <published>2020-11-11T07:50:10Z</published>
    <title>PHONI: Streamed Matching Statistics with Multi-Genome References</title>
    <summary>  Computing the matching statistics of patterns with respect to a text is a
fundamental task in bioinformatics, but a formidable one when the text is a
highly compressed genomic database. Bannai et al. gave an efficient solution
for this case, which Rossi et al. recently implemented, but it uses two passes
over the patterns and buffers a pointer for each character during the first
pass. In this paper, we simplify their solution and make it streaming, at the
cost of slowing it down slightly. This means that, first, we can compute the
matching statistics of several long patterns (such as whole human chromosomes)
in parallel while still using a reasonable amount of RAM; second, we can
compute matching statistics online with low latency and thus quickly recognize
when a pattern becomes incompressible relative to the database.
</summary>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Dominik K√∂ppl</name>
    </author>
    <author>
      <name>Ben Langmead</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Alejandro Pacheco</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Our code is available at https://github.com/koeppl/phoni</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.05610v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.05610v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.04010">
    <id>http://arxiv.org/abs/2011.04010v1</id>
    <updated>2020-11-08T16:09:20Z</updated>
    <published>2020-11-08T16:09:20Z</published>
    <title>Scout Algorithm For Fast Substring Matching</title>
    <summary>  Exact substring matching is a common task in many software applications.
Despite the existence of several algorithms for finding whether or not a
pattern string is present in a target string, the most common implementation is
a na\"ive, brute force approach. Alternative approaches either do not provide
enough of a benefit for the added complexity, or are impractical for modern
character sets, e.g., Unicode. We present a new algorithm, Scout, that is
straightforward, quick and appropriate for all applications. We also compare
the performance characteristics of the Scout algorithm with several others.
</summary>
    <author>
      <name>Anand Natrajan</name>
    </author>
    <author>
      <name>Mallige Anand</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.04010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.04010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.08589">
    <id>http://arxiv.org/abs/2012.08589v1</id>
    <updated>2020-12-15T20:00:01Z</updated>
    <published>2020-12-15T20:00:01Z</published>
    <title>Sorting Lists with Equal Keys Using Mergesort in Linear Time</title>
    <summary>  This article introduces a new optimization method to improve mergesort's
runtime complexity, when sorting sequences that have equal keys to $O(n log_2
k)$, where $k$ is the number of distinct keys in the sequence. When $k$ is
constant, it is evident that mergesort is capable of achieving linear time by
utilizing linked lists as its underlying data structure. Mergesort linked list
implementations can be optimized by introducing a new mechanism to group
elements with equal keys together, thus allowing merge algorithm to achieve
linear time.
</summary>
    <author>
      <name>Albert Tedja</name>
    </author>
    <link href="http://arxiv.org/abs/2012.08589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.08878">
    <id>http://arxiv.org/abs/2012.08878v1</id>
    <updated>2020-12-16T11:41:45Z</updated>
    <published>2020-12-16T11:41:45Z</published>
    <title>Greedy-reduction from Shortest Linear Superstring to Shortest Circular
  Superstring</title>
    <summary>  A superstring of a set of strings correspond to a string which contains all
the other strings as substrings. The problem of finding the Shortest Linear
Superstring is a well-know and well-studied problem in stringology. We present
here a variant of this problem, the Shortest Circular Superstring problem where
the sought superstring is a circular string. We show a strong link between
these two problems and prove that the Shortest Circular Superstring problem is
NP-complete. Moreover, we propose a new conjecture on the approximation ratio
of the Shortest Circular Superstring problem.
</summary>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Eric Rivals</name>
    </author>
    <link href="http://arxiv.org/abs/2012.08878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.10092">
    <id>http://arxiv.org/abs/2012.10092v2</id>
    <updated>2021-02-04T03:26:22Z</updated>
    <published>2020-12-18T07:53:48Z</published>
    <title>The Parameterized Suffix Tray</title>
    <summary>  Let $\Sigma$ and $\Pi$ be disjoint alphabets, respectively called the static
alphabet and the parameterized alphabet. Two strings $x$ and $y$ over $\Sigma
\cup \Pi$ of equal length are said to parameterized match (p-match) if there
exists a renaming bijection $f$ on $\Sigma$ and $\Pi$ which is identity on
$\Sigma$ and maps the characters of $x$ to those of $y$ so that the two strings
become identical. The indexing version of the problem of finding p-matching
occurrences of a given pattern in the text is a well-studied topic in string
matching. In this paper, we present a state-of-the-art indexing structure for
p-matching called the parameterized suffix tray of an input text $T$, denoted
by $\mathsf{PSTray}(T)$. We show that $\mathsf{PSTray}(T)$ occupies $O(n)$
space and supports pattern matching queries in $O(m + \log (\sigma+\pi) +
\mathit{occ})$ time, where $n$ is the length of $T$, $m$ is the length of a
query pattern $P$, $\pi$ is the number of distinct symbols of $|\Pi|$ in $T$,
$\sigma$ is the number of distinct symbols of $|\Sigma|$ in $T$ and
$\mathit{occ}$ is the number of p-matching occurrences of $P$ in $T$. We also
present how to build $\mathsf{PSTray}(T)$ in $O(n)$ time from the parameterized
suffix tree of $T$.
</summary>
    <author>
      <name>Noriki Fujisato</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for CIAC 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.10092v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.10092v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.09376">
    <id>http://arxiv.org/abs/2012.09376v2</id>
    <updated>2020-12-25T12:45:42Z</updated>
    <published>2020-12-17T03:13:45Z</published>
    <title>Quantum Algorithm for Lexicographically Minimal String Rotation</title>
    <summary>  Lexicographically minimal string rotation (LMSR) is a problem to find the
minimal one among all rotations of a string in the lexicographical order, which
is widely used in equality checking of graphs, polygons, automata and chemical
structures.
  In this paper, we propose an $O(n^{3/4})$ quantum query algorithm for LMSR.
In particular, the algorithm has average-case query complexity $O(\sqrt n \log
n)$, which is shown to be asymptotically optimal up to a polylogarithmic
factor, compared with its $\Omega\left(\sqrt{n/\log n}\right)$ lower bound.
Furthermore, we claim that our quantum algorithm outperforms any (classical)
randomized algorithms in both worst-case and average-case query complexities by
showing that every (classical) randomized algorithm for LMSR has worst-case
query complexity $\Omega(n)$ and average-case query complexity $\Omega(n/\log
n)$.
  Our quantum algorithm for LMSR is developed in a framework of nested quantum
algorithms, based on two new results: (i) an $O(\sqrt{n})$ (optimal) quantum
minimum finding on bounded-error quantum oracles; and (ii) its $O\left(\sqrt{n
\log(1/\varepsilon)}\right)$ (optimal) error reduction. As a byproduct, we
obtain some better upper bounds of independent interest: (i) $O(\sqrt{N})$
(optimal) for constant-depth MIN-MAX trees on $N$ variables; and (ii)
$O(\sqrt{n \log m})$ for pattern matching which removes
$\operatorname{polylog}(n)$ factors.
</summary>
    <author>
      <name>Qisheng Wang</name>
    </author>
    <author>
      <name>Mingsheng Ying</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 6 algorithms, minor corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.09376v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.09376v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.07892">
    <id>http://arxiv.org/abs/2012.07892v1</id>
    <updated>2020-12-14T19:15:19Z</updated>
    <published>2020-12-14T19:15:19Z</published>
    <title>A New Approach to Regular &amp; Indeterminate Strings</title>
    <summary>  In this paper we propose a new, more appropriate definition of regular and
indeterminate strings. A regular string is one that is "isomorphic" to a string
whose entries all consist of a single letter, but which nevertheless may itself
include entries containing multiple letters. A string that is not regular is
said to be indeterminate. We begin by proposing a new model for the
representation of strings, regular or indeterminate, then go on to describe a
linear time algorithm to determine whether or not a string $x = x[1..n]$ is
regular and, if so, to replace it by a lexicographically least (lex-least)
string $y$ whose entries are all single letters. Furthermore, we connect the
regularity of a string to the transitive closure problem on a graph, which in
our special case can be efficiently solved. We then introduce the idea of a
feasible palindrome array MP of a string, and prove that every feasible MP
corresponds to some (regular or indeterminate) string. We describe an algorithm
that constructs a string $x$ corresponding to given feasible MP, while ensuring
that whenever possible $x$ is regular and if so, then lex-least. A final
section outlines new research directions suggested by this changed perspective
on regular and indeterminate strings.
</summary>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Neerja Mhaskar</name>
    </author>
    <author>
      <name>W. F. Smyth</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2020.12.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2020.12.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to TCS</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.07892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.03926">
    <id>http://arxiv.org/abs/2012.03926v1</id>
    <updated>2020-12-07T18:55:11Z</updated>
    <published>2020-12-07T18:55:11Z</published>
    <title>Counting ternary square-free words quickly</title>
    <summary>  An efficient, when compared to exhaustive enumeration, algorithm for
computing the number of square-free words of length $n$ over the alphabet $\{a,
b, c\}$ is presented.
</summary>
    <author>
      <name>Vladislav Makarov</name>
    </author>
    <link href="http://arxiv.org/abs/2012.03926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.03926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.03996">
    <id>http://arxiv.org/abs/2012.03996v1</id>
    <updated>2020-12-07T19:08:31Z</updated>
    <published>2020-12-07T19:08:31Z</published>
    <title>Galloping in natural merge sorts</title>
    <summary>  We study the algorithm TimSort and the sub-routine it uses to merge monotonic
(non-decreasing) sub-arrays, hereafter called runs. More precisely, we look at
the impact on the number of element comparisons performed of using this
sub-routine instead of a naive routine.
  In this article, we introduce a new object for measuring the complexity of
arrays. This notion dual to the notion of runs on which TimSort built its
success so far, hence we call it dual runs. It induces complexity measures that
are dual to those induced by runs. We prove, for this new complexity measure,
results that are similar to those already known when considering standard
run-induced measures. Although our new results do not lead to any improvement
on the number of element moves performed, they may lead to dramatic
improvements on the number of element comparisons performed by the algorithm.
  In order to do so, we introduce new notions of fast- and middle-growth for
natural merge sorts, which allow deriving the same upper bounds. After using
these notions successfully on TimSort, we prove that they can be applied to a
wealth of variants of TimSort and other natural merge sorts.
</summary>
    <author>
      <name>Vincent Jug√©</name>
    </author>
    <author>
      <name>Ghazal Khalighinejad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.03996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.03996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.00866">
    <id>http://arxiv.org/abs/2012.00866v1</id>
    <updated>2020-12-01T22:07:59Z</updated>
    <published>2020-12-01T22:07:59Z</published>
    <title>Huskysort</title>
    <summary>  Much of the copious literature on the subject of sorting has concentrated on
minimizing the number of comparisons and/or exchanges/copies. However, a more
appropriate yardstick for the performance of sorting algorithms is based on the
total number of array accesses that are required (the "work"). For a sort that
is based on divide-and-conquer (including iterative variations on that theme),
we can divide the work into linear, i.e. $\textbf{O}(N)$, work and
linearithmic, i.e. $\textbf{O}(N log N)$, work. An algorithm that moves work
from the linearithmic phase to the linear phase may be able to reduce the total
number of array accesses and, indirectly, processing time. This paper describes
an approach to sorting which reduces the number of expensive comparisons in the
linearithmic phase as much as possible by substituting inexpensive comparisons.
In Java, the two system sorts are dual-pivot quicksort (for primitives) and
Timsort for objects. We demonstrate that a combination of these two algorithms
can run significantly faster than either algorithm alone for the types of
objects which are expensive to compare. We call this improved sorting algorithm
Huskysort.
</summary>
    <author>
      <name>R. C. Hillyard</name>
    </author>
    <author>
      <name>Yunlu Liaozheng</name>
    </author>
    <author>
      <name>Sai Vineeth K. R</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, Github repo for the algorithm included</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.00866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.00866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.12742">
    <id>http://arxiv.org/abs/2011.12742v1</id>
    <updated>2020-11-25T13:52:42Z</updated>
    <published>2020-11-25T13:52:42Z</published>
    <title>Left Lyndon tree construction</title>
    <summary>  We extend the left-to-right Lyndon factorisation of a word to the left Lyndon
tree construction of a Lyndon word. It yields an algorithm to sort the prefixes
of a Lyndon word according to the infinite ordering defined by Dolce et al.
(2019). A straightforward variant computes the left Lyndon forest of a word.
All algorithms run in linear time on a general alphabet, that is, in the
letter-comparison model.
</summary>
    <author>
      <name>Golnaz Badkobeh</name>
    </author>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.12742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32, 68W27" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.12898">
    <id>http://arxiv.org/abs/2011.12898v1</id>
    <updated>2020-11-25T17:26:46Z</updated>
    <published>2020-11-25T17:26:46Z</published>
    <title>Grammar Compression By Induced Suffix Sorting</title>
    <summary>  A grammar compression algorithm, called GCIS, is introduced in this work.
GCIS is based on the induced suffix sorting algorithm SAIS, presented by Nong
et al. in 2009. The proposed solution builds on the factorization performed by
SAIS during suffix sorting. A context-free grammar is used to replace factors
by non-terminals. The algorithm is then recursively applied on the shorter
sequence of non-terminals. The resulting grammar is encoded by exploiting some
redundancies, such as common prefixes between right-hands of rules, sorted
according to SAIS. GCIS excels for its low space and time required for
compression while obtaining competitive compression ratios. Our experiments on
regular and repetitive, moderate and very large texts, show that GCIS stands as
a very convenient choice compared to well-known compressors such as Gzip,
7-Zip, and RePair, the gold standard in grammar compression. In exchange, GCIS
is slow at decompressing. Yet, grammar compressors are more convenient than
Lempel-Ziv compressors in that one can access text substrings directly in
compressed form, without ever decompressing the text. We demonstrate that GCIS
is an excellent candidate for this scenario because it shows to be competitive
among its RePair based alternatives. We also show, how GCIS relation with SAIS
makes it a good intermediate structure to build the suffix array and the LCP
array during decompression of the text.
</summary>
    <author>
      <name>Daniel S. N. Nunes</name>
    </author>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Mauricio Ayala-Rinc√≥n</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/2011.12898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.05329">
    <id>http://arxiv.org/abs/2101.05329v1</id>
    <updated>2021-01-13T20:11:47Z</updated>
    <published>2021-01-13T20:11:47Z</published>
    <title>Improving Run Length Encoding by Preprocessing</title>
    <summary>  The Run Length Encoding (RLE) compression method is a long standing simple
lossless compression scheme which is easy to implement and achieves a good
compression on input data which contains repeating consecutive symbols. In its
pure form RLE is not applicable on natural text or other input data with short
sequences of identical symbols. We present a combination of preprocessing steps
that turn arbitrary input data in a byte-wise encoding into a bit-string which
is highly suitable for RLE compression. The main idea is to first read all most
significant bits of the input byte-string, followed by the second most
significant bit, and so on. We combine this approach by a dynamic byte
remapping as well as a Burrows-Wheeler-Scott transform on a byte level.
Finally, we apply a Huffman Encoding on the output of the bit-wise RLE encoding
to allow for more dynamic lengths of code words encoding runs of the RLE. With
our technique we can achieve a lossless average compression which is better
than the standard RLE compression by a factor of 8 on average.
</summary>
    <author>
      <name>Sven Fiergolla</name>
    </author>
    <author>
      <name>Petra Wolf</name>
    </author>
    <link href="http://arxiv.org/abs/2101.05329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.05329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P30, 94A08" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.03978">
    <id>http://arxiv.org/abs/2101.03978v1</id>
    <updated>2021-01-11T15:36:35Z</updated>
    <published>2021-01-11T15:36:35Z</published>
    <title>Strictly In-Place Algorithms for Permuting and Inverting Permutations</title>
    <summary>  We revisit the problem of permuting an array of length $n$ according to a
given permutation in place, that is, using only a small number of bits of extra
storage. Fich, Munro and Poblete [FOCS 1990, SICOMP 1995] obtained an elegant
$\mathcal{O}(n\log n)$-time algorithm using only $\mathcal{O}(\log^{2}n)$ bits
of extra space for this basic problem by designing a procedure that scans the
permutation and outputs exactly one element from each of its cycles. However,
in the strict sense in place should be understood as using only an
asymptotically optimal $\mathcal{O}(\log n)$ bits of extra space, or storing a
constant number of indices. The problem of permuting in this version is, in
fact, a well-known interview question, with the expected solution being a
quadratic-time algorithm. Surprisingly, no faster algorithm seems to be known
in the literature.
  Our first contribution is a strictly in-place generalisation of the method of
Fich et al. that works in $\mathcal{O}_{\varepsilon}(n^{1+\varepsilon})$ time,
for any $\varepsilon > 0$. Then, we build on this generalisation to obtain a
strictly in-place algorithm for inverting a given permutation on $n$ elements
working in the same complexity. This is a significant improvement on a recent
result of Gu\'spiel [arXiv 2019], who designed an $\mathcal{O}(n^{1.5})$-time
algorithm.
</summary>
    <author>
      <name>Bart≈Çomiej Dudek</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Karol Pokorski</name>
    </author>
    <link href="http://arxiv.org/abs/2101.03978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.03978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.03165">
    <id>http://arxiv.org/abs/2101.03165v1</id>
    <updated>2021-01-08T18:50:28Z</updated>
    <published>2021-01-08T18:50:28Z</published>
    <title>Cantor Mapping Technique</title>
    <summary>  A new technique specific to String ordering utilizing a method called "Cantor
Mapping" is explained in this paper and used to perform string comparative sort
in loglinear time while utilizing linear extra space.
</summary>
    <author>
      <name>Kaustubh Joshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, 2 algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.03165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.03165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.12369">
    <id>http://arxiv.org/abs/2012.12369v1</id>
    <updated>2020-12-22T21:44:42Z</updated>
    <published>2020-12-22T21:44:42Z</published>
    <title>Integer Division by Constants: Optimal Bounds</title>
    <summary>  The integer division of a numerator n by a divisor d gives a quotient q and a
remainder r. Optimizing compilers accelerate software by replacing the division
of n by d with the division of c * n (or c * n + c) by m for convenient
integers c and m chosen so that they approximate the reciprocal: c/m ~= 1/d.
Such techniques are especially advantageous when m is chosen to be a power of
two and when d is a constant so that c and m can be precomputed. The literature
contains many bounds on the distance between c/m and the divisor d. Some of
these bounds are optimally tight, while others are not. Using accessible
mathematics, we present optimally tight bounds for quotient and remainder
computations.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Colin Bartlett</name>
    </author>
    <author>
      <name>Owen Kaser</name>
    </author>
    <link href="http://arxiv.org/abs/2012.12369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.12369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.00314">
    <id>http://arxiv.org/abs/2101.00314v1</id>
    <updated>2021-01-01T20:14:33Z</updated>
    <published>2021-01-01T20:14:33Z</published>
    <title>SetSketch: Filling the Gap between MinHash and HyperLogLog</title>
    <summary>  MinHash and HyperLogLog are sketching algorithms that have become
indispensable for set summaries in big data applications. While HyperLogLog
allows counting different elements with very little space, MinHash is suitable
for the fast comparison of sets as it allows estimating the Jaccard similarity
and other joint quantities. This work presents a new data structure called
SetSketch that is able to continuously fill the gap between both use cases. Its
commutative and idempotent insert operation and its mergeable state make it
suitable for distributed environments. Robust and easy-to-implement estimators
for cardinality and joint quantities, as well as the ability to use SetSketch
for similarity search, enable versatile applications. The developed methods can
also be used for HyperLogLog sketches and allow estimation of joint quantities
such as the intersection size with a smaller error compared to the common
estimation approach based on the inclusion-exclusion principle.
</summary>
    <author>
      <name>Otmar Ertl</name>
    </author>
    <link href="http://arxiv.org/abs/2101.00314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.00172">
    <id>http://arxiv.org/abs/2101.00172v1</id>
    <updated>2021-01-01T05:45:56Z</updated>
    <published>2021-01-01T05:45:56Z</published>
    <title>Chunk List: Concurrent Data Structures</title>
    <summary>  Chunking data is obviously no new concept; however, I had never found any
data structures that used chunking as the basis of their implementation. I
figured that by using chunking alongside concurrency, I could create an
extremely fast run-time in regards to particular methods as searching and/or
sorting. By using chunking and concurrency to my advantage, I came up with the
chunk list - a dynamic list-based data structure that would separate large
amounts of data into specifically sized chunks, each of which should be able to
be searched at the exact same time by searching each chunk on a separate
thread. As a result of implementing this concept into its own class, I was able
to create something that almost consistently gives around 20x-300x faster
results than a regular ArrayList. However, should speed be a particular issue
even after implementation, users can modify the size of the chunks and
benchmark the speed of using smaller or larger chunks, depending on the amount
of data being stored.
</summary>
    <author>
      <name>Daniel Szelogowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 3 figures A full implementation can be found at
  https://github.com/danielathome19/Chunk-List</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.00172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.00718">
    <id>http://arxiv.org/abs/2101.00718v1</id>
    <updated>2021-01-03T22:21:51Z</updated>
    <published>2021-01-03T22:21:51Z</published>
    <title>Text Searching Allowing for Non-Overlapping Adjacent Unbalanced
  Translocations</title>
    <summary>  In this paper we investigate the \emph{approximate string matching problem}
when the allowed edit operations are \emph{non-overlapping unbalanced
translocations of adjacent factors}. Such kind of edit operations take place
when two adjacent sub-strings of the text swap, resulting in a modified string.
The two involved substrings are allowed to be of different lengths.
  Such large-scale modifications on strings have various applications. They are
among the most frequent chromosomal alterations, accounted for 30\% of all
losses of heterozygosity, a major genetic event causing inactivation of cancer
suppressor genes. In addition, among other applications, they are frequent
modifications accounted in musical or in natural language information
retrieval. However, despite of their central role in so many fields of text
processing, little attention has been devoted to the problem of matching
strings allowing for this kind of edit operation.
  In this paper we present three algorithms for solving the problem, all of
them with a $\bigO(nm^3)$ worst-case and a $\bigO(m^2)$-space complexity, where
$m$ and $n$ are the length of the pattern and of the text, respectively. % In
particular, our first algorithm is based on the dynamic-programming approach.
Our second solution improves the previous one by making use of the Directed
Acyclic Word Graph of the pattern. Finally our third algorithm is based on an
alignment procedure. We also show that under the assumptions of equiprobability
and independence of characters, our second algorithm has a
$\bigO(n\log^2_{\sigma} m)$ average time complexity, for an alphabet of size
$\sigma \geq 4$.
</summary>
    <author>
      <name>Domenico Cantone</name>
    </author>
    <author>
      <name>Simone Faro</name>
    </author>
    <author>
      <name>Arianna Pavone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1812.00421</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.00718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.01108">
    <id>http://arxiv.org/abs/2101.01108v1</id>
    <updated>2021-01-04T17:32:47Z</updated>
    <published>2021-01-04T17:32:47Z</published>
    <title>Binary Dynamic Time Warping in Linear Time</title>
    <summary>  Dynamic time warping distance (DTW) is a widely used distance measure between
time series $x, y \in \Sigma^n$. It was shown by Abboud, Backurs, and Williams
that in the \emph{binary case}, where $|\Sigma| = 2$, DTW can be computed in
time $O(n^{1.87})$. We improve this running time $O(n)$.
  Moreover, if $x$ and $y$ are run-length encoded, then there is an algorithm
running in time $\tilde{O}(k + \ell)$, where $k$ and $\ell$ are the number of
runs in $x$ and $y$, respectively. This improves on the previous best bound of
$O(k\ell)$ due to Dupont and Marteau.
</summary>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <link href="http://arxiv.org/abs/2101.01108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.01108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.13698">
    <id>http://arxiv.org/abs/2012.13698v1</id>
    <updated>2020-12-26T08:08:46Z</updated>
    <published>2020-12-26T08:08:46Z</published>
    <title>Arithmetic Binary Search Trees: Static Optimality in the Matching Model</title>
    <summary>  Motivated by recent developments in optical switching and reconfigurable
network design, we study dynamic binary search trees (BSTs) in the matching
model. In the classical dynamic BST model, the cost of both link traversal and
basic reconfiguration (rotation) is $O(1)$. However, in the matching model, the
BST is defined by two optical switches (that represent two matchings in an
abstract way), and each switch (or matching) reconfiguration cost is $\alpha$
while a link traversal cost is still $O(1)$. In this work, we propose
Arithmetic BST (A-BST), a simple dynamic BST algorithm that is based on dynamic
Shannon-Fano-Elias coding, and show that A-BST is statically optimal for
sequences of length $\Omega(n \alpha \log \alpha)$ where $n$ is the number of
nodes (keys) in the tree.
</summary>
    <author>
      <name>Chen Avin</name>
    </author>
    <link href="http://arxiv.org/abs/2012.13698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.13698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.06840">
    <id>http://arxiv.org/abs/2012.06840v5</id>
    <updated>2021-01-07T21:42:26Z</updated>
    <published>2020-12-12T15:34:12Z</published>
    <title>String Attractors for Automatic Sequences</title>
    <summary>  We show that it is decidable, given an automatic sequence $\bf s$ and a
constant $c$, whether all prefixes of $\bf s$ have a string attractor of size
$\leq c$. Using a decision procedure based on this result, we show that all
prefixes of the period-doubling sequence of length $\geq 2$ have a string
attractor of size $2$. We also prove analogous results for other sequences,
including the Thue-Morse sequence and the Tribonacci sequence.
  We also provide general upper and lower bounds on string attractor size for
different kinds of sequences. For example, if $\bf s$ has a finite appearance
constant, then there is a string attractor for ${\bf s}[0..n-1]$ of size
$O(\log n)$. If further $\bf s$ is linearly recurrent, then there is a string
attractor for ${\bf s}[0..n-1]$ of size $O(1)$. For automatic sequences, the
size of the smallest string attractor for ${\bf s}[0..n-1]$ is either
$\Theta(1)$ or $\Theta(\log n)$, and it is decidable which case occurs.
Finally, we close with some remarks about greedy string attractors.
</summary>
    <author>
      <name>Luke Schaeffer</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">revision adding significant new results due to Luke Schaeffer</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.06840v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.06840v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.03961">
    <id>http://arxiv.org/abs/2102.03961v1</id>
    <updated>2021-02-08T02:10:34Z</updated>
    <published>2021-02-08T02:10:34Z</published>
    <title>Efficient construction of the extended BWT from grammar-compressed DNA
  sequencing reads</title>
    <summary>  We present an algorithm for building the extended BWT (eBWT) of a string
collection from its grammar-compressed representation. Our technique exploits
the string repetitions captured by the grammar to boost the computation of the
eBWT. Thus, the more repetitive the collection is, the lower are the resources
we use per input symbol. We rely on a new grammar recently proposed at DCC'21
whose nonterminals serve as building blocks for inducing the eBWT. A relevant
application for this idea is the construction of self-indexes for analyzing
sequencing reads -- massive and repetitive string collections of raw genomic
data. Self-indexes have become increasingly popular in Bioinformatics as they
can encode more information in less space. Our efficient eBWT construction
opens the door to perform accurate bioinformatic analyses on more massive
sequence datasets, which are not tractable with current eBWT construction
techniques.
</summary>
    <author>
      <name>Diego Diaz-Dominguez annd Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/2102.03961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.03961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.02505">
    <id>http://arxiv.org/abs/2102.02505v1</id>
    <updated>2021-02-04T09:32:52Z</updated>
    <published>2021-02-04T09:32:52Z</published>
    <title>Gapped Indexing for Consecutive Occurrences</title>
    <summary>  The classic string indexing problem is to preprocess a string S into a
compact data structure that supports efficient pattern matching queries.
Typical queries include existential queries (decide if the pattern occurs in
S), reporting queries (return all positions where the pattern occurs), and
counting queries (return the number of occurrences of the pattern). In this
paper we consider a variant of string indexing, where the goal is to compactly
represent the string such that given two patterns P1 and P2 and a gap range
[\alpha,\beta] we can quickly find the consecutive occurrences of P1 and P2
with distance in [\alpha,\beta], i.e., pairs of occurrences immediately
following each other and with distance within the range. We present data
structures that use \~O(n) space and query time \~O(|P1|+|P2|+n^(2/3)) for
existence and counting and \~O(|P1|+|P2|+n^(2/3)*occ^(1/3)) for reporting. We
complement this with a conditional lower bound based on the set intersection
problem showing that any solution using \~O(n) space must use
\tilde{\Omega}}(|P1|+|P2|+\sqrt{n}) query time. To obtain our results we
develop new techniques and ideas of independent interest including a new suffix
tree decomposition and hardness of a variant of the set intersection problem.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li G√∏rtz</name>
    </author>
    <author>
      <name>Max Rish√∏j Pedersen</name>
    </author>
    <author>
      <name>Teresa Anna Steiner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.02505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.02505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.12341">
    <id>http://arxiv.org/abs/2101.12341v1</id>
    <updated>2021-01-29T01:12:29Z</updated>
    <published>2021-01-29T01:12:29Z</published>
    <title>$r$-indexing Wheeler graphs</title>
    <summary>  Let $G$ be a Wheeler graph and $r$ be the number of runs in a Burrows-Wheeler
Transform of $G$, and suppose $G$ can be decomposed into $\upsilon$
edge-disjoint directed paths whose internal vertices each have in- and
out-degree exactly 1. We show how to store $G$ in $O (r + \upsilon)$ space such
that later, given a pattern $P$, in $O (|P| \log \log |G|)$ time we can count
the vertices of $G$ reachable by directed paths labelled $P$, and then report
those vertices in $O (\log \log |G|)$ time per vertex.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/2101.12341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.12341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.11421">
    <id>http://arxiv.org/abs/2101.11421v1</id>
    <updated>2021-01-27T14:15:46Z</updated>
    <published>2021-01-27T14:15:46Z</published>
    <title>Deriving monadic quicksort (Declarative Pearl)</title>
    <summary>  To demonstrate derivation of monadic programs, we present a specification of
sorting using the non-determinism monad, and derive pure quicksort on lists and
state-monadic quicksort on arrays. In the derivation one may switch between
point-free and pointwise styles, and deploy techniques familiar to functional
programmers such as pattern matching and induction on structures or on sizes.
Derivation of stateful programs resembles reasoning backwards from the
postcondition.
</summary>
    <author>
      <name>Shin-Cheng Mu</name>
    </author>
    <author>
      <name>Tsung-Ju Chiang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-59025-3_8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-59025-3_8" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Nakano K., Sagonas K. (eds) Functional and Logic Programming
  (FLOPS 2020). LNCS 12073. pp 124-138. 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.11421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.11421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.11350">
    <id>http://arxiv.org/abs/2101.11350v1</id>
    <updated>2021-01-27T12:29:03Z</updated>
    <published>2021-01-27T12:29:03Z</published>
    <title>Entropy of Mersenne-Twisters</title>
    <summary>  The Mersenne-Twister is one of the most popular generators of uniform
pseudo-random numbers. It is used in many numerical libraries and software. In
this paper, we look at the Komolgorov entropy of the original Mersenne-Twister,
as well as of more modern variations such as the 64-bit Mersenne-Twisters, the
Well generators, and the Melg generators.
</summary>
    <author>
      <name>Fabien Le Floc'h</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.11350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.11350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.11408">
    <id>http://arxiv.org/abs/2101.11408v4</id>
    <updated>2021-03-23T00:52:54Z</updated>
    <published>2021-01-11T20:31:27Z</published>
    <title>Number Parsing at a Gigabyte per Second</title>
    <summary>  With disks and networks providing gigabytes per second, parsing decimal
numbers from strings becomes a bottleneck. We consider the problem of parsing
decimal numbers to the nearest binary floating-point value. The general problem
requires variable-precision arithmetic. However, we need at most 17 digits to
represent 64-bit standard floating-point numbers (IEEE 754). Thus we can
represent the decimal significand with a single 64-bit word. By combining the
significand and precomputed tables, we can compute the nearest floating-point
number using as few as one or two 64-bit multiplications. Our implementation
can be several times faster than conventional functions present in standard C
libraries on modern 64-bit systems (Intel, AMD, ARM and POWER9). Our work is
available as open source software used by major systems such as Apache Arrow
and Yandex ClickHouse. The Go standard library has adopted a version of our
approach.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Software at https://github.com/fastfloat/fast_float and
  https://github.com/lemire/simple_fastfloat_benchmark/</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.11408v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.11408v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.10890">
    <id>http://arxiv.org/abs/2101.10890v1</id>
    <updated>2021-01-25T18:37:35Z</updated>
    <published>2021-01-25T18:37:35Z</published>
    <title>Spanner Evaluation over SLP-Compressed Documents</title>
    <summary>  We consider the problem of evaluating regular spanners over compressed
documents, i.e., we wish to solve evaluation tasks directly on the compressed
data, without decompression. As compressed forms of the documents we use
straight-line programs (SLPs) -- a lossless compression scheme for textual data
widely used in different areas of theoretical computer science and particularly
well-suited for algorithmics on compressed data. In terms of data complexity,
our results are as follows. For a regular spanner M and an SLP S that
represents a document D, we can solve the tasks of model checking and of
checking non-emptiness in time O(size(S)). Computing the set M(D) of all
span-tuples extracted from D can be done in time O(size(S) size(M(D))), and
enumeration of M(D) can be done with linear preprocessing O(size(S)) and a
delay of O(depth(S)), where depth(S) is the depth of S's derivation tree. Note
that size(S) can be exponentially smaller than the document's size |D|; and,
due to known balancing results for SLPs, we can always assume that depth(S) =
O(log(|D|)) independent of D's compressibility. Hence, our enumeration
algorithm has a delay logarithmic in the size of the non-compressed data and a
preprocessing time that is at best (i.e., in the case of highly compressible
documents) also logarithmic, but at worst still linear. Therefore, in a
big-data perspective, our enumeration algorithm for SLP-compressed documents
may nevertheless beat the known linear preprocessing and constant delay
algorithms for non-compressed documents.
</summary>
    <author>
      <name>Markus L. Schmid</name>
    </author>
    <author>
      <name>Nicole Schweikardt</name>
    </author>
    <link href="http://arxiv.org/abs/2101.10890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.10890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.11514">
    <id>http://arxiv.org/abs/2009.11514v1</id>
    <updated>2020-09-24T06:50:27Z</updated>
    <published>2020-09-24T06:50:27Z</published>
    <title>On One-way Functions and Kolmogorov Complexity</title>
    <summary>  We prove that the equivalence of two fundamental problems in the theory of
computing. For every polynomial $t(n)\geq (1+\varepsilon)n, \varepsilon>0$, the
following are equivalent:
  - One-way functions exists (which in turn is equivalent to the existence of
secure private-key encryption schemes, digital signatures, pseudorandom
generators, pseudorandom functions, commitment schemes, and more);
  - $t$-time bounded Kolmogorov Complexity, $K^t$, is mildly hard-on-average
(i.e., there exists a polynomial $p(n)>0$ such that no PPT algorithm can
compute $K^t$, for more than a $1-\frac{1}{p(n)}$ fraction of $n$-bit strings).
  In doing so, we present the first natural, and well-studied, computational
problem characterizing the feasibility of the central private-key primitives
and protocols in Cryptography.
</summary>
    <author>
      <name>Yanyi Liu</name>
    </author>
    <author>
      <name>Rafael Pass</name>
    </author>
    <link href="http://arxiv.org/abs/2009.11514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.08840">
    <id>http://arxiv.org/abs/2010.08840v1</id>
    <updated>2020-10-17T18:24:08Z</updated>
    <published>2020-10-17T18:24:08Z</published>
    <title>Lazy Search Trees</title>
    <summary>  We introduce the lazy search tree data structure. The lazy search tree is a
comparison-based data structure on the pointer machine that supports
order-based operations such as rank, select, membership, predecessor,
successor, minimum, and maximum while providing dynamic operations insert,
delete, change-key, split, and merge. We analyze the performance of our data
structure based on a partition of current elements into a set of gaps
$\{\Delta_i\}$ based on rank. A query falls into a particular gap and splits
the gap into two new gaps at a rank $r$ associated with the query operation. If
we define $B = \sum_i |\Delta_i| \log_2(n/|\Delta_i|)$, our performance over a
sequence of $n$ insertions and $q$ distinct queries is $O(B + \min(n \log \log
n, n \log q))$. We show $B$ is a lower bound.
  Effectively, we reduce the insertion time of binary search trees from
$\Theta(\log n)$ to $O(\min(\log(n/|\Delta_i|) + \log \log |\Delta_i|, \; \log
q))$, where $\Delta_i$ is the gap in which the inserted element falls. Over a
sequence of $n$ insertions and $q$ queries, a time bound of $O(n \log q + q
\log n)$ holds; better bounds are possible when queries are non-uniformly
distributed. As an extreme case of non-uniformity, if all queries are for the
minimum element, the lazy search tree performs as a priority queue with $O(\log
\log n)$ time insert and decrease-key operations. The same data structure
supports queries for any rank, interpolating between binary search trees and
efficient priority queues.
  Lazy search trees can be implemented to operate mostly on arrays, requiring
only $O(\min(q, n))$ pointers. Via direct reduction, our data structure also
supports the efficient access theorems of the splay tree, providing a powerful
data structure for non-uniform element access, both when the number of accesses
is small and large.
</summary>
    <author>
      <name>Bryce Sandlund</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in FOCS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.08840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.07360">
    <id>http://arxiv.org/abs/2101.07360v1</id>
    <updated>2021-01-18T22:55:32Z</updated>
    <published>2021-01-18T22:55:32Z</published>
    <title>Dynamic Longest Increasing Subsequence and the Erd√∂s-Szekeres
  Partitioning Problem</title>
    <summary>  In this paper, we provide new approximation algorithms for dynamic variations
of the longest increasing subsequence (\textsf{LIS}) problem, and the
complementary distance to monotonicity (\textsf{DTM}) problem. In this setting,
operations of the following form arrive sequentially: (i) add an element, (ii)
remove an element, or (iii) substitute an element for another. At every point
in time, the algorithm has an approximation to the longest increasing
subsequence (or distance to monotonicity). We present a
$(1+\epsilon)$-approximation algorithm for \textsf{DTM} with polylogarithmic
worst-case update time and a constant factor approximation algorithm for
\textsf{LIS} with worst-case update time $\tilde O(n^\epsilon)$ for any
constant $\epsilon > 0$.% $n$ in the runtime denotes the size of the array at
the time the operation arrives.
  Our dynamic algorithm for \textsf{LIS} leads to an almost optimal algorithm
for the Erd\"{o}s-Szekeres partitioning problem. Erd\"{o}s-Szekeres
partitioning problem was introduced by Erd\"{o}s and Szekeres in 1935 and was
known to be solvable in time $O(n^{1.5}\log n)$. Subsequent work improve the
runtime to $O(n^{1.5})$ only in 1998. Our dynamic \textsf{LIS} algorithm leads
to a solution for Erd\"{o}s-Szekeres partitioning problem with runtime $\tilde
O_{\epsilon}(n^{1+\epsilon})$ for any constant $\epsilon > 0$.
</summary>
    <author>
      <name>Michael Mitzenmacher</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <link href="http://arxiv.org/abs/2101.07360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.07360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.00462">
    <id>http://arxiv.org/abs/2103.00462v1</id>
    <updated>2021-02-28T11:28:13Z</updated>
    <published>2021-02-28T11:28:13Z</published>
    <title>Weighted Ancestors in Suffix Trees Revisited</title>
    <summary>  The weighted ancestor problem is a well-known generalization of the
predecessor problem to trees. It is known that it requires $\Omega(\log\log n)$
time for queries provided $O(n\mathop{\mathrm{polylog}} n)$ space is available
and weights are from $[0..n]$, where $n$ is the number of tree nodes. However,
when applied to suffix trees, the problem, surprisingly, admits an $O(n)$-space
solution with constant query time as was shown by Gawrychowski, Lewenstein, and
Nicholson. This variant of the problem can be reformulated as follows: given
the suffix tree of a string $s$, we need a data structure that can locate in
the tree any substring $s[p..q]$ of $s$ in $O(1)$ time (as if one descended
from the root reading $s[p..q]$ along the way). Unfortunately, the data
structure of Gawrychowski et al. has no efficient construction algorithm, which
apparently prevents its wide usage. In this paper we resolve this issue
describing a data structure for weighted ancestors in suffix trees with
constant query time and a linear construction algorithm. Our solution is based
on a novel approach using so-called irreducible LCP values.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.00462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.00462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.00713">
    <id>http://arxiv.org/abs/2103.00713v1</id>
    <updated>2021-03-01T02:49:34Z</updated>
    <published>2021-03-01T02:49:34Z</published>
    <title>Lower Bounds and Improved Algorithms for Asymmetric Streaming Edit
  Distance and Longest Common Subsequence</title>
    <summary>  In this paper, we study edit distance (ED) and longest common subsequence
(LCS) in the asymmetric streaming model, introduced by Saks and Seshadhri
[SS13]. As an intermediate model between the random access model and the
streaming model, this model allows one to have streaming access to one string
and random access to the other string.
  Our first main contribution is a systematic study of space lower bounds for
ED and LCS in the asymmetric streaming model. Previously, there are no
explicitly stated results in this context, although some lower bounds about LCS
can be inferred from the lower bounds for longest increasing subsequence (LIS)
in [SW07][GG10][EJ08]. Yet these bounds only work for large alphabet size. In
this paper, we develop several new techniques to handle ED in general and LCS
for small alphabet size, thus establishing strong lower bounds for both
problems. In particular, our lower bound for ED provides an exponential
separation between edit distance and Hamming distance in the asymmetric
streaming model. Our lower bounds also extend to LIS and longest non-decreasing
sequence (LNS) in the standard streaming model. Together with previous results,
our bounds provide an almost complete picture for these two problems.
  As our second main contribution, we give improved algorithms for ED and LCS
in the asymmetric streaming model. For ED, we improve the space complexity of
the constant factor approximation algorithms in [FHRS20][CJLZ20] from
$\tilde{O}(\frac{n^\delta}{\delta})$ to
$O(\frac{d^\delta}{\delta}\;\mathsf{polylog}(n))$, where $n$ is the length of
each string and $d$ is the edit distance between the two strings. For LCS, we
give the first $1/2+\epsilon$ approximation algorithm with space $n^{\delta}$
for any constant $\delta>0$, over a binary alphabet.
</summary>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Yu Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/2103.00713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.00713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.01052">
    <id>http://arxiv.org/abs/2103.01052v2</id>
    <updated>2021-03-09T14:45:51Z</updated>
    <published>2021-03-01T14:50:29Z</published>
    <title>On the Cost of Unsuccessful Searches in Search Trees with Two-way
  Comparisons</title>
    <summary>  Search trees are commonly used to implement access operations to a set of
stored keys. If this set is static and the probabilities of membership queries
are known in advance, then one can precompute an optimal search tree, namely
one that minimizes the expected access cost. For a non-key query, a search tree
can determine its approximate location by returning the inter-key interval
containing the query. This is in contrast to other dictionary data structures,
like hash tables, that only report a failed search. We address the question
"what is the additional cost of determining approximate locations for non-key
queries"? We prove that for two-way comparison trees this additional cost is at
most 1. Our proof is based on a novel probabilistic argument that involves
converting a search tree that does not identify non-key queries into a random
tree that does.
</summary>
    <author>
      <name>Marek Chrobak</name>
    </author>
    <author>
      <name>Mordecai Golin</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Neal E. Young</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ic.2021.104707</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ic.2021.104707" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v2 has updated bibliography</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information and Computation (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.01052v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.01052v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P10, 68P30, 68W25, 94A45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; G.1.6; G.2.2; H.3.1; I.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.01084">
    <id>http://arxiv.org/abs/2103.01084v2</id>
    <updated>2021-03-09T16:14:30Z</updated>
    <published>2021-03-01T15:53:28Z</published>
    <title>A Simple Algorithm for Optimal Search Trees with Two-Way Comparisons</title>
    <summary>  We present a simple $O(n^4)$-time algorithm for computing optimal search
trees with two-way comparisons. The only previous solution to this problem, by
Anderson et al., has the same running time, but is significantly more
complicated and is restricted to the variant where only successful queries are
allowed. Our algorithm extends directly to solve the standard full variant of
the problem, which also allows unsuccessful queries and for which no
polynomial-time algorithm was previously known. The correctness proof of our
algorithm relies on a new structural theorem for two-way-comparison search
trees.
</summary>
    <author>
      <name>Marek Chrobak</name>
    </author>
    <author>
      <name>Mordecai Golin</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Neal E. Young</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v2 has updated references</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.01084v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.01084v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P10, 68P30, 68W25, 94A45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; G.1.6; G.2.2; H.3.1; I.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.12824">
    <id>http://arxiv.org/abs/2102.12824v2</id>
    <updated>2021-02-27T07:28:28Z</updated>
    <published>2021-02-25T12:51:33Z</published>
    <title>A Linear Time Algorithm for Constructing Hierarchical Overlap Graphs</title>
    <summary>  The hierarchical overlap graph (HOG) is a graph that encodes overlaps from a
given set P of n strings, as the overlap graph does. A best known algorithm
constructs HOG in O(||P|| log n) time and O(||P||) space, where ||P|| is the
sum of lengths of the strings in P. In this paper we present a new algorithm to
construct HOG in O(||P||) time and space. Hence, the construction time and
space of HOG are better than those of the overlap graph, which are O(||P|| +
n^2).
</summary>
    <author>
      <name>Sangsoo Park</name>
    </author>
    <author>
      <name>Sung Gwan Park</name>
    </author>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Kunsoo Park</name>
    </author>
    <author>
      <name>Eric Rivals</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, submitted to CPM 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.12824v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12824v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.12822">
    <id>http://arxiv.org/abs/2102.12822v3</id>
    <updated>2021-03-17T18:39:58Z</updated>
    <published>2021-02-25T12:47:13Z</published>
    <title>Algorithms and Complexity on Indexing Founder Graphs</title>
    <summary>  We introduce a compact pangenome representation based on an optimal
segmentation concept that aims to reconstruct founder sequences from a multiple
sequence alignment (MSA). Such founder sequences have the feature that each row
of the MSA is a recombination of the founders. Several linear time dynamic
programming algorithms have been previously devised to optimize segmentations
that induce founder blocks that then can be concatenated into a set of founder
sequences. All possible concatenation orders can be expressed as a founder
graph. We observe a key property of such graphs: if the node labels (founder
segments) do not repeat in the paths of the graph, such graphs can be indexed
for efficient string matching. We call such graphs repeat-free founder graphs
when constructed from a gapless MSA and repeat-free elastic founder graphs when
constructed from a general MSA with gaps. We give a linear time algorithm and a
parameterized near linear time algorithm to construct a repeat-free founder
graph and a repeat-free elastic founder graph, respectively. We derive a
tailored succinct index structure to support queries of arbitrary length in the
paths of a repeat-free (elastic) founder graph. In addition, we show how to
turn a repeat-free (elastic) founder graph into a Wheeler graph in polynomial
time. Furthermore, we show that a property such as repeat-freeness is essential
for indexability. In particular, we show that unless the Strong Exponential
Time Hypothesis (SETH) fails, one cannot build an index on an elastic founder
graph in polynomial time to support fast queries.
</summary>
    <author>
      <name>Massimo Equi</name>
    </author>
    <author>
      <name>Tuukka Norri</name>
    </author>
    <author>
      <name>Jarno Alanko</name>
    </author>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Alexandru I. Tomescu</name>
    </author>
    <author>
      <name>Veli M√§kinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extended version of WABI 2020 paper
  (https://doi.org/10.4230/LIPIcs.WABI.2020.7), whose preprint is in
  arXiv:2005.09342</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.12822v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12822v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.4; F.1.3; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.11797">
    <id>http://arxiv.org/abs/2102.11797v1</id>
    <updated>2021-02-23T17:04:25Z</updated>
    <published>2021-02-23T17:04:25Z</published>
    <title>Conditional Lower Bounds for Variants of Dynamic LIS</title>
    <summary>  In this note, we consider the complexity of maintaining the longest
increasing subsequence (LIS) of an array under (i) inserting an element, and
(ii) deleting an element of an array. We show that no algorithm can support
queries and updates in time $\mathcal{O}(n^{1/2-\epsilon})$ and
$\mathcal{O}(n^{1/3-\epsilon})$ for the dynamic LIS problem, for any constant
$\epsilon>0$, when the elements are weighted or the algorithm supports
1D-queries (on subarrays), respectively, assuming the All-Pairs Shortest Paths
(APSP) conjecture or the Online Boolean Matrix-Vector Multiplication (OMv)
conjecture. The main idea in our construction comes from the work of Abboud and
Dahlgaard [FOCS 2016], who proved conditional lower bounds for dynamic planar
graph algorithm. However, this needs to be appropriately adjusted and
translated to obtain an instance of the dynamic LIS problem.
</summary>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Wojciech Janczewski</name>
    </author>
    <link href="http://arxiv.org/abs/2102.11797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.11797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.09463">
    <id>http://arxiv.org/abs/2102.09463v1</id>
    <updated>2021-02-18T16:25:13Z</updated>
    <published>2021-02-18T16:25:13Z</published>
    <title>Range Minimum Queries in Minimal Space</title>
    <summary>  We consider the problem of computing a sequence of range minimum queries. We
assume a sequence of commands that contains values and queries. Our goal is to
quickly determine the minimum value that exists between the current position
and a previous position $i$. Range minimum queries are used as a sub-routine of
several algorithms, namely related to string processing. We propose a data
structure that can process these commands sequences. We obtain efficient
results for several variations of the problem, in particular we obtain $O(1)$
time per command for the offline version and $O(\alpha(n))$ amortized time for
the online version, where $\alpha(n)$ is the inverse Ackermann function and $n$
the number of values in the sequence. This data structure also has very small
space requirements, namely $O(\ell)$ where $\ell$ is the maximum number active
$i$ positions. We implemented our data structure and show that it is
competitive against existing alternatives. We obtain comparable command
processing time, in the nano second range, and much smaller space requirements.
</summary>
    <author>
      <name>Lu√≠s M. S. Russo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 3 figures, 3 tables, 6 algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.09463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.09463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.08670">
    <id>http://arxiv.org/abs/2102.08670v1</id>
    <updated>2021-02-17T10:25:06Z</updated>
    <published>2021-02-17T10:25:06Z</published>
    <title>Linear Time Runs over General Ordered Alphabets</title>
    <summary>  A run in a string is a maximal periodic substring. For example, the string
$\texttt{bananatree}$ contains the runs $\texttt{anana} = (\texttt{an})^{3/2}$
and $\texttt{ee} = \texttt{e}^2$. There are less than $n$ runs in any
length-$n$ string, and computing all runs for a string over a linearly-sortable
alphabet takes $\mathcal{O}(n)$ time (Bannai et al., SODA 2015). Kosolobov
conjectured that there also exists a linear time runs algorithm for general
ordered alphabets (Inf. Process. Lett. 2016). The conjecture was almost proven
by Crochemore et al., who presented an $\mathcal{O}(n\alpha(n))$ time algorithm
(where $\alpha(n)$ is the extremely slowly growing inverse Ackermann function).
We show how to achieve $\mathcal{O}(n)$ time by exploiting combinatorial
properties of the Lyndon array, thus proving Kosolobov's conjecture.
</summary>
    <author>
      <name>Jonas Ellert</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to ICALP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.08670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.08670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.06798">
    <id>http://arxiv.org/abs/2102.06798v1</id>
    <updated>2021-02-12T22:25:30Z</updated>
    <published>2021-02-12T22:25:30Z</published>
    <title>Which Regular Languages can be Efficiently Indexed?</title>
    <summary>  In the present work, we study the hierarchy of $p$-sortable languages:
regular languages accepted by automata of width $p$. In this hierarchy, regular
languages are sorted according to the new fundamental measure of NFA complexity
$p$. Our main contributions are the following: (i) we show that the hierarchy
is strict and does not collapse, (ii) we provide (exponential) upper and lower
bounds relating the minimum widths of equivalent NFAs and DFAs, and (iii) we
characterize DFAs of minimum $p$ for a given $\mathcal L$ via a
co-lexicographic variant of the Myhill-Nerode theorem. Our findings imply that
in polynomial time we can build an index breaking the worst-case conditional
lower bound of $\Omega(\pi m)$, whenever the input NFA's width is at most
$\epsilon \log_2 m$, for any constant $0 \leq \epsilon &lt; 1/2$.
</summary>
    <author>
      <name>Nicola Cotumaccio</name>
    </author>
    <author>
      <name>Giovanna D'Agostino</name>
    </author>
    <author>
      <name>Alberto Policriti</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/2102.06798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.06798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.05460">
    <id>http://arxiv.org/abs/2103.05460v1</id>
    <updated>2021-03-09T14:50:49Z</updated>
    <published>2021-03-09T14:50:49Z</published>
    <title>Dynamic Range Mode Enumeration</title>
    <summary>  The range mode problem is a fundamental problem and there is a lot of work
about it. There is also some work for the dynamic version of it and the
enumerating version of it, but there is no previous research about the dynamic
and enumerating version of it. We found an efficient algorithm for it.
</summary>
    <author>
      <name>Tetto Obata</name>
    </author>
    <link href="http://arxiv.org/abs/2103.05460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.05460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.03468">
    <id>http://arxiv.org/abs/2103.03468v1</id>
    <updated>2021-03-05T04:35:35Z</updated>
    <published>2021-03-05T04:35:35Z</published>
    <title>Compressed Communication Complexity of Hamming Distance</title>
    <summary>  We consider the communication complexity of the Hamming distance of two
strings. Bille et al. [SPIRE 2018] considered the communication complexity of
the longest common prefix (LCP) problem in the setting where the two parties
have their strings in a compressed form, i.e., represented by the Lempel-Ziv 77
factorization (LZ77) with/without self-references. We present a randomized
public-coin protocol for a joint computation of the Hamming distance of two
strings represented by LZ77 without self-references. While our scheme is
heavily based on Bille et al.'s LCP protocol, our complexity analysis is
original which uses Crochemore's C-factorization and Rytter's AVL-grammar. As a
byproduct, we also show that LZ77 with/without self-references are not
monotonic in the sense that their sizes can increase by a factor of 4/3 when a
prefix of the string is removed.
</summary>
    <author>
      <name>Shiori Mitsuya</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2103.03468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.03468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.03294">
    <id>http://arxiv.org/abs/2103.03294v1</id>
    <updated>2021-03-04T20:07:00Z</updated>
    <published>2021-03-04T20:07:00Z</published>
    <title>An Almost Optimal Edit Distance Oracle</title>
    <summary>  We consider the problem of preprocessing two strings $S$ and $T$, of lengths
$m$ and $n$, respectively, in order to be able to efficiently answer the
following queries: Given positions $i,j$ in $S$ and positions $a,b$ in $T$,
return the optimal alignment of $S[i \mathinner{.\,.} j]$ and $T[a
\mathinner{.\,.} b]$. Let $N=mn$. We present an oracle with preprocessing time
$N^{1+o(1)}$ and space $N^{1+o(1)}$ that answers queries in $\log^{2+o(1)}N$
time. In other words, we show that we can query the alignment of every two
substrings in almost the same time it takes to compute just the alignment of
$S$ and $T$. Our oracle uses ideas from our distance oracle for planar graphs
[STOC 2019] and exploits the special structure of the alignment graph.
Conditioned on popular hardness conjectures, this result is optimal up to
subpolynomial factors. Our results apply to both edit distance and longest
common subsequence (LCS).
  The best previously known oracle with construction time and size
$\mathcal{O}(N)$ has slow $\Omega(\sqrt{N})$ query time [Sakai, TCS 2019], and
the one with size $N^{1+o(1)}$ and query time $\log^{2+o(1)}N$ (using a planar
graph distance oracle) has slow $\Omega(N^{3/2})$ construction time [Long &amp;
Pettie, SODA 2021]. We improve both approaches by roughly a $\sqrt N$ factor.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Shay Mozes</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <link href="http://arxiv.org/abs/2103.03294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.03294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.13457">
    <id>http://arxiv.org/abs/2104.13457v1</id>
    <updated>2021-04-27T20:11:47Z</updated>
    <published>2021-04-27T20:11:47Z</published>
    <title>Hypersuccinct Trees -- New universal tree source codes for optimal
  compressed tree data structures</title>
    <summary>  We present a new universal source code for unlabeled binary and ordinal trees
that achieves asymptotically optimal compression for all tree sources covered
by existing universal codes. At the same time, it supports answering many
navigational queries on the compressed representation in constant time on the
word-RAM; this is not known to be possible for any existing tree compression
method. The resulting data structures, "hypersuccinct trees", hence combine the
compression achieved by the best known universal codes with the operation
support of the best succinct tree data structures. Compared to prior work on
succinct data structures, we do not have to tailor our data structure to
specific applications; hypersuccinct trees automatically adapt to the trees at
hand. We show that it simultaneously achieves the asymptotically optimal space
usage for a wide range of distributions over tree shapes, including: random
binary search trees (BSTs) / Cartesian trees of random arrays, random
fringe-balanced BSTs, binary trees with a given number of binary/unary/leaf
nodes, random binary tries generated from memoryless sources, full binary
trees, unary paths, as well as uniformly chosen weight-balanced BSTs, AVL
trees, and left-leaning red-black trees. Using hypersuccinct trees, we further
obtain the first data structure that answers range-minimum queries on a random
permutation of $n$ elements in constant time and using the optimal $1.736n +
o(n)$ bits on average, solving an open problem of Davoodi et al. (2014) and
Golin et al. (2016).
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Patrick K. Nicholson</name>
    </author>
    <author>
      <name>Louisa Seelbach Benkner</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <link href="http://arxiv.org/abs/2104.13457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.13457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.10939">
    <id>http://arxiv.org/abs/2104.10939v1</id>
    <updated>2021-04-22T09:10:31Z</updated>
    <published>2021-04-22T09:10:31Z</published>
    <title>HINT: A Hierarchical Index for Intervals in Main Memory</title>
    <summary>  Indexing intervals is a fundamental problem, finding a wide range of
applications. Recent work on managing large collections of intervals in main
memory focused on overlap joins and temporal aggregation problems. In this
paper, we propose novel and efficient in-memory indexing techniques for
intervals, with a focus on interval range queries, which are a basic component
of many search and analysis tasks. First, we propose an optimized version of a
single-level (flat) domain-partitioning approach, which may have large space
requirements due to excessive replication. Then, we propose a hierarchical
partitioning approach, which assigns each interval to at most two partitions
per level and has controlled space requirements. Novel elements of our
techniques include the division of the intervals at each partition into groups
based on whether they begin inside or before the partition boundaries, reducing
the information stored at each partition to the absolutely necessary, and the
effective handling of data sparsity and skew. Experimental results on real and
synthetic interval sets of different characteristics show that our approaches
are typically one order of magnitude faster than the state-of-the-art.
</summary>
    <author>
      <name>George Christodoulou</name>
    </author>
    <author>
      <name>Panagiotis Bouros</name>
    </author>
    <author>
      <name>Nikos Mamoulis</name>
    </author>
    <link href="http://arxiv.org/abs/2104.10939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.10939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.10402">
    <id>http://arxiv.org/abs/2104.10402v2</id>
    <updated>2021-05-28T08:58:38Z</updated>
    <published>2021-04-21T08:22:07Z</published>
    <title>PTHash: Revisiting FCH Minimal Perfect Hashing</title>
    <summary>  Given a set $S$ of $n$ distinct keys, a function $f$ that bijectively maps
the keys of $S$ into the range $\{0,\ldots,n-1\}$ is called a minimal perfect
hash function for $S$. Algorithms that find such functions when $n$ is large
and retain constant evaluation time are of practical interest; for instance,
search engines and databases typically use minimal perfect hash functions to
quickly assign identifiers to static sets of variable-length keys such as
strings. The challenge is to design an algorithm which is efficient in three
different aspects: time to find $f$ (construction time), time to evaluate $f$
on a key of $S$ (lookup time), and space of representation for $f$. Several
algorithms have been proposed to trade-off between these aspects. In 1992, Fox,
Chen, and Heath (FCH) presented an algorithm at SIGIR providing very fast
lookup evaluation. However, the approach received little attention because of
its large construction time and higher space consumption compared to other
subsequent techniques. Almost thirty years later we revisit their framework and
present an improved algorithm that scales well to large sets and reduces space
consumption altogether, without compromising the lookup time. We conduct an
extensive experimental assessment and show that the algorithm finds functions
that are competitive in space with state-of-the art techniques and provide
$2-4\times$ better lookup time.
</summary>
    <author>
      <name>Giulio Ermanno Pibiri</name>
    </author>
    <author>
      <name>Roberto Trani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3404835.3462849</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3404835.3462849" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SIGIR 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.10402v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.10402v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.06740">
    <id>http://arxiv.org/abs/2104.06740v1</id>
    <updated>2021-04-14T09:47:06Z</updated>
    <published>2021-04-14T09:47:06Z</published>
    <title>Engineering Predecessor Data Structures for Dynamic Integer Sets</title>
    <summary>  We present highly optimized data structures for the dynamic predecessor
problem, where the task is to maintain a set $S$ of $w$-bit numbers under
insertions, deletions, and predecessor queries (return the largest element in
$S$ no larger than a given key). The problem of finding predecessors can be
viewed as a generalized form of the membership problem, or as a simple version
of the nearest neighbour problem. It lies at the core of various real-world
problems such as internet routing.
  In this work, we engineer (1) a simple implementation of the idea of universe
reduction, similar to van-Emde-Boas trees (2) variants of y-fast tries
[Willard, IPL'83], and (3) B-trees with different strategies for organizing the
keys contained in the nodes, including an implementation of dynamic fusion
nodes [P\v{a}tra\c{s}cu and Thorup, FOCS'14]. We implement our data structures
for $w=32,40,64$, which covers most typical scenarios.
  Our data structures finish workloads faster than previous approaches while
being significantly more space-efficient, e.g., they clearly outperform
standard implementations of the STL by finishing up to four times as fast using
less than a third of the memory. Our tests also provide more general insights
on data structure design, such as how small sets should be stored and handled
and if and when new CPU instructions such as advanced vector extensions pay
off.
</summary>
    <author>
      <name>Patrick Dinklage</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Alexander Herlez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages plus 5 page appendix, to be published in the proceedings of
  the 19th Symposium on Experimental Algorithms (SEA) 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.06740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.06740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.09985">
    <id>http://arxiv.org/abs/2104.09985v1</id>
    <updated>2021-04-19T08:13:39Z</updated>
    <published>2021-04-19T08:13:39Z</published>
    <title>A Separation of $Œ≥$ and $b$ via Thue--Morse Words</title>
    <summary>  We prove that for $n\geq 2$, the size $b(t_n)$ of the smallest bidirectional
scheme for the $n$th Thue--Morse word $t_n$ is $n+2$. Since Kutsukake et al.
[SPIRE 2020] show that the size $\gamma(t_n)$ of the smallest string attractor
for $t_n$ is $4$ for $n \geq 4$, this shows for the first time that there is a
separation between the size of the smallest string attractor $\gamma$ and the
size of the smallest bidirectional scheme $b$, i.e., there exist string
families such that $\gamma = o(b)$.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Dominik Koeppl</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <link href="http://arxiv.org/abs/2104.09985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.09985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.08751">
    <id>http://arxiv.org/abs/2104.08751v1</id>
    <updated>2021-04-18T07:22:29Z</updated>
    <published>2021-04-18T07:22:29Z</published>
    <title>Load-Balancing Succinct B Trees</title>
    <summary>  We propose a B tree representation storing $n$ keys, each of $k$ bits, in
either (a) $nk + O(nk / \lg n)$ bits or (b) $nk + O(nk \lg \lg n/ \lg n)$ bits
of space supporting all B tree operations in either (a) $O(\lg n )$ time or (b)
$O(\lg n / \lg \lg n)$ time, respectively. We can augment each node with an
aggregate value such as the minimum value within its subtree, and maintain
these aggregate values within the same space and time complexities. Finally, we
give the sparse suffix tree as an application, and present a linear-time
algorithm computing the sparse longest common prefix array from the suffix AVL
tree of Irving et al. [JDA'2003].
</summary>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Dominik K√∂ppl</name>
    </author>
    <link href="http://arxiv.org/abs/2104.08751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.06740">
    <id>http://arxiv.org/abs/2104.06740v1</id>
    <updated>2021-04-14T09:47:06Z</updated>
    <published>2021-04-14T09:47:06Z</published>
    <title>Engineering Predecessor Data Structures for Dynamic Integer Sets</title>
    <summary>  We present highly optimized data structures for the dynamic predecessor
problem, where the task is to maintain a set $S$ of $w$-bit numbers under
insertions, deletions, and predecessor queries (return the largest element in
$S$ no larger than a given key). The problem of finding predecessors can be
viewed as a generalized form of the membership problem, or as a simple version
of the nearest neighbour problem. It lies at the core of various real-world
problems such as internet routing.
  In this work, we engineer (1) a simple implementation of the idea of universe
reduction, similar to van-Emde-Boas trees (2) variants of y-fast tries
[Willard, IPL'83], and (3) B-trees with different strategies for organizing the
keys contained in the nodes, including an implementation of dynamic fusion
nodes [P\v{a}tra\c{s}cu and Thorup, FOCS'14]. We implement our data structures
for $w=32,40,64$, which covers most typical scenarios.
  Our data structures finish workloads faster than previous approaches while
being significantly more space-efficient, e.g., they clearly outperform
standard implementations of the STL by finishing up to four times as fast using
less than a third of the memory. Our tests also provide more general insights
on data structure design, such as how small sets should be stored and handled
and if and when new CPU instructions such as advanced vector extensions pay
off.
</summary>
    <author>
      <name>Patrick Dinklage</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Alexander Herlez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages plus 5 page appendix, to be published in the proceedings of
  the 19th Symposium on Experimental Algorithms (SEA) 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.06740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.06740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.02461">
    <id>http://arxiv.org/abs/2104.02461v1</id>
    <updated>2021-04-06T12:39:28Z</updated>
    <published>2021-04-06T12:39:28Z</published>
    <title>Sorted Range Reporting</title>
    <summary>  In sorted range selection problem, the aim is to preprocess a given array
A[1: n] so as to answers queries of type: given two indices i,j ($1 \le i\le j
\le n$) and an integer k, report k smallest elements in sorted order present in
the sub-array A[i: j] Brodal et.al.[2] have shown that the problem can be
solved in O(k) time after O(n log n) preprocessing in linear space. In this
paper we discuss another tradeoff. We reduce preprocessing time to O(n), but
query time is O(k log k), again using linear space. Our method is very simple.
</summary>
    <author>
      <name>Waseem Akram</name>
    </author>
    <author>
      <name>Sanjeev Saxena</name>
    </author>
    <link href="http://arxiv.org/abs/2104.02461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.15329">
    <id>http://arxiv.org/abs/2103.15329v1</id>
    <updated>2021-03-29T04:54:14Z</updated>
    <published>2021-03-29T04:54:14Z</published>
    <title>A Fast and Small Subsampled R-index</title>
    <summary>  The $r$-index (Gagie et al., JACM 2020) represented a breakthrough in
compressed indexing of repetitive text collections, outperforming its
alternatives by orders of magnitude. Its space usage, $\mathcal{O}(r)$ where
$r$ is the number of runs in the Burrows-Wheeler Transform of the text, is
however larger than Lempel-Ziv and grammar-based indexes, and makes it
uninteresting in various real-life scenarios of milder repetitiveness. In this
paper we introduce the $sr$-index, a variant that limits the space to
$\mathcal{O}(\min(r,n/s))$ for a text of length $n$ and a given parameter $s$,
at the expense of multiplying by $s$ the time per occurrence reported. The
$sr$-index is obtained by carefully subsampling the text positions indexed by
the $r$-index, in a way that we prove is still able to support pattern matching
with guaranteed performance. Our experiments demonstrate that the $sr$-index
sharply outperforms virtually every other compressed index on repetitive texts,
both in time and space, even matching the performance of the $r$-index while
using 1.5--3.0 times less space. Only some Lempel-Ziv-based indexes achieve
better compression than the $sr$-index, using about half the space, but they
are an order of magnitude slower.
</summary>
    <author>
      <name>Dustin Cobas</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/2103.15329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.07073">
    <id>http://arxiv.org/abs/2105.07073v1</id>
    <updated>2021-05-14T21:24:34Z</updated>
    <published>2021-05-14T21:24:34Z</published>
    <title>N-ary Huffman Encoding Using High-Degree Trees -- A Performance
  Comparison</title>
    <summary>  In this paper we implement an n-ary Huffman Encoding and Decoding application
using different degrees of tree structures. Our goal is to compare the
performance of the algorithm in terms of compression ratio, decompression speed
and weighted path length when using higher degree trees, compared to the 2-ary
Huffman Code. The Huffman tree degrees that we compare are 2-ary, 3-ary, 4-ary,
5-ary, 6-ary, 7-ary, 8-ary and 16-mal. We also present the impact that branch
prediction has on the performance of the n-ary Huffman Decoding.
</summary>
    <author>
      <name>Ioannis S. Xezonakis</name>
    </author>
    <author>
      <name>Svoronos Leivadaros</name>
    </author>
    <link href="http://arxiv.org/abs/2105.07073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.07073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.10027">
    <id>http://arxiv.org/abs/2102.10027v5</id>
    <updated>2021-05-07T13:33:26Z</updated>
    <published>2021-02-19T16:53:54Z</published>
    <title>Sorting Short Integers</title>
    <summary>  We build boolean circuits of size $O(nm^2)$ and depth $O(\log(n) + m
\log(m))$ for sorting $n$ integers each of $m$-bits. We build also circuits
that sort $n$ integers each of $m$-bits according to their first $k$ bits that
are of size $O(nmk(1 + \log^*(n) - \log^*(m)))$ and depth $O(\log^{3}(n))$.
This improves on the result of Asharov et al. arXiv:2010.09884 and resolves
some of their open questions.
</summary>
    <author>
      <name>Michal Kouck√Ω</name>
    </author>
    <author>
      <name>Karel Kr√°l</name>
    </author>
    <link href="http://arxiv.org/abs/2102.10027v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.10027v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.10327">
    <id>http://arxiv.org/abs/2105.10327v1</id>
    <updated>2021-05-21T12:59:19Z</updated>
    <published>2021-05-21T12:59:19Z</published>
    <title>Weighted Burrows-Wheeler Compression</title>
    <summary>  A weight based dynamic compression method has recently been proposed, which
is especially suitable for the encoding of files with locally skewed
distributions. Its main idea is to assign larger weights to closer to be
encoded symbols by means of an increasing weight function, rather than
considering each position in the text evenly. A well known transformation that
tends to convert input files into files with a more skewed distribution is the
Burrows-Wheeler Transform. This paper employs the weighted approach on
Burrows-Wheeler transformed files and provides empirical evidence of the
efficiency of this combination.
</summary>
    <author>
      <name>Aharon Fruchtman</name>
    </author>
    <author>
      <name>Yoav Gross</name>
    </author>
    <author>
      <name>Shmuel T. Klein</name>
    </author>
    <author>
      <name>Dana Shapira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.10327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.10327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.08496">
    <id>http://arxiv.org/abs/2105.08496v1</id>
    <updated>2021-05-18T13:15:44Z</updated>
    <published>2021-05-18T13:15:44Z</published>
    <title>Combinatorics of minimal absent words for a sliding window</title>
    <summary>  A string $w$ is called a minimal absent word (MAW) for another string $T$ if
$w$ does not occur in $T$ but the proper substrings of $w$ occur in $T$. For
example, let $\Sigma = \{\mathtt{a, b, c}\}$ be the alphabet. Then, the set of
MAWs for string $w = \mathtt{abaab}$ is $\{\mathtt{aaa, aaba, bab, bb, c}\}$.
In this paper, we study combinatorial properties of MAWs in the sliding window
model, namely, how the set of MAWs changes when a sliding window of fixed
length $d$ is shifted over the input string $T$ of length $n$, where $1 \leq d
&lt; n$. We present \emph{tight} upper and lower bounds on the maximum number of
changes in the set of MAWs for a sliding window over $T$, both in the cases of
general alphabets and binary alphabets. Our bounds improve on the previously
known best bounds [Crochemore et al., 2020].
</summary>
    <author>
      <name>Tooru Akagi</name>
    </author>
    <author>
      <name>Yuki Kuhara</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A part of the results of this article appeared in Proc. SOFSEM 2020.
  arXiv admin note: text overlap with arXiv:1909.02804</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.08496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.08496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.06166">
    <id>http://arxiv.org/abs/2105.06166v1</id>
    <updated>2021-05-13T09:51:51Z</updated>
    <published>2021-05-13T09:51:51Z</published>
    <title>The Dynamic k-Mismatch Problem</title>
    <summary>  The text-to-pattern Hamming distances problem asks to compute the Hamming
distances between a given pattern of length $m$ and all length-$m$ substrings
of a given text of length $n\ge m$. We focus on the $k$-mismatch version of the
problem, where a distance needs to be returned only if it does not exceed a
threshold $k$. We assume $n\le 2m$ (in general, one can partition the text into
overlapping blocks). In this work, we show data structures for the dynamic
version of this problem supporting two operations: An update performs a
single-letter substitution in the pattern or the text, and a query, given an
index $i$, returns the Hamming distance between the pattern and the text
substring starting at position $i$, or reports that it exceeds $k$.
  First, we show a data structure with $\tilde{O}(1)$ update and $\tilde{O}(k)$
query time. Then we show that $\tilde{O}(k)$ update and $\tilde{O}(1)$ query
time is also possible. These two provide an optimal trade-off for the dynamic
$k$-mismatch problem with $k \le \sqrt{n}$: we prove that, conditioned on the
strong 3SUM conjecture, one cannot simultaneously achieve $k^{1-\Omega(1)}$
time for all operations.
  For $k\ge \sqrt{n}$, we give another lower bound, conditioned on the Online
Matrix-Vector conjecture, that excludes algorithms taking $n^{1/2-\Omega(1)}$
time per operation. This is tight for constant-sized alphabets: Clifford et al.
(STACS 2018) achieved $\tilde{O}(\sqrt{n})$ time per operation in that case,
but with $\tilde{O}(n^{3/4})$ time per operation for large alphabets. We
improve and extend this result with an algorithm that, given $1\le x\le k$,
achieves update time $\tilde{O}(\frac{n}{k} +\sqrt{\frac{nk}{x}})$ and query
time $\tilde{O}(x)$. In particular, for $k\ge \sqrt{n}$, an appropriate choice
of $x$ yields $\tilde{O}(\sqrt[3]{nk})$ time per operation, which is
$\tilde{O}(n^{2/3})$ when no threshold $k$ is provided.
</summary>
    <author>
      <name>Rapha√´l Clifford</name>
    </author>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Daniel P. Martin</name>
    </author>
    <author>
      <name>Przemys≈Çaw Uzna≈Ñski</name>
    </author>
    <link href="http://arxiv.org/abs/2105.06166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.06166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.04802">
    <id>http://arxiv.org/abs/2105.04802v1</id>
    <updated>2021-05-11T06:29:06Z</updated>
    <published>2021-05-11T06:29:06Z</published>
    <title>Tree Edit Distance with Variables. Measuring the Similarity between
  Mathematical Formulas</title>
    <summary>  In this article, we propose tree edit distance with variables, which is an
extension of the tree edit distance to handle trees with variables and has a
potential application to measuring the similarity between mathematical
formulas, especially, those appearing in mathematical models of biological
systems. We analyze the computational complexities of several variants of this
new model. In particular, we show that the problem is NP-complete for ordered
trees. We also show for unordered trees that the problem of deciding whether or
not the distance is 0 is graph isomorphism complete but can be solved in
polynomial time if the maximum outdegree of input trees is bounded by a
constant. This distance model is then extended for measuring the
difference/similarity between two systems of differential equations, for which
results of preliminary computational experiments using biological models are
provided.
</summary>
    <author>
      <name>Tatsuya Akutsu</name>
    </author>
    <author>
      <name>Tomoya Mori</name>
    </author>
    <author>
      <name>Naotoshi Nakamura</name>
    </author>
    <author>
      <name>Satoshi Kozawa</name>
    </author>
    <author>
      <name>Yuhei Ueno</name>
    </author>
    <author>
      <name>Thomas N. Sato</name>
    </author>
    <link href="http://arxiv.org/abs/2105.04802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.04802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.04965">
    <id>http://arxiv.org/abs/2105.04965v1</id>
    <updated>2021-05-11T12:01:51Z</updated>
    <published>2021-05-11T12:01:51Z</published>
    <title>Compact Euler Tours of Trees with Small Maximum Degree</title>
    <summary>  We show how an Euler tour for a tree on $n$ vertices with maximum degree $d$
can be stored in $2 n + o (n)$ bits such that queries take $O (\log n)$ time
and updates take $O (d \log^{1 + \epsilon} n)$ time.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/2105.04965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.04965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.03782">
    <id>http://arxiv.org/abs/2105.03782v1</id>
    <updated>2021-05-08T21:24:55Z</updated>
    <published>2021-05-08T21:24:55Z</published>
    <title>Construction of Sparse Suffix Trees and LCE Indexes in Optimal Time and
  Space</title>
    <summary>  The notions of synchronizing and partitioning sets are recently introduced
variants of locally consistent parsings with great potential in
problem-solving. In this paper we propose a deterministic algorithm that
constructs for a given readonly string of length $n$ over the alphabet
$\{0,1,\ldots,n^{\mathcal{O}(1)}\}$ a version of $\tau$-partitioning set with
size $\mathcal{O}(b)$ and $\tau = \frac{n}{b}$ using $\mathcal{O}(b)$ space and
$\mathcal{O}(\frac{1}{\epsilon}n)$ time provided $b \ge n^\epsilon$, for
$\epsilon > 0$. As a corollary, for $b \ge n^\epsilon$ and constant $\epsilon >
0$, we obtain linear construction algorithms with $\mathcal{O}(b)$ space on top
of the string for two major small-space indexes: a sparse suffix tree, which is
a compacted trie built on $b$ chosen suffixes of the string, and a longest
common extension (LCE) index, which occupies $\mathcal{O}(b)$ space and allows
us to compute the longest common prefix for any pair of substrings in
$\mathcal{O}(n/b)$ time. For both, the $\mathcal{O}(b)$ construction storage is
asymptotically optimal since the tree itself takes $\mathcal{O}(b)$ space and
any LCE index with $\mathcal{O}(n/b)$ query time must occupy at least
$\mathcal{O}(b)$ space by a known trade-off (at least for $b \ge \Omega(n /
\log n)$). In case of arbitrary $b \ge \Omega(\log^2 n)$, we present
construction algorithms for the partitioning set, sparse suffix tree, and LCE
index with $\mathcal{O}(n\log_b n)$ running time and $\mathcal{O}(b)$ space,
thus also improving the state of the art.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Nikita Sivukhin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.03782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.03782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.03028">
    <id>http://arxiv.org/abs/2105.03028v1</id>
    <updated>2021-05-07T01:44:13Z</updated>
    <published>2021-05-07T01:44:13Z</published>
    <title>Improved Approximation for Longest Common Subsequence over Small
  Alphabets</title>
    <summary>  This paper investigates the approximability of the Longest Common Subsequence
(LCS) problem. The fastest algorithm for solving the LCS problem exactly runs
in essentially quadratic time in the length of the input, and it is known that
under the Strong Exponential Time Hypothesis the quadratic running time cannot
be beaten. There are no such limitations for the approximate computation of the
LCS however, except in some limited scenarios. There is also a scarcity of
approximation algorithms. When the two given strings are over an alphabet of
size $k$, returning the subsequence formed by the most frequent symbol
occurring in both strings achieves a $1/k$ approximation for the LCS. It is an
open problem whether a better than $1/k$ approximation can be achieved in truly
subquadratic time ($O(n^{2-\delta})$ time for constant $\delta>0$).
  A recent result [Rubinstein and Song SODA'2020] showed that a $1/2+\epsilon$
approximation for the LCS over a binary alphabet is possible in truly
subquadratic time, provided the input strings have the same length. In this
paper we show that if a $1/2+\epsilon$ approximation (for $\epsilon>0$) is
achievable for binary LCS in truly subquadratic time when the input strings can
be unequal, then for every constant $k$, there is a truly subquadratic time
algorithm that achieves a $1/k+\delta$ approximation for $k$-ary alphabet LCS
for some $\delta>0$. Thus the binary case is the hardest. We also show that for
every constant $k$, if one is given two strings of \emph{equal} length over a
$k$-ary alphabet, one can obtain a $1/k+\epsilon$ approximation for some
constant $\epsilon>0$ in truly subquadratic time, thus extending the Rubinstein
and Song result to all alphabets of constant size.
</summary>
    <author>
      <name>Shyan Akmal</name>
    </author>
    <author>
      <name>Virginia Vassilevska Williams</name>
    </author>
    <link href="http://arxiv.org/abs/2105.03028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.03028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.03106">
    <id>http://arxiv.org/abs/2105.03106v1</id>
    <updated>2021-05-07T08:19:25Z</updated>
    <published>2021-05-07T08:19:25Z</published>
    <title>Faster Algorithms for Longest Common Substring</title>
    <summary>  In the classic longest common substring (LCS) problem, we are given two
strings $S$ and $T$, each of length at most $n$, over an alphabet of size
$\sigma$, and we are asked to find a longest string occurring as a fragment of
both $S$ and $T$. Weiner, in his seminal paper that introduced the suffix tree,
presented an $\mathcal{O}(n \log \sigma)$-time algorithm for this problem [SWAT
1973]. For polynomially-bounded integer alphabets, the linear-time construction
of suffix trees by Farach yielded an $\mathcal{O}(n)$-time algorithm for the
LCS problem [FOCS 1997]. However, for small alphabets, this is not necessarily
optimal for the LCS problem in the word RAM model of computation, in which the
strings can be stored in $\mathcal{O}(n \log \sigma/\log n )$ space and read in
$\mathcal{O}(n \log \sigma/\log n )$ time. We show that, in this model, we can
compute an LCS in time $\mathcal{O}(n \log \sigma / \sqrt{\log n})$, which is
sublinear in $n$ if $\sigma=2^{o(\sqrt{\log n})}$ (in particular, if
$\sigma=\mathcal{O}(1)$), using optimal space $\mathcal{O}(n \log \sigma/\log
n)$.
  We then lift our ideas to the problem of computing a $k$-mismatch LCS, which
has received considerable attention in recent years. In this problem, the aim
is to compute a longest substring of $S$ that occurs in $T$ with at most $k$
mismatches. Thankachan et al.~showed how to compute a $k$-mismatch LCS in
$\mathcal{O}(n \log^k n)$ time for $k=\mathcal{O}(1)$ [J. Comput. Biol. 2016].
We show an $\mathcal{O}(n \log^{k-1/2} n)$-time algorithm, for any constant
$k>0$ and irrespective of the alphabet size, using $\mathcal{O}(n)$ space as
the previous approaches. We thus notably break through the well-known $n \log^k
n$ barrier, which stems from a recursive heavy-path decomposition technique
that was first introduced in the seminal paper of Cole et al. [STOC 2004] for
string indexing with $k$ errors.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <link href="http://arxiv.org/abs/2105.03106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.03106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.02428">
    <id>http://arxiv.org/abs/2105.02428v1</id>
    <updated>2021-05-06T03:54:27Z</updated>
    <published>2021-05-06T03:54:27Z</published>
    <title>Faster Algorithms for Bounded Tree Edit Distance</title>
    <summary>  Tree edit distance is a well-studied measure of dissimilarity between rooted
trees with node labels. It can be computed in $O(n^3)$ time [Demaine, Mozes,
Rossman, and Weimann, ICALP 2007], and fine-grained hardness results suggest
that the weighted version of this problem cannot be solved in truly subcubic
time unless the APSP conjecture is false [Bringmann, Gawrychowski, Mozes, and
Weimann, SODA 2018].
  We consider the unweighted version of tree edit distance, where every
insertion, deletion, or relabeling operation has unit cost. Given a parameter
$k$ as an upper bound on the distance, the previous fastest algorithm for this
problem runs in $O(nk^3)$ time [Touzet, CPM 2005], which improves upon the
cubic-time algorithm for $k\ll n^{2/3}$. In this paper, we give a faster
algorithm taking $O(nk^2 \log n)$ time, improving both of the previous results
for almost the full range of $\log n \ll k\ll n/\sqrt{\log n}$.
</summary>
    <author>
      <name>Shyan Akmal</name>
    </author>
    <author>
      <name>Ce Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICALP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.02428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.02428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.14903">
    <id>http://arxiv.org/abs/2105.14903v1</id>
    <updated>2021-05-31T12:06:09Z</updated>
    <published>2021-05-31T12:06:09Z</published>
    <title>Lower Bounds for the Number of Repetitions in 2D Strings</title>
    <summary>  A two-dimensional string is simply a two-dimensional array. We continue the
study of the combinatorial properties of repetitions in such strings over the
binary alphabet, namely the number of distinct tandems, distinct quartics, and
runs. First, we construct an infinite family of $n\times n$ 2D strings with
$\Omega(n^{3})$ distinct tandems. Second, we construct an infinite family of
$n\times n$ 2D strings with $\Omega(n^{2}\log n)$ distinct quartics. Third, we
construct an infinite family of $n\times n$ 2D strings with $\Omega(n^{2}\log
n)$ runs. This resolves an open question of Charalampopoulos, Radoszewski,
Rytter, Wale\'n, and Zuba [ESA 2020], who asked if the number of distinct
quartics and runs in an $n\times n$ 2D string is $\mathcal{O}(n^{2})$.
</summary>
    <author>
      <name>Pawe≈Ç Gawrychowski</name>
    </author>
    <author>
      <name>Samah Ghazawi</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <link href="http://arxiv.org/abs/2105.14903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.14903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.14990">
    <id>http://arxiv.org/abs/2105.14990v1</id>
    <updated>2021-05-31T14:22:20Z</updated>
    <published>2021-05-31T14:22:20Z</published>
    <title>A new distance based on minimal absent words and applications to
  biological sequences</title>
    <summary>  A minimal absent word of a sequence x, is a sequence yt hat is not a factorof
x, but all of its proper factors are factors of x as well. The set of minimal
absent words uniquely defines the sequence itself. In recent times minimal
absent words have been used in order to compare sequences. In fact, to do this,
one can compare the sets of their minimal absent words. Chairungasee and
Crochemorein [2] define a distance between pairs of sequences x and y, where
the symmetric difference of the sets of minimal absent words of x and y is
involved. Here, weconsider a different distance, introduced in [1], based on a
specific subset of such symmetric difference that, in our opinion, better
capture the different features ofthe considered sequences. We show the result
of some experiments where the distance is tested on a dataset of genetic
sequences by 11 living species, in order to compare the new distance with the
ones existing in literature.
</summary>
    <author>
      <name>Giuseppa Castiglione</name>
    </author>
    <author>
      <name>Jia Gao</name>
    </author>
    <author>
      <name>Sabrina Mantaci</name>
    </author>
    <author>
      <name>Antonio Restivo</name>
    </author>
    <link href="http://arxiv.org/abs/2105.14990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.14990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.13595">
    <id>http://arxiv.org/abs/2105.13595v1</id>
    <updated>2021-05-28T05:24:53Z</updated>
    <published>2021-05-28T05:24:53Z</published>
    <title>On Stricter Reachable Repetitiveness Measures*</title>
    <summary>  The size $b$ of the smallest bidirectional macro scheme, which is arguably
the most general copy-paste scheme to generate a given sequence, is considered
to be the strictest reachable measure of repetitiveness. It is strictly
lower-bounded by measures like $\gamma$ and $\delta$, which are known or
believed to be unreachable and to capture the entropy of repetitiveness. In
this paper we study another sequence generation mechanism, namely compositions
of a morphism. We show that these form another plausible mechanism to
characterize repetitive sequences and define NU-systems, which combine such a
mechanism with macro schemes. We show that the size $\nu \leq b$ of the
smallest NU-system is reachable and can be $o(\delta)$ for some string
families, thereby implying that the limit of compressibility of repetitive
sequences can be even smaller than previously thought. We also derive several
other results characterizing $\nu$.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Chile</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CeBiB</arxiv:affiliation>
    </author>
    <author>
      <name>Cristian Urbina</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Chile</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CeBiB</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Funded in part by Basal Funds FB0001, Fondecyt Grant 1-200038, and a
  Conicyt Doctoral Scholarship, ANID, Chile</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.13595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.13595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.13744">
    <id>http://arxiv.org/abs/2105.13744v1</id>
    <updated>2021-05-28T11:15:05Z</updated>
    <published>2021-05-28T11:15:05Z</published>
    <title>Grammar Index By Induced Suffix Sorting</title>
    <summary>  Pattern matching is the most central task for text indices. Most recent
indices leverage compression techniques to make pattern matching feasible for
massive but highly-compressible datasets. Within this kind of indices, we
propose a new compressed text index built upon a grammar compression based on
induced suffix sorting [Nunes et al., DCC'18]. We show that this grammar
exhibits a locality sensitive parsing property, which allows us to specify,
given a pattern $P$, certain substrings of $P$, called cores, that are
similarly parsed in the text grammar whenever these occurrences are extensible
to occurrences of $P$. Supported by the cores, given a pattern of length $m$,
we can locate all its $occ$ occurrences in a text $T$ of length $n$ within $O(m
\lg |\mathcal{S}| + occ_C \lg|\mathcal{S}| \lg n + occ)$ time, where
$\mathcal{S}$ is the set of all characters and non-terminals, $occ$ is the
number of occurrences, and $occ_C$ is the number of occurrences of a chosen
core $C$ of $P$ in the right hand side of all production rules of the grammar
of $T$. Our grammar index requires $O(g)$ words of space and can be built in
$O(n)$ time using $O(g)$ working space, where $g$ is the sum of the right hand
sides of all production rules. We underline the strength of our grammar index
with an exhaustive practical evaluation that gives evidence that our proposed
solution excels at locating long patterns in highly-repetitive texts.
</summary>
    <author>
      <name>Tooru Akagi</name>
    </author>
    <author>
      <name>Dominik K√∂ppl</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Our implementation is available at
  https://github.com/TooruAkagi/GCIS_Index</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.13744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.13744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.11693">
    <id>http://arxiv.org/abs/2105.11693v1</id>
    <updated>2021-05-25T06:22:04Z</updated>
    <published>2021-05-25T06:22:04Z</published>
    <title>Minimal unique palindromic substrings after single-character
  substitution</title>
    <summary>  A palindrome is a string that reads the same forward and backward. A
palindromic substring $w$ of a string $T$ is called a minimal unique
palindromic substring (MUPS) of $T$ if $w$ occurs only once in $T$ and any
proper palindromic substring of $w$ occurs at least twice in $T$. MUPSs are
utilized for answering the shortest unique palindromic substring problem, which
is motivated by molecular biology [Inoue et al., 2018]. Given a string $T$ of
length $n$, all MUPSs of $T$ can be computed in $O(n)$ time. In this paper, we
study the problem of updating the set of MUPSs when a character in the input
string $T$ is substituted by another character. We first analyze the number $d$
of changes of MUPSs when a character is substituted, and show that $d$ is in
$O(\log n)$. Further, we present an algorithm that uses $O(n)$ time and space
for preprocessing, and updates the set of MUPSs in $O(\log\sigma + (\log\log
n)^2 + d)$ time where $\sigma$ is the alphabet size. We also propose a variant
of the algorithm, which runs in optimal $O(d)$ time when the alphabet size is
constant.
</summary>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <link href="http://arxiv.org/abs/2105.11693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.10622">
    <id>http://arxiv.org/abs/2105.10622v1</id>
    <updated>2021-05-22T02:33:34Z</updated>
    <published>2021-05-22T02:33:34Z</published>
    <title>Support Optimality and Adaptive Cuckoo Filters</title>
    <summary>  Filters (such as Bloom Filters) are data structures that speed up network
routing and measurement operations by storing a compressed representation of a
set. Filters are space efficient, but can make bounded one-sided errors: with
tunable probability epsilon, they may report that a query element is stored in
the filter when it is not. This is called a false positive. Recent research has
focused on designing methods for dynamically adapting filters to false
positives, reducing the number of false positives when some elements are
queried repeatedly.
  Ideally, an adaptive filter would incur a false positive with bounded
probability epsilon for each new query element, and would incur o(epsilon)
total false positives over all repeated queries to that element. We call such a
filter support optimal.
  In this paper we design a new Adaptive Cuckoo Filter and show that it is
support optimal (up to additive logarithmic terms) over any n queries when
storing a set of size n. Our filter is simple: fixing previous false positives
requires a simple cuckoo operation, and the filter does not need to store any
additional metadata. This data structure is the first practical data structure
that is support optimal, and the first filter that does not require additional
space to fix false positives.
  We complement these bounds with experiments showing that our data structure
is effective at fixing false positives on network traces, outperforming
previous Adaptive Cuckoo Filters.
  Finally, we investigate adversarial adaptivity, a stronger notion of
adaptivity in which an adaptive adversary repeatedly queries the filter, using
the result of previous queries to drive the false positive rate as high as
possible. We prove a lower bound showing that a broad family of filters,
including all known Adaptive Cuckoo Filters, can be forced by such an adversary
to incur a large number of false positives.
</summary>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Samuel McCauley</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <link href="http://arxiv.org/abs/2105.10622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.10622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.11052">
    <id>http://arxiv.org/abs/2105.11052v1</id>
    <updated>2021-05-24T00:27:53Z</updated>
    <published>2021-05-24T00:27:53Z</published>
    <title>Fast and Space-Efficient Construction of AVL Grammars from the LZ77
  Parsing</title>
    <summary>  Grammar compression is, next to Lempel-Ziv (LZ77) and run-length
Burrows-Wheeler transform (RLBWT), one of the most flexible approaches to
representing and processing highly compressible strings. The main idea is to
represent a text as a context-free grammar whose language is precisely the
input string. This is called a straight-line grammar (SLG). An AVL grammar,
proposed by Rytter [Theor. Comput. Sci., 2003] is a type of SLG that
additionally satisfies the AVL-property: the heights of parse-trees for
children of every nonterminal differ by at most one. In contrast to other SLG
constructions, AVL grammars can be constructed from the LZ77 parsing in
compressed time: $\mathcal{O}(z \log n)$ where $z$ is the size of the LZ77
parsing and $n$ is the length of the input text. Despite these advantages, AVL
grammars are thought to be too large to be practical.
  We present a new technique for rapidly constructing a small AVL grammar from
an LZ77 or LZ77-like parse. Our algorithm produces grammars that are always at
least five times smaller than those produced by the original algorithm, and
never more than double the size of grammars produced by the practical Re-Pair
compressor [Larsson and Moffat, Proc. IEEE, 2000]. Our algorithm also achieves
low peak RAM usage. By combining this algorithm with recent advances in
approximating the LZ77 parsing, we show that our method has the potential to
construct a run-length BWT from an LZ77 parse in about one third of the time
and peak RAM required by other approaches. Overall, we show that AVL grammars
are surprisingly practical, opening the door to much faster construction of key
compressed data structures.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Ben Langmead</name>
    </author>
    <link href="http://arxiv.org/abs/2105.11052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry></articles>