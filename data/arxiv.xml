<?xml version="1.0" encoding="UTF-8"?><articles>
<entry id="1910.06169" xmlns="http://www.w3.org/2005/Atom">
    <id>http://arxiv.org/abs/1910.06169v1</id>
    <updated>2019-10-14T14:25:25Z</updated>
    <published>2019-10-14T14:25:25Z</published>
    <title>The PGM-index: a multicriteria, compressed and learned approach to data
  indexing</title>
    <summary>  The recent introduction of learned indexes has shaken the foundations of the
decades-old field of indexing data structures. Combining, or even replacing,
classic design elements such as B-tree nodes with machine learning models has
proven to give outstanding improvements in the space footprint and time
efficiency of data systems. However, these novel approaches are based on
heuristics, thus they lack any guarantees both in their time and space
requirements. We propose the Piecewise Geometric Model index (shortly,
PGM-index), which achieves guaranteed I/O-optimality in query operations,
learns an optimal number of linear models, and its peculiar recursive
construction makes it a purely learned data structure, rather than a hybrid of
traditional and learned indexes (such as RMI and FITing-tree). We show that the
PGM-index improves the space of the FITing-tree by 63.3% and of the B-tree by
more than four orders of magnitude, while achieving their same or even better
query time efficiency. We complement this result by proposing three variants of
the PGM-index. First, we design a compressed PGM-index that further reduces its
space footprint by exploiting the repetitiveness at the level of the learned
linear models it is composed of. Second, we design a PGM-index that adapts
itself to the distribution of the queries, thus resulting in the first known
distribution-aware learned index to date. Finally, given its flexibility in the
offered space-time trade-offs, we propose the multicriteria PGM-index that
efficiently auto-tune itself in a few seconds over hundreds of millions of keys
to the possibly evolving space-time constraints imposed by the application of
use.
  We remark to the reader that this paper is an extended and improved version
of our previous paper titled "Superseding traditional indexes by orchestrating
learning and geometry" (arXiv:1903.00507).
</summary>
    <author>
      <name>Paolo Ferragina</name>
    </author>
    <author>
      <name>Giorgio Vinciguerra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">We remark to the reader that this paper is an extended and improved
  version of our previous paper titled "Superseding traditional indexes by
  orchestrating learning and geometry" (arXiv:1903.00507)</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.4; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.04640">
    <id>http://arxiv.org/abs/1910.04640v1</id>
    <updated>2019-10-10T15:19:19Z</updated>
    <published>2019-10-10T15:19:19Z</published>
    <title>E2FM: an encrypted and compressed full-text index for collections of
  genomic sequences</title>
    <summary>  Next Generation Sequencing (NGS) platforms and, more generally,
high-throughput technologies are giving rise to an exponential growth in the
size of nucleotide sequence databases. Moreover, many emerging applications of
nucleotide datasets -- as those related to personalized medicine -- require the
compliance with regulations about the storage and processing of sensitive data.
We have designed and carefully engineered E2FM-index, a new full-text index in
minute space which was optimized for compressing and encrypting nucleotide
sequence collections in FASTA format and for performing fast pattern-search
queries. E2FM-index allows to build self-indexes which occupy till to 1/20 of
the storage required by the input FASTA file, thus permitting to save about 95%
of storage when indexing collections of highly similar sequences; moreover, it
can exactly search the built indexes for patterns in times ranging from few
milliseconds to a few hundreds milliseconds, depending on pattern length.
Supplementary material and supporting datasets are available through
Bioinformatics Online and https://figshare.com/s/6246ee9c1bd730a8bf6e.
</summary>
    <author>
      <name>Ferdinando Montecuollo</name>
    </author>
    <author>
      <name>Giovannni Schmid</name>
    </author>
    <author>
      <name>Roberto Tagliaferri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/bioinformatics/btx313</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/bioinformatics/btx313" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages with pseudo-code and experimental results</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bioinformatics, 33(18), 2017, 2808-2817</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.04640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.04640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.04728">
    <id>http://arxiv.org/abs/1910.04728v1</id>
    <updated>2019-10-10T17:41:53Z</updated>
    <published>2019-10-10T17:41:53Z</published>
    <title>LISA: Towards Learned DNA Sequence Search</title>
    <summary>  Next-generation sequencing (NGS) technologies have enabled affordable
sequencing of billions of short DNA fragments at high throughput, paving the
way for population-scale genomics. Genomics data analytics at this scale
requires overcoming performance bottlenecks, such as searching for short DNA
sequences over long reference sequences. In this paper, we introduce LISA
(Learned Indexes for Sequence Analysis), a novel learning-based approach to DNA
sequence search. As a first proof of concept, we focus on accelerating one of
the most essential flavors of the problem, called exact search. LISA builds on
and extends FM-index, which is the state-of-the-art technique widely deployed
in genomics tool-chains. Initial experiments with human genome datasets
indicate that LISA achieves up to a factor of 4X performance speedup against
its traditional counterpart.
</summary>
    <author>
      <name>Darryl Ho</name>
    </author>
    <author>
      <name>Jialin Ding</name>
    </author>
    <author>
      <name>Sanchit Misra</name>
    </author>
    <author>
      <name>Nesime Tatbul</name>
    </author>
    <author>
      <name>Vikram Nathan</name>
    </author>
    <author>
      <name>Vasimuddin Md</name>
    </author>
    <author>
      <name>Tim Kraska</name>
    </author>
    <link href="http://arxiv.org/abs/1910.04728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.04728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.03578">
    <id>http://arxiv.org/abs/1910.03578v1</id>
    <updated>2019-10-08T09:12:47Z</updated>
    <published>2019-10-08T09:12:47Z</published>
    <title>Stack Sorting with Increasing and Decreasing Stacks</title>
    <summary>  We introduce a sorting machine consisting of $k+1$ stacks in series: the
first $k$ stacks can only contain elements in decreasing order from top to
bottom, while the last one has the opposite restriction. This device
generalizes \cite{SM}, which studies the case $k=1$. Here we show that, for
$k=2$, the set of sortable permutations is a class with infinite basis, by
explicitly finding an antichain of minimal nonsortable permutations. This
construction can easily be adapted to each $k \ge 3$. Next we describe an
optimal sorting algorithm, again for the case $k=2$. We then analyze two types
of left-greedy sorting procedures, obtaining complete results in one case and
only some partial results in the other one. We close the paper by discussing a
few open questions.
</summary>
    <author>
      <name>Giulio Cerbai</name>
    </author>
    <author>
      <name>Lapo Cioni</name>
    </author>
    <author>
      <name>Luca Ferrari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.03578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.03578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.02611">
    <id>http://arxiv.org/abs/1910.02611v1</id>
    <updated>2019-10-07T05:15:27Z</updated>
    <published>2019-10-07T05:15:27Z</published>
    <title>RAMBO: Repeated And Merged Bloom Filter for Multiple Set Membership
  Testing (MSMT) in Sub-linear time</title>
    <summary>  Approximate set membership is a common problem with wide applications in
databases, networking, and search. Given a set S and a query q, the task is to
determine whether q in S. The Bloom Filter (BF) is a popular data structure for
approximate membership testing due to its simplicity. In particular, a BF
consists of a bit array that can be incrementally updated. A related problem
concerning this paper is the Multiple Set Membership Testing (MSMT) problem.
Here we are given K different sets, and for any given query q the goal is the
find all of the sets containing the query element. Trivially, a multiple set
membership instance can be reduced to K membership testing instances, each with
the same q, leading to O(K) query time. A simple array of Bloom Filters can
achieve that. In this paper, we show the first non-trivial data-structure for
streaming keys, RAMBO (Repeated And Merged Bloom Filter) that achieves expected
O(sqrt(K) logK) query time with an additional worst case memory cost factor of
O(logK) than the array of Bloom Filters. The proposed data-structure is simply
a count-min sketch arrangement of Bloom Filters and retains all its favorable
properties. We replace the addition operation with a set union and the minimum
operation with a set intersection during estimation.
</summary>
    <author>
      <name>Gaurav Gupta</name>
    </author>
    <author>
      <name>Benjamin Coleman</name>
    </author>
    <author>
      <name>Tharun Medini</name>
    </author>
    <author>
      <name>Vijai Mohan</name>
    </author>
    <author>
      <name>Anshumali Shrivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.02611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.07145">
    <id>http://arxiv.org/abs/1910.07145v2</id>
    <updated>2020-03-21T01:05:37Z</updated>
    <published>2019-10-16T03:14:03Z</published>
    <title>Practical Random Access to Large SLP-Compressed Texts</title>
    <summary>  Grammar-based compression is a popular and powerful approach to compressing
repetitive texts but until recently its relatively poor time-space trade-offs
in real life made it impractical for truly massive datasets such as genomic
databases. In a recent paper (SPIRE 2019) we showed how simple pre-processing
can dramatically improve those trade-offs. Now that grammar-based compression
itself is reasonably scalable, in this paper we turn our attention to one of
the features that make grammar-based compression so attractive: the possibility
of supporting fast random access. In this paper we introduce a new encoding in
which we identify symbols by their offsets among those with the same expansion
sizes, thus tightly integrating our encodings of the symbols in the parse tree
and its shape.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <author>
      <name>Louisa Seelbach Benkner</name>
    </author>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <link href="http://arxiv.org/abs/1910.07145v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07145v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.12370">
    <id>http://arxiv.org/abs/1904.12370v2</id>
    <updated>2019-10-14T14:04:03Z</updated>
    <published>2019-04-28T19:04:24Z</published>
    <title>Compact Fenwick trees for dynamic ranking and selection</title>
    <summary>  The Fenwick tree is a classical implicit data structure that stores an array
in such a way that modifying an element, accessing an element, computing a
prefix sum and performing a predecessor search on prefix sums all take
logarithmic time. We introduce a number of variants which improve the classical
implementation of the tree: in particular, we can reduce its size when an upper
bound on the array element is known, and we can perform much faster predecessor
searches. Our aim is to use our variants to implement an efficient dynamic bit
vector: our structure is able to perform updates, ranking and selection in
logarithmic time, with a space overhead in the order of a few percents,
outperforming existing data structures with the same purpose. Along the way, we
highlight the pernicious interplay between the arithmetic behind the Fenwick
tree and the structure of current CPU caches, suggesting simple solutions that
improve performance significantly.
</summary>
    <author>
      <name>Stefano Marchini</name>
    </author>
    <author>
      <name>Sebastiano Vigna</name>
    </author>
    <link href="http://arxiv.org/abs/1904.12370v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.12370v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.06437">
    <id>http://arxiv.org/abs/1910.06437v2</id>
    <updated>2019-11-14T14:50:14Z</updated>
    <published>2019-10-14T21:44:14Z</published>
    <title>It is high time we let go of the Mersenne Twister</title>
    <summary>  When the Mersenne Twister made his first appearance in 1997 it was a powerful
example of how linear maps on $\mathbf F_2$ could be used to generate
pseudorandom numbers. In particular, the easiness with which generators with
long periods could be defined gave the Mersenne Twister a large following, in
spite of the fact that such long periods are not a measure of quality, and they
require a large amount of memory. Even at the time of its publication, several
defects of the Mersenne Twister were predictable, but they were somewhat
obscured by other interesting properties. Today the Mersenne Twister is the
default generator in C compilers, the Python language, the Maple mathematical
computation system, and in many other environments. Nonetheless, knowledge
accumulated in the last $20$ years suggests that the Mersenne Twister has, in
fact, severe defects, and should never be used as a general-purpose
pseudorandom number generator. Many of these results are folklore, or are
scattered through very specialized literature. This paper surveys these results
for the non-specialist, providing new, simple, understandable examples, and it
is intended as a guide for the final user, or for language implementors, so
that they can take an informed decision about whether to use the Mersenne
Twister or not.
</summary>
    <author>
      <name>Sebastiano Vigna</name>
    </author>
    <link href="http://arxiv.org/abs/1910.06437v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06437v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.06920">
    <id>http://arxiv.org/abs/1910.06920v1</id>
    <updated>2019-10-15T16:45:59Z</updated>
    <published>2019-10-15T16:45:59Z</published>
    <title>Apply Sorting Algorithms to FAST Problem</title>
    <summary>  FAST problem is finding minimum feedback arc set problem in tournaments. In
this paper we present some algorithms that are similar to sorting algorithms
for FAST problem and we analyze them. We present Pseudo_InsertionSort algorithm
for FAST problem and we show that average number of all backward edges in
output of that is equal to ((n^2-5n+8)/4)-2^(1-n). We introduce
Pseudo_MergeSort algorithm and we find the probability of being backward for an
edge. Finally we introduce other algorithms for this problem.
</summary>
    <author>
      <name>Sadra Mohammadshirazi</name>
    </author>
    <author>
      <name>Alireza Bagheri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.06416">
    <id>http://arxiv.org/abs/1910.06416v2</id>
    <updated>2019-11-30T11:47:44Z</updated>
    <published>2019-10-14T20:50:22Z</published>
    <title>RecSplit: Minimal Perfect Hashing via Recursive Splitting</title>
    <summary>  A minimal perfect hash function bijectively maps a key set $S$ out of a
universe $U$ into the first $|S|$ natural numbers. Minimal perfect hash
functions are used, for example, to map irregularly-shaped keys, such as
string, in a compact space so that metadata can then be simply stored in an
array. While it is known that just $1.44$ bits per key are necessary to store a
minimal perfect function, no published technique can go below $2$ bits per key
in practice. We propose a new technique for storing minimal perfect hash
functions with expected linear construction time and expected constant lookup
time that makes it possible to build for the first time, for example,
structures which need $1.56$ bits per key, that is, within $8.3$% of the lower
bound, in less than $2$ ms per key. We show that instances of our construction
are able to simultaneously beat the construction time, space usage and lookup
time of the state-of-the-art data structure reaching $2$ bits per key.
Moreover, we provide parameter choices giving structures which are competitive
with alternative, larger-size data structures in terms of space and lookup
time. The construction of our data structures can be easily parallelized or
mapped on distributed computational units (e.g., within the MapReduce
framework), and structures larger than the available RAM can be directly built
in mass storage.
</summary>
    <author>
      <name>Emmanuel Esposito</name>
    </author>
    <author>
      <name>Thomas Mueller Graf</name>
    </author>
    <author>
      <name>Sebastiano Vigna</name>
    </author>
    <link href="http://arxiv.org/abs/1910.06416v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06416v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.11564">
    <id>http://arxiv.org/abs/1910.11564v1</id>
    <updated>2019-10-25T08:17:10Z</updated>
    <published>2019-10-25T08:17:10Z</published>
    <title>Non-Rectangular Convolutions and (Sub-)Cadences with Three Elements</title>
    <summary>  The discrete acyclic convolution computes the 2n-1 sums sum_{i+j=k; (i,j) in
[0,1,2,...,n-1]^2} (a_i b_j) in O(n log n) time. By using suitable offsets and
setting some of the variables to zero, this method provides a tool to calculate
all non-zero sums sum_{i+j=k; (i,j) in (P cap Z^2)} (a_i b_j) in a rectangle P
with perimeter p in O(p log p) time.
  This paper extends this geometric interpretation in order to allow arbitrary
convex polygons P with k vertices and perimeter p. Also, this extended
algorithm only needs O(k + p(log p)^2 log k) time.
  Additionally, this paper presents fast algorithms for counting sub-cadences
and cadences with 3 elements using this extended method.
</summary>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Julian Pape-Lange</name>
    </author>
    <link href="http://arxiv.org/abs/1910.11564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.10406">
    <id>http://arxiv.org/abs/1910.10406v1</id>
    <updated>2019-10-23T08:24:15Z</updated>
    <published>2019-10-23T08:24:15Z</published>
    <title>Analyzing Trade-offs in Reversible Linear and Binary Search Algorithms</title>
    <summary>  Reversible algorithms are algorithms in which each step represents a partial
injective function; they are useful for performance optimization in reversible
systems. In this study, using Janus, a reversible imperative high-level
programming language, we have developed reversible linear and binary search
algorithms. We have analyzed the non-trivial space-time trade-offs between
them, focusing on the memory usage disregarding original inputs and outputs,
the size of the output garbage disregarding the original inputs, and the
maximum amount of traversal of the input. The programs in this study can easily
be adapted to other reversible programming languages. Our analysis reveals that
the change of the output data and/or the data structure affects the design of
efficient reversible algorithms. For example, the number of input data
traversals depends on whether the search has succeeded or failed, while it
expectedly never changes in corresponding irreversible linear and binary
searches. Our observations indicate the importance of the selection of data
structures and what is regarded as the output with the aim of the reversible
algorithm design.
</summary>
    <author>
      <name>Hiroki Masuda</name>
    </author>
    <author>
      <name>Tetsuo Yokoyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Third Workshop on Software Foundations for Data
  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.10406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.10631">
    <id>http://arxiv.org/abs/1910.10631v1</id>
    <updated>2019-10-23T15:54:53Z</updated>
    <published>2019-10-23T15:54:53Z</published>
    <title>Resolution of the Burrows-Wheeler Transform Conjecture</title>
    <summary>  Burrows-Wheeler Transform (BWT) is an invertible text transformation that
permutes symbols of a text according to the lexicographical order of its
suffixes. BWT is the main component of some of the most popular lossless
compression methods as well as of compressed indexes, central in modern
bioinformatics. The compression ratio of BWT-based compressors, such as bzip2,
is quantified by the number $r$ of maximal equal-letter runs in the BWT. This
is also (up to ${\rm polylog}\,n$ factors, where $n$ is the length of the text)
the space used by the state-of-the-art BWT-based indexes, such as the recent
$r$-index [Gagie et al., SODA 2018]. The output size of virtually every known
compression method is known to be either within a ${\rm polylog}\,n$ factor
from $z$, the size of Lempel-Ziv (LZ77) parsing of the text, or significantly
larger (by a $n^{\epsilon}$ factor for $\epsilon > 0$). The value of $r$ has
resisted, however, all attempts and until now, no non-trivial upper bounds on
$r$ were known.
  In this paper, we show that every text satisfies $r=\mathcal{O}(z\log^2 n)$.
This result has a number of immediate implications: (1) it proves that a large
body of work related to BWT automatically applies to the so-far disjoint field
of Lempel--Ziv indexing and compression, e.g., it is possible to obtain full
functionality of the suffix tree and the suffix array in $\mathcal{O}(z\,{\rm
polylog}\,n)$ space; (2) it lets us relate the number of runs in the BWT of the
text and its reverse; (3) it shows that many fundamental text processing tasks
can be solved in the optimal time assuming that the text is compressible by a
sufficiently large ${\rm polylog}\,n$ factor using LZ77.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <link href="http://arxiv.org/abs/1910.10631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.07819">
    <id>http://arxiv.org/abs/1910.07819v1</id>
    <updated>2019-10-17T10:41:47Z</updated>
    <published>2019-10-17T10:41:47Z</published>
    <title>EvoZip: Efficient Compression of Large Collections of Evolutionary Trees</title>
    <summary>  Phylogenetic trees represent evolutionary relationships among sets of
organisms. Popular phylogenetic reconstruction approaches typically yield
hundreds to thousands of trees on a common leafset. Storing and sharing such
large collection of trees requires considerable amount of space and bandwidth.
Furthermore, the huge size of phylogenetic tree databases can make search and
retrieval operations time-consuming. Phylogenetic compression techniques are
specialized compression techniques that exploit redundant topological
information to achieve better compression of phylogenetic trees. Here, we
present EvoZip, a new approach for phylogenetic tree compression. On average,
EvoZip achieves 71.6% better compression and takes 80.71% less compression time
and 60.47% less decompression time than TreeZip, the current state-of-the-art
algorithm for phylogenetic tree compression. While EvoZip is based on TreeZip,
it betters TreeZip due to (a) an improved bipartition and support list encoding
scheme, (b) use of Deflate compression algorithm, and (c) use of an efficient
tree reconstruction algorithm. EvoZip is freely available online for use by the
scientific community.
</summary>
    <author>
      <name>Balanand Jha</name>
    </author>
    <author>
      <name>David Fernández-Baca</name>
    </author>
    <author>
      <name>Akshay Deepak</name>
    </author>
    <author>
      <name>Kumar Abhishek</name>
    </author>
    <link href="http://arxiv.org/abs/1910.07819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.07849">
    <id>http://arxiv.org/abs/1910.07849v2</id>
    <updated>2019-10-28T10:22:03Z</updated>
    <published>2019-10-17T12:15:09Z</published>
    <title>Engineering Top-Down Weight-Balanced Trees</title>
    <summary>  Weight-balanced trees are a popular form of self-balancing binary search
trees. Their popularity is due to desirable guarantees, for example regarding
the required work to balance annotated trees.
  While usual weight-balanced trees perform their balancing operations in a
bottom-up fashion after a modification to the tree is completed, there exists a
top-down variant which performs these balancing operations during descend. This
variant has so far received only little attention. We provide an in-depth
analysis and engineering of these top-down weight-balanced trees, demonstrating
their superior performance. We also gaining insights into how the balancing
parameters necessary for a weight-balanced tree should be chosen - with the
surprising observation that it is often beneficial to choose parameters which
are not feasible in the sense of the correctness proofs for the rebalancing
algorithm.
</summary>
    <author>
      <name>Lukas Barth</name>
    </author>
    <author>
      <name>Dorothea Wagner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/1.9781611976007.13</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/1.9781611976007.13" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at ALENEX 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.07849v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07849v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.13479">
    <id>http://arxiv.org/abs/1910.13479v1</id>
    <updated>2019-10-29T18:58:48Z</updated>
    <published>2019-10-29T18:58:48Z</published>
    <title>Practical Repetition-Aware Grammar Compression</title>
    <summary>  The goal of grammar compression is to construct a small sized context free
grammar which uniquely generates the input text data. Among grammar compression
methods, RePair is known for its good practical compression performance.
MR-RePair was recently proposed as an improvement to RePair for constructing
small-sized context free grammar for repetitive text data. However, a compact
encoding scheme has not been discussed for MR-RePair. We propose a practical
encoding method for MR-RePair and show its effectiveness through comparative
experiments. Moreover, we extend MR-RePair to run-length context free grammar
and design a novel variant for it called RL-MR-RePair. We experimentally
demonstrate that a compression scheme consisting of RL-MR-RePair and the
proposed encoding method show good performance on real repetitive datasets.
</summary>
    <author>
      <name>Isamu Furuya</name>
    </author>
    <link href="http://arxiv.org/abs/1910.13479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.13479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.11993">
    <id>http://arxiv.org/abs/1910.11993v1</id>
    <updated>2019-10-26T04:17:37Z</updated>
    <published>2019-10-26T04:17:37Z</published>
    <title>Selection on $X_1+X_2+\cdots + X_m$ with layer-ordered heaps</title>
    <summary>  Selection on $X_1+X_2+\cdots + X_m$ is an important problem with many
applications in areas such as max-convolution, max-product Bayesian inference,
calculating most probable isotopes, and computing non-parametric test
statistics, among others. Faster-than-na\"{i}ve approaches exist for $m=2$:
Johnson \&amp; Mizoguchi (1978) find the smallest $k$ values in $A+B$ with runtime
$O(n \log(n))$. Frederickson \&amp; Johnson (1982) created a method for finding the
$k$ smallest values in $A+B$ with runtime $O(n +
\min(k,n)\log(\frac{k}{\min(k,n)}))$. In 1993, Frederickson published an
optimal algorithm for selection on $A+B$, which runs in $O(n+k)$. In 2018,
Kaplan \emph{et al.} described another optimal algorithm in terms Chazelle's of
soft heaps. No fast methods exist for $m>2$. Johnson \&amp; Mizoguchi (1978)
introduced a method to compute the minimal $k$ terms when $m>2$, but that
method runs in $O(m\cdot n^{\frac{m}{2}} \log(n))$ and is inefficient when $m
\gg 1$.
  In this paper, we introduce the first efficient methods for problems where
$m>2$. We introduce the ``layer-ordered heap,'' a simple special class of heap
with which we produce a new, fast selection algorithm on the Cartesian product.
Using this new algorithm to perform $k$-selection on the Cartesian product of
$m$ arrays of length $n$ has runtime $\in o(m\cdot n + k\cdot m)$. We also
provide implementations of the algorithms proposed and their performance in
practice.
</summary>
    <author>
      <name>Patrick Kreitzberg</name>
    </author>
    <author>
      <name>Kyle Lucke</name>
    </author>
    <author>
      <name>Oliver Serang</name>
    </author>
    <link href="http://arxiv.org/abs/1910.11993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.01851">
    <id>http://arxiv.org/abs/1911.01851v1</id>
    <updated>2019-11-02T14:56:53Z</updated>
    <published>2019-11-02T14:56:53Z</published>
    <title>Lyndon words versus inverse Lyndon words: queries on suffixes and
  bordered words</title>
    <summary>  Lyndon words have been largely investigated and showned to be a useful tool
to prove interesting combinatorial properties of words. In this paper we state
new properties of both Lyndon and inverse Lyndon factorizations of a word $w$,
with the aim of exploring their use in some classical queries on $w$.
  The main property we prove is related to a classical query on words. We prove
that there are relations between the length of the longest common extension (or
longest common prefix) $lcp(x,y)$ of two different suffixes $x,y$ of a word $w$
and the maximum length $\mathcal{M}$ of two consecutive factors of the inverse
Lyndon factorization of $w$. More precisely, $\mathcal{M}$ is an upper bound on
the length of $lcp(x,y)$. This result is in some sense stronger than the
compatibility property, proved by Mantaci, Restivo, Rosone and Sciortino for
the Lyndon factorization and here for the inverse Lyndon factorization.
Roughly, the compatibility property allows us to extend the mutual order
between local suffixes of (inverse) Lyndon factors to the suffixes of the whole
word.
  A main tool used in the proof of the above results is a property that we
state for factors $m_i$ with nonempty borders in an inverse Lyndon
factorization: a nonempty border of $m_i$ cannot be a prefix of the next factor
$m_{i+1}$. The last property we prove shows that if two words share a common
overlap, then their Lyndon factorizations can be used to capture the common
overlap of the two words.
  The above results open to the study of new applications of Lyndon words and
inverse Lyndon words in the field of string comparison.
</summary>
    <author>
      <name>Paola Bonizzoni</name>
    </author>
    <author>
      <name>Clelia De Felice</name>
    </author>
    <author>
      <name>Rocco Zaccagnino</name>
    </author>
    <author>
      <name>Rosalba Zizza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1705.10277</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.01851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.01763">
    <id>http://arxiv.org/abs/1911.01763v1</id>
    <updated>2019-11-05T13:36:15Z</updated>
    <published>2019-11-05T13:36:15Z</published>
    <title>An Efficient Word Lookup System by using Improved Trie Algorithm</title>
    <summary>  Efficiently word storing and searching is an important task in computer
science. An application space complexity, time complexity, and overall
performance depend on this string data. Many word searching data structures and
algorithms exist in the current world but few of them have space compress
ability. Trie is a popular data structure for word searching for its linear
searching capability. It is the basic and important part of various computer
applications such as information retrieval, natural language processing,
database system, compiler, and computer network. But currently, the available
version of trie tree cannot be used widely because of its high memory
requirement. This paper proposes a new Radix trie based data structure for word
storing and searching which can share not only just prefix but also infix and
suffix and thus reduces memory requirement. We propose a new emptiness property
to Radix trie. Proposed trie has character cell reduction capability and it can
dramatically reduce any application runtime memory size. Using it as data tank
to an operating system the overall main memory requirement of a device can be
reduced to a large extent.
</summary>
    <author>
      <name>Rahat Yeasin Emon</name>
    </author>
    <author>
      <name>Sharmistha Chanda Tista</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.01763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="null" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.01644">
    <id>http://arxiv.org/abs/1911.01644v1</id>
    <updated>2019-11-05T06:59:56Z</updated>
    <published>2019-11-05T06:59:56Z</published>
    <title>Fast Multiple Pattern Cartesian Tree Matching</title>
    <summary>  Cartesian tree matching is the problem of finding all substrings in a given
text which have the same Cartesian trees as that of a given pattern. In this
paper, we deal with Cartesian tree matching for the case of multiple patterns.
We present two fingerprinting methods, i.e., the parent-distance encoding and
the binary encoding. By combining an efficient fingerprinting method and a
conventional multiple string matching algorithm, we can efficiently solve
multiple pattern Cartesian tree matching. We propose three practical algorithms
for multiple pattern Cartesian tree matching based on the Wu-Manber algorithm,
the Rabin-Karp algorithm, and the Alpha Skip Search algorithm, respectively. In
the experiments we compare our solutions against the previous algorithm [18].
Our solutions run faster than the previous algorithm as the pattern lengths
increase. Especially, our algorithm based on Wu-Manber runs up to 33 times
faster.
</summary>
    <author>
      <name>Geonmo Gu</name>
    </author>
    <author>
      <name>Siwoo Song</name>
    </author>
    <author>
      <name>Simone Faro</name>
    </author>
    <author>
      <name>Thierry Lecroq</name>
    </author>
    <author>
      <name>Kunsoo Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to WALCOM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.01644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.01414">
    <id>http://arxiv.org/abs/1911.01414v2</id>
    <updated>2019-11-12T18:55:43Z</updated>
    <published>2019-11-04T18:57:04Z</published>
    <title>Counting Small Permutation Patterns</title>
    <summary>  A sample of n generic points in the xy-plane defines a permutation that
relates their ranks along the two axes. Every subset of k points similarly
defines a pattern, which occurs in that permutation. The number of occurrences
of small patterns in a large permutation arises in many areas, including
nonparametric statistics. It is therefore desirable to count them more
efficiently than the straightforward ~O(n^k) time algorithm.
  This work proposes new algorithms for counting patterns. We show that all
patterns of order 2 and 3, as well as eight patterns of order 4, can be counted
in nearly linear time. To that end, we develop an algebraic framework that we
call corner tree formulas. Our approach generalizes the existing methods and
allows a systematic study of their scope.
  Using the machinery of corner trees, we find twenty-three independent linear
combinations of order-4 patterns, that can be computed in time ~O(n). We also
describe an algorithm that counts another 4-pattern, and hence all 4-patterns,
in time ~O(n^(3/2)).
  As a practical application, we provide a nearly linear time computation of a
statistic by Yanagimoto (1970), Bergsma and Dassios (2010). This statistic
yields a natural and strongly consistent variant of Hoeffding's test for
independence of X and Y, given a random sample as above. This improves upon the
so far most efficient ~O(n^2) algorithm.
</summary>
    <author>
      <name>Chaim Even-Zohar</name>
    </author>
    <author>
      <name>Calvin Leng</name>
    </author>
    <link href="http://arxiv.org/abs/1911.01414v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01414v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.01348">
    <id>http://arxiv.org/abs/1911.01348v1</id>
    <updated>2019-11-04T17:18:44Z</updated>
    <published>2019-11-04T17:18:44Z</published>
    <title>Nearly Optimal Static Las Vegas Succinct Dictionary</title>
    <summary>  Given a set $S$ of $n$ (distinct) keys from key space $[U]$, each associated
with a value from $\Sigma$, the \emph{static dictionary} problem asks to
preprocess these (key, value) pairs into a data structure, supporting
value-retrieval queries: for any given $x\in [U]$, $\mathtt{valRet}(x)$ must
return the value associated with $x$ if $x\in S$, or return $\bot$ if $x\notin
S$. The special case where $|\Sigma|=1$ is called the \emph{membership}
problem. The "textbook" solution is to use a hash table, which occupies linear
space and answers each query in constant time. On the other hand, the minimum
possible space to encode all (key, value) pairs is only $\mathtt{OPT}:=
\lceil\lg_2\binom{U}{n}+n\lg_2|\Sigma|\rceil$ bits, which could be much less.
  In this paper, we design a randomized dictionary data structure using
$\mathtt{OPT}+\mathrm{poly}\lg n+O(\lg\lg\lg\lg\lg U)$ bits of space, and it
has \emph{expected constant} query time, assuming the query algorithm can
access an external lookup table of size $n^{0.001}$. The lookup table depends
only on $U$, $n$ and $|\Sigma|$, and not the input. Previously, even for
membership queries and $U\leq n^{O(1)}$, the best known data structure with
constant query time requires $\mathtt{OPT}+n/\mathrm{poly}\lg n$ bits of space
(Pagh [Pag01] and P\v{a}tra\c{s}cu [Pat08]); the best-known using
$\mathtt{OPT}+n^{0.999}$ space has query time $O(\lg n)$; the only known
non-trivial data structure with $\mathtt{OPT}+n^{0.001}$ space has $O(\lg n)$
query time and requires a lookup table of size $\geq n^{2.99}$ (!). Our new
data structure answers open questions by P\v{a}tra\c{s}cu and Thorup
[Pat08,Tho13].
</summary>
    <author>
      <name>Huacheng Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1911.01348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.01169">
    <id>http://arxiv.org/abs/1911.01169v1</id>
    <updated>2019-11-04T12:45:25Z</updated>
    <published>2019-11-04T12:45:25Z</published>
    <title>Optimal Adaptive Detection of Monotone Patterns</title>
    <summary>  We investigate adaptive sublinear algorithms for detecting monotone patterns
in an array. Given fixed $2 \leq k \in \mathbb{N}$ and $\varepsilon > 0$,
consider the problem of finding a length-$k$ increasing subsequence in an array
$f \colon [n] \to \mathbb{R}$, provided that $f$ is $\varepsilon$-far from free
of such subsequences. Recently, it was shown that the non-adaptive query
complexity of the above task is $\Theta((\log n)^{\lfloor \log_2 k \rfloor})$.
In this work, we break the non-adaptive lower bound, presenting an adaptive
algorithm for this problem which makes $O(\log n)$ queries. This is optimal,
matching the classical $\Omega(\log n)$ adaptive lower bound by Fischer [2004]
for monotonicity testing (which corresponds to the case $k=2$), and implying in
particular that the query complexity of testing whether the longest increasing
subsequence (LIS) has constant length is $\Theta(\log n)$.
</summary>
    <author>
      <name>Omri Ben-Eliezer</name>
    </author>
    <author>
      <name>Shoham Letzter</name>
    </author>
    <author>
      <name>Erik Waingarten</name>
    </author>
    <link href="http://arxiv.org/abs/1911.01169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.00044">
    <id>http://arxiv.org/abs/1911.00044v3</id>
    <updated>2020-01-17T16:14:20Z</updated>
    <published>2019-10-31T18:19:55Z</published>
    <title>Edge minimization in de Bruijn graphs</title>
    <summary>  This paper introduces the de Bruijn graph edge minimization problem, which is
related to the compression of de Bruijn graphs: find the order-k de Bruijn
graph with minimum edge count among all orders. We describe an efficient
algorithm that solves this problem. Since the edge minimization problem is
connected to the BWT compression technique called "tunneling", the paper also
describes a way to minimize the length of a tunneled BWT in such a way that
useful properties for sequence analysis are preserved. Although being a
restriction, this is significant progress towards a solution to the open
problem of finding optimal disjoint blocks that minimize space, as stated in
Alanko et al. (DCC 2019).
</summary>
    <author>
      <name>Uwe Baier</name>
    </author>
    <author>
      <name>Thomas Büchler</name>
    </author>
    <author>
      <name>Enno Ohlebusch</name>
    </author>
    <author>
      <name>Pascal Weber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Data Compression Conference 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.00044v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00044v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.14508">
    <id>http://arxiv.org/abs/1910.14508v2</id>
    <updated>2019-11-15T17:03:58Z</updated>
    <published>2019-10-31T14:44:42Z</published>
    <title>ALLSAT compressed with wildcards: Frequent Set Mining</title>
    <summary>  Like any simplicial complex the simplicial complex of all frequent sets can
be compressed with wildcards once the maximal frequent sets (=facets) are
known. Namely, the task (a particular kind of ALLSAT problem) is achieved by
the author's recent algorithm Facets-To-Faces. But how to get the facets in the
first place? The novel algorithm Find-All-Facets determines all facets of any
(decidable) finite simplicial complex by replacing costly hypergraph
dualization (Dualize+Advance and its variants) with the cheaper calculation of
the minimal members of certain set families. The latter task is sped up by
Vertical Layout. While all of this concerns arbitrary simplicial complexes, the
impact to Frequent Set Mining (FSM) seems particularly high.
</summary>
    <author>
      <name>Marcel Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">I solicite the help of FSM practitioners for the final version of
  this article!</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.14508v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.14508v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.06347">
    <id>http://arxiv.org/abs/1911.06347v1</id>
    <updated>2019-11-14T19:07:20Z</updated>
    <published>2019-11-14T19:07:20Z</published>
    <title>In Search of the Fastest Concurrent Union-Find Algorithm</title>
    <summary>  Union-Find (or Disjoint-Set Union) is one of the fundamental problems in
computer science; it has been well-studied from both theoretical and practical
perspectives in the sequential case. Recently, there has been mounting interest
in analyzing this problem in the concurrent scenario, and several
asymptotically-efficient algorithms have been proposed. Yet, to date, there is
very little known about the practical performance of concurrent Union-Find.
  This work addresses this gap. We evaluate and analyze the performance of
several concurrent Union-Find algorithms and optimization strategies across a
wide range of platforms (Intel, AMD, and ARM) and workloads (social, random,
and road networks, as well as integrations into more complex algorithms). We
first observe that, due to the limited computational cost, the number of
induced cache misses is the critical determining factor for the performance of
existing algorithms. We introduce new techniques to reduce this cost by storing
node priorities implicitly and by using plain reads and writes in a way that
does not affect the correctness of the algorithms. Finally, we show that
Union-Find implementations are an interesting application for Transactional
Memory (TM): one of the fastest algorithm variants we discovered is a
sequential one that uses coarse-grained locking with the lock elision
optimization to reduce synchronization cost and increase scalability.
</summary>
    <author>
      <name>Dan Alistarh</name>
    </author>
    <author>
      <name>Alexander Fedorov</name>
    </author>
    <author>
      <name>Nikita Koval</name>
    </author>
    <link href="http://arxiv.org/abs/1911.06347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.05676">
    <id>http://arxiv.org/abs/1911.05676v1</id>
    <updated>2019-11-13T17:55:06Z</updated>
    <published>2019-11-13T17:55:06Z</published>
    <title>Enumerative Data Compression with Non-Uniquely Decodable Codes</title>
    <summary>  Non-uniquely decodable codes can be defined as the codes that cannot be
uniquely decoded without additional disambiguation information. These are
mainly the class of non-prefix-free codes, where a codeword can be a prefix of
other(s), and thus, the codeword boundary information is essential for correct
decoding. Although the codeword bit stream consumes significantly less space
when compared to prefix--free codes, the additional disambiguation information
makes it difficult to catch the performance of prefix-free codes in total.
Previous studies considered compression with non-prefix-free codes by
integrating rank/select dictionaries or wavelet trees to mark the code-word
boundaries. In this study we focus on another dimension with a block--wise
enumeration scheme that improves the compression ratios of the previous studies
significantly. Experiments conducted on a known corpus showed that the proposed
scheme successfully represents a source within its entropy, even performing
better than the Huffman and arithmetic coding in some cases. The non-uniquely
decodable codes also provides an intrinsic security feature due to lack of
unique-decodability. We investigate this dimension as an opportunity to provide
compressed data security without (or with less) encryption, and discuss various
possible practical advantages supported by such codes.
</summary>
    <author>
      <name>M. Oğuzhan Külekci</name>
    </author>
    <author>
      <name>Yasin Öztürk</name>
    </author>
    <author>
      <name>Elif Altunok</name>
    </author>
    <author>
      <name>Can Altıniğne</name>
    </author>
    <link href="http://arxiv.org/abs/1911.05676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.05676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.05060">
    <id>http://arxiv.org/abs/1911.05060v1</id>
    <updated>2019-11-12T18:35:28Z</updated>
    <published>2019-11-12T18:35:28Z</published>
    <title>Fully-Dynamic Space-Efficient Dictionaries and Filters with Constant
  Number of Memory Accesses</title>
    <summary>  A fully-dynamic dictionary is a data structure for maintaining sets that
supports insertions, deletions and membership queries. A filter approximates
membership queries with a one-sided error. We present two designs:
  1. The first space-efficient fully-dynamic dictionary that maintains both
sets and random multisets and supports queries, insertions, and deletions with
a constant number of memory accesses in the worst case with high probability.
The comparable dictionary of Arbitman, Naor, and Segev [FOCS 2010] works only
for sets.
  2. By a reduction from our dictionary for random multisets, we obtain a
space-efficient fully-dynamic filter that supports queries, insertions, and
deletions with a constant number of memory accesses in the worst case with high
probability (as long as the false positive probability is $2^{-O(w)}$, where
$w$ denotes the word length). This is the first in-memory space-efficient
fully-dynamic filter design that provably achieves these properties.
  We also present an application of the techniques used to design our
dictionary to the static Retrieval Problem.
</summary>
    <author>
      <name>Ioana O. Bercea</name>
    </author>
    <author>
      <name>Guy Even</name>
    </author>
    <link href="http://arxiv.org/abs/1911.05060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.05060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.03542">
    <id>http://arxiv.org/abs/1911.03542v2</id>
    <updated>2019-12-10T13:59:09Z</updated>
    <published>2019-11-08T21:15:21Z</published>
    <title>Space Efficient Construction of Lyndon Arrays in Linear Time</title>
    <summary>  We present the first linear time algorithm to construct the $2n$-bit version
of the Lyndon array for a string of length $n$ using only $o(n)$ bits of
working space. A simpler variant of this algorithm computes the plain ($n\lg
n$-bit) version of the Lyndon array using only $\mathcal{O}(1)$ words of
additional working space. All previous algorithms are either not linear, or use
at least $n\lg n$ bits of additional working space. Also in practice, our new
algorithms outperform the previous best ones by an order of magnitude, both in
terms of time and space.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Jonas Ellert</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Florian Kurpicz</name>
    </author>
    <author>
      <name>Ian Munro</name>
    </author>
    <author>
      <name>Eva Rotenberg</name>
    </author>
    <link href="http://arxiv.org/abs/1911.03542v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03542v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.04202">
    <id>http://arxiv.org/abs/1911.04202v1</id>
    <updated>2019-11-11T12:05:37Z</updated>
    <published>2019-11-11T12:05:37Z</published>
    <title>Dv2v: A Dynamic Variable-to-Variable Compressor</title>
    <summary>  We present Dv2v, a new dynamic (one-pass) variable-to-variable compressor.
Variable-to-variable compression aims at using a modeler that gathers
variable-length input symbols and a variable-length statistical coder that
assigns shorter codewords to the more frequent symbols. In Dv2v, we process the
input text word-wise to gather variable-length symbols that can be either
terminals (new words) or non-terminals, subsequences of words seen before in
the input text. Those input symbols are set in a vocabulary that is kept sorted
by frequency. Therefore, those symbols can be easily encoded with dense codes.
Our Dv2v permits real-time transmission of data, i.e. compression/transmission
can begin as soon as data become available. Our experiments show that Dv2v is
able to overcome the compression ratios of the v2vDC, the state-of-the-art
semi-static variable-to-variable compressor, and to almost reach p7zip values.
It also draws a competitive performance at both compression and decompression.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Antonio Fariña</name>
    </author>
    <author>
      <name>Adrián Gómez-Brandón</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Tirso V. Rodeiro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DCC.2019.00016</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DCC.2019.00016" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dv2v: A Dynamic Variable-to-Variable Compressor. In 2019 Data
  Compression Conference (DCC) (pp. 83-92). IEEE</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.04202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.04202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.04198">
    <id>http://arxiv.org/abs/1911.04198v1</id>
    <updated>2019-11-11T11:54:20Z</updated>
    <published>2019-11-11T11:54:20Z</published>
    <title>GraCT: A Grammar-based Compressed Index for Trajectory Data</title>
    <summary>  We introduce a compressed data structure for the storage of free trajectories
of moving objects (such as ships and planes) that efficiently supports various
spatio-temporal queries. Our structure, dubbed GraCT, stores the absolute
positions of all the objects at regular time intervals (snapshots) using a
$k^2$-tree, which is a space- and time-efficient version of a region quadtree.
Positions between snapshots are represented as logs of relative movements and
compressed using Re-Pair, a grammar-based compressor. The nonterminals of this
grammar are enhanced with MBR information to enable fast queries.
  The GraCT structure of a dataset occupies less than the raw data compressed
with a powerful traditional compressor such as p7zip. Further, instead of
requiring full decompression to access the data like a traditional compressor,
GraCT supports direct access to object trajectories or to their position at
specific time instants, as well as spatial range and nearest-neighbor queries
on time instants and/or time intervals.
  Compared to traditional methods for storing and indexing spatio-temporal
data, GraCT requires two orders of magnitude less space, and is competitive in
query times. In particular, thanks to its compressed representation, the GraCT
structure may reside in main memory in situations where any classical
uncompressed index must resort to disk, thereby being one or two orders of
magnitude faster.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Adrián Gómez-Brandón</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>José R. Paramá</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2019.01.035</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2019.01.035" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Sciences, 2019, vol. 483, p. 106-135</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.04198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.04198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.03028">
    <id>http://arxiv.org/abs/1911.03028v1</id>
    <updated>2019-11-08T03:55:54Z</updated>
    <published>2019-11-08T03:55:54Z</published>
    <title>Lock-Free Hopscotch Hashing</title>
    <summary>  In this paper we present a lock-free version of Hopscotch Hashing. Hopscotch
Hashing is an open addressing algorithm originally proposed by Herlihy, Shavit,
and Tzafrir, which is known for fast performance and excellent cache locality.
The algorithm allows users of the table to skip or jump over irrelevant
entries, allowing quick search, insertion, and removal of entries. Unlike
traditional linear probing, Hopscotch Hashing is capable of operating under a
high load factor, as probe counts remain small. Our lock-free version improves
on both speed, cache locality, and progress guarantees of the original, being a
chimera of two concurrent hash tables. We compare our data structure to various
other lock-free and blocking hashing algorithms and show that its performance
is in many cases superior to existing strategies. The proposed lock-free
version overcomes some of the drawbacks associated with the original blocking
version, leading to a substantial boost in scalability while maintaining
attractive features like physical deletion or probe-chain compression.
</summary>
    <author>
      <name>Robert Kelly</name>
    </author>
    <author>
      <name>Barak A. Pearlmutter</name>
    </author>
    <author>
      <name>Phil Maguire</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, to appear in APOCS20</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.03028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.02889">
    <id>http://arxiv.org/abs/1911.02889v2</id>
    <updated>2019-11-15T11:28:31Z</updated>
    <published>2019-11-07T13:24:50Z</published>
    <title>Towards Better Compressed Representations</title>
    <summary>  We introduce the problem of computing a parsing where each phrase is of
length at most $m$ and which minimizes the zeroth order entropy of parsing.
Based on the recent theoretical results we devise a heuristic for this problem.
The solution has straightforward application in succinct text representations
and gives practical improvements. Moreover the proposed heuristic yields
structure whose size can be bounded both by $|S|H_{m-1}(S)$ and by
$|S|/m(H_0(S) + \cdots + H_{m-1})$, where $H_{k}(S)$ is the $k$-th order
empirical entropy of $S$. We also consider a similar problem in which the
first-order entropy is minimized.
</summary>
    <author>
      <name>Michał Gańczorz</name>
    </author>
    <link href="http://arxiv.org/abs/1911.02889v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.02889v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.03035">
    <id>http://arxiv.org/abs/1911.03035v2</id>
    <updated>2020-02-18T18:21:38Z</updated>
    <published>2019-11-08T04:03:40Z</published>
    <title>On the Complexity of BWT-runs Minimization via Alphabet Reordering</title>
    <summary>  The Burrows-Wheeler Transform (BWT) has been an essential tool in text
compression and indexing. First introduced in 1994, it went on to provide the
backbone for the first encoding of the classic suffix tree data structure in
space close to the entropy-based lower bound. Recently, there has been the
development of compact suffix trees in space proportional to "$r$", the number
of runs in the BWT, as well as the appearance of $r$ in the time complexity of
new algorithms. Unlike other popular measures of compression, the parameter $r$
is sensitive to the lexicographic ordering given to the text's alphabet.
Despite several past attempts to exploit this, a provably efficient algorithm
for finding, or approximating, an alphabet ordering which minimizes $r$ has
been open for years.
  We present the first set of results on the computational complexity of
minimizing BWT-runs via alphabet reordering. We prove that the decision version
of this problem is NP-complete and cannot be solved in time $2^{o(\sigma +
\sqrt{n})}$ unless the Exponential Time Hypothesis fails, where $\sigma$ is the
size of the alphabet and $n$ is the length of the text. We also show that the
optimization problem is APX-hard. In doing so, we relate two previously
disparate topics: the optimal traveling salesperson path and the number of runs
in the BWT of a text, providing a surprising connection between problems on
graphs and text compression. Also, by relating recent results in the field of
dictionary compression, we illustrate that an arbitrary alphabet ordering
provides a $O(\log^2 n)$-approximation.
  We provide an optimal linear-time algorithm for the problem of finding a run
minimizing ordering on a subset of symbols (occurring only once) under ordering
constraints, and prove a generalization of this problem to a class of graphs
with BWT like properties called Wheeler graphs is NP-complete.
</summary>
    <author>
      <name>Jason Bentley</name>
    </author>
    <author>
      <name>Daniel Gibney</name>
    </author>
    <author>
      <name>Sharma V. Thankachan</name>
    </author>
    <link href="http://arxiv.org/abs/1911.03035v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03035v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.03195">
    <id>http://arxiv.org/abs/1911.03195v2</id>
    <updated>2019-12-06T12:12:28Z</updated>
    <published>2019-11-08T11:35:29Z</published>
    <title>On dynamic succinct graph representations</title>
    <summary>  We address the problem of representing dynamic graphs using $k^2$-trees. The
$k^2$-tree data structure is one of the succinct data structures proposed for
representing static graphs, and binary relations in general. It relies on
compact representations of bit vectors. Hence, by relying on compact
representations of dynamic bit vectors, we can also represent dynamic graphs.
In this paper we follow instead the ideas by Munro {\em et al.}, and we present
an alternative implementation for representing dynamic graphs using
$k^2$-trees. Our experimental results show that this new implementation is
competitive in practice.
</summary>
    <author>
      <name>Miguel E. Coimbra</name>
    </author>
    <author>
      <name>Alexandre P. Francisco</name>
    </author>
    <author>
      <name>Luís M. S. Russo</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Susana Ladra</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.03195v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03195v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.11048">
    <id>http://arxiv.org/abs/1911.11048v1</id>
    <updated>2019-11-25T17:00:13Z</updated>
    <published>2019-11-25T17:00:13Z</published>
    <title>Listing Conflicting Triples in Optimal Time</title>
    <summary>  Different sources of information might tell different stories about the
evolutionary history of a given set of species. This leads to (rooted)
phylogenetic trees that "disagree" on triples of species, which we call
"conflict triples". An important subtask of computing consensus trees which is
interesting in its own regard is the enumeration of all conflicts exhibited by
a pair of phylogenetic trees (on the same set of $n$ taxa). As it is possible
that a significant part of the $n^3$ triples are in conflict, the trivial
${\Theta}(n^3)$-time algorithm that checks for each triple whether it
constitutes a conflict, was considered optimal. It turns out, however, that we
can do way better in the case that there are only few conflicts. In particular,
we show that we can enumerate all d conflict triples between a pair of
phylogenetic trees in $O(n + d)$ time. Since any deterministic algorithm has to
spend ${\Theta}(n)$ time reading the input and ${\Theta}(d)$ time writing the
output, no deterministic algorithm can solve this task faster than we do (up to
constant factors).
</summary>
    <author>
      <name>Mathias Weller</name>
    </author>
    <link href="http://arxiv.org/abs/1911.11048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.09498">
    <id>http://arxiv.org/abs/1911.09498v1</id>
    <updated>2019-11-21T14:42:08Z</updated>
    <published>2019-11-21T14:42:08Z</published>
    <title>Implementing the Topological Model Succinctly</title>
    <summary>  We show that the topological model, a semantically rich standard to represent
GIS data, can be encoded succinctly while efficiently answering a number of
topology-related queries. We build on recent succinct planar graph
representations so as to encode a model with $m$ edges within $4m+o(m)$ bits
and answer various queries relating nodes, edges, and faces in $o(\log\log m)$
time, or any time in $\omega(\log m)$ for a few complex ones.
</summary>
    <author>
      <name>José Fuentes-Sepúlveda</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Diego Seco</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-32686-9_35</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-32686-9_35" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. Conference version
  presented at SPIRE 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.09498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.09498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.09370">
    <id>http://arxiv.org/abs/1911.09370v1</id>
    <updated>2019-11-21T09:56:37Z</updated>
    <published>2019-11-21T09:56:37Z</published>
    <title>Energy consumption in compact integer vectors: A study case</title>
    <summary>  In the field of algorithms and data structures analysis and design, most of
the researchers focus only on the space/time trade-off, and little attention
has been paid to energy consumption. Moreover, most of the efforts in the field
of Green Computing have been devoted to hardware-related issues, being green
software in its infancy. Optimizing the usage of computing resources,
minimizing power consumption or increasing battery life are some of the goals
of this field of research.
  As an attempt to address the most recent sustainability challenges, we must
incorporate the energy consumption as a first-class constraint when designing
new compact data structures. Thus, as a preliminary work to reach that goal, we
first need to understand the factors that impact on the energy consumption and
their relation with compression. In this work, we study the energy consumption
required by several integer vector representations. We execute typical
operations over datasets of different nature. We can see that, as commonly
believed, energy consumption is highly related to the time required by the
process, but not always. We analyze other parameters, such as number of
instructions, number of CPU cycles, memory loads, among others.
</summary>
    <author>
      <name>José Fuentes-Sepúlveda</name>
    </author>
    <author>
      <name>Susana Ladra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2019.2949655</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2019.2949655" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access 7, pp. 155625-155636 (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.09370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.09370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.08971">
    <id>http://arxiv.org/abs/1911.08971v1</id>
    <updated>2019-11-20T15:35:20Z</updated>
    <published>2019-11-20T15:35:20Z</published>
    <title>Faster Dynamic Compressed d-ary Relations</title>
    <summary>  The $k^2$-tree is a successful compact representation of binary relations
that exhibit sparseness and/or clustering properties. It can be extended to $d$
dimensions, where it is called a $k^d$-tree. The representation boils down to a
long bitvector. We show that interpreting the $k^d$-tree as a dynamic trie on
the Morton codes of the points, instead of as a dynamic representation of the
bitvector as done in previous work, yields operation times that are below the
lower bound of dynamic bitvectors and offers improved time performance in
practice.
</summary>
    <author>
      <name>Diego Arroyuelo</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-32686-9_30</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-32686-9_30" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. SPIRE 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.08971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.08971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.09077">
    <id>http://arxiv.org/abs/1911.09077v2</id>
    <updated>2019-11-21T21:32:34Z</updated>
    <published>2019-11-20T18:29:30Z</published>
    <title>Grammar Compressed Sequences with Rank/Select Support</title>
    <summary>  Sequence representations supporting not only direct access to their symbols,
but also rank/select operations, are a fundamental building block in many
compressed data structures. Several recent applications need to represent
highly repetitive sequences, and classical statistical compression proves
ineffective. We introduce, instead, grammar-based representations for
repetitive sequences, which use up to 6% of the space needed by statistically
compressed representations, and support direct access and rank/select
operations within tens of microseconds. We demonstrate the impact of our
structures in text indexing applications.
</summary>
    <author>
      <name>Alberto Ordóñez</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jda.2016.10.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jda.2016.10.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Discrete Algorithms 43, pp. 54-71 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.09077v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.09077v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.08376">
    <id>http://arxiv.org/abs/1911.08376v1</id>
    <updated>2019-11-19T16:18:15Z</updated>
    <published>2019-11-19T16:18:15Z</published>
    <title>Extending General Compact Querieable Representations to GIS Applications</title>
    <summary>  The raster model is commonly used for the representation of images in many
domains, and is especially useful in Geographic Information Systems (GIS) to
store information about continuous variables of the space (elevation,
temperature, etc.). Current representations of raster data are usually designed
for external memory or, when stored in main memory, lack efficient query
capabilities. In this paper we propose compact representations to efficiently
store and query raster datasets in main memory. We present different
representations for binary raster data, general raster data and time-evolving
raster data. We experimentally compare our proposals with traditional storage
mechanisms such as linear quadtrees or compressed GeoTIFF files. Results show
that our structures are up to 10 times smaller than classical linear quadtrees,
and even comparable in space to non-querieable representations of raster data,
while efficiently answering a number of typical queries.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Ana Cerdeira-Pena</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Oscar Pedreira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2019.08.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2019.08.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941,</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Sciences 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.08376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.08376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.09415">
    <id>http://arxiv.org/abs/1907.09415v1</id>
    <updated>2019-07-19T08:56:18Z</updated>
    <published>2019-07-19T08:56:18Z</published>
    <title>Quantum Computing: Lecture Notes</title>
    <summary>  This is a set of lecture notes suitable for a Master's course on quantum
computation and information from the perspective of theoretical computer
science. The first version was written in 2011, with many extensions and
improvements in subsequent years. The first 10 chapters cover the circuit model
and the main quantum algorithms (Deutsch-Jozsa, Simon, Shor, Hidden Subgroup
Problem, Grover, quantum walks, Hamiltonian simulation and HHL). They are
followed by 2 chapters about complexity, 4 chapters about distributed ("Alice
and Bob") settings, and a final chapter about quantum error correction.
Appendices A and B give a brief introduction to the required linear algebra and
some other mathematical and computer science background. All chapters come with
exercises, with some hints provided in Appendix C.
</summary>
    <author>
      <name>Ronald de Wolf</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">QuSoft, CWI and University of Amsterdam</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">165 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.09415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.09415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.08372">
    <id>http://arxiv.org/abs/1911.08372v1</id>
    <updated>2019-11-19T16:07:49Z</updated>
    <published>2019-11-19T16:07:49Z</published>
    <title>Improved Compressed String Dictionaries</title>
    <summary>  We introduce a new family of compressed data structures to efficiently store
and query large string dictionaries in main memory. Our main technique is a
combination of hierarchical Front-coding with ideas from longest-common-prefix
computation in suffix arrays. Our data structures yield relevant space-time
tradeoffs in real-world dictionaries. We focus on two domains where string
dictionaries are extensively used and efficient compression is required: URL
collections, a key element in Web graphs and applications such as Web mining;
and collections of URIs and literals, the basic components of RDF datasets. Our
experiments show that our data structures achieve better compression than the
state-of-the-art alternatives while providing very competitive query times.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Ana Cerdeira-Pena</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3357384.3357972</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3357384.3357972" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 28th ACM International Conference on Information and
  Knowledge Management (CIKM 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.08372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.08372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.07124">
    <id>http://arxiv.org/abs/1911.07124v2</id>
    <updated>2019-12-11T00:15:07Z</updated>
    <published>2019-11-17T01:18:03Z</published>
    <title>Faster Integer Multiplication Using Preprocessing</title>
    <summary>  A New Number Theoretic Transform(NTT), which is a form of FFT, is introduced,
that is faster than FFTs. Also, a multiplication algorithm is introduced that
uses this to perform integer multiplication faster than O(n log n). It uses
preprocessing to achieve an upper bounds of (n log n/(log log n/ log log log
n).
  Also, we explore the possibility of O(n) time multiplication via NTTs that
require only O(n) operations, using preprocessing.
</summary>
    <author>
      <name>Matt Groff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.07124v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07124v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.06985">
    <id>http://arxiv.org/abs/1911.06985v1</id>
    <updated>2019-11-16T08:04:25Z</updated>
    <published>2019-11-16T08:04:25Z</published>
    <title>Constructing the Bijective BWT</title>
    <summary>  The Burrows-Wheeler transform (BWT) is a permutation whose applications are
prevalent in data compression and text indexing. The bijective BWT (BBWT) is a
bijective variant of it. Although it is known that the BWT can be constructed
in linear time for integer alphabets by using a linear time suffix array
construction algorithm, it was up to now only conjectured that the BBWT can
also be constructed in linear time. We confirm this conjecture by proposing a
construction algorithm that is based on SAIS, improving the best known result
of $O(n \lg n /\lg \lg n)$ time to linear.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Marcin Picatkowski</name>
    </author>
    <link href="http://arxiv.org/abs/1911.06985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.10140">
    <id>http://arxiv.org/abs/1912.10140v1</id>
    <updated>2019-12-20T23:12:09Z</updated>
    <published>2019-12-20T23:12:09Z</published>
    <title>String factorisations with maximum or minimum dimension</title>
    <summary>  In this paper we consider two problems concerning string factorisation.
Specifically given a string $w$ and an integer $k$ find a factorisation of $w$
where each factor has length bounded by $k$ and has the minimum (the FmD
problem) or the maximum (the FMD problem) number of different factors. The FmD
has been proved to be NP-hard even if $k=2$ in [9] and for this case we provide
a $3/2$-approximation algorithm. The FMD problem, up to our knowledge has not
been considered in the literature. We show that this problem is NP-hard for any
$k\geq 3$. In view of this we propose a $2$-approximation algorithm (for any
$k$) an exact exponential algorithm. We conclude with some open problems.
</summary>
    <author>
      <name>Angelo Monti</name>
    </author>
    <author>
      <name>Blerina Sinaimeri</name>
    </author>
    <link href="http://arxiv.org/abs/1912.10140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.09783">
    <id>http://arxiv.org/abs/1912.09783v2</id>
    <updated>2020-02-11T08:33:09Z</updated>
    <published>2019-12-20T12:16:29Z</published>
    <title>Circ-Tree: A B+-Tree Variant with Circular Design for Persistent Memory</title>
    <summary>  Several B+-tree variants have been developed to exploit the performance
potential of byte-addressable non-volatile memory (NVM). In this paper, we
attentively investigate the properties of B+-tree and find that, a conventional
B+-tree node is a linear structure in which key-value (KV) pairs are maintained
from the zero offset of the node. These pairs are shifted in a unidirectional
fashion for insertions and deletions. Inserting and deleting one KV pair may
inflict a large amount of write amplifications due to shifting KV pairs. This
badly impairs the performance of in-NVM B+-tree. In this paper, we propose a
novel circular design for B+-tree. With regard to NVM's byte-addressability,
our Circ-tree design embraces tree nodes in a circular structure without a
fixed base address, and bidirectionally shifts KV pairs in a node for
insertions and deletions to minimize write amplifications. We have implemented
a prototype for Circ-Tree and conducted extensive experiments. Experimental
results show that Circ-Tree significantly outperforms two state-of-the-art
in-NVM B+-tree variants, i.e., NV-tree and FAST+FAIR, by up to 1.6x and 8.6x,
respectively, in terms of write performance. The end-to-end comparison by
running YCSB to KV store systems built on NV-tree, FAST+FAIR, and Circ-Tree
reveals that Circ-Tree yields up to 29.3% and 47.4% higher write performance,
respectively, than NV-tree and FAST+FAIR.
</summary>
    <author>
      <name>Chundong Wang</name>
    </author>
    <author>
      <name>Gunavaran Brihadiswarn</name>
    </author>
    <author>
      <name>Xingbin Jiang</name>
    </author>
    <author>
      <name>Sudipta Chattopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.09783v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.09783v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.08258">
    <id>http://arxiv.org/abs/1912.08258v3</id>
    <updated>2020-01-27T18:27:49Z</updated>
    <published>2019-12-17T20:07:53Z</published>
    <title>Xor Filters: Faster and Smaller Than Bloom and Cuckoo Filters</title>
    <summary>  The Bloom filter provides fast approximate set membership while using little
memory. Engineers often use these filters to avoid slow operations such as disk
or network accesses. As an alternative, a cuckoo filter may need less space
than a Bloom filter and it is faster. Chazelle et al. proposed a generalization
of the Bloom filter called the Bloomier filter. Dietzfelbinger and Pagh
described a variation on the Bloomier filter that can be used effectively for
approximate membership queries. It has never been tested empirically, to our
knowledge. We review an efficient implementation of their approach, which we
call the xor filter. We find that xor filters can be faster than Bloom and
cuckoo filters while using less memory. We further show that a more compact
version of xor filters (xor+) can use even less space than highly compact
alternatives (e.g., Golomb-compressed sequences) while providing speeds
competitive with Bloom filters.
</summary>
    <author>
      <name>Thomas Mueller Graf</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <link href="http://arxiv.org/abs/1912.08258v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08258v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.08147">
    <id>http://arxiv.org/abs/1912.08147v1</id>
    <updated>2019-12-17T17:29:45Z</updated>
    <published>2019-12-17T17:29:45Z</published>
    <title>New Bounds on Antipowers in Binary Words</title>
    <summary>  Fici et al. defined a word to be a k-power if it is the concatenation of k
consecutive identical blocks, and an r-antipower if it is the concatenation of
r pairwise distinct blocks of the same size. They defined N(k, r) as the
shortest length l such that every binary word of length l contains either a
k-power or an r-antipower. In this note we obtain some new upper and lower
bounds on N(k, r).
</summary>
    <author>
      <name>Lukas Fleischer</name>
    </author>
    <author>
      <name>Samin Riasat</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/1912.08147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.09625">
    <id>http://arxiv.org/abs/1903.09625v2</id>
    <updated>2019-12-10T17:58:58Z</updated>
    <published>2019-03-22T17:46:15Z</published>
    <title>Matching strings in encoded sequences</title>
    <summary>  We investigate the longest common substring problem for encoded sequences and
its asymptotic behaviour. The main result is a strong law of large numbers for
a re-scaled version of this quantity, which presents an explicit relation with
the R\'enyi entropy of the source. We apply this result to the zero-inflated
contamination model and the stochastic scrabble. In the case of dynamical
systems, this problem is equivalent to the shortest distance between two
observed orbits and its limiting relationship with the correlation dimension of
the pushforward measure. An extension to the shortest distance between orbits
for random dynamical systems is also provided.
</summary>
    <author>
      <name>Adriana Coutinho</name>
    </author>
    <author>
      <name>Rodrigo Lambert</name>
    </author>
    <author>
      <name>Jérôme Rousseau</name>
    </author>
    <link href="http://arxiv.org/abs/1903.09625v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.09625v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.02900">
    <id>http://arxiv.org/abs/1912.02900v2</id>
    <updated>2019-12-21T20:45:16Z</updated>
    <published>2019-12-05T22:13:34Z</published>
    <title>Pinning Down the Strong Wilber 1 Bound for Binary Search Trees</title>
    <summary>  The dynamic optimality conjecture, postulating the existence of an
$O(1)$-competitive online algorithm for binary search trees (BSTs), is among
the most fundamental open problems in dynamic data structures. Despite
extensive work and some notable progress, including, for example, the Tango
Trees (Demaine et al., FOCS 2004), that give the best currently known $O(\log
\log n)$-competitive algorithm, the conjecture remains widely open. One of the
main hurdles towards settling the conjecture is that we currently do not have
approximation algorithms achieving better than an $O(\log \log
n)$-approximation, even in the offline setting. All known non-trivial
algorithms for BST's so far rely on comparing the algorithm's cost with the
so-called Wilber's first bound (WB-1). Therefore, establishing the worst-case
relationship between this bound and the optimal solution cost appears crucial
for further progress, and it is an interesting open question in its own right.
  Our contribution is two-fold. First, we show that the gap between the WB-1
bound and the optimal solution value can be as large as $\Omega(\log \log n/
\log \log \log n)$; in fact, the gap holds even for several stronger variants
of the bound. Second, we provide a simple algorithm, that, given an integer
$D>0$, obtains an $O(D)$-approximation in time $\exp\left(O\left
(n^{1/2^{\Omega(D)}}\log n\right )\right )$. In particular, this gives a
constant-factor approximation sub-exponential time algorithm. Moreover, we
obtain a simpler and cleaner efficient $O(\log \log n)$-approximation algorithm
that can be used in an online setting. Finally, we suggest a new bound, that we
call {\em Guillotine Bound}, that is stronger than WB, while maintaining its
algorithm-friendly nature, that we hope will lead to better algorithms. All our
results use the geometric interpretation of the problem, leading to cleaner and
simpler analysis.
</summary>
    <author>
      <name>Parinya Chalermsook</name>
    </author>
    <author>
      <name>Julia Chuzhoy</name>
    </author>
    <author>
      <name>Thatchaphol Saranurak</name>
    </author>
    <link href="http://arxiv.org/abs/1912.02900v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.02900v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.00692">
    <id>http://arxiv.org/abs/1912.00692v2</id>
    <updated>2019-12-03T13:52:57Z</updated>
    <published>2019-12-02T11:41:02Z</published>
    <title>Gardens of Eden in the Game of Life</title>
    <summary>  We prove that in the Game of Life, if the thickness-four zero-padding of a
rectangular pattern is not an orphan, then the corresponding finite-support
configuration is not a Garden of Eden, and that the preimage of every
finite-support configuration has dense semilinear configurations. In particular
finite-support Gardens of Eden are in co-NP.
</summary>
    <author>
      <name>Ville Salo</name>
    </author>
    <author>
      <name>Ilkka Törmä</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages + 5 pages of code; some figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.00692v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.00692v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.12464">
    <id>http://arxiv.org/abs/1911.12464v3</id>
    <updated>2020-01-04T12:44:38Z</updated>
    <published>2019-11-27T23:40:26Z</published>
    <title>Words With Few Palindromes, Revisited</title>
    <summary>  In 2013, Fici and Zamboni proved a number of theorems about finite and
infinite words having only a small number of factors that are palindromes. In
this paper we rederive some of their results, and obtain some new ones, by a
different method based on finite automata.
</summary>
    <author>
      <name>Lukas Fleischer</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor typo corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.12464v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.12464v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.11704">
    <id>http://arxiv.org/abs/1911.11704v2</id>
    <updated>2019-12-09T15:41:13Z</updated>
    <published>2019-11-26T17:14:56Z</published>
    <title>Words Avoiding Reversed Factors, Revisited</title>
    <summary>  In 2005, Rampersad and the second author proved a number of theorems about
infinite words x with the property that if w is any sufficiently long finite
factor of x, then its reversal w^R is not a factor of x. In this note we
revisit these results, reproving them in more generality, using machine
computations only. Two different techniques are presented.
</summary>
    <author>
      <name>Lukas Fleischer</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/1911.11704v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11704v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.11637">
    <id>http://arxiv.org/abs/1911.11637v1</id>
    <updated>2019-11-25T08:42:50Z</updated>
    <published>2019-11-25T08:42:50Z</published>
    <title>Fast Fibonacci heaps with worst case extensions</title>
    <summary>  We are concentrating on reducing overhead of heaps based on comparisons with
optimal worstcase behaviour. The paper is inspired by Strict Fibonacci Heaps
[1], where G. S. Brodal, G. Lagogiannis, and R. E. Tarjan implemented the heap
with DecreaseKey and Meld interface in assymptotically optimal worst case times
(based on key comparisons). In the paper [2], the ideas were elaborated and it
was shown that the same asymptotical times could be achieved with a strategy
loosing much less information from previous comparisons. There is big overhead
with maintainance of violation lists in these heaps. We propose simple
alternative reducing this overhead. It allows us to implement fast amortized
Fibonacci heaps, where user could call some methods in variants guaranting
worst case time. If he does so, the heaps are not guaranted to be Fibonacci
until an amortized version of a method is called. Of course we could call worst
case versions all the time, but as there is an overhead with the guarantee,
calling amortized versions is prefered choice if we are not concentrated on
complexity of the separate operation.
  We have shown, we could implement full DecreaseKey-Meld interface, but Meld
interface is not natural for these heaps, so if Meld is not needed, much
simpler implementation suffices. As I don't know application requiring Meld, we
would concentrate on noMeld variant, but we will show the changes could be
applied on Meld including variant as well. The papers [1], [2] shown the heaps
could be implemented on pointer machine model. For fast practical
implementations we would rather use arrays. Our goal is to reduce number of
pointer manipulations. Maintainance of ranks by pointers to rank lists would be
unnecessary overhead.
</summary>
    <author>
      <name>Vladan Majerech</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages at all, 4+2/2 pages of tables, 1 figure. arXiv admin note:
  text overlap with arXiv:1911.04372</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.11637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.11944">
    <id>http://arxiv.org/abs/1912.11944v1</id>
    <updated>2019-12-26T22:51:13Z</updated>
    <published>2019-12-26T22:51:13Z</published>
    <title>On the Reproducibility of Experiments of Indexing Repetitive Document
  Collections</title>
    <summary>  This work introduces a companion reproducible paper with the aim of allowing
the exact replication of the methods, experiments, and results discussed in a
previous work [5]. In that parent paper, we proposed many and varied techniques
for compressing indexes which exploit that highly repetitive collections are
formed mostly of documents that are near-copies of others. More concretely, we
describe a replication framework, called uiHRDC (universal indexes for Highly
Repetitive Document Collections), that allows our original experimental setup
to be easily replicated using various document collections. The corresponding
experimentation is carefully explained, providing precise details about the
parameters that can be tuned for each indexing solution. Finally, note that we
also provide uiHRDC as reproducibility package.
</summary>
    <author>
      <name>Antonio Fariña</name>
    </author>
    <author>
      <name>Miguel A. Martínez-Prieto</name>
    </author>
    <author>
      <name>Francisco Claude</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Juan J. Lastra-Díaz</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Diego Seco</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.is.2019.03.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.is.2019.03.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. Replication framework
  available at: https://github.com/migumar2/uiHRDC/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Systems; Volume 83, July 2019; pages 181-194</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.11944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.11866">
    <id>http://arxiv.org/abs/1912.11866v1</id>
    <updated>2019-12-26T13:50:46Z</updated>
    <published>2019-12-26T13:50:46Z</published>
    <title>Efficient processing of raster and vector data</title>
    <summary>  In this work, we propose a framework to store and manage spatial data, which
includes new efficient algorithms to perform operations accepting as input a
raster dataset and a vector dataset. More concretely, we present algorithms for
solving a spatial join between a raster and a vector dataset imposing a
restriction on the values of the cells of the raster; and an algorithm for
retrieving K objects of a vector dataset that overlap cells of a raster
dataset, such that the K objects are those overlapping the highest (or lowest)
cell values among all objects.
  The raster data is stored using a compact data structure, which can directly
manipulate compressed data without the need for prior decompression. This leads
to better running times and lower memory consumption. In our experimental
evaluation comparing our solution to other baselines, we obtain the best
space/time trade-offs.
</summary>
    <author>
      <name>Fernando Silva-Coira</name>
    </author>
    <author>
      <name>José R. Paramá</name>
    </author>
    <author>
      <name>Susana Ladra</name>
    </author>
    <author>
      <name>Juan R. López</name>
    </author>
    <author>
      <name>Gilberto Gutiérrez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0226943</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0226943" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941 To appear in PLOS One (2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.11866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.11388">
    <id>http://arxiv.org/abs/1912.11388v1</id>
    <updated>2019-12-23T03:07:12Z</updated>
    <published>2019-12-23T03:07:12Z</published>
    <title>The Weak Circular Repetition Threshold Over Large Alphabets</title>
    <summary>  The repetition threshold for words on $n$ letters, denoted $\mbox{RT}(n)$, is
the infimum of the set of all $r$ such that there are arbitrarily long $r$-free
words over $n$ letters. A repetition threshold for circular words on $n$
letters can be defined in three natural ways, which gives rise to the weak,
intermediate, and strong circular repetition thresholds for $n$ letters,
denoted $\mbox{CRT}_{\mbox{W}}(n)$, $\mbox{CRT}_{\mbox{I}}(n)$, and
$\mbox{CRT}_{\mbox{S}}(n)$, respectively. Currie and the present authors
conjectured that
$\mbox{CRT}_{\mbox{I}}(n)=\mbox{CRT}_{\mbox{W}}(n)=\mbox{RT}(n)$ for all $n\geq
4$. We prove that $\mbox{CRT}_{\mbox{W}}(n)=\mbox{RT}(n)$ for all $n\geq 45$,
which confirms a weak version of this conjecture for all but finitely many
values of $n$.
</summary>
    <author>
      <name>Lucas Mol</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1911.05779</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.11388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15 (primary), 05C15 (secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.11417">
    <id>http://arxiv.org/abs/1912.11417v1</id>
    <updated>2019-12-24T15:39:46Z</updated>
    <published>2019-12-24T15:39:46Z</published>
    <title>Flat combined Red Black Trees</title>
    <summary>  Flat combining is a concurrency threaded technique whereby one thread
performs all the operations in batch by scanning a queue of operations
to-be-done and performing them together. Flat combining makes sense as long as
k operations each taking O(n) separately can be batched together and done in
less than O(k*n). Red black tree is a balanced binary search tree with
permanent balancing warranties. Operations in red black tree are hard to batch
together: for example inserting nodes in two different branches of the tree
affect different areas of the tree. In this paper we investigate alternatives
to making a flat combine approach work for red black trees.
</summary>
    <author>
      <name>Sergio Sainz-Palacios</name>
    </author>
    <link href="http://arxiv.org/abs/1912.11417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.05239">
    <id>http://arxiv.org/abs/2001.05239v1</id>
    <updated>2020-01-15T11:13:17Z</updated>
    <published>2020-01-15T11:13:17Z</published>
    <title>Optimal Skeleton Huffman Trees Revisited</title>
    <summary>  A skeleton Huffman tree is a Huffman tree in which all disjoint maximal
perfect subtrees are shrunk into leaves. Skeleton Huffman trees, besides saving
storage space, are also used for faster decoding and for speeding up
Huffman-shaped wavelet trees. In 2017 Klein et al. introduced an optimal
skeleton tree: for given symbol frequencies, it has the least number of nodes
among all optimal prefix-free code trees (not necessarily Huffman's) with
shrunk perfect subtrees. Klein et al. described a simple algorithm that, for
fixed codeword lengths, finds a skeleton tree with the least number of nodes;
with this algorithm one can process each set of optimal codeword lengths to
find an optimal skeleton tree. However, there are exponentially many such sets
in the worst case. We describe an $O(n^2\log n)$-time algorithm that, given $n$
symbol frequencies, constructs an optimal skeleton tree and its corresponding
optimal code.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Oleg Merkurev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.05239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.04505">
    <id>http://arxiv.org/abs/2001.04505v1</id>
    <updated>2020-01-13T19:20:14Z</updated>
    <published>2020-01-13T19:20:14Z</published>
    <title>Fast Generation of Big Random Binary Trees</title>
    <summary>  random_tree() is a linear time and space C++ implementation able to create
trees of up to a billion nodes for genetic programming and genetic improvement
experiments. A 3.60GHz CPU can generate more than 18 million random nodes for
GP program trees per second.
</summary>
    <author>
      <name>William B. Langdon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">C++ code:
  http://www.cs.ucl.ac.uk/staff/W.Langdon/ftp/gp-code/rand_tree.cc_r1.43</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.04505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.04760">
    <id>http://arxiv.org/abs/2001.04760v1</id>
    <updated>2020-01-14T13:19:41Z</updated>
    <published>2020-01-14T13:19:41Z</published>
    <title>Simulation computation in grammar-compressed graphs</title>
    <summary>  Like [1], we present an algorithm to compute the simulation of a query
pattern in a graph of labeled nodes and unlabeled edges. However, our algorithm
works on a compressed graph grammar, instead of on the original graph. The
speed-up of our algorithm compared to the algorithm in [1] grows with the size
of the graph and with the compression strength.
</summary>
    <author>
      <name>Stefan Böttcher</name>
    </author>
    <author>
      <name>Rita Hartel</name>
    </author>
    <author>
      <name>Sven Peeters</name>
    </author>
    <link href="http://arxiv.org/abs/2001.04760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.03147">
    <id>http://arxiv.org/abs/2001.03147v1</id>
    <updated>2020-01-09T18:23:11Z</updated>
    <published>2020-01-09T18:23:11Z</published>
    <title>Age-Partitioned Bloom Filters</title>
    <summary>  Bloom filters (BF) are widely used for approximate membership queries over a
set of elements. BF variants allow removals, sets of unbounded size or querying
a sliding window over an unbounded stream. However, for this last case the best
current approaches are dictionary based (e.g., based on Cuckoo Filters or
TinyTable), and it may seem that BF-based approaches will never be competitive
to dictionary-based ones. In this paper we present Age-Partitioned Bloom
Filters, a BF-based approach for duplicate detection in sliding windows that
not only is competitive in time-complexity, but has better space usage than
current dictionary-based approaches (e.g., SWAMP), at the cost of some moderate
slack. APBFs retain the BF simplicity, unlike dictionary-based approaches,
important for hardware-based implementations, and can integrate known
improvements such as double hashing or blocking. We present an Age-Partitioned
Blocked Bloom Filter variant which can operate with 2-3 cache-line accesses per
insertion and around 2-4 per query, even for high accuracy filters.
</summary>
    <author>
      <name>Ariel Shtul</name>
    </author>
    <author>
      <name>Carlos Baquero</name>
    </author>
    <author>
      <name>Paulo Sérgio Almeida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.03147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.03147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.02172">
    <id>http://arxiv.org/abs/2001.02172v1</id>
    <updated>2020-01-07T16:56:23Z</updated>
    <published>2020-01-07T16:56:23Z</published>
    <title>Data Structure Primitives on Persistent Memory: An Evaluation</title>
    <summary>  Persistent Memory (PM), as already available e.g. with Intel Optane DC
Persistent Memory, represents a very promising, next generation memory solution
with a significant impact on database architectures. Several data structures
for this new technology and its properties have already been proposed. However,
primarily merely complete structures were presented and evaluated hiding the
impact of the individual ideas and PM characteristics. Therefore, in this
paper, we disassemble the structures presented so far, identify their
underlying design primitives, and assign them to appropriate design goals
regarding PM. As a result of our comprehensive experiments on real PM hardware,
we were able to reveal the trade-offs of the primitives at the micro level.
From this, performance profiles could be derived for selected primitives. With
these it is possible to precisely identify their best use cases as well as
vulnerabilities. Beside our general insights regarding PM-based data structure
design, we also discovered new promising combinations not considered in the
literature so far.
</summary>
    <author>
      <name>Philipp Götze</name>
    </author>
    <author>
      <name>Arun Kumar Tharanatha</name>
    </author>
    <author>
      <name>Kai-Uwe Sattler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 15 figures, submitted to PVLDB</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.02172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.02172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.02139">
    <id>http://arxiv.org/abs/2001.02139v2</id>
    <updated>2020-01-17T13:49:47Z</updated>
    <published>2020-01-07T15:55:52Z</published>
    <title>Computing the rearrangement distance of natural genomes</title>
    <summary>  The computation of genomic distances has been a very active field of
computational comparative genomics over the last 25 years. Substantial results
include the polynomial-time computability of the inversion distance by
Hannenhalli and Pevzner in 1995 and the introduction of the double-cut and join
(DCJ) distance by Yancopoulos et al. in 2005. Both results, however, rely on
the assumption that the genomes under comparison contain the same set of unique
markers (syntenic genomic regions, sometimes also referred to as genes). In
2015, Shao, Lin and Moret relax this condition by allowing for duplicate
markers in the analysis. This generalized version of the genomic distance
problem is NP-hard, and they give an ILP solution that is efficient enough to
be applied to real-world datasets. A restriction of their approach is that it
can be applied only to balanced genomes, that have equal numbers of duplicates
of any marker. Therefore it still needs a delicate preprocessing of the input
data in which excessive copies of unbalanced markers have to be removed.
  In this paper we present an algorithm solving the genomic distance problem
for natural genomes, in which any marker may occur an arbitrary number of
times. Our method is based on a new graph data structure, the multi-relational
diagram, that allows an elegant extension of the ILP by Shao, Lin and Moret to
count runs of markers that are under- or over-represented in one genome with
respect to the other and need to be inserted or deleted, respectively. With
this extension, previous restrictions on the genome configurations are lifted,
for the first time enabling an uncompromising rearrangement analysis. Any
marker sequence can directly be used for the distance calculation.
  The evaluation of our approach shows that it can be used to analyze genomes
with up to a few ten thousand markers, which we demonstrate on simulated and
real data.
</summary>
    <author>
      <name>Leonard Bohnenkämper</name>
    </author>
    <author>
      <name>Marília D. V. Braga</name>
    </author>
    <author>
      <name>Daniel Doerr</name>
    </author>
    <author>
      <name>Jens Stoye</name>
    </author>
    <link href="http://arxiv.org/abs/2001.02139v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.02139v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.01914">
    <id>http://arxiv.org/abs/2001.01914v1</id>
    <updated>2020-01-07T07:22:02Z</updated>
    <published>2020-01-07T07:22:02Z</published>
    <title>Quantum Algorithms for the Most Frequently String Search, Intersection
  of Two String Sequences and Sorting of Strings Problems</title>
    <summary>  We study algorithms for solving three problems on strings. The first one is
the Most Frequently String Search Problem. The problem is the following. Assume
that we have a sequence of $n$ strings of length $k$. The problem is finding
the string that occurs in the sequence most often. We propose a quantum
algorithm that has a query complexity $\tilde{O}(n \sqrt{k})$. This algorithm
shows speed-up comparing with the deterministic algorithm that requires
$\Omega(nk)$ queries. The second one is searching intersection of two sequences
of strings. All strings have the same length $k$. The size of the first set is
$n$ and the size of the second set is $m$. We propose a quantum algorithm that
has a query complexity $\tilde{O}((n+m) \sqrt{k})$. This algorithm shows
speed-up comparing with the deterministic algorithm that requires
$\Omega((n+m)k)$ queries. The third problem is sorting of $n$ strings of length
$k$. On the one hand, it is known that quantum algorithms cannot sort objects
asymptotically faster than classical ones. On the other hand, we focus on
sorting strings that are not arbitrary objects. We propose a quantum algorithm
that has a query complexity $O(n (\log n)^2 \sqrt{k})$. This algorithm shows
speed-up comparing with the deterministic algorithm (radix sort) that requires
$\Omega((n+d)k)$ queries, where $d$ is a size of the alphabet.
</summary>
    <author>
      <name>Kamil Khadiev</name>
    </author>
    <author>
      <name>Artem Ilikaev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-34500-6_17</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-34500-6_17" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">THe paper was presented on TPNC 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">TPNC 2019. Lecture Notes in Computer Science, vol 11934. Springer,
  Cham</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2001.01914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.01661">
    <id>http://arxiv.org/abs/2001.01661v2</id>
    <updated>2020-01-24T11:22:10Z</updated>
    <published>2020-01-06T16:52:44Z</published>
    <title>A Hybrid Approach to Temporal Pattern Matching</title>
    <summary>  The primary objective of graph pattern matching is to find all appearances of
an input graph pattern query in a large data graph. Such appearances are called
matches. In this paper, we are interested in finding matches of interaction
patterns in temporal graphs. To this end, we propose a hybrid approach that
achieves effective filtering of potential matches based both on structure and
time. Our approach exploits a graph representation where edges are ordered by
time. We present experiments with real datasets that illustrate the efficiency
of our approach.
</summary>
    <author>
      <name>Konstantinos Semertzidis</name>
    </author>
    <author>
      <name>Evaggelia Pitoura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.01661v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01661v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.00211">
    <id>http://arxiv.org/abs/2001.00211v1</id>
    <updated>2020-01-01T14:03:31Z</updated>
    <published>2020-01-01T14:03:31Z</published>
    <title>Approximating Text-to-Pattern Hamming Distances</title>
    <summary>  We revisit a fundamental problem in string matching: given a pattern of
length m and a text of length n, both over an alphabet of size $\sigma$,
compute the Hamming distance between the pattern and the text at every
location. Several $(1+\epsilon)$-approximation algorithms have been proposed in
the literature, with running time of the form $O(\epsilon^{-O(1)}n\log n\log
m)$, all using fast Fourier transform (FFT). We describe a simple
$(1+\epsilon)$-approximation algorithm that is faster and does not need FFT.
Combining our approach with additional ideas leads to numerous new results:
  - We obtain the first linear-time approximation algorithm; the running time
is $O(\epsilon^{-2}n)$.
  - We obtain a faster exact algorithm computing all Hamming distances up to a
given threshold k; its running time improves previous results by logarithmic
factors and is linear if $k\le\sqrt m$.
  - We obtain approximation algorithms with better $\epsilon$-dependence using
rectangular matrix multiplication. The time-bound is $\~O(n)$ when the pattern
is sufficiently long: $m\ge \epsilon^{-28}$. Previous algorithms require
$\~O(\epsilon^{-1}n)$ time.
  - When k is not too small, we obtain a truly sublinear-time algorithm to find
all locations with Hamming distance approximately (up to a constant factor)
less than k, in $O((n/k^{\Omega(1)}+occ)n^{o(1)})$ time, where occ is the
output size. The algorithm leads to a property tester, returning true if an
exact match exists and false if the Hamming distance is more than $\delta m$ at
every location, running in $\~O(\delta^{-1/3}n^{2/3}+\delta^{-1}n/m)$ time.
  - We obtain a streaming algorithm to report all locations with Hamming
distance approximately less than k, using $\~O(\epsilon^{-2}\sqrt k)$ space.
Previously, streaming algorithms were known for the exact problem with \~O(k)
space or for the approximate problem with $\~O(\epsilon^{-O(1)}\sqrt m)$ space.
</summary>
    <author>
      <name>Timothy M. Chan</name>
    </author>
    <author>
      <name>Shay Golan</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.00218">
    <id>http://arxiv.org/abs/2001.00218v3</id>
    <updated>2020-02-22T16:09:43Z</updated>
    <published>2020-01-01T15:04:43Z</published>
    <title>Lossless Compression of Deep Neural Networks</title>
    <summary>  Deep neural networks have been successful in many predictive modeling tasks,
such as image and language recognition, where large neural networks are often
used to obtain good accuracy. Consequently, it is challenging to deploy these
networks under limited computational resources, such as in mobile devices. In
this work, we introduce an algorithm that removes units and layers of a neural
network while not changing the output that is produced, which thus implies a
lossless compression. This algorithm, which we denote as LEO (Lossless
Expressiveness Optimization), relies on Mixed-Integer Linear Programming (MILP)
to identify Rectified Linear Units (ReLUs) with linear behavior over the input
domain. By using L1 regularization to induce such behavior, we can benefit from
training over a larger architecture than we would later use in the environment
where the trained neural network is deployed.
</summary>
    <author>
      <name>Thiago Serra</name>
    </author>
    <author>
      <name>Abhinav Kumar</name>
    </author>
    <author>
      <name>Srikumar Ramalingam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CPAIOR 2020 (to appear)</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.00218v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00218v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.03459">
    <id>http://arxiv.org/abs/2002.03459v1</id>
    <updated>2020-02-09T21:58:36Z</updated>
    <published>2020-02-09T21:58:36Z</published>
    <title>Approximating Text-to-Pattern Distance via Dimensionality Reduction</title>
    <summary>  Text-to-pattern distance is a fundamental problem in string matching, where
given a pattern of length $m$ and a text of length $n$, over integer alphabet,
we are asked to compute the distance between pattern and text at every
location. The distance function can be e.g. Hamming distance or $\ell_p$
distance for some parameter $p > 0$. Almost all state-of-the-art exact and
approximate algorithms developed in the past $\sim 40$ years were using FFT as
a black-box. In this work we present $\widetilde{O}(n/\varepsilon^2)$ time
algorithms for $(1\pm\varepsilon)$-approximation of $\ell_2$ distances, and
$\widetilde{O}(n/\varepsilon^3)$ algorithm for approximation of Hamming and
$\ell_1$ distances, all without use of FFT. This is independent to the very
recent development by Chan et al. [STOC 2020], where $O(n/\varepsilon^2)$
algorithm for Hamming distances not using FFT was presented -- although their
algorithm is much more "combinatorial", our techniques apply to other norms
than Hamming.
</summary>
    <author>
      <name>Przemysław Uznański</name>
    </author>
    <link href="http://arxiv.org/abs/2002.03459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1601.05706">
    <id>http://arxiv.org/abs/1601.05706v1</id>
    <updated>2016-01-21T16:52:27Z</updated>
    <published>2016-01-21T16:52:27Z</published>
    <title>Pachinko</title>
    <summary>  Inspired by the Japanese game Pachinko, we study simple (perfectly
"inelastic" collisions) dynamics of a unit ball falling amidst point obstacles
(pins) in the plane. A classic example is that a checkerboard grid of pins
produces the binomial distribution, but what probability distributions result
from different pin placements? In the 50-50 model, where the pins form a subset
of this grid, not all probability distributions are possible, but surprisingly
the uniform distribution is possible for $\{1,2,4,8,16\}$ possible drop
locations. Furthermore, every probability distribution can be approximated
arbitrarily closely, and every dyadic probability distribution can be divided
by a suitable power of $2$ and then constructed exactly (along with extra
"junk" outputs). In a more general model, if a ball hits a pin off center, it
falls left or right accordingly. Then we prove a universality result: any
distribution of $n$ dyadic probabilities, each specified by $k$ bits, can be
constructed using $O(n k^2)$ pins, which is close to the information-theoretic
lower bound of $\Omega(n k)$.
</summary>
    <author>
      <name>Hugo A. Akitaya</name>
    </author>
    <author>
      <name>Erik D. Demaine</name>
    </author>
    <author>
      <name>Martin L. Demaine</name>
    </author>
    <author>
      <name>Adam Hesterberg</name>
    </author>
    <author>
      <name>Ferran Hurtado</name>
    </author>
    <author>
      <name>Jason S. Ku</name>
    </author>
    <author>
      <name>Jayson Lynch</name>
    </author>
    <link href="http://arxiv.org/abs/1601.05706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.05706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.11218">
    <id>http://arxiv.org/abs/2001.11218v2</id>
    <updated>2020-03-16T09:53:32Z</updated>
    <published>2020-01-30T09:08:41Z</published>
    <title>Reconstructing Words from Right-Bounded-Block Words</title>
    <summary>  A reconstruction problem of words from scattered factors asks for the minimal
information, like multisets of scattered factors of a given length or the
number of occurrences of scattered factors from a given set, necessary to
uniquely determine a word. We show that a word $w \in \{a, b\}^{*}$ can be
reconstructed from the number of occurrences of at most $\min(|w|_a, |w|_b)+ 1$
scattered factors of the form $a^{i} b$. Moreover, we generalize the result to
alphabets of the form $\{1,\ldots,q\}$ by showing that at most $
\sum^{q-1}_{i=1} |w|_i (q-i+1)$ scattered factors suffices to reconstruct $w$.
Both results improve on the upper bounds known so far. Complexity time bounds
on reconstruction algorithms are also considered here.
</summary>
    <author>
      <name>Pamela Fleischmann</name>
    </author>
    <author>
      <name>Marie Lejeune</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Dirk Nowotka</name>
    </author>
    <author>
      <name>Michel Rigo</name>
    </author>
    <link href="http://arxiv.org/abs/2001.11218v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11218v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.11763">
    <id>http://arxiv.org/abs/2001.11763v1</id>
    <updated>2020-01-31T10:51:37Z</updated>
    <published>2020-01-31T10:51:37Z</published>
    <title>Lengths of extremal square-free ternary words</title>
    <summary>  A square-free word $w$ over a fixed alphabet $\Sigma$ is extremal if every
word obtained from $w$ by inserting a single letter from $\Sigma$ (at any
position) contains a square. Grytczuk et al. recently introduced the concept of
extremal square-free word, and demonstrated that there are arbitrarily long
extremal square-free ternary words. We find all lengths which admit an extremal
square-free ternary word. In particular, we show that there is an extremal
square-free ternary word of every sufficiently large length. We also solve the
analogous problem for circular words.
</summary>
    <author>
      <name>Lucas Mol</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <link href="http://arxiv.org/abs/2001.11763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.11732">
    <id>http://arxiv.org/abs/2001.11732v1</id>
    <updated>2020-01-31T09:27:41Z</updated>
    <published>2020-01-31T09:27:41Z</published>
    <title>On the binomial equivalence classes of finite words</title>
    <summary>  Two finite words $u$ and $v$ are $k$-binomially equivalent if, for each word
$x$ of length at most $k$, $x$ appears the same number of times as a
subsequence (i.e., as a scattered subword) of both $u$ and $v$. This notion
generalizes abelian equivalence. In this paper, we study the equivalence
classes induced by the $k$-binomial equivalence with a special focus on the
cardinalities of the classes. We provide an algorithm generating the
$2$-binomial equivalence class of a word. For $k \geq 2$ and alphabet of $3$ or
more symbols, the language made of lexicographically least elements of every
$k$-binomial equivalence class and the language of singletons, i.e., the words
whose $k$-binomial equivalence class is restricted to a single element, are
shown to be non context-free. As a consequence of our discussions, we also
prove that the submonoid generated by the generators of the free nil-$2$ group
on $m$ generators is isomorphic to the quotient of the free monoid $\{ 1,
\ldots , m\}^{*}$ by the $2$-binomial equivalence.
</summary>
    <author>
      <name>Marie Lejeune</name>
    </author>
    <author>
      <name>Michel Rigo</name>
    </author>
    <author>
      <name>Matthieu Rosenfeld</name>
    </author>
    <link href="http://arxiv.org/abs/2001.11732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.08516">
    <id>http://arxiv.org/abs/2001.08516v1</id>
    <updated>2020-01-23T13:57:47Z</updated>
    <published>2020-01-23T13:57:47Z</published>
    <title>Communication-Efficient String Sorting</title>
    <summary>  There has been surprisingly little work on algorithms for sorting strings on
distributed-memory parallel machines. We develop efficient algorithms for this
problem based on the multi-way merging principle. These algorithms inspect only
characters that are needed to determine the sorting order. Moreover,
communication volume is reduced by also communicating (roughly) only those
characters and by communicating repetitions of the same prefixes only once.
Experiments on up to 1280 cores reveal that these algorithm are often more than
five times faster than previous algorithms.
</summary>
    <author>
      <name>Timo Bingmann</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <author>
      <name>Matthias Schimek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version to appear at IPDPS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.08516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.08516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.08679">
    <id>http://arxiv.org/abs/2001.08679v1</id>
    <updated>2020-01-23T17:20:36Z</updated>
    <published>2020-01-23T17:20:36Z</published>
    <title>$O(\log \log n)$ Worst-Case Local Decoding and Update Efficiency for
  Data Compression</title>
    <summary>  This paper addresses the problem of data compression with local decoding and
local update. A compression scheme has worst-case local decoding $d_{wc}$ if
any bit of the raw file can be recovered by probing at most $d_{wc}$ bits of
the compressed sequence, and has update efficiency of $u_{wc}$ if a single bit
of the raw file can be updated by modifying at most $u_{wc}$ bits of the
compressed sequence. This article provides an entropy-achieving compression
scheme for memoryless sources that simultaneously achieves $ O(\log\log n) $
local decoding and update efficiency. Key to this achievability result is a
novel succinct data structure for sparse sequences which allows efficient local
decoding and local update. Under general assumptions on the local decoder and
update algorithms, a converse result shows that $d_{wc}$ and $u_{wc}$ must grow
as $ \Omega(\log\log n) $.
</summary>
    <author>
      <name>Shashank Vatedka</name>
    </author>
    <author>
      <name>Venkat Chandar</name>
    </author>
    <author>
      <name>Aslan Tchamkerten</name>
    </author>
    <link href="http://arxiv.org/abs/2001.08679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.08679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.06864">
    <id>http://arxiv.org/abs/2001.06864v1</id>
    <updated>2020-01-19T16:58:58Z</updated>
    <published>2020-01-19T16:58:58Z</published>
    <title>Chaining with overlaps revisited</title>
    <summary>  Chaining algorithms aim to form a semi-global alignment of two sequences
based on a set of anchoring local alignments as input. Depending on the
optimization criteria and the exact definition of a chain, there are several
$O(n \log n)$ time algorithms to solve this problem optimally, where $n$ is the
number of input anchors.
  In this paper, we focus on a formulation allowing the anchors to overlap in a
chain. This formulation was studied by Shibuya and Kurochin (WABI 2003), but
their algorithm comes with no proof of correctness. We revisit and modify their
algorithm to consider a strict definition of precedence relation on anchors,
adding the required derivation to convince on the correctness of the resulting
algorithm that runs in $O(n \log^2 n)$ time on anchors formed by exact matches.
With the more relaxed definition of precedence relation considered by Shibuya
and Kurochin or when anchors are non-nested such as matches of uniform length
($k$-mers), the algorithm takes $O(n \log n)$ time.
  We also establish a connection between chaining with overlaps to the widely
studied longest common subsequence (LCS) problem.
</summary>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <author>
      <name>Kristoffer Sahlin</name>
    </author>
    <link href="http://arxiv.org/abs/2001.06864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.06864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.05976">
    <id>http://arxiv.org/abs/2001.05976v1</id>
    <updated>2020-01-16T18:20:04Z</updated>
    <published>2020-01-16T18:20:04Z</published>
    <title>Generalised Pattern Matching Revisited</title>
    <summary>  In the problem of $\texttt{Generalised Pattern Matching}\ (\texttt{GPM})$
[STOC'94, Muthukrishnan and Palem], we are given a text $T$ of length $n$ over
an alphabet $\Sigma_T$, a pattern $P$ of length $m$ over an alphabet
$\Sigma_P$, and a matching relationship $\subseteq \Sigma_T \times \Sigma_P$,
and must return all substrings of $T$ that match $P$ (reporting) or the number
of mismatches between each substring of $T$ of length $m$ and $P$ (counting).
In this work, we improve over all previously known algorithms for this problem
for various parameters describing the input instance:
  * $\mathcal{D}\,$ being the maximum number of characters that match a fixed
character,
  * $\mathcal{S}\,$ being the number of pairs of matching characters,
  * $\mathcal{I}\,$ being the total number of disjoint intervals of characters
that match the $m$ characters of the pattern $P$.
  At the heart of our new deterministic upper bounds for $\mathcal{D}\,$ and
$\mathcal{S}\,$ lies a faster construction of superimposed codes, which solves
an open problem posed in [FOCS'97, Indyk] and can be of independent interest.
To conclude, we demonstrate first lower bounds for $\texttt{GPM}$. We start by
showing that any deterministic or Monte Carlo algorithm for $\texttt{GPM}$ must
use $\Omega(\mathcal{S})$ time, and then proceed to show higher lower bounds
for combinatorial algorithms. These bounds show that our algorithms are almost
optimal, unless a radically new approach is developed.
</summary>
    <author>
      <name>Bartłomiej Dudek</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <link href="http://arxiv.org/abs/2001.05976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.05671">
    <id>http://arxiv.org/abs/2001.05671v1</id>
    <updated>2020-01-16T06:30:29Z</updated>
    <published>2020-01-16T06:30:29Z</published>
    <title>Faster STR-EC-LCS Computation</title>
    <summary>  The longest common subsequence (LCS) problem is a central problem in
stringology that finds the longest common subsequence of given two strings $A$
and $B$. More recently, a set of four constrained LCS problems (called
generalized constrained LCS problem) were proposed by Chen and Chao [J. Comb.
Optim, 2011]. In this paper, we consider the substring-excluding constrained
LCS (STR-EC-LCS) problem. A string $Z$ is said to be an STR-EC-LCS of two given
strings $A$ and $B$ excluding $P$ if, $Z$ is one of the longest common
subsequences of $A$ and $B$ that does not contain $P$ as a substring. Wang et
al. proposed a dynamic programming solution which computes an STR-EC-LCS in
$O(mnr)$ time and space where $m = |A|, n = |B|, r = |P|$ [Inf. Process. Lett.,
2013]. In this paper, we show a new solution for the STR-EC-LCS problem. Our
algorithm computes an STR-EC-LCS in $O(n|\Sigma| + (L+1)(m-L+1)r)$ time where
$|\Sigma| \leq \min\{m, n\}$ denotes the set of distinct characters occurring
in both $A$ and $B$, and $L$ is the length of the STR-EC-LCS. This algorithm is
faster than the $O(mnr)$-time algorithm for short/long STR-EC-LCS (namely, $L
\in O(1)$ or $m-L \in O(1)$), and is at least as efficient as the $O(mnr)$-time
algorithm for all cases.
</summary>
    <author>
      <name>Kohei Yamada</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2001.05671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.06265">
    <id>http://arxiv.org/abs/2002.06265v1</id>
    <updated>2020-02-14T22:02:10Z</updated>
    <published>2020-02-14T22:02:10Z</published>
    <title>On Extensions of Maximal Repeats in Compressed Strings</title>
    <summary>  This paper provides an upper bound for several subsets of maximal repeats and
maximal pairs in compressed strings and also presents a formerly unknown
relationship between maximal pairs and the run-length Burrows-Wheeler
transform.
  This relationship is used to obtain a different proof for the Burrows-Wheeler
conjecture which has recently been proven by Kempa and Kociumaka in "Resolution
of the Burrows-Wheeler Transform Conjecture".
  More formally, this paper proves that a string $S$ with $z$ LZ77-factors and
without $q$-th powers has at most $73(\log_2 |S|)(z+2)^2$ runs in the
run-length Burrows-Wheeler transform and the number of arcs in the compacted
directed acyclic word graph of $S$ is bounded from above by $18q(1+\log_q
|S|)(z+2)^2$.
</summary>
    <author>
      <name>Julian Pape-Lange</name>
    </author>
    <link href="http://arxiv.org/abs/2002.06265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.06764">
    <id>http://arxiv.org/abs/2002.06764v1</id>
    <updated>2020-02-17T04:16:05Z</updated>
    <published>2020-02-17T04:16:05Z</published>
    <title>Computing Covers under Substring Consistent Equivalence Relations</title>
    <summary>  Covers are a kind of quasiperiodicity in strings. A string $C$ is a cover of
another string $T$ if any position of $T$ is inside some occurrence of $C$ in
$T$. The literature has proposed linear-time algorithms computing longest and
shortest cover arrays taking border arrays as input. An equivalence relation
$\approx$ over strings is called a substring consistent equivalence relation
(SCER) iff $X \approx Y$ implies (1) $|X| = |Y|$ and (2) $X[i:j] \approx
Y[i:j]$ for all $1 \le i \le j \le |X|$. In this paper, we generalize the
notion of covers for SCERs and prove that existing algorithms to compute the
shortest cover array and the longest cover array of a string $T$ under the
identity relation will work for any SCERs taking the accordingly generalized
border arrays.
</summary>
    <author>
      <name>Natsumi Kikuchi</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.06764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.06786">
    <id>http://arxiv.org/abs/2002.06786v1</id>
    <updated>2020-02-17T06:08:01Z</updated>
    <published>2020-02-17T06:08:01Z</published>
    <title>DAWGs for parameterized matching: online construction and related
  indexing structures</title>
    <summary>  Two strings $x$ and $y$ over $\Sigma \cup \Pi$ of equal length are said to
parameterized match (p-match) if there is a renaming bijection $f:\Sigma \cup
\Pi \rightarrow \Sigma \cup \Pi$ that is identity on $\Sigma$ and transforms
$x$ to $y$ (or vice versa). The p-matching problem is to look for substrings in
a text that p-match a given pattern. In this paper, we propose parameterized
suffix automata (p-suffix automata) and parameterized directed acyclic word
graphs (PDAWGs) which are the p-matching versions of suffix automata and DAWGs.
While suffix automata and DAWGs are equivalent for standard strings, we show
that p-suffix automata can have $\Theta(n^2)$ nodes and edges but PDAWGs have
only $O(n)$ nodes and edges, where $n$ is the length of an input string. We
also give $O(n |\Pi| \log (|\Pi| + |\Sigma|))$-time $O(n)$-space algorithm that
builds the PDAWG in a left-to-right online manner. We then show that an
implicit representation for the PDAWG can be built in $O(n \log (|\Pi| +
|\Sigma|))$ time and $O(n)$ space from left to right. As a byproduct, it is
shown that the parameterized suffix tree for the reversed string can also be
built in the same time and space, in a right-to-left online manner. We also
discuss parameterized compact DAWGs.
</summary>
    <author>
      <name>Katsuhito Nakashima</name>
    </author>
    <author>
      <name>Noriki Fujisato</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.06786v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06786v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.06796">
    <id>http://arxiv.org/abs/2002.06796v1</id>
    <updated>2020-02-17T06:39:57Z</updated>
    <published>2020-02-17T06:39:57Z</published>
    <title>Detecting $k$-(Sub-)Cadences and Equidistant Subsequence Occurrences</title>
    <summary>  The equidistant subsequence pattern matching problem is considered. Given a
pattern string $P$ and a text string $T$, we say that $P$ is an
\emph{equidistant subsequence} of $T$ if $P$ is a subsequence of the text such
that consecutive symbols of $P$ in the occurrence are equally spaced. We can
consider the problem of equidistant subsequences as generalizations of
(sub-)cadences. We give bit-parallel algorithms that yield $o(n^2)$ time
algorithms for finding $k$-(sub-)cadences and equidistant subsequences.
Furthermore, $O(n\log^2 n)$ and $O(n\log n)$ time algorithms, respectively for
equidistant and Abelian equidistant matching for the case $|P| = 3$, are shown.
The algorithms make use of a technique that was recently introduced which can
efficiently compute convolutions with linear constraints.
</summary>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <link href="http://arxiv.org/abs/2002.06796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.05599">
    <id>http://arxiv.org/abs/2002.05599v1</id>
    <updated>2020-02-13T16:22:11Z</updated>
    <published>2020-02-13T16:22:11Z</published>
    <title>Engineering Faster Sorters for Small Sets of Items</title>
    <summary>  Sorting a set of items is a task that can be useful by itself or as a
building block for more complex operations. That is why a lot of effort has
been put into finding sorting algorithms that sort large sets as fast as
possible. But the more sophisticated the algorithms become, the less efficient
they are for small sets of items due to large constant factors. We aim to
determine if there is a faster way than insertion sort to sort small sets of
items to provide a more efficient base case sorter. We looked at sorting
networks, at how they can improve the speed of sorting few elements, and how to
implement them in an efficient manner by using conditional moves. Since sorting
networks need to be implemented explicitly for each set size, providing
networks for larger sizes becomes less efficient due to increased code sizes.
To also enable the sorting of slightly larger base cases, we adapted sample
sort to Register Sample Sort, to break down those larger sets into sizes that
can in turn be sorted by sorting networks. From our experiments we found that
when sorting only small sets, the sorting networks outperform insertion sort by
a factor of at least 1.76 for any array size between six and sixteen, and by a
factor of 2.72 on average across all machines and array sizes. When integrating
sorting networks as a base case sorter into Quicksort, we achieved far less
performance improvements, which is probably due to the networks having a larger
code size and cluttering the L1 instruction cache. But for x86 machines with a
larger L1 instruction cache of 64 KiB or more, we obtained speedups of 12.7%
when using sorting networks as a base case sorter in std::sort. In conclusion,
the desired improvement in speed could only be achieved under special
circumstances, but the results clearly show the potential of using conditional
moves in the field of sorting algorithms.
</summary>
    <author>
      <name>Timo Bingmann</name>
    </author>
    <author>
      <name>Jasper Marianczuk</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <link href="http://arxiv.org/abs/2002.05599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.05600">
    <id>http://arxiv.org/abs/2002.05600v2</id>
    <updated>2020-02-14T11:42:55Z</updated>
    <published>2020-02-13T16:22:26Z</published>
    <title>On Two Measures of Distance between Fully-Labelled Trees</title>
    <summary>  The last decade brought a significant increase in the amount of data and a
variety of new inference methods for reconstructing the detailed evolutionary
history of various cancers. This brings the need of designing efficient
procedures for comparing rooted trees representing the evolution of mutations
in tumor phylogenies. Bernardini et al. [CPM 2019] recently introduced a notion
of the rearrangement distance for fully-labelled trees motivated by this
necessity. This notion originates from two operations: one that permutes the
labels of the nodes, the other that affects the topology of the tree. Each
operation alone defines a distance that can be computed in polynomial time,
while the actual rearrangement distance, that combines the two, was proven to
be NP-hard.
  We answer two open question left unanswered by the previous work. First, what
is the complexity of computing the permutation distance? Second, is there a
constant-factor approximation algorithm for estimating the rearrangement
distance between two arbitrary trees? We answer the first one by showing, via a
two-way reduction, that calculating the permutation distance between two trees
on $n$ nodes is equivalent, up to polylogarithmic factors, to finding the
largest cardinality matching in a sparse bipartite graph. In particular, by
plugging in the algorithm of Liu and Sidford [ArXiv 2019], we obtain an
$O(n^{11/8})$ time algorithm for computing the permutation distance between two
trees on $n$ nodes. Then we answer the second question positively, and design a
linear-time constant-factor approximation algorithm that does not need any
assumption on the trees.
</summary>
    <author>
      <name>Giulia Bernardini</name>
    </author>
    <author>
      <name>Paola Bonizzoni</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.05600v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05600v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.04979">
    <id>http://arxiv.org/abs/2002.04979v1</id>
    <updated>2020-02-12T13:37:23Z</updated>
    <published>2020-02-12T13:37:23Z</published>
    <title>On Rearrangement of Items Stored in Stacks</title>
    <summary>  There are $n \ge 2$ stacks, each filled with $d$ items (its full capacity),
and one empty stack with capacity $d$. A robot arm, in one stack operation
(move), may pop one item from the top of a non-empty stack and subsequently
push it into a stack that is not at capacity. In a {\em labeled} problem, all
$nd$ items are distinguishable and are initially randomly scattered in the $n$
stacks. The items must be rearranged using pop-and-push moves so that at the
end, the $k^{\rm th}$ stack holds items $(k-1)d +1, \ldots, kd$, in that order,
from the top to the bottom for all $1 \le k \le n$. In an {\em unlabeled}
problem, the $nd$ items are of $n$ types of $d$ each. The goal is to rearrange
items so that items of type $k$ are located in the $k^{\rm th}$ stack for all
$1 \le k \le n$. In carrying out the rearrangement, a natural question is to
find the least number of required pop-and-push moves.
  In terms of the required number of moves for solving the rearrangement
problems, the labeled and unlabeled version have lower bounds $\Omega(nd +
nd{\frac{\log d}{\log n}})$ and $\Omega(nd)$, respectively. Our main
contribution is the design of an algorithm with a guaranteed upper bound of
$O(nd)$ for both versions when $d \le cn$ for arbitrary fixed positive number
$c$. In addition, a subroutine for a problem that we call the Rubik table
problem is of independent interest, with applications to problems including
multi-robot motion planning.
</summary>
    <author>
      <name>Mario Szegedy</name>
    </author>
    <author>
      <name>Jingjin Yu</name>
    </author>
    <link href="http://arxiv.org/abs/2002.04979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.04979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.05034">
    <id>http://arxiv.org/abs/2002.05034v2</id>
    <updated>2020-03-18T14:25:21Z</updated>
    <published>2020-02-12T14:50:40Z</published>
    <title>Uniform Linked Lists Contraction</title>
    <summary>  We present a parallel algorithm (EREW PRAM algorithm) for linked lists
contraction. We show that when we contract a linked list from size $n$ to size
$n/c$ for a suitable constant $c$ we can pack the linked list into an array of
size $n/d$ for a constant $1 &lt; d\leq c$ in the time of 3 coloring the list.
Thus for a set of linked lists with a total of $n$ elements and the longest
list has $l$ elements our algorithm contracts them in $O(n\log
i/p+(\log^{(i)}n+\log i )\log \log l+ \log l)$ time, for an arbitrary
constructible integer $i$, with $p$ processors on the EREW PRAM, where
$\log^{(1)} n =\log n$ and $\log^{(t)}n=\log \log^{(t-1)} n$ and $\log^*n=\min
\{ i|\log^{(i)} n &lt; 10\}$. When $i$ is a constant we get time
$O(n/p+\log^{(i)}n\log \log l+\log l)$. Thus when $l=\Omega (\log^{(c)}n)$ for
any constant $c$ we achieve $O(n/p+\log l)$ time. The previous best
deterministic EREW PRAM algorithm has time $O(n/p+\log n)$ and best CRCW PRAM
algorithm has time $O(n/p+\log n/\log \log n+\log l)$.
  Keywords: Parallel algorithms, linked list, linked list contraction, uniform
linked list contraction, EREW PRAM.
</summary>
    <author>
      <name>Yijie Han</name>
    </author>
    <link href="http://arxiv.org/abs/2002.05034v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05034v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W10, 68W40" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.03965">
    <id>http://arxiv.org/abs/2002.03965v2</id>
    <updated>2020-02-16T17:57:08Z</updated>
    <published>2020-02-10T17:27:34Z</published>
    <title>Palindromic k-Factorization in Pure Linear Time</title>
    <summary>  Given a string $s$ of length $n$ over a general alphabet and an integer $k$,
the problem is to decide whether $s$ is a concatenation of $k$ nonempty
palindromes. Two previously known solutions for this problem work in time
$O(kn)$ and $O(n\log n)$ respectively. Here we settle the complexity of this
problem in the word-RAM model, presenting an $O(n)$-time online deciding
algorithm. The algorithm simultaneously finds the minimum odd number of factors
and the minimum even number of factors in a factorization of a string into
nonempty palindromes. We also demonstrate how to get an explicit factorization
of $s$ into $k$ palindromes with an $O(n)$-time offline postprocessing.
</summary>
    <author>
      <name>Mikhail Rubinchik</name>
    </author>
    <author>
      <name>Arseny M. Shur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to CPM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.03965v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03965v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.03057">
    <id>http://arxiv.org/abs/2002.03057v3</id>
    <updated>2020-02-19T11:50:25Z</updated>
    <published>2020-02-08T00:54:19Z</published>
    <title>The Bloom Tree</title>
    <summary>  We introduce a data structure that allows for efficient (probabilistic)
presence proofs and non-probabilistic absence proofs in a bandwidth efficient
and secure way. The Bloom tree combines the idea of Bloom filters with that of
Merkle trees. Bloom filters are used to verify the presence, or absence of
elements in a set. In the case of the Bloom tree, we are interested to verify
and transmit the presence, or absence of an element in a secure and bandwidth
efficient way to another party. Instead of sending the whole Bloom filter to
check for the presence, or absence of an element, the Bloom tree achieves
efficient verification by using a compact Merkle multiproof.
</summary>
    <author>
      <name>Lum Ramabaja</name>
    </author>
    <author>
      <name>Arber Avdullahu</name>
    </author>
    <link href="http://arxiv.org/abs/2002.03057v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03057v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.11157">
    <id>http://arxiv.org/abs/2002.11157v1</id>
    <updated>2020-02-25T19:56:08Z</updated>
    <published>2020-02-25T19:56:08Z</published>
    <title>2-Dimensional Palindromes with $k$ Mismatches</title>
    <summary>  This paper extends the problem of 2-dimensional palindrome search into the
area of approximate matching. Using the Hamming distance as the measure, we
search for 2D palindromes that allow up to $k$ mismatches. We consider two
different definitions of 2D palindromes and describe efficient algorithms for
both of them. The first definition implies a square, while the second
definition (also known as a \emph{centrosymmetric factor}), can be any
rectangular shape. Given a text of size $n \times m$, the time complexity of
the first algorithm is $O(nm (\log m + \log n + k))$ and for the second
algorithm it is $O(nm(\log m + k) + occ)$ where $occ$ is the size of the
output.
</summary>
    <author>
      <name>Dina Sokol</name>
    </author>
    <link href="http://arxiv.org/abs/2002.11157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.11342">
    <id>http://arxiv.org/abs/2002.11342v1</id>
    <updated>2020-02-26T08:04:35Z</updated>
    <published>2020-02-26T08:04:35Z</published>
    <title>Streaming with Oracle: New Streaming Algorithms for Edit Distance and
  LCS</title>
    <summary>  The edit distance (ED) and longest common subsequence (LCS) are two
fundamental problems which quantify how similar two strings are to one another.
In this paper, we consider these problems in the streaming model where one
string is available via oracle queries and the other string comes as a stream
of characters. Our main contribution is a constant factor approximation
algorithm in this setting for ED with memory $O(n^{\delta})$ for any $\delta >
0$. In addition to this, we present an upper bound of $\tilde O(\sqrt{n})$ on
the memory needed to approximate ED or LCS within a factor $1+o(1)$ in our
setting. All our algorithms run in a single pass.
  For approximating ED within a constant factor, we discover yet another
application of triangle inequality, this time in the context of streaming
algorithms. Triangle inequality has been previously used to obtain subquadratic
time approximation algorithms for ED. Our technique is novel and elegantly
utilizes triangle inequality to save memory at the expense of an exponential
increase in the runtime.
</summary>
    <author>
      <name>Alireza Farhadi</name>
    </author>
    <author>
      <name>MohammadTaghi Hajiaghayi</name>
    </author>
    <author>
      <name>Aviad Rubinstein</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <link href="http://arxiv.org/abs/2002.11342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.11691">
    <id>http://arxiv.org/abs/2002.11691v1</id>
    <updated>2020-02-26T18:32:32Z</updated>
    <published>2020-02-26T18:32:32Z</published>
    <title>Bitvectors with runs and the successor/predecessor problem</title>
    <summary>  The successor and predecessor problem consists of obtaining the closest value
in a set of integers, greater/smaller than a given value. This problem has
interesting applications, like the intersection of inverted lists. It can be
easily modeled by using a bitvector of size $n$ and its operations rank and
select. However, there is a practical approach, which keeps the best
theoretical bounds, and allows to solve successor and predecessor more
efficiently. Based on that technique, we designed a novel compact data
structure for bitvectors with $k$ runs that achieves access, rank, and
successor/predecessor in $O(1)$ time by consuming space $O(\sqrt{kn})$ bits. In
practice, it obtains a compression ratio of $0.04\%-26.33\%$ when the runs are
larger than $100$, and becomes the fastest technique, which considers
compressibility, in successor/predecessor queries. Besides, we present a
recursive variant of our structure, which tends to $O(k)$ bits and takes
$O(\log \frac{n}{k})$ time.
</summary>
    <author>
      <name>Adrián Gómez-Brandón</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2020 Data Compression Conference (DCC)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.11691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.10303">
    <id>http://arxiv.org/abs/2002.10303v1</id>
    <updated>2020-02-24T15:20:33Z</updated>
    <published>2020-02-24T15:20:33Z</published>
    <title>Wheeler Languages</title>
    <summary>  The recently introduced class of Wheeler graphs, inspired by the
Burrows-Wheeler Transform (BWT) of a given string, admits an efficient index
data structure for searching for subpaths with a given path label, and lifts
the applicability of the Burrows-Wheeler transform from strings to languages.
In this paper we study the regular languages accepted by automata having a
Wheeler graph as transition function, and prove results on determination,
Myhill_Nerode characterization, decidability, and closure properties for this
class of languages.
</summary>
    <author>
      <name>Jarno Alanko</name>
    </author>
    <author>
      <name>Giovanna D'Agostino</name>
    </author>
    <author>
      <name>Alberto Policriti</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/2002.10303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.09707">
    <id>http://arxiv.org/abs/2002.09707v1</id>
    <updated>2020-02-22T14:18:53Z</updated>
    <published>2020-02-22T14:18:53Z</published>
    <title>Compression with wildcards: All spanning trees</title>
    <summary>  By processing all minimal cutsets of a graph G, and by using novel wildcards,
all spanning trees of G can be compactly encoded. Thus, different from all
previous enumeration schemes, the spanning trees are not generated one-by-one.
The Mathematica implementation of one of our algorithms generated for a random
(11,50)-graph its 819'603'181 spanning trees, in bundles of size about 400,
within 52 seconds.
</summary>
    <author>
      <name>Marcel Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.09707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.09511">
    <id>http://arxiv.org/abs/2002.09511v3</id>
    <updated>2020-03-02T11:44:07Z</updated>
    <published>2020-02-21T19:16:07Z</published>
    <title>Chronofold: a data structure for versioned text</title>
    <summary>  Chronofold is a replicated data structure for versioned text, based on the
extended Causal Tree model. Past models of this kind either retrofitted local
linear orders to a distributed system (the OT approach) or employed distributed
data models locally (the CRDT approach). That caused either extreme fragility
in a distributed setting or egregious overheads in local use. Overall, that
local/distributed impedance mismatch is cognitively taxing and causes lots of
complexity. We solve that by using subjective linear orders locally at each
replica, while inter-replica communication uses a distributed model. A separate
translation layer insulates local data structures from the distributed
environment. We modify the Lamport timestamping scheme to make that translation
as trivial as possible. We believe our approach has applications beyond the
domain of collaborative editing.
</summary>
    <author>
      <name>Victor Grishchenko</name>
    </author>
    <author>
      <name>Mikhail Patrakeev</name>
    </author>
    <link href="http://arxiv.org/abs/2002.09511v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09511v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.09041">
    <id>http://arxiv.org/abs/2002.09041v1</id>
    <updated>2020-02-20T22:15:41Z</updated>
    <published>2020-02-20T22:15:41Z</published>
    <title>Compressed Data Structures for Binary Relations in Practice</title>
    <summary>  Binary relations are commonly used in Computer Science for modeling data. In
addition to classical representations using matrices or lists, some compressed
data structures have recently been proposed to represent binary relations in
compact space, such as the $k^2$-tree and the Binary Relation Wavelet Tree
(BRWT). Knowing their storage needs, supported operations and time performance
is key for enabling an appropriate choice of data representation given a domain
or application, its data distribution and typical operations that are computed
over the data.
  In this work, we present an empirical comparison among several compressed
representations for binary relations. We analyze their space usage and the
speed of their operations using different (synthetic and real) data
distributions. We include both neighborhood and set operations, also proposing
algorithms for set operations for the BRWT, which were not presented before in
the literature. We conclude that there is not a clear choice that outperforms
the rest, but we give some recommendations of usage of each compact
representation depending on the data distribution and types of operations
performed over the data. We also include a scalability study of the data
representations.
</summary>
    <author>
      <name>Carlos Quijada-Fuentes</name>
    </author>
    <author>
      <name>Miguel R. Penabad</name>
    </author>
    <author>
      <name>Susana Ladra</name>
    </author>
    <author>
      <name>Gilberto Gutiérrez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2020.2970983</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2020.2970983" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access 8, pp. 25949-25963 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.09041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.08498">
    <id>http://arxiv.org/abs/2002.08498v3</id>
    <updated>2020-03-13T13:52:20Z</updated>
    <published>2020-02-19T23:33:39Z</published>
    <title>Space Efficient Deterministic Approximation of String Measures</title>
    <summary>  We study approximation algorithms for the following three string measures
that are widely used in practice: edit distance, longest common subsequence,
and longest increasing sequence.\ All three problems can be solved exactly by
standard algorithms that run in polynomial time with roughly $O(n)$ space,
where $n$ is the input length, and our goal is to design deterministic
approximation algorithms that run in polynomial time with significantly smaller
space. Towards this, we design several algorithms that achieve $1+\epsilon$ or
$1-\epsilon$ approximation for all three problems, where $\epsilon>0$ can be
any constant. Our algorithms use space $n^{\delta}$ for any constant $\delta>0$
and have running time essentially the same as or slightly more than the
standard algorithms. Our algorithms significantly improve previous results in
terms of space complexity, where all known results need to use space at least
$\Omega(\sqrt{n})$. Some of our algorithms can also be adapted to work in the
asymmetric streaming model \cite{saks2013space}, and output the corresponding
sequence.
  Our algorithms are based on the idea of using recursion as in Savitch's
theorem \cite{Savitch70}, and a careful modification of previous techniques to
make the recursion work. Along the way we also give a new logspace reduction
from longest common subsequence to longest increasing sequence, which may be of
independent interest.
</summary>
    <author>
      <name>Kuan Cheng</name>
    </author>
    <author>
      <name>Zhengzhong Jin</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Yu Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/2002.08498v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08498v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.08061">
    <id>http://arxiv.org/abs/2002.08061v1</id>
    <updated>2020-02-19T08:51:38Z</updated>
    <published>2020-02-19T08:51:38Z</published>
    <title>Translating Between Wavelet Tree and Wavelet Matrix Construction</title>
    <summary>  The wavelet tree (Grossi et al. [SODA, 2003]) and wavelet matrix (Claude et
al. [Inf. Syst., 2015]) are compact data structures with many applications such
as text indexing or computational geometry. By continuing the recent research
of Fischer et al. [ALENEX, 2018], we explore the similarities and differences
of these heavily related data structures with focus on their construction. We
develop a data structure to modify construction algorithms for either the
wavelet tree or matrix to construct instead the other. This modification is
efficient, in that it does not worsen the asymptotic time and space
requirements of any known wavelet tree or wavelet matrix construction
algorithm.
</summary>
    <author>
      <name>Patrick Dinklage</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper originally submitted to and presented at the Prague Stringology
  Conference 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.08061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.08004">
    <id>http://arxiv.org/abs/2002.08004v1</id>
    <updated>2020-02-19T04:58:41Z</updated>
    <published>2020-02-19T04:58:41Z</published>
    <title>Fast and linear-time string matching algorithms based on the distances
  of $q$-gram occurrences</title>
    <summary>  Given a text $T$ of length $n$ and a pattern $P$ of length $m$, the string
matching problem is a task to find all occurrences of $P$ in $T$. In this
study, we propose an algorithm that solves this problem in $O((n + m)q)$ time
considering the distance between two adjacent occurrences of the same $q$-gram
contained in $P$. We also propose a theoretical improvement of it which runs in
$O(n + m)$ time, though it is not necessarily faster in practice. We compare
the execution times of our and existing algorithms on various kinds of real and
artificial datasets such as an English text, a genome sequence and a Fibonacci
string. The experimental results show that our algorithm is as fast as the
state-of-the-art algorithms in many cases, particularly when a pattern
frequently appears in a text.
</summary>
    <author>
      <name>Satoshi Kobayashi</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <link href="http://arxiv.org/abs/2002.08004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.03801">
    <id>http://arxiv.org/abs/2003.03801v1</id>
    <updated>2020-03-08T15:38:08Z</updated>
    <published>2020-03-08T15:38:08Z</published>
    <title>Multiset Synchronization with Counting Cuckoo Filters</title>
    <summary>  Set synchronization is a fundamental task in distributed applications and
implementations. Existing methods that synchronize simple sets are mainly based
on compact data structures such as Bloom filter and its variants. However,
these methods are infeasible to synchronize a pair of multisets which allow an
element to appear for multiple times. To this end, in this paper, we propose to
leverage the counting cuckoo filter (CCF), a novel variant of cuckoo filter, to
represent and thereafter synchronize a pair of multisets. The cuckoo filter
(CF) is a minimized hash table that uses cuckoo hashing to resolve collisions.
CF has an array of buckets, each of which has multiple slots to store element
fingerprints. Based on CF, CCF extends each slot as two fields, the fingerprint
field and the counter field. The fingerprint field records the fingerprint of
element which is stored by this slot; while the counter field counts the
multiplicity of the stored element. With such a design, CCF is competent to
represent any multiset. After generating and exchanging the respective CCFs
which represent the local multi-sets, we propose the query-based and the
decoding-based methods to identify the different elements between the given
multisets. The comprehensive evaluation results indicate that CCF outperforms
the counting Bloom filter (CBF) when they are used to synchronize multisets, in
terms of both synchronization accuracy and the space-efficiency, at the cost of
a little higher time-consumption.
</summary>
    <author>
      <name>Shangsen Li</name>
    </author>
    <author>
      <name>Lailong Luo</name>
    </author>
    <author>
      <name>Deke Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2003.03801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.03801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.03959">
    <id>http://arxiv.org/abs/2003.03959v1</id>
    <updated>2020-03-09T07:55:06Z</updated>
    <published>2020-03-09T07:55:06Z</published>
    <title>Adaptive Fibonacci and Pairing Heaps</title>
    <summary>  This brief note presents two adaptive heap data structures and conjectures on
running times.
</summary>
    <author>
      <name>Andrew Frohmader</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.03959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.03959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.03222">
    <id>http://arxiv.org/abs/2003.03222v1</id>
    <updated>2020-03-05T07:43:53Z</updated>
    <published>2020-03-05T07:43:53Z</published>
    <title>Generating a Gray code for prefix normal words in amortized
  polylogarithmic time per word</title>
    <summary>  A prefix normal word is a binary word with the property that no substring has
more 1s than the prefix of the same length. By proving that the set of prefix
normal words is a bubble language, we can exhaustively list all prefix normal
words of length n as a combinatorial Gray code, where successive strings differ
by at most two swaps or bit flips. This Gray code can be generated in O(log^2
n) amortized time per word, while the best generation algorithm hitherto has
O(n) running time per word. We also present a membership tester for prefix
normal words, as well as a novel characterization of bubble languages.
</summary>
    <author>
      <name>Péter Burcsi</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <author>
      <name>Joe Sawada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1401.6346</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.03222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.03222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.02336">
    <id>http://arxiv.org/abs/2003.02336v1</id>
    <updated>2020-03-04T21:31:42Z</updated>
    <published>2020-03-04T21:31:42Z</published>
    <title>Approximating Optimal Bidirectional Macro Schemes</title>
    <summary>  Lempel-Ziv is an easy-to-compute member of a wide family of so-called macro
schemes; it restricts pointers to go in one direction only. Optimal
bidirectional macro schemes are NP-complete to find, but they may provide much
better compression on highly repetitive sequences. We consider the problem of
approximating optimal bidirectional macro schemes. We describe a simulated
annealing algorithm that usually converges quickly. Moreover, in some cases, we
obtain bidirectional macro schemes that are provably a 2-approximation of the
optimal. We test our algorithm on a number of artificial repetitive texts and
verify that it is efficient in practice and outperforms Lempel-Ziv, sometimes
by a wide margin.
</summary>
    <author>
      <name>Luís M. S. Russo</name>
    </author>
    <author>
      <name>Ana D. Correia</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Alexandre P. Francisco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.02336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.02016">
    <id>http://arxiv.org/abs/2003.02016v1</id>
    <updated>2020-03-04T11:48:05Z</updated>
    <published>2020-03-04T11:48:05Z</published>
    <title>Time-Space Tradeoffs for Finding a Long Common Substring</title>
    <summary>  We consider the problem of finding, given two documents of total length $n$,
a longest string occurring as a substring of both documents. This problem,
known as the Longest Common Substring (LCS) problem, has a classic $O(n)$-time
solution dating back to the discovery of suffix trees (Weiner, 1973) and their
efficient construction for integer alphabets (Farach-Colton, 1997). However,
these solutions require $\Theta(n)$ space, which is prohibitive in many
applications. To address this issue, Starikovskaya and Vildh{\o}j (CPM 2013)
showed that for $n^{2/3} \le s \le n^{1-o(1)}$, the LCS problem can be solved
in $O(s)$ space and $O(\frac{n^2}{s})$ time. Kociumaka et al. (ESA 2014)
generalized this tradeoff to $1 \leq s \leq n$, thus providing a smooth
time-space tradeoff from constant to linear space. In this paper, we obtain a
significant speed-up for instances where the length $L$ of the sought LCS is
large. For $1 \leq s \leq n$, we show that the LCS problem can be solved in
$O(s)$ space and $\tilde{O}(\frac{n^2}{L\cdot s}+n)$ time. The result is based
on techniques originating from the LCS with Mismatches problem (Flouri et al.,
2015; Charalampopoulos et al., CPM 2018), on space-efficient locally consistent
parsing (Birenzwige et al., SODA 2020), and on the structure of maximal
repetitions (runs) in the input documents.
</summary>
    <author>
      <name>Stav Ben Nun</name>
    </author>
    <author>
      <name>Shay Golan</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Matan Kraus</name>
    </author>
    <link href="http://arxiv.org/abs/2003.02016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.12570">
    <id>http://arxiv.org/abs/2002.12570v1</id>
    <updated>2020-02-28T06:51:40Z</updated>
    <published>2020-02-28T06:51:40Z</published>
    <title>Learning Directly from Grammar Compressed Text</title>
    <summary>  Neural networks using numerous text data have been successfully applied to a
variety of tasks. While massive text data is usually compressed using
techniques such as grammar compression, almost all of the previous machine
learning methods assume already decompressed sequence data as their input. In
this paper, we propose a method to directly apply neural sequence models to
text data compressed with grammar compression algorithms without decompression.
To encode the unique symbols that appear in compression rules, we introduce
composer modules to incrementally encode the symbols into vector
representations. Through experiments on real datasets, we empirically showed
that the proposal model can achieve both memory and computational efficiency
while maintaining moderate performance.
</summary>
    <author>
      <name>Yoichi Sasaki</name>
    </author>
    <author>
      <name>Kosuke Akimoto</name>
    </author>
    <author>
      <name>Takanori Maehara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 Postscript figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.12570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.12570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.01203">
    <id>http://arxiv.org/abs/2003.01203v1</id>
    <updated>2020-03-02T21:43:46Z</updated>
    <published>2020-03-02T21:43:46Z</published>
    <title>Concurrent Disjoint Set Union</title>
    <summary>  We develop and analyze concurrent algorithms for the disjoint set union
(union-find) problem in the shared memory, asynchronous multiprocessor model of
computation, with CAS (compare and swap) or DCAS (double compare and swap) as
the synchronization primitive. We give a deterministic bounded wait-free
algorithm that uses DCAS and has a total work bound of $O(m \cdot (\log(np/m +
1) + \alpha(n, m/(np)))$ for a problem with $n$ elements and $m$ operations
solved by $p$ processes, where $\alpha$ is a functional inverse of Ackermann's
function. We give two randomized algorithms that use only CAS and have the same
work bound in expectation. The analysis of the second randomized algorithm is
valid even if the scheduler is adversarial. Our DCAS and randomized algorithms
take $O(\log n)$ steps per operation, worst-case for the DCAS algorithm,
high-probability for the randomized algorithms. Our work and step bounds grow
only logarithmically with $p$, making our algorithms truly scalable. We prove
that for a class of symmetric algorithms that includes ours, no better step or
work bound is possible.
</summary>
    <author>
      <name>Siddhartha V. Jayanti</name>
    </author>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, combines ideas in two previous PODC papers</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.01203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.12662">
    <id>http://arxiv.org/abs/2002.12662v1</id>
    <updated>2020-02-28T11:33:30Z</updated>
    <published>2020-02-28T11:33:30Z</published>
    <title>Fast Indexes for Gapped Pattern Matching</title>
    <summary>  We describe indexes for searching large data sets for variable-length-gapped
(VLG) patterns. VLG patterns are composed of two or more subpatterns, between
each adjacent pair of which is a gap-constraint specifying upper and lower
bounds on the distance allowed between subpatterns. VLG patterns have numerous
applications in computational biology (motif search), information retrieval
(e.g., for language models, snippet generation, machine translation) and
capture a useful subclass of the regular expressions commonly used in practice
for searching source code. Our best approach provides search speeds several
times faster than prior art across a broad range of patterns and texts.
</summary>
    <author>
      <name>Manuel Cáceres</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Chile</arxiv:affiliation>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Helsinki</arxiv:affiliation>
    </author>
    <author>
      <name>Bella Zhukova</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Helsinki</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-38919-2_40</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-38919-2_40" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research is supported by Academy of Finland through grant 319454
  and has received funding from the European Union's Horizon 2020 research and
  innovation programme under the Marie Sklodowska-Curie Actions
  H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SOFSEM 2020: Theory and Practice of Computer Science</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.12662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.12662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.12050">
    <id>http://arxiv.org/abs/2002.12050v2</id>
    <updated>2020-02-28T13:20:14Z</updated>
    <published>2020-02-27T11:48:33Z</published>
    <title>Semantrix: A Compressed Semantic Matrix</title>
    <summary>  We present a compact data structure to represent both the duration and length
of homogeneous segments of trajectories from moving objects in a way that, as a
data warehouse, it allows us to efficiently answer cumulative queries. The
division of trajectories into relevant segments has been studied in the
literature under the topic of Trajectory Segmentation. In this paper, we design
a data structure to compactly represent them and the algorithms to answer the
more relevant queries. We experimentally evaluate our proposal in the real
context of an enterprise with mobile workers (truck drivers) where we aim at
analyzing the time they spend in different activities. To test our proposal
under higher stress conditions we generated a huge amount of synthetic
realistic trajectories and evaluated our system with those data to have a good
idea about its space needs and its efficiency when answering different types of
queries.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Antonio Fariña</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Tirso V. Rodeiro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, Data Compression Conference 2020. This research has
  received funding from the European Union's Horizon 2020 research and
  innovation programme under the Marie Sk{\l}odowska-Curie Actions
  H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.12050v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.12050v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.11622">
    <id>http://arxiv.org/abs/2002.11622v1</id>
    <updated>2020-02-26T17:03:28Z</updated>
    <published>2020-02-26T17:03:28Z</published>
    <title>Revisiting compact RDF stores based on k2-trees</title>
    <summary>  We present a new compact representation to efficiently store and query large
RDF datasets in main memory. Our proposal, called BMatrix, is based on the
k2-tree, a data structure devised to represent binary matrices in a compressed
way, and aims at improving the results of previous state-of-the-art
alternatives, especially in datasets with a relatively large number of
predicates. We introduce our technique, together with some improvements on the
basic k2-tree that can be applied to our solution in order to boost
compression. Experimental results in the flagship RDF dataset DBPedia show that
our proposal achieves better compression than existing alternatives, while
yielding competitive query times, particularly in the most frequent triple
patterns and in queries with unbound predicate, in which we outperform existing
solutions.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Ana Cerdeira-Pena</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Antonio Fariña</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.11622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.08211">
    <id>http://arxiv.org/abs/2003.08211v2</id>
    <updated>2020-03-19T01:23:39Z</updated>
    <published>2020-03-17T04:26:35Z</published>
    <title>An Efficient Implementation of Manacher's Algorithm</title>
    <summary>  Manacher's algorithm has been shown to be optimal to the longest palindromic
substring problem. Many of the existing implementations of this algorithm,
however, unanimously required in-memory construction of an augmented string
that is twice as long as the original string. Although it has found widespread
use, we found that this preprocessing is neither economic nor necessary. We
present a more efficient implementation of Manacher's algorithm based on index
mapping that makes the string augmentation process obsolete.
</summary>
    <author>
      <name>Shoupu Wan</name>
    </author>
    <link href="http://arxiv.org/abs/2003.08211v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08211v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.6; F.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.08097">
    <id>http://arxiv.org/abs/2003.08097v1</id>
    <updated>2020-03-18T08:47:16Z</updated>
    <published>2020-03-18T08:47:16Z</published>
    <title>Grammar compression with probabilistic context-free grammar</title>
    <summary>  We propose a new approach for universal lossless text compression, based on
grammar compression. In the literature, a target string $T$ has been compressed
as a context-free grammar $G$ in Chomsky normal form satisfying $L(G) = \{T\}$.
Such a grammar is often called a \emph{straight-line program} (SLP). In this
paper, we consider a probabilistic grammar $G$ that generates $T$, but not
necessarily as a unique element of $L(G)$. In order to recover the original
text $T$ unambiguously, we keep both the grammar $G$ and the derivation tree of
$T$ from the start symbol in $G$, in compressed form. We show some simple
evidence that our proposal is indeed more efficient than SLPs for certain
texts, both from theoretical and practical points of view.
</summary>
    <author>
      <name>Hiroaki Naganuma</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <author>
      <name>Naoki Kobayashi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, accepted for poster presentation at DCC 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.08097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.07285">
    <id>http://arxiv.org/abs/2003.07285v1</id>
    <updated>2020-03-16T15:45:53Z</updated>
    <published>2020-03-16T15:45:53Z</published>
    <title>Approximating LCS in Linear Time: Beating the $\sqrt{n}$ Barrier</title>
    <summary>  Longest common subsequence (LCS) is one of the most fundamental problems in
combinatorial optimization. Apart from theoretical importance, LCS has enormous
applications in bioinformatics, revision control systems, and data comparison
programs. Although a simple dynamic program computes LCS in quadratic time, it
has been recently proven that the problem admits a conditional lower bound and
may not be solved in truly subquadratic time. In addition to this, LCS is
notoriously hard with respect to approximation algorithms. Apart from a trivial
sampling technique that obtains a $n^{x}$ approximation solution in time
$O(n^{2-2x})$ nothing else is known for LCS. This is in sharp contrast to its
dual problem edit distance for which several linear time solutions are obtained
in the past two decades.
</summary>
    <author>
      <name>MohammadTaghi Hajiaghayi</name>
    </author>
    <author>
      <name>Masoud Seddighin</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <author>
      <name>Xiaorui Sun</name>
    </author>
    <link href="http://arxiv.org/abs/2003.07285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.07285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.06742">
    <id>http://arxiv.org/abs/2003.06742v1</id>
    <updated>2020-03-15T03:00:13Z</updated>
    <published>2020-03-15T03:00:13Z</published>
    <title>Four-Dimensional Dominance Range Reporting in Linear Space</title>
    <summary>  In this paper we study the four-dimensional dominance range reporting problem
and present data structures with linear or almost-linear space usage. Our
results can be also used to answer four-dimensional queries that are bounded on
five sides. The first data structure presented in this paper uses linear space
and answers queries in $O(\log^{1+\varepsilon}n + k\log^{\varepsilon} n)$ time,
where $k$ is the number of reported points, $n$ is the number of points in the
data structure, and $\varepsilon$ is an arbitrarily small positive constant.
Our second data structure uses $O(n \log^{\varepsilon} n)$ space and answers
queries in $O(\log n+k)$ time.
  These are the first data structures for this problem that use linear (resp.
$O(n\log^{\varepsilon} n)$) space and answer queries in poly-logarithmic time.
For comparison the fastest previously known linear-space or
$O(n\log^{\varepsilon} n)$-space data structure supports queries in
$O(n^{\varepsilon} + k)$ time (Bentley and Mauer, 1980). Our results can be
generalized to $d\ge 4$ dimensions. For example, we can answer $d$-dimensional
dominance range reporting queries in $O(\log\log n (\log n/\log\log n)^{d-3} +
k)$ time using $O(n\log^{d-4+\varepsilon}n)$ space. Compared to the fastest
previously known result (Chan, 2013), our data structure reduces the space
usage by $O(\log n)$ without increasing the query time.
</summary>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a SoCG'20 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.06742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.06691">
    <id>http://arxiv.org/abs/2003.06691v1</id>
    <updated>2020-03-14T19:44:44Z</updated>
    <published>2020-03-14T19:44:44Z</published>
    <title>Shorter Labels for Routing in Trees</title>
    <summary>  A routing labeling scheme assigns a binary string, called a label, to each
node in a network, and chooses a distinct port number from $\{1,\ldots,d\}$ for
every edge outgoing from a node of degree $d$. Then, given the labels of $u$
and $w$ and no other information about the network, it should be possible to
determine the port number corresponding to the first edge on the shortest path
from $u$ to $w$. In their seminal paper, Thorup and Zwick [SPAA 2001] designed
several routing methods for general weighted networks. An important technical
ingredient in their paper that according to the authors ``may be of independent
practical and theoretical interest'' is a routing labeling scheme for trees of
arbitrary degrees. For a tree on $n$ nodes, their scheme constructs labels
consisting of $(1+o(1))\log n$ bits such that the sought port number can be
computed in constant time. Looking closer at their construction, the labels
consist of $\log n + O(\log n\cdot \log\log\log n / \log\log n)$ bits. Given
that the only known lower bound is $\log n+\Omega(\log\log n)$, a natural
question that has been asked for other labeling problems in trees is to
determine the asymptotics of the smaller-order term.
  We make the first (and significant) progress in 19 years on determining the
correct second-order term for the length of a label in a routing labeling
scheme for trees on $n$ nodes. We design such a scheme with labels of length
$\log n+O((\log\log n)^{2})$. Furthermore, we modify the scheme to allow for
computing the port number in constant time at the expense of slightly
increasing the length to $\log n+O((\log\log n)^{3})$.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Wojciech Janczewski</name>
    </author>
    <author>
      <name>Jakub Łopuszański</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.06691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.04629">
    <id>http://arxiv.org/abs/2003.04629v1</id>
    <updated>2020-03-10T10:51:05Z</updated>
    <published>2020-03-10T10:51:05Z</published>
    <title>Scattered Factor-Universality of Words</title>
    <summary>  A word $u=u_1\dots u_n$ is a scattered factor of a word $w$ if $u$ can be
obtained from $w$ by deleting some of its letters: there exist the (potentially
empty) words $v_0,v_1,..,v_n$ such that $w = v_0u_1v_1...u_nv_n$. The set of
all scattered factors up to length $k$ of a word is called its full
$k$-spectrum. Firstly, we show an algorithm deciding whether the $k$-spectra
for given $k$ of two words are equal or not, running in optimal time. Secondly,
we consider a notion of scattered-factors universality: the word $w$, with
$\letters(w)=\Sigma$, is called $k$-universal if its $k$-spectrum includes all
words of length $k$ over the alphabet $\Sigma$; we extend this notion to
$k$-circular universality. After a series of preliminary combinatorial results,
we present an algorithm computing, for a given $k'$-universal word $w$ the
minimal $i$ such that $w^i$ is $k$-universal for some $k>k'$. Several other
connected problems~are~also~considered.
</summary>
    <author>
      <name>Laura Barker</name>
    </author>
    <author>
      <name>Pamela Fleischmann</name>
    </author>
    <author>
      <name>Katharina Harwardt</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Dirk Nowotka</name>
    </author>
    <link href="http://arxiv.org/abs/2003.04629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.04629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.06794">
    <id>http://arxiv.org/abs/1909.06794v2</id>
    <updated>2019-10-01T05:13:05Z</updated>
    <published>2019-09-15T12:48:00Z</published>
    <title>Run-Length Encoding in a Finite Universe</title>
    <summary>  Text compression schemes and compact data structures usually combine
sophisticated probability models with basic coding methods whose average
codeword length closely match the entropy of known distributions. In the
frequent case where basic coding represents run-lengths of outcomes that have
probability $p$, i.e. the geometric distribution $\Pr(i)=p^i(1-p)$, a
\emph{Golomb code} is an optimal instantaneous code, which has the additional
advantage that codewords can be computed using only an integer parameter
calculated from $p$, without need for a large or sophisticated data structure.
Golomb coding does not, however, gracefully handle the case where run-lengths
are bounded by a known integer~$n$. In this case, codewords allocated for the
case $i>n$ are wasted. While negligible for large $n$, this makes Golomb coding
unattractive in situations where $n$ is recurrently small, e.g., when
representing many short lists of integers drawn from limited ranges, or when
the range of $n$ is narrowed down by a recursive algorithm. We address the
problem of choosing a code for this case, considering efficiency from both
information-theoretic and computational perspectives, and arrive at a simple
code that allows computing a codeword using only $O(1)$ simple computer
operations and $O(1)$ machine words. We demonstrate experimentally that the
resulting representation length is very close (equal in a majority of tested
cases) to the optimal Huffman code, to the extent that the expected difference
is practically negligible. We describe efficient branch-free implementation of
encoding and decoding.
</summary>
    <author>
      <name>N. Jesper Larsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SPIRE 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.06794v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.06794v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.06444">
    <id>http://arxiv.org/abs/1909.06444v3</id>
    <updated>2019-10-14T07:57:50Z</updated>
    <published>2019-09-13T20:51:30Z</published>
    <title>Local Decode and Update for Big Data Compression</title>
    <summary>  This paper investigates data compression that simultaneously allows local
decoding and local update. The main result is a universal compression scheme
for memoryless sources with the following features. The rate can be made
arbitrarily close to the entropy of the underlying source, contiguous fragments
of the source can be recovered or updated by probing or modifying a number of
codeword bits that is on average linear in the size of the fragment, and the
overall encoding and decoding complexity is quasilinear in the blocklength of
the source. In particular, the local decoding or update of a single message
symbol can be performed by probing or modifying a constant number of codeword
bits. This latter part improves over previous best known results for which
local decodability or update efficiency grows logarithmically with blocklength.
</summary>
    <author>
      <name>Shashank Vatedka</name>
    </author>
    <author>
      <name>Aslan Tchamkerten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 4 figures. v2: updated references</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.06444v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.06444v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.07538">
    <id>http://arxiv.org/abs/1909.07538v2</id>
    <updated>2019-11-05T07:36:36Z</updated>
    <published>2019-09-17T01:08:27Z</published>
    <title>Generalized Dictionary Matching under Substring Consistent Equivalence
  Relations</title>
    <summary>  Given a set of patterns called a dictionary and a text, the dictionary
matching problem is a task to find all occurrence positions of all patterns in
the text. The dictionary matching problem can be solved efficiently by using
the Aho-Corasick algorithm. Recently, Matsuoka et al. [TCS, 2016] proposed a
generalization of pattern matching problem under substring consistent
equivalence relations and presented a generalization of the Knuth-Morris-Pratt
algorithm to solve this problem. An equivalence relation $\approx$ is a
substring consistent equivalence relation (SCER) if for two strings $X,Y$, $X
\approx Y$ implies $|X| = |Y|$ and $X[i:j] \approx Y[i:j]$ for all $1 \le i \le
j \le |X|$. In this paper, we propose a generalization of the dictionary
matching problem and present a generalization of the Aho-Corasick algorithm for
the dictionary matching under SCER. We present an algorithm that constructs
SCER automata and an algorithm that performs dictionary matching under SCER by
using the automata. Moreover, we show the time and space complexity of our
algorithms with respect to the size of input strings.
</summary>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.07538v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.07538v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.08006">
    <id>http://arxiv.org/abs/1909.08006v1</id>
    <updated>2019-09-17T18:10:04Z</updated>
    <published>2019-09-17T18:10:04Z</published>
    <title>Leyenda: An Adaptive, Hybrid Sorting Algorithm for Large Scale Data with
  Limited Memory</title>
    <summary>  Sorting is the one of the fundamental tasks of modern data management
systems. With Disk I/O being the most-accused performance bottleneck and more
computation-intensive workloads, it has come to our attention that in
heterogeneous environment, performance bottleneck may vary among different
infrastructure. As a result, sort kernels need to be adaptive to changing
hardware conditions. In this paper, we propose Leyenda, a hybrid, parallel and
efficient Radix Most-Significant-Bit (MSB) MergeSort algorithm, with
utilization of local thread-level CPU cache and efficient disk/memory I/O.
Leyenda is capable of performing either internal or external sort efficiently,
based on different I/O and processing conditions. We benchmarked Leyenda with
three different workloads from Sort Benchmark, targeting three unique use
cases, including internal, partially in-memory and external sort, and we found
Leyenda to outperform GNU's parallel in-memory quick/merge sort implementations
by up to three times. Leyenda is also ranked the second best external sort
algorithm on ACM 2019 SIGMOD programming contest and forth overall.
</summary>
    <author>
      <name>Yuanjing Shi</name>
    </author>
    <author>
      <name>Zhaoxing Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.08006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.08006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.11433">
    <id>http://arxiv.org/abs/1909.11433v1</id>
    <updated>2019-09-25T12:19:38Z</updated>
    <published>2019-09-25T12:19:38Z</published>
    <title>Weighted Shortest Common Supersequence Problem Revisited</title>
    <summary>  A weighted string, also known as a position weight matrix, is a sequence of
probability distributions over some alphabet. We revisit the Weighted Shortest
Common Supersequence (WSCS) problem, introduced by Amir et al. [SPIRE 2011],
that is, the SCS problem on weighted strings. In the WSCS problem, we are given
two weighted strings $W_1$ and $W_2$ and a threshold $\mathit{Freq}$ on
probability, and we are asked to compute the shortest (standard) string $S$
such that both $W_1$ and $W_2$ match subsequences of $S$ (not necessarily the
same) with probability at least $\mathit{Freq}$. Amir et al. showed that this
problem is NP-complete if the probabilities, including the threshold
$\mathit{Freq}$, are represented by their logarithms (encoded in binary). We
present an algorithm that solves the WSCS problem for two weighted strings of
length $n$ over a constant-sized alphabet in $\mathcal{O}(n^2\sqrt{z} \log{z})$
time. Notably, our upper bound matches known conditional lower bounds stating
that the WSCS problem cannot be solved in $\mathcal{O}(n^{2-\varepsilon})$ time
or in $\mathcal{O}^*(z^{0.5-\varepsilon})$ time unless there is a breakthrough
improving upon long-standing upper bounds for fundamental NP-hard problems
(CNF-SAT and Subset Sum, respectively). We also discover a fundamental
difference between the WSCS problem and the Weighted Longest Common Subsequence
(WLCS) problem, introduced by Amir et al. [JDA 2010]. We show that the WLCS
problem cannot be solved in $\mathcal{O}(n^{f(z)})$ time, for any function
$f(z)$, unless $\mathrm{P}=\mathrm{NP}$.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszyński</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SPIRE'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.11433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.11433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.11577">
    <id>http://arxiv.org/abs/1909.11577v1</id>
    <updated>2019-09-25T16:08:19Z</updated>
    <published>2019-09-25T16:08:19Z</published>
    <title>Internal Dictionary Matching</title>
    <summary>  We introduce data structures answering queries concerning the occurrences of
patterns from a given dictionary $\mathcal{D}$ in fragments of a given string
$T$ of length $n$. The dictionary is internal in the sense that each pattern in
$\mathcal{D}$ is given as a fragment of $T$. This way, $\mathcal{D}$ takes
space proportional to the number of patterns $d=|\mathcal{D}|$ rather than
their total length, which could be $\Theta(n\cdot d)$.
  In particular, we consider the following types of queries: reporting and
counting all occurrences of patterns from $\mathcal{D}$ in a fragment $T[i..j]$
and reporting distinct patterns from $\mathcal{D}$ that occur in $T[i..j]$. We
show how to construct, in $\mathcal{O}((n+d) \log^{\mathcal{O}(1)} n)$ time, a
data structure that answers each of these queries in time
$\mathcal{O}(\log^{\mathcal{O}(1)} n+|output|)$.
  The case of counting patterns is much more involved and needs a combination
of a locally consistent parsing with orthogonal range searching. Reporting
distinct patterns, on the other hand, uses the structure of maximal repetitions
in strings. Finally, we provide tight---up to subpolynomial factors---upper and
lower bounds for the case of a dynamic dictionary.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Manal Mohamed</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A short version of this paper was accepted for presentation at ISAAC
  2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.11577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.11577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.11930">
    <id>http://arxiv.org/abs/1909.11930v2</id>
    <updated>2020-01-15T12:05:06Z</updated>
    <published>2019-09-26T06:34:01Z</published>
    <title>String Indexing with Compressed Patterns</title>
    <summary>  Given a string $S$ of length $n$, the classic string indexing problem is to
preprocess $S$ into a compact data structure that supports efficient subsequent
pattern queries. In this paper we consider the basic variant where the pattern
is given in compressed form and the goal is to achieve query time that is fast
in terms of the compressed size of the pattern. This captures the common
client-server scenario, where a client submits a query and communicates it in
compressed form to a server. Instead of the server decompressing the query
before processing it, we consider how to efficiently process the compressed
query directly. Our main result is a novel linear space data structure that
achieves near-optimal query time for patterns compressed with the classic
Lempel-Ziv compression scheme. Along the way we develop several data structural
techniques of independent interest, including a novel data structure that
compactly encodes all LZ77 compressed suffixes of a string in linear space and
a general decomposition of tries that reduces the search time from logarithmic
in the size of the trie to logarithmic in the length of the pattern.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Teresa Anna Steiner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures; added figure for section 5, included discussion
  of open problems, revision of explanations in sections 3, 4 and 5, fixed
  typos</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.11930v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.11930v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.13670">
    <id>http://arxiv.org/abs/1909.13670v4</id>
    <updated>2019-11-08T18:23:08Z</updated>
    <published>2019-09-23T02:21:18Z</published>
    <title>RECIPE : Converting Concurrent DRAM Indexes to Persistent-Memory Indexes</title>
    <summary>  We present Recipe, a principled approach for converting concurrent DRAM
indexes into crash-consistent indexes for persistent memory (PM). The main
insight behind Recipe is that isolation provided by a certain class of
concurrent in-memory indexes can be translated with small changes to
crash-consistency when the same index is used in PM. We present a set of
conditions that enable the identification of this class of DRAM indexes, and
the actions to be taken to convert each index to be persistent. Based on these
conditions and conversion actions, we modify five different DRAM indexes based
on B+ trees, tries, radix trees, and hash tables to their crash-consistent PM
counterparts. The effort involved in this conversion is minimal, requiring
30-200 lines of code. We evaluated the converted PM indexes on Intel DC
Persistent Memory, and found that they outperform state-of-the-art,
hand-crafted PM indexes in multi-threaded workloads by up-to 5.2x. For example,
we built P-CLHT, our PM implementation of the CLHT hash table by modifying only
30 LOC. When running YCSB workloads, P-CLHT performs up to 2.4x better than
Cacheline-Conscious Extendible Hashing (CCEH), the state-of-the-art PM hash
table.
</summary>
    <author>
      <name>Se Kwon Lee</name>
    </author>
    <author>
      <name>Jayashree Mohan</name>
    </author>
    <author>
      <name>Sanidhya Kashyap</name>
    </author>
    <author>
      <name>Taesoo Kim</name>
    </author>
    <author>
      <name>Vijay Chidambaram</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3341301.3359635</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3341301.3359635" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3pages: Added one more reference</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.13670v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.13670v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.02151">
    <id>http://arxiv.org/abs/1910.02151v2</id>
    <updated>2020-03-08T20:06:38Z</updated>
    <published>2019-10-04T21:17:16Z</published>
    <title>Towards a Definitive Measure of Repetitiveness</title>
    <summary>  Unlike in statistical compression, where Shannon's entropy is a definitive
lower bound, no such clear measure exists for the compressibility of repetitive
sequences. Since statistical entropy does not capture repetitiveness, ad-hoc
measures like the size $z$ of the Lempel--Ziv parse are frequently used to
estimate repetitiveness. Recently, a more principled measure, the size $\gamma$
of the smallest string \emph{attractor}, was introduced. The measure $\gamma$
lower bounds all the previous relevant ones, yet length-$n$ strings can be
represented and efficiently indexed within space
$O(\gamma\log\frac{n}{\gamma})$, which also upper bounds most measures. While
$\gamma$ is certainly a better measure of repetitiveness than $z$, it is
NP-complete to compute, and no $o(\gamma\log n)$-space representation of
strings is known.
  In this paper, we study a smaller measure, $\delta \le \gamma$, which can be
computed in linear time. We show that $\delta$ better captures the
compressibility of repetitive strings. For every length $n$ and every value
$\delta \ge 2$, we construct a string such that $\gamma = \Omega(\delta \log
\frac{n}{\delta})$. Still, we show a representation of any string $S$ in
$O(\delta\log\frac{n}{\delta})$ space that supports direct access to any
character $S[i]$ in time $O(\log\frac{n}{\delta})$ and finds the $occ$
occurrences of any pattern $P[1..m]$ in time $O(m\log n + occ\log^\epsilon n)$.
Further, we prove that no $o(\delta\log n)$-space representation exists: for
every $n$ and every $2\le \delta\le n^{1-\epsilon}$, we exhibit a string family
whose elements can only be encoded in $\Omega(\delta\log \frac{n}{\delta})$
space. We complete our characterization of $\delta$ by showing that the
smallest context-free grammar can be of size $\Omega(\delta \log^2 n / \log\log
n)$. No such separation is known for $\gamma$.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1910.02151v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02151v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.02851">
    <id>http://arxiv.org/abs/1910.02851v1</id>
    <updated>2019-10-07T15:19:44Z</updated>
    <published>2019-10-07T15:19:44Z</published>
    <title>ER-index: a referential index for encrypted genomic databases</title>
    <summary>  Huge DBMSs storing genomic information are being created and engineerized for
doing large-scale, comprehensive and in-depth analysis of human beings and
their diseases. However, recent regulations like the GDPR require that
sensitive data are stored and elaborated thanks to privacy-by-design methods
and software. We designed and implemented ER-index, a new full-text index in
minute space which was optimized for compressing and encrypting collections of
genomic sequences, and for performing on them fast pattern-search queries. Our
new index complements the E2FM-index, which was introduced to compress and
encrypt collections of nucleotide sequences without relying on a reference
sequence. When used on collections of highly similar sequences, the ER-index
allows to obtain compression ratios which are an order of magnitude smaller
than those achieved with the E2FM-index, but maintaining its very good search
performance. Moreover, thanks to the ER-index multi-user and multiple-keys
encryption model, a single index can store the sequences related to a
population of individuals so that users may perform search operations only on
the sequences to which they were granted access. The ER-index C++ source code
plus scripts and data to assess the tool performance are available at:
https://github.com/EncryptedIndexes/erindex.
</summary>
    <author>
      <name>Ferdinando Montecuollo</name>
    </author>
    <author>
      <name>Giovannni Schmid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages with detailed pseudocodes</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.02851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P15, 68P25, 68P30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.2; E.3; H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.08111">
    <id>http://arxiv.org/abs/1908.08111v1</id>
    <updated>2019-08-21T20:31:34Z</updated>
    <published>2019-08-21T20:31:34Z</published>
    <title>Engineering Faster Sorters for Small Sets of Items</title>
    <summary>  Sorting a set of items is a task that can be useful by itself or as a
building block for more complex operations. The more sophisticated and fast
sorting algorithms become asymptotically, the less efficient they are for small
sets of items due to large constant factor. This thesis aims to determine if
there is a faster way to sort base case sizes than using insertion sort. For
that we looked at sorting networks and how to implement them efficiently.
Because sorting networks need to be implemented explicitly for each input size,
providing networks for larger sizes becomess less efficient. That is why we
modified Super Scalar Sample Sort to break down larger sets into sizes that can
in turn be sorted by sorting networks. We show that the task of sorting only
small sets can be greatly improved by at least 25% when using sorting networks
compared to insertion sort, but that when integrating them into other sorting
algorithms the speed-up is hindered by the limited L1 instruction cache size.
On a machine with 64KiB of L1 instruction cache we achieved over 6% of
improvement when using sorting networks as a base case sorter instead of
insertion sort.
</summary>
    <author>
      <name>Jasper Marianczuk</name>
    </author>
    <link href="http://arxiv.org/abs/1908.08111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.08762">
    <id>http://arxiv.org/abs/1908.08762v1</id>
    <updated>2019-08-23T11:27:47Z</updated>
    <published>2019-08-23T11:27:47Z</published>
    <title>Revisiting Consistent Hashing with Bounded Loads</title>
    <summary>  Dynamic load balancing lies at the heart of distributed caching. Here, the
goal is to assign objects (load) to servers (computing nodes) in a way that
provides load balancing while at the same time dynamically adjusts to the
addition or removal of servers. One essential requirement is that the
assignment time (or hashing time) should be independent of the number of
servers. Addition or removal of small servers should not require us to
recompute the complete assignment. A popular and widely adopted solution is the
two-decade-old Consistent Hashing (CH). Recently, an elegant extension was
provided to account for server bounds. In this paper, we identify that existing
methodologies for CH and its variants suffer from cascaded overflow, leading to
poor load balancing. This cascading effect leads to decreasing performance of
the hashing procedure with increasing load. To overcome the cascading effect,
we propose a simple solution to CH based on recent advances in fast minwise
hashing. We show, both theoretically and empirically, that our proposed
solution is significantly superior for load balancing and is optimal in many
senses.
</summary>
    <author>
      <name>John Chen</name>
    </author>
    <author>
      <name>Ben Coleman</name>
    </author>
    <author>
      <name>Anshumali Shrivastava</name>
    </author>
    <link href="http://arxiv.org/abs/1908.08762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.09125">
    <id>http://arxiv.org/abs/1908.09125v2</id>
    <updated>2020-03-04T10:00:03Z</updated>
    <published>2019-08-24T11:26:37Z</published>
    <title>When a Dollar Makes a BWT</title>
    <summary>  The Burrows-Wheeler-Transform (BWT) is a reversible string transformation
which plays a central role in text compression and is fundamental in many
modern bioinformatics applications. The BWT is a permutation of the characters,
which is in general better compressible and allows to answer several different
query types more efficiently than the original string.
  It is easy to see that not every string is a BWT image, and exact
characterizations of BWT images are known. We investigate a related
combinatorial question. In many applications, a sentinel character dollar is
added to mark the end of the string, and thus the BWT of a string ending with
dollar contains exactly one dollar-character. Given a string w, we ask in which
positions, if any, the dollar-character can be inserted to turn w into the BWT
image of a word ending with dollar. We show that this depends only on the
standard permutation of w and present a O(n log n)-time algorithm for
identifying all such positions, improving on the naive quadratic time
algorithm. We also give a combinatorial characterization of such positions and
develop bounds on their number and value.
</summary>
    <author>
      <name>Sara Giuliani</name>
    </author>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <author>
      <name>Romeo Rizzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at ICTCS 2019 (20th Italian Conference on Theoretical
  Computer Science, 9-11 Sept. 2019, Como, Italy)</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.09125v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.09125v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.01960">
    <id>http://arxiv.org/abs/1902.01960v2</id>
    <updated>2019-02-25T19:39:39Z</updated>
    <published>2019-02-05T22:29:03Z</published>
    <title>On the Hardness and Inapproximability of Recognizing Wheeler Graphs</title>
    <summary>  In recent years several compressed indexes based on variants of the
Burrows-Wheeler transformation have been introduced. Some of these index
structures far more complex than a single string, as was originally done with
the FM-index [Ferragina and Manzini, J. ACM 2005]. As such, there has been an
effort to better understand under which conditions such an indexing scheme is
possible. This led to the introduction of Wheeler graphs [Gagie it et al.,
Theor. Comput. Sci., 2017]. A Wheeler graph is a directed graph with edge
labels which satisfies two simple axioms. Wheeler graphs can be indexed in a
way which is space efficient and allows for fast traversal. Gagie et al. showed
that de Bruijn graphs, generalized compressed suffix arrays, and several other
BWT related structures can be represented as Wheeler graphs. Here we answer the
open question of whether or not there exists an efficient algorithm for
recognizing if a graph is a Wheeler graph. We demonstrate:(i) Recognizing if a
graph is a Wheeler graph is NP-complete for any edge label alphabet of size
$\sigma \geq 2$, even for DAGs. It can be solved in linear time for $\sigma
=1$; (ii) An optimization variant called Wheeler Graph Violation (WGV) which
aims to remove the minimum number of edges needed to obtain a Wheeler graph is
APX-hard, even for DAGs. Hence, unless P = NP, there exists constant $C > 1$
such that there is no $C$-approximation algorithm. We show conditioned on the
Unique Games Conjecture, for every constant $C \geq 1$, it is NP-hard to find a
$C$-approximation to WGV; (iii) The Wheeler Subgraph problem (WS) which aims to
find the largest Wheeler subgraph is in APX for $\sigma=O(1)$; (iv) For the
above problems there exist efficient exponential time exact algorithms, relying
on graph isomorphism being computed in strictly sub-exponential time; (v) A
class of graphs where the recognition problem is polynomial time solvable.
</summary>
    <author>
      <name>Daniel Gibney</name>
    </author>
    <author>
      <name>Sharma V. Thankachan</name>
    </author>
    <link href="http://arxiv.org/abs/1902.01960v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01960v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.10159">
    <id>http://arxiv.org/abs/1908.10159v1</id>
    <updated>2019-08-27T12:33:32Z</updated>
    <published>2019-08-27T12:33:32Z</published>
    <title>Partial Sums on the Ultra-Wide Word RAM</title>
    <summary>  We consider the classic partial sums problem on the ultra-wide word RAM model
of computation. This model extends the classic $w$-bit word RAM model with
special ultrawords of length $w^2$ bits that support standard arithmetic and
boolean operation and scattered memory access operations that can access $w$
(non-contiguous) locations in memory. The ultra-wide word RAM model captures
(and idealizes) modern vector processor architectures.
  Our main result is a new in-place data structure for the partial sum problem
that only stores a constant number of ultraword in addition to the input and
supports operations in doubly logarithmic time. This matches the best known
time bounds for the problem (among polynomial space data structures) while
improving the space from superlinear to a constant number of ultrawords. Our
results are based on a simple and elegant in-place word RAM data structure,
known as the Fenwick tree. Our main technical contribution is a new efficient
parallel ultra-wide word RAM implementation of the Fenwick tree, which is
likely of independent interest.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Frederik Rye Skjoldjensen</name>
    </author>
    <link href="http://arxiv.org/abs/1908.10159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.10644">
    <id>http://arxiv.org/abs/1908.10644v1</id>
    <updated>2019-08-28T11:13:03Z</updated>
    <published>2019-08-28T11:13:03Z</published>
    <title>Bloom filter variants for multiple sets: a comparative assessment</title>
    <summary>  In this paper we compare two probabilistic data structures for association
queries derived from the well-known Bloom filter: the shifting Bloom filter
(ShBF), and the spatial Bloom filter (SBF). With respect to the original data
structure, both variants add the ability to store multiple subsets in the same
filter, using different strategies. We analyse the performance of the two data
structures with respect to false positive probability, and the inter-set error
probability (the probability for an element in the set of being recognised as
belonging to the wrong subset). As part of our analysis, we extended the
functionality of the shifting Bloom filter, optimising the filter for any
non-trivial number of subsets. We propose a new generalised ShBF definition
with applications outside of our specific domain, and present new probability
formulas. Results of the comparison show that the ShBF provides better space
efficiency, but at a significantly higher computational cost than the SBF.
</summary>
    <author>
      <name>Luca Calderoni</name>
    </author>
    <author>
      <name>Dario Maio</name>
    </author>
    <author>
      <name>Paolo Palmieri</name>
    </author>
    <link href="http://arxiv.org/abs/1908.10644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.10843">
    <id>http://arxiv.org/abs/1908.10843v2</id>
    <updated>2020-01-08T00:30:51Z</updated>
    <published>2019-08-26T21:05:26Z</published>
    <title>An Incompressibility Theorem for Automatic Complexity</title>
    <summary>  Shallit and Wang showed that the automatic complexity $A(x)\ge n/13$ for
almost all $x\in\{0,1\}^n$. They also stated that Holger Petersen had informed
them that the constant 13 can be reduced to 7. Here we show that it can be
reduced to $2+\epsilon$ for any $\epsilon>0$.
</summary>
    <author>
      <name>Bjørn Kjos-Hanssen</name>
    </author>
    <link href="http://arxiv.org/abs/1908.10843v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10843v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.11181">
    <id>http://arxiv.org/abs/1908.11181v2</id>
    <updated>2019-10-31T17:48:20Z</updated>
    <published>2019-08-29T12:36:15Z</published>
    <title>Compacted binary trees admit a stretched exponential</title>
    <summary>  A compacted binary tree is a directed acyclic graph encoding a binary tree in
which common subtrees are factored and shared, such that they are represented
only once. We show that the number of compacted binary trees of size $n$ grows
asymptotically like $$\Theta\left( n! \, 4^n e^{3a_1n^{1/3}} n^{3/4} \right),$$
where $a_1\approx-2.338$ is the largest root of the Airy function. Our method
involves a new two parameter recurrence which yields an algorithm of quadratic
arithmetic complexity. We use empirical methods to estimate the values of all
terms defined by the recurrence, then we prove by induction that these
estimates are sufficiently accurate for large $n$ to determine the asymptotic
form. Our results also lead to new bounds on the number of minimal finite
automata recognizing a finite language on a binary alphabet. As a consequence,
these also exhibit a stretched exponential.
</summary>
    <author>
      <name>Andrew Elvey Price</name>
    </author>
    <author>
      <name>Wenjie Fang</name>
    </author>
    <author>
      <name>Michael Wallner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.11181v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.11181v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05C30, 05A16, 05C20, 05C05" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.10598">
    <id>http://arxiv.org/abs/1908.10598v1</id>
    <updated>2019-08-28T08:29:14Z</updated>
    <published>2019-08-28T08:29:14Z</published>
    <title>Techniques for Inverted Index Compression</title>
    <summary>  The data structure at the core of large-scale search engines is the inverted
index, which is essentially a collection of sorted integer sequences called
inverted lists. Because of the many documents indexed by such engines and
stringent performance requirements imposed by the heavy load of queries, the
inverted index stores billions of integers that must be searched efficiently.
In this scenario, index compression is essential because it leads to a better
exploitation of the computer memory hierarchy for faster query processing and,
at the same time, allows reducing the number of storage machines. The aim of
this article is twofold: first, surveying the encoding algorithms suitable for
inverted index compression and, second, characterizing the performance of the
inverted index through experimentation.
</summary>
    <author>
      <name>Giulio Ermanno Pibiri</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <link href="http://arxiv.org/abs/1908.10598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.02804">
    <id>http://arxiv.org/abs/1909.02804v2</id>
    <updated>2019-09-13T06:16:46Z</updated>
    <published>2019-09-06T10:17:30Z</published>
    <title>Minimal Unique Substrings and Minimal Absent Words in a Sliding Window</title>
    <summary>  A substring $u$ of a string $T$ is called a minimal unique substring (MUS) of
$T$ if $u$ occurs exactly once in $T$ and any proper substring of $u$ occurs at
least twice in $T$. A string $w$ is called a minimal absent word (MAW) of $T$
if $w$ does not occur in $T$ and any proper substring of $w$ occurs in $T$. In
this paper, we study the problems of computing MUSs and MAWs in a sliding
window over a given string $T$. We first show how the set of MUSs can change in
a sliding window over $T$, and present an $O(n\log\sigma)$-time and
$O(d)$-space algorithm to compute MUSs in a sliding window of width $d$ over
$T$, where $\sigma$ is the maximum number of distinct characters in every
window. We then give tight upper and lower bounds on the maximum number of
changes in the set of MAWs in a sliding window over $T$. Our bounds improve on
the previous results in [Crochemore et al., 2017].
</summary>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Yuki Kuhara</name>
    </author>
    <author>
      <name>Tooru Akagi</name>
    </author>
    <author>
      <name>Yuta Fujishige</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1909.02804v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02804v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.02741">
    <id>http://arxiv.org/abs/1908.02741v4</id>
    <updated>2019-10-10T17:58:10Z</updated>
    <published>2019-08-07T17:35:50Z</published>
    <title>Parallel Finger Search Structures</title>
    <summary>  In this paper we present two versions of a parallel finger structure FS on p
processors that supports searches, insertions and deletions, and has a finger
at each end. This is to our knowledge the first implementation of a parallel
search structure that is work-optimal with respect to the finger bound and yet
has very good parallelism (within a factor of O( (log p)^2 ) of optimal). We
utilize an extended implicit batching framework that transparently facilitates
the use of FS by any parallel program P that is modelled by a dynamically
generated DAG D where each node is either a unit-time instruction or a call to
FS.
  The total work done by either version of FS is bounded by the finger bound
F[L] (for some linearization L of D ), i.e. each operation on an item with
finger distance r takes O( log r + 1 ) amortized work; it is cheaper for items
closer to a finger. Running P using the simpler version takes O( ( T[1] + F[L]
) / p + T[inf] + d * ( (log p)^2 + log n ) ) time on a greedy scheduler, where
T[1],T[inf] are the size and span of D respectively, and n is the maximum
number of items in FS, and d is the maximum number of calls to FS along any
path in D. Using the faster version, this is reduced to O( ( T[1] + F[L] ) / p
+ T[inf] + d * (log p)^2 + s[L] ) time, where s[L] is the weighted span of D
where each call to FS is weighted by its cost according to F[L]. We also sketch
how to extend FS to support a fixed number of movable fingers.
  The data structures in our paper fit into the dynamic multithreading
paradigm, and their performance bounds are directly composable with other data
structures given in the same paradigm. Also, the results can be translated to
practical implementations using work-stealing schedulers.
</summary>
    <author>
      <name>Seth Gilbert</name>
    </author>
    <author>
      <name>Wei Quan Lim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper published in DISC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.02741v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02741v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.03169">
    <id>http://arxiv.org/abs/1908.03169v4</id>
    <updated>2020-02-06T20:38:44Z</updated>
    <published>2019-08-08T17:02:16Z</published>
    <title>The repetition threshold for binary rich words</title>
    <summary>  A word of length $n$ is rich if it contains $n$ nonempty palindromic factors.
An infinite word is rich if all of its finite factors are rich. Baranwal and
Shallit produced an infinite binary rich word with critical exponent
$2+\sqrt{2}/2$ ($\approx 2.707$) and conjectured that this was the least
possible critical exponent for infinite binary rich words (i.e., that the
repetition threshold for binary rich words is $2+\sqrt{2}/2$). In this article,
we give a structure theorem for infinite binary rich words that avoid
$14/5$-powers (i.e., repetitions with exponent at least 2.8). As a consequence,
we deduce that the repetition threshold for binary rich words is
$2+\sqrt{2}/2$, as conjectured by Baranwal and Shallit. This resolves an open
problem of Vesti for the binary alphabet; the problem remains open for larger
alphabets.
</summary>
    <author>
      <name>James D. Currie</name>
    </author>
    <author>
      <name>Lucas Mol</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.03169v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.03169v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.04686">
    <id>http://arxiv.org/abs/1908.04686v1</id>
    <updated>2019-08-12T10:58:49Z</updated>
    <published>2019-08-12T10:58:49Z</published>
    <title>Space-Efficient Construction of Compressed Suffix Trees</title>
    <summary>  We show how to build several data structures of central importance to string
processing, taking as input the Burrows-Wheeler transform (BWT) and using small
extra working space. Let $n$ be the text length and $\sigma$ be the alphabet
size. We first provide two algorithms that enumerate all LCP values and suffix
tree intervals in $O(n\log\sigma)$ time using just $o(n\log\sigma)$ bits of
working space on top of the input BWT. Using these algorithms as building
blocks, for any parameter $0 &lt; \epsilon \leq 1$ we show how to build the PLCP
bitvector and the balanced parentheses representation of the suffix tree
topology in $O\left(n(\log\sigma + \epsilon^{-1}\cdot \log\log n)\right)$ time
using at most $n\log\sigma \cdot(\epsilon + o(1))$ bits of working space on top
of the input BWT and the output. In particular, this implies that we can build
a compressed suffix tree from the BWT using just succinct working space (i.e.
$o(n\log\sigma)$ bits) and any time in $\Theta(n\log\sigma) + \omega(n\log\log
n)$. This improves the previous most space-efficient algorithms, which worked
in $O(n)$ bits and $O(n\log n)$ time. We also consider the problem of merging
BWTs of string collections, and provide a solution running in $O(n\log\sigma)$
time and using just $o(n\log\sigma)$ bits of working space. An efficient
implementation of our LCP construction and BWT merge algorithms use (in RAM) as
few as $n$ bits on top of a packed representation of the input/output and
process data as fast as $2.92$ megabases per second.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1901.05226</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.04686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.04517">
    <id>http://arxiv.org/abs/1908.04517v1</id>
    <updated>2019-08-13T07:14:01Z</updated>
    <published>2019-08-13T07:14:01Z</published>
    <title>Beyond the Inverted Index</title>
    <summary>  In this paper, a new data structure named group-list is proposed. The
group-list is as simple as the inverted index. However, the group-list divides
document identifiers in an inverted index into groups, which makes it more
efficient when it is used to perform the intersection or union operation on
document identifiers. The experimental results on a synthetic dataset show that
the group-list outperforms the inverted index.
</summary>
    <author>
      <name>Zhi-Hong Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, and 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.04517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.04937">
    <id>http://arxiv.org/abs/1908.04937v1</id>
    <updated>2019-08-14T03:14:21Z</updated>
    <published>2019-08-14T03:14:21Z</published>
    <title>Fast Cartesian Tree Matching</title>
    <summary>  Cartesian tree matching is the problem of finding all substrings of a given
text which have the same Cartesian trees as that of a given pattern. So far
there is one linear-time solution for Cartesian tree matching, which is based
on the KMP algorithm. We improve the running time of the previous solution by
introducing new representations. We present the framework of a binary
filtration method and an efficient verification technique for Cartesian tree
matching. Any exact string matching algorithm can be used as a filtration for
Cartesian tree matching on our framework. We also present a SIMD solution for
Cartesian tree matching suitable for short patterns. By experiments we show
that known string matching algorithms combined on our framework of binary
filtration and efficient verification produce algorithms of good performances
for Cartesian tree matching.
</summary>
    <author>
      <name>Siwoo Song</name>
    </author>
    <author>
      <name>Cheol Ryu</name>
    </author>
    <author>
      <name>Simone Faro</name>
    </author>
    <author>
      <name>Thierry Lecroq</name>
    </author>
    <author>
      <name>Kunsoo Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures, Submitted to SPIRE 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.04937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.04933">
    <id>http://arxiv.org/abs/1908.04933v3</id>
    <updated>2019-11-16T07:48:28Z</updated>
    <published>2019-08-14T02:42:36Z</published>
    <title>Re-Pair In Small Space</title>
    <summary>  Re-Pair is a grammar compression scheme with favorably good compression
rates. The computation of Re-Pair comes with the cost of maintaining large
frequency tables, which makes it hard to compute Re-Pair on large scale data
sets. As a solution for this problem we present, given a text of length $n$
whose characters are drawn from an integer alphabet, an $O(n^2) \cap O(n^2 \lg
\log_\tau n \lg \lg \lg n / \log_\tau n)$ time algorithm computing Re-Pair in
$n \lg \max(n,\tau)$ bits of space including the text space, where $\tau$ is
the number of terminals and non-terminals. The algorithm works in the restore
model, supporting the recovery of the original input in the time for the
Re-Pair computation with $O(\lg n)$ additional bits of working space. We give
variants of our solution working in parallel or in the external memory model.
</summary>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Isamu Furuya</name>
    </author>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <author>
      <name>Kensuke Sakai</name>
    </author>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <link href="http://arxiv.org/abs/1908.04933v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04933v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.04810">
    <id>http://arxiv.org/abs/1908.04810v1</id>
    <updated>2019-08-13T18:21:04Z</updated>
    <published>2019-08-13T18:21:04Z</published>
    <title>On Occupancy Moments and Bloom Filter Efficiency</title>
    <summary>  Two multivariate committee distributions are shown to belong to Berg's family
of factorial series distributions and Kemp's family of generalized
hypergeometric factorial moment distributions. Exact moment formulas, upper and
lower bounds, and statistical parameter estimators are provided for the classic
occupancy and committee distributions. The derived moment equations are used to
determine exact formulas for the false-positive rate and efficiency of Bloom
filters -- probabilistic data structures used to solve the set membership
problem. This study reveals that the conventional Bloom filter analysis
overestimates the number of hash functions required to minimize the
false-positive rate, and shows that Bloom filter efficiency is monotonic in the
number of hash functions.
</summary>
    <author>
      <name>Jonathan Burns</name>
    </author>
    <link href="http://arxiv.org/abs/1908.04810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60C05, 68R05 (Primary) 94A24, 33C20 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.04056">
    <id>http://arxiv.org/abs/1908.04056v1</id>
    <updated>2019-08-12T08:58:39Z</updated>
    <published>2019-08-12T08:58:39Z</published>
    <title>New Results on Nyldon Words Derived Using an Algorithm from Hall Set
  Theory</title>
    <summary>  Grinberg defined Nyldon words as those words which cannot be factorized into
a sequence of lexicographically nondecreasing smaller Nyldon words. He was
inspired by Lyndon words, defined the same way except with "nondecreasing"
replaced by "nonincreasing." Charlier, Philibert, and Stipulanti proved that,
like Lyndon words, any word has a unique nondecreasing factorization into
Nyldon words. They also show that the Nyldon words form a right Lazard set, and
equivalently, a right Hall set. In this paper, we provide a new proof of unique
factorization into Nyldon words related to Hall set theory and resolve several
questions of Charlier et al. In particular, we prove that Nyldon words of a
fixed length form a circular code, we prove a result on factorizing powers of
words into Nyldon words, and we investigate the Lazard procedure for generating
Nyldon words.
</summary>
    <author>
      <name>Swapnil Garg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.04056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.05930">
    <id>http://arxiv.org/abs/1908.05930v1</id>
    <updated>2019-08-16T11:11:06Z</updated>
    <published>2019-08-16T11:11:06Z</published>
    <title>Efficient Online String Matching Based on Characters Distance Text
  Sampling</title>
    <summary>  Searching for all occurrences of a pattern in a text is a fundamental problem
in computer science with applications in many other fields, like natural
language processing, information retrieval and computational biology. Sampled
string matching is an efficient approach recently introduced in order to
overcome the prohibitive space requirements of an index construction, on the
one hand, and drastically reduce searching time for the online solutions, on
the other hand. In this paper we present a new algorithm for the sampled string
matching problem, based on a characters distance sampling approach. The main
idea is to sample the distances between consecutive occurrences of a given
pivot character and then to search online the sampled data for any occurrence
of the sampled pattern, before verifying the original text. From a theoretical
point of view we prove that, under suitable conditions, our solution can
achieve both linear worst-case time complexity and optimal average-time
complexity. From a practical point of view it turns out that our solution shows
a sub-linear behaviour in practice and speeds up online searching by a factor
of up to 9, using limited additional space whose amount goes from 11% to 2.8%
of the text size, with a gain up to 50% if compared with previous solutions.
</summary>
    <author>
      <name>Simone Faro</name>
    </author>
    <author>
      <name>Arianna Pavone</name>
    </author>
    <author>
      <name>Francesco Pio Marino</name>
    </author>
    <link href="http://arxiv.org/abs/1908.05930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.05930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.06428">
    <id>http://arxiv.org/abs/1908.06428v1</id>
    <updated>2019-08-18T11:38:09Z</updated>
    <published>2019-08-18T11:38:09Z</published>
    <title>The smallest grammar problem revisited</title>
    <summary>  In a seminal paper of Charikar et al. on the smallest grammar problem, the
authors derive upper and lower bounds on the approximation ratios for several
grammar-based compressors, but in all cases there is a gap between the lower
and upper bound. Here the gaps for $\mathsf{LZ78}$ and $\mathsf{BISECTION}$ are
closed by showing that the approximation ratio of $\mathsf{LZ78}$ is $\Theta(
(n/\log n)^{2/3})$, whereas the approximation ratio of $\mathsf{BISECTION}$ is
$\Theta(\sqrt{n/\log n})$. In addition, the lower bound for $\mathsf{RePair}$
is improved from $\Omega(\sqrt{\log n})$ to $\Omega(\log n/\log\log n)$.
Finally, results of Arpe and Reischuk relating grammar-based compression for
arbitrary alphabets and binary alphabets are improved.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Momoko Hirayama</name>
    </author>
    <author>
      <name>Danny Hucke</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Artur Jez</name>
    </author>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Carl Philipp Reh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A short version of this paper appeared in the Proceedings of SPIRE
  2016. This work has been supported by the DFG research project LO 748/10-1
  (QUANT-KOMP)</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.06428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.06428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.10984">
    <id>http://arxiv.org/abs/1907.10984v1</id>
    <updated>2019-07-25T11:48:57Z</updated>
    <published>2019-07-25T11:48:57Z</published>
    <title>Enumerating Range Modes</title>
    <summary>  We consider the range mode problem where given a sequence and a query range
in it, we want to find items with maximum frequency in the range. We give time-
and space- efficient algorithms for this problem. Our algorithms are efficient
for small maximum frequency cases. We also consider a natural generalization of
the problem: the range mode enumeration problem, for which there has been no
known efficient algorithms. Our algorithms have query time complexities which
is linear to the output size plus small terms.
</summary>
    <author>
      <name>Kentaro Sumigawa</name>
    </author>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <link href="http://arxiv.org/abs/1907.10984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.10874">
    <id>http://arxiv.org/abs/1907.10874v1</id>
    <updated>2019-07-25T07:37:42Z</updated>
    <published>2019-07-25T07:37:42Z</published>
    <title>How to Store a Random Walk</title>
    <summary>  Motivated by storage applications, we study the following data structure
problem: An encoder wishes to store a collection of jointly-distributed files
$\overline{X}:=(X_1,X_2,\ldots, X_n) \sim \mu$ which are \emph{correlated}
($H_\mu(\overline{X}) \ll \sum_i H_\mu(X_i)$), using as little (expected)
memory as possible, such that each individual file $X_i$ can be recovered
quickly with few (ideally constant) memory accesses.
  In the case of independent random files, a dramatic result by \Pat (FOCS'08)
and subsequently by Dodis, \Pat and Thorup (STOC'10) shows that it is possible
to store $\overline{X}$ using just a \emph{constant} number of extra bits
beyond the information-theoretic minimum space, while at the same time decoding
each $X_i$ in constant time. However, in the (realistic) case where the files
are correlated, much weaker results are known, requiring at least
$\Omega(n/poly\lg n)$ extra bits for constant decoding time, even for "simple"
joint distributions $\mu$.
  We focus on the natural case of compressing\emph{Markov chains}, i.e.,
storing a length-$n$ random walk on any (possibly directed) graph $G$. Denoting
by $\kappa(G,n)$ the number of length-$n$ walks on $G$, we show that there is a
succinct data structure storing a random walk using $\lg_2 \kappa(G,n) + O(\lg
n)$ bits of space, such that any vertex along the walk can be decoded in $O(1)$
time on a word-RAM. For the harder task of matching the \emph{point-wise}
optimal space of the walk, i.e., the empirical entropy $\sum_{i=1}^{n-1} \lg
(deg(v_i))$, we present a data structure with $O(1)$ extra bits at the price of
$O(\lg n)$ decoding time, and show that any improvement on this would lead to
an improved solution on the long-standing Dictionary problem. All of our data
structures support the \emph{online} version of the problem with constant
update and query time.
</summary>
    <author>
      <name>Emanuele Viola</name>
    </author>
    <author>
      <name>Omri Weinstein</name>
    </author>
    <author>
      <name>Huacheng Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1907.10874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.11232">
    <id>http://arxiv.org/abs/1907.11232v1</id>
    <updated>2019-07-24T20:50:16Z</updated>
    <published>2019-07-24T20:50:16Z</published>
    <title>Exhaustive Exact String Matching: The Analysis of the Full Human Genome</title>
    <summary>  Exact string matching has been a fundamental problem in computer science for
decades because of many practical applications. Some are related to common
procedures, such as searching in files and text editors, or, more recently, to
more advanced problems such as pattern detection in Artificial Intelligence and
Bioinformatics. Tens of algorithms and methodologies have been developed for
pattern matching and several programming languages, packages, applications and
online systems exist that can perform exact string matching in biological
sequences. These techniques, however, are limited to searching for specific and
predefined strings in a sequence. In this paper a novel methodology (called
Ex2SM) is presented, which is a pipeline of execution of advanced data
structures and algorithms, explicitly designed for text mining, that can detect
every possible repeated string in multivariate biological sequences. In
contrast to known algorithms in literature, the methodology presented here is
string agnostic, i.e., it does not require an input string to search for it,
rather it can detect every string that exists at least twice, regardless of its
attributes such as length, frequency, alphabet, overlapping etc. The complexity
of the problem solved and the potential of the proposed methodology is
demonstrated with the experimental analysis performed on the entire human
genome. More specifically, all repeated strings with a length of up to 50
characters have been detected, an achievement which is practically impossible
using other algorithms due to the exponential number of possible permutations
of such long strings.
</summary>
    <author>
      <name>Konstantinos F. Xylogiannopoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3341161.3343517</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3341161.3343517" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted for publication at IEEE/ACM ASONAM 2019 conference,
  Vancouver, BC, Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.11232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.12034">
    <id>http://arxiv.org/abs/1907.12034v2</id>
    <updated>2019-10-30T07:58:05Z</updated>
    <published>2019-07-28T07:32:58Z</published>
    <title>Minimal Absent Words in Rooted and Unrooted Trees</title>
    <summary>  We extend the theory of minimal absent words to (rooted and unrooted) trees,
having edges labeled by letters from an alphabet $\Sigma$ of cardinality
$\sigma$. We show that the set $\text{MAW}(T)$ of minimal absent words of a
rooted (resp. unrooted) tree $T$ with $n$ nodes has cardinality $O(n\sigma)$
(resp. $O(n^{2}\sigma)$), and we show that these bounds are realized. Then, we
exhibit algorithms to compute all minimal absent words in a rooted (resp.
unrooted) tree in output-sensitive time $O(n+|\text{MAW}(T)|)$ (resp.
$O(n^{2}+|\text{MAW}(T)|)$ assuming an integer alphabet of size polynomial in
$n$.
</summary>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a slightly modified version of the paper that appeared in the
  proceedings of SPIRE 2019, which contained an error in the example showed in
  Fig.1, now corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.12034v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12034v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.00848">
    <id>http://arxiv.org/abs/1908.00848v1</id>
    <updated>2019-08-02T13:33:47Z</updated>
    <published>2019-08-02T13:33:47Z</published>
    <title>Competitive Online Search Trees on Trees</title>
    <summary>  We consider the design of adaptive data structures for searching elements of
a tree-structured space. We use a natural generalization of the rotation-based
online binary search tree model in which the underlying search space is the set
of vertices of a tree. This model is based on a simple structure for
decomposing graphs, previously known under several names including elimination
trees, vertex rankings, and tubings. The model is equivalent to the classical
binary search tree model exactly when the underlying tree is a path. We
describe an online $O(\log \log n)$-competitive search tree data structure in
this model, matching the best known competitive ratio of binary search trees.
Our method is inspired by Tango trees, an online binary search tree algorithm,
but critically needs several new notions including one which we call
Steiner-closed search trees, which may be of independent interest. Moreover our
technique is based on a novel use of two levels of decomposition, first from
search space to a set of Steiner-closed trees, and secondly from these trees
into paths.
</summary>
    <author>
      <name>Prosenjit Bose</name>
    </author>
    <author>
      <name>Jean Cardinal</name>
    </author>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Grigorios Koumoutsos</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <link href="http://arxiv.org/abs/1908.00848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.00848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.00563">
    <id>http://arxiv.org/abs/1908.00563v1</id>
    <updated>2019-08-01T18:08:19Z</updated>
    <published>2019-08-01T18:08:19Z</published>
    <title>Dynamic Optimality Refuted -- For Tournament Heaps</title>
    <summary>  We prove a separation between offline and online algorithms for finger-based
tournament heaps undergoing key modifications. These heaps are implemented by
binary trees with keys stored on leaves, and intermediate nodes tracking the
min of their respective subtrees. They represent a natural starting point for
studying self-adjusting heaps due to the need to access the root-to-leaf path
upon modifications. We combine previous studies on the competitive ratios of
unordered binary search trees by [Fredman WADS2011] and on order-by-next
request by [Mart\'inez-Roura TCS2000] and [Munro ESA2000] to show that for any
number of fingers, tournament heaps cannot handle a sequence of modify-key
operations with competitive ratio in $o(\sqrt{\log{n}})$. Critical to this
analysis is the characterization of the modifications that a heap can undergo
upon an access. There are $\exp(\Theta(n \log{n}))$ valid heaps on $n$ keys,
but only $\exp(\Theta(n))$ binary search trees. We parameterize the
modification power through the well-studied concept of fingers: additional
pointers the data structure can manipulate arbitrarily. Here we demonstrate
that fingers can be significantly more powerful than servers moving on a static
tree by showing that access to $k$ fingers allow an offline algorithm to handle
any access sequence with amortized cost $O(\log_{k}(n) + 2^{\lg^{*}n})$.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Richard Peng</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <author>
      <name>Lingyi Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1908.00563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.00563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.01664">
    <id>http://arxiv.org/abs/1908.01664v1</id>
    <updated>2019-08-05T14:54:19Z</updated>
    <published>2019-08-05T14:54:19Z</published>
    <title>On the cyclic regularities of strings</title>
    <summary>  Regularities in strings are often related to periods and covers, which have
extensively been studied, and algorithms for their efficient computation have
broad application. In this paper we concentrate on computing cyclic
regularities of strings, in particular, we propose several efficient algorithms
for computing: (i) cyclic periodicity; (ii) all cyclic periodicity; (iii)
maximal local cyclic periodicity; (iv) cyclic covers.
</summary>
    <author>
      <name>Oluwole Ajala</name>
    </author>
    <author>
      <name>Miznah Alshammary</name>
    </author>
    <author>
      <name>Mai Alzamel</name>
    </author>
    <author>
      <name>Jia Gao</name>
    </author>
    <author>
      <name>Costas Iliopoulos</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Bruce Watson</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.01263">
    <id>http://arxiv.org/abs/1908.01263v1</id>
    <updated>2019-08-04T03:31:16Z</updated>
    <published>2019-08-04T03:31:16Z</published>
    <title>Matching reads to many genomes with the $r$-index</title>
    <summary>  The $r$-index is a tool for compressed indexing of genomic databases for
exact pattern matching, which can be used to completely align reads that
perfectly match some part of a genome in the database or to find seeds for
reads that do not. This paper shows how to download and install the programs
ri-buildfasta and ri-align; how to call ri-buildfasta on a FASTA file to build
an $r$-index for that file; and how to query that index with ri-align.
  Availability: The source code for these programs is released under GPLv3 and
available at https://github.com/alshai/r-index .
</summary>
    <author>
      <name>Taher Mun</name>
    </author>
    <author>
      <name>Alan Kuhnle</name>
    </author>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Ben Langmead</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.02211">
    <id>http://arxiv.org/abs/1908.02211v2</id>
    <updated>2019-11-29T13:06:54Z</updated>
    <published>2019-08-06T15:25:54Z</published>
    <title>An Index for Sequencing Reads Based on The Colored de Bruijn Graph</title>
    <summary>  In this article, we show how to transform a colored de Bruijn graph (dBG)
into a practical index for processing massive sets of sequencing reads. Similar
to previous works, we encode an instance of a colored dBG of the set using BOSS
and a color matrix C. To reduce the space requirements, we devise an algorithm
that produces a smaller and more sparse version of C. The novelties in this
algorithm are (i) an incomplete coloring of the graph and (ii) a greedy
coloring approach that tries to reuse the same colors for different strings
when possible. We also propose two algorithms that work on top of the index;
one is for reconstructing reads, and the other is for contig assembly.
Experimental results show that our data structure uses about half the space of
the plain representation of the set (1 Byte per DNA symbol) and that more than
99% of the reads can be reconstructed just from the index.
</summary>
    <author>
      <name>Diego Diaz-Domínguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.02211v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02211v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1908.01812">
    <id>http://arxiv.org/abs/1908.01812v2</id>
    <updated>2020-01-09T15:28:33Z</updated>
    <published>2019-08-05T19:25:57Z</published>
    <title>Optimal Joins using Compact Data Structures</title>
    <summary>  Worst-case optimal join algorithms have gained a lot of attention in the
database literature. We now count with several algorithms that are optimal in
the worst case, and many of them have been implemented and validated in
practice. However, the implementation of these algorithms often requires an
enhanced indexing structure: to achieve optimality we either need to build
completely new indexes, or we must populate the database with several
instantiations of indexes such as B$+$-trees. Either way, this means spending
an extra amount of storage space that may be non-negligible.
  We show that optimal algorithms can be obtained directly from a
representation that regards the relations as point sets in variable-dimensional
grids, without the need of extra storage. Our representation is a compact quad
tree for the static indexes, and a dynamic quadtree sharing subtrees (which we
dub a qdag) for intermediate results. We develop a compositional algorithm to
process full join queries under this representation, and show that the running
time of this algorithm is worst-case optimal in data complexity. Remarkably, we
can extend our framework to evaluate more expressive queries from relational
algebra by introducing a lazy version of qdags (lqdags). Once again, we can
show that the running time of our algorithms is worst-case optimal.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Juan L. Reutter</name>
    </author>
    <author>
      <name>Javiel Rojas-Ledesma</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01812v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01812v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.07330">
    <id>http://arxiv.org/abs/1812.07330v1</id>
    <updated>2018-12-18T12:39:42Z</updated>
    <published>2018-12-18T12:39:42Z</published>
    <title>Computing the $k$-binomial complexity of the Thue--Morse word</title>
    <summary>  Two words are $k$-binomially equivalent whenever they share the same
subwords, i.e., subsequences, of length at most $k$ with the same
multiplicities. This is a refinement of both abelian equivalence and the Simon
congruence. The $k$-binomial complexity of an infinite word $\mathbf{x}$ maps
the integer $n$ to the number of classes in the quotient, by this $k$-binomial
equivalence relation, of the set of factors of length $n$ occurring in
$\mathbf{x}$. This complexity measure has not been investigated very much. In
this paper, we characterize the $k$-binomial complexity of the Thue--Morse
word. The result is striking, compared to more familiar complexity functions.
Although the Thue--Morse word is aperiodic, its $k$-binomial complexity
eventually takes only two values. In this paper, we first obtain general
results about the number of occurrences of subwords appearing in iterates of
the form $\Psi^\ell(w)$ for an arbitrary morphism $\Psi$. We also thoroughly
describe the factors of the Thue--Morse word by introducing a relevant new
equivalence relation.
</summary>
    <author>
      <name>Marie Lejeune</name>
    </author>
    <author>
      <name>Julien Leroy</name>
    </author>
    <author>
      <name>Michel Rigo</name>
    </author>
    <link href="http://arxiv.org/abs/1812.07330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.07330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.07223">
    <id>http://arxiv.org/abs/1905.07223v1</id>
    <updated>2019-05-17T12:24:01Z</updated>
    <published>2019-05-17T12:24:01Z</published>
    <title>Separating many words by counting occurrences of factors</title>
    <summary>  For a given language $L$, we study the languages $X$ such that for all
distinct words $u, v \in L$, there exists a word $x \in X$ that appears a
different number of times as a factor in $u$ and in $v$. In particular, we are
interested in the following question: For which languages $L$ does there exist
a finite language $X$ satisfying the above condition? We answer this question
for all regular languages and for all sets of factors of infinite words.
</summary>
    <author>
      <name>Aleksi Saarela</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages. Full version of an article to appear in the proceedings of
  the 23rd International Conference on Developments in Language Theory (DLT
  2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.07223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.08579">
    <id>http://arxiv.org/abs/1907.08579v1</id>
    <updated>2019-07-19T17:17:44Z</updated>
    <published>2019-07-19T17:17:44Z</published>
    <title>On Approximate Range Mode and Range Selection</title>
    <summary>  For any $\epsilon \in (0,1)$, a $(1+\epsilon)$-approximate range mode query
asks for the position of an element whose frequency in the query range is at
most a factor $(1+\epsilon)$ smaller than the true mode. For this problem, we
design an $O(n/\epsilon)$ bit data structure supporting queries in
$O(\lg(1/\epsilon))$ time. This is an encoding data structure which does not
require access to the input sequence; we prove the space cost is asymptotically
optimal for constant $\epsilon$. Our solution improves the previous best result
of Greve et al. (Cell Probe Lower Bounds and Approximations for Range Mode,
ICALP'10) by reducing the space cost by a factor of $\lg n$ while achieving the
same query time. We also design an $O(n)$-word dynamic data structure that
answers queries in $O(\lg n /\lg\lg n)$ time and supports insertions and
deletions in $O(\lg n)$ time, for any constant $\epsilon \in (0,1)$. This is
the first result on dynamic approximate range mode; it can also be used to
obtain the first static data structure for approximate 3-sided range mode
queries in two dimensions.
  We also consider approximate range selection. For any $\alpha \in (0,1/2)$,
an $\alpha$-approximate range selection query asks for the position of an
element whose rank in the query range is in $[k - \alpha s, k + \alpha s]$,
where $k$ is a rank given by the query and $s$ is the size of the query range.
When $\alpha$ is a constant, we design an $O(n)$-bit encoding data structure
that can answer queries in constant time and prove this space cost is
asymptotically optimal. The previous best result by Krizanc et al. (Range Mode
and Range Median Queries on Lists and Trees, Nordic Journal of Computing, 2005)
uses $O(n\lg n)$ bits, or $O(n)$ words, to achieve constant approximation for
range median only. Thus we not only improve the space cost, but also provide
support for any arbitrary $k$ given at query time.
</summary>
    <author>
      <name>Hicham El-Zein</name>
    </author>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Bryce Sandlund</name>
    </author>
    <link href="http://arxiv.org/abs/1907.08579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.08579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.08355">
    <id>http://arxiv.org/abs/1907.08355v2</id>
    <updated>2019-11-09T23:21:07Z</updated>
    <published>2019-07-19T03:02:25Z</published>
    <title>Data Structures Meet Cryptography: 3SUM with Preprocessing</title>
    <summary>  This paper shows several connections between data structure problems and
cryptography against preprocessing attacks. Our results span data structure
upper bounds, cryptographic applications, and data structure lower bounds, as
summarized next.
  First, we apply Fiat--Naor inversion, a technique with cryptographic origins,
to obtain a data structure upper bound. In particular, our technique yields a
suite of algorithms with space $S$ and (online) time $T$ for a preprocessing
version of the $N$-input 3SUM problem where $S^3\cdot T = \widetilde{O}(N^6)$.
This disproves a strong conjecture (Goldstein et al., WADS 2017) that there is
no data structure that solves this problem for $S=N^{2-\delta}$ and $T =
N^{1-\delta}$ for any constant $\delta>0$.
  Secondly, we show equivalence between lower bounds for a broad class of
(static) data structure problems and one-way functions in the random oracle
model that resist a very strong form of preprocessing attack. Concretely, given
a random function $F: [N] \to [N]$ (accessed as an oracle) we show how to
compile it into a function $G^F: [N^2] \to [N^2]$ which resists $S$-bit
preprocessing attacks that run in query time $T$ where
$ST=O(N^{2-\varepsilon})$ (assuming a corresponding data structure lower bound
on 3SUM). In contrast, a classical result of Hellman tells us that $F$ itself
can be more easily inverted, say with $N^{2/3}$-bit preprocessing in $N^{2/3}$
time. We also show that much stronger lower bounds follow from the hardness of
kSUM. Our results can be equivalently interpreted as security against
adversaries that are very non-uniform, or have large auxiliary input, or as
security in the face of a powerfully backdoored random oracle.
  Thirdly, we give lower bounds for 3SUM and a range of geometric problems
which match the best known lower bounds for static data structure problems
(Larsen, FOCS 2012).
</summary>
    <author>
      <name>Alexander Golovnev</name>
    </author>
    <author>
      <name>Siyao Guo</name>
    </author>
    <author>
      <name>Thibaut Horel</name>
    </author>
    <author>
      <name>Sunoo Park</name>
    </author>
    <author>
      <name>Vinod Vaikuntanathan</name>
    </author>
    <link href="http://arxiv.org/abs/1907.08355v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.08355v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.08246">
    <id>http://arxiv.org/abs/1907.08246v1</id>
    <updated>2019-07-18T18:55:41Z</updated>
    <published>2019-07-18T18:55:41Z</published>
    <title>Finding First and Most-Beautiful Queens by Integer Programming</title>
    <summary>  The n-queens puzzle is a well-known combinatorial problem that requires to
place n queens on an n x n chessboard so that no two queens can attack each
other. Since the 19th century, this problem was studied by many mathematicians
and computer scientists. While finding any solution to the n-queens puzzle is
rather straightforward, it is very challenging to find the lexicographically
first (or smallest) feasible solution. Solutions for this type are known in the
literature for n &lt;= 55, while for some larger chessboards only partial
solutions are known. The present paper was motivated by the question of whether
Integer Linear Programming (ILP) can be used to compute solutions for some open
instances. We describe alternative ILP-based solution approaches, and show that
they are indeed able to compute (sometimes in unexpectedly-short computing
times) many new lexicographically optimal solutions for n ranging from 56 to
115. One of the proposed algorithms is a pure cutting plane method based on a
combinatorial variant of classical Gomory cuts. We also address an intriguing
"lexicographic bottleneck" (or min-max) variant of the problem that requires
finding a most beautiful (in a well defined sense) placement, and report its
solution for n up to 176.
</summary>
    <author>
      <name>Matteo Fischetti</name>
    </author>
    <author>
      <name>Domenico Salvagnin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.08246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.08246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R05" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.09271">
    <id>http://arxiv.org/abs/1907.09271v1</id>
    <updated>2019-07-22T12:29:03Z</updated>
    <published>2019-07-22T12:29:03Z</published>
    <title>Succinct Representation for (Non)Deterministic Finite Automata</title>
    <summary>  Deterministic finite automata are one of the simplest and most practical
models of computation studied in automata theory. Their conceptual extension is
the non-deterministic finite automata which also have plenty of applications.
In this article, we study these models through the lens of succinct data
structures where our ultimate goal is to encode these mathematical objects
using information-theoretically optimal number of bits along with supporting
queries on them efficiently. Towards this goal, we first design a succinct data
structure for representing any deterministic finite automaton $\mathcal{D}$
having $n$ states over a $\sigma$-letter alphabet $\Sigma$ using $(\sigma-1)
n\log n + O(n \log \sigma)$ bits of space, which can determine, given an input
string $x$ over $\Sigma$, whether $\mathcal{D}$ accepts $x$ in $O(|x| \log
\sigma)$ time, using constant words of working space. When the input
deterministic finite automaton is acyclic, not only we can improve the above
space-bound significantly to $(\sigma -1) (n-1)\log n+ 3n + O(\log^2 \sigma) +
o(n)$ bits, we also obtain optimal query time for string acceptance checking.
More specifically, using our succinct representation, we can check if a given
input string $x$ can be accepted by the acyclic deterministic finite automaton
using time proportional to the length of $x$, hence, the optimal query time. We
also exhibit a succinct data structure for representing a non-deterministic
finite automaton $\mathcal{N}$ having $n$ states over a $\sigma$-letter
alphabet $\Sigma$ using $\sigma n^2+n$ bits of space, such that given an input
string $x$, we can decide whether $\mathcal{N}$ accepts $x$ efficiently in
$O(n^2|x|)$ time. Finally, we also provide time and space-efficient algorithms
for performing several standard operations such as union, intersection, and
complement on the languages accepted by deterministic finite automata.
</summary>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <link href="http://arxiv.org/abs/1907.09271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.09271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.09280">
    <id>http://arxiv.org/abs/1907.09280v1</id>
    <updated>2019-07-22T12:41:59Z</updated>
    <published>2019-07-22T12:41:59Z</published>
    <title>Optimal In-place Algorithms for Basic Graph Problems</title>
    <summary>  We present linear time {\it in-place} algorithms for several basic and
fundamental graph problems including the well-known graph search methods (like
depth-first search, breadth-first search, maximum cardinality search),
connectivity problems (like biconnectivity, $2$-edge connectivity),
decomposition problem (like chain decomposition) among various others,
improving the running time (by polynomial multiplicative factor) of the recent
results of Chakraborty et al. [ESA, 2018] who designed $O(n^3 \lg n)$ time
in-place algorithms for a strict subset of the above mentioned problems. The
running times of all our algorithms are essentially optimal as they run in
linear time. One of the main ideas behind obtaining these algorithms is the
detection and careful exploitation of sortedness present in the input
representation for any graph without loss of generality. This observation alone
is powerful enough to design some basic linear time in-place algorithms, but
more non-trivial graph problems require extra techniques which, we believe, may
find other applications while designing in-place algorithms for different graph
problems in the future.
</summary>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <link href="http://arxiv.org/abs/1907.09280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.09280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0706.4107">
    <id>http://arxiv.org/abs/0706.4107v1</id>
    <updated>2007-06-27T22:04:40Z</updated>
    <published>2007-06-27T22:04:40Z</published>
    <title>Radix Sorting With No Extra Space</title>
    <summary>  It is well known that n integers in the range [1,n^c] can be sorted in O(n)
time in the RAM model using radix sorting. More generally, integers in any
range [1,U] can be sorted in O(n sqrt{loglog n}) time. However, these
algorithms use O(n) words of extra memory. Is this necessary?
  We present a simple, stable, integer sorting algorithm for words of size
O(log n), which works in O(n) time and uses only O(1) words of extra memory on
a RAM model. This is the integer sorting case most useful in practice. We
extend this result with same bounds to the case when the keys are read-only,
which is of theoretical interest. Another interesting question is the case of
arbitrary c. Here we present a black-box transformation from any RAM sorting
algorithm to a sorting algorithm which uses only O(1) extra space and has the
same running time. This settles the complexity of in-place sorting in terms of
the complexity of sorting.
</summary>
    <author>
      <name>Gianni Franceschini</name>
    </author>
    <author>
      <name>S. Muthukrishnan</name>
    </author>
    <author>
      <name>Mihai Patrascu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of paper accepted to ESA 2007. (17 pages)</arxiv:comment>
    <link href="http://arxiv.org/abs/0706.4107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.4107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.10444">
    <id>http://arxiv.org/abs/1907.10444v1</id>
    <updated>2019-07-24T13:39:59Z</updated>
    <published>2019-07-24T13:39:59Z</published>
    <title>Constant Delay Traversal of Grammar-Compressed Graphs with Bounded Rank</title>
    <summary>  We present a pointer-based data structure for constant time traversal of the
edges of an edge-labeled (alphabet $\Sigma$) directed hypergraph (a graph where
edges can be incident to more than two vertices, and the incident vertices are
ordered) given as hyperedge-replacement grammar $G$. It is assumed that the
grammar has a fixed rank $\kappa$ (maximal number of vertices connected to a
nonterminal hyperedge) and that each vertex of the represented graph is
incident to at most one $\sigma$-edge per direction ($\sigma \in \Sigma$).
Precomputing the data structure needs $O(|G||\Sigma|\kappa r h)$ space and
$O(|G||\Sigma|\kappa rh^2)$ time, where $h$ is the height of the derivation
tree of $G$ and $r$ is the maximal rank of a terminal edge occurring in the
grammar.
</summary>
    <author>
      <name>Sebastian Maneth</name>
    </author>
    <author>
      <name>Fabian Peternek</name>
    </author>
    <link href="http://arxiv.org/abs/1907.10444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.11206">
    <id>http://arxiv.org/abs/1907.11206v1</id>
    <updated>2019-07-25T17:11:11Z</updated>
    <published>2019-07-25T17:11:11Z</published>
    <title>The Strong 3SUM-INDEXING Conjecture is False</title>
    <summary>  In the 3SUM-Indexing problem the goal is to preprocess two lists of elements
from $U$, $A=(a_1,a_2,\ldots,a_n)$ and $B=(b_1,b_2,...,b_n)$, such that given
an element $c\in U$ one can quickly determine whether there exists a pair
$(a,b)\in A \times B$ where $a+b=c$. Goldstein et al.~[WADS'2017] conjectured
that there is no algorithm for 3SUM-Indexing which uses $n^{2-\Omega(1)}$ space
and $n^{1-\Omega(1)}$ query time.
  We show that the conjecture is false by reducing the 3SUM-Indexing problem to
the problem of inverting functions, and then applying an algorithm of Fiat and
Naor [SICOMP'1999] for inverting functions.
</summary>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <link href="http://arxiv.org/abs/1907.11206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.05369">
    <id>http://arxiv.org/abs/1907.05369v1</id>
    <updated>2019-07-04T21:14:28Z</updated>
    <published>2019-07-04T21:14:28Z</published>
    <title>Abelian-square factors and binary words</title>
    <summary>  In this work, we affirm the conjecture proposed by Gabriele Fici and Filippo
Mignosi at the 10th Conference on Combinatorics on Words.
</summary>
    <author>
      <name>Salah Triki</name>
    </author>
    <link href="http://arxiv.org/abs/1907.05369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.06310">
    <id>http://arxiv.org/abs/1907.06310v1</id>
    <updated>2019-07-15T02:02:40Z</updated>
    <published>2019-07-15T02:02:40Z</published>
    <title>New Paths from Splay to Dynamic Optimality</title>
    <summary>  Consider the task of performing a sequence of searches in a binary search
tree. After each search, an algorithm is allowed to arbitrarily restructure the
tree, at a cost proportional to the amount of restructuring performed. The cost
of an execution is the sum of the time spent searching and the time spent
optimizing those searches with restructuring operations. This notion was
introduced by Sleator and Tarjan in (JACM, 1985), along with an algorithm and a
conjecture. The algorithm, Splay, is an elegant procedure for performing
adjustments while moving searched items to the top of the tree. The conjecture,
called "dynamic optimality," is that the cost of splaying is always within a
constant factor of the optimal algorithm for performing searches. The
conjecture stands to this day. In this work, we attempt to lay the foundations
for a proof of the dynamic optimality conjecture.
</summary>
    <author>
      <name>Caleb C. Levy</name>
    </author>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An earlier version of this work appeared in the Proceedings of the
  Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms. arXiv admin note:
  text overlap with arXiv:1907.06309</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.06310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.06310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.06309">
    <id>http://arxiv.org/abs/1907.06309v1</id>
    <updated>2019-07-15T01:45:56Z</updated>
    <published>2019-07-15T01:45:56Z</published>
    <title>Splaying Preorders and Postorders</title>
    <summary>  Let $T$ be a binary search tree. We prove two results about the behavior of
the Splay algorithm (Sleator and Tarjan 1985). Our first result is that
inserting keys into an empty binary search tree via splaying in the order of
either $T$'s preorder or $T$'s postorder takes linear time. Our proof uses the
fact that preorders and postorders are pattern-avoiding: i.e. they contain no
subsequences that are order-isomorphic to $(2,3,1)$ and $(3,1,2)$,
respectively. Pattern-avoidance implies certain constraints on the manner in
which items are inserted. We exploit this structure with a simple potential
function that counts inserted nodes lying on access paths to uninserted nodes.
Our methods can likely be extended to permutations that avoid more general
patterns. Second, if $T'$ is any other binary search tree with the same keys as
$T$ and $T$ is weight-balanced (Nievergelt and Reingold 1973), then splaying
$T$'s preorder sequence or $T$'s postorder sequence starting from $T'$ takes
linear time. To prove this, we demonstrate that preorders and postorders of
balanced search trees do not contain many large "jumps" in symmetric order, and
exploit this fact by using the dynamic finger theorem (Cole et al. 2000). Both
of our results provide further evidence in favor of the elusive "dynamic
optimality conjecture."
</summary>
    <author>
      <name>Caleb C. Levy</name>
    </author>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <link href="http://arxiv.org/abs/1907.06309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.06309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.08142">
    <id>http://arxiv.org/abs/1907.08142v1</id>
    <updated>2019-07-18T16:24:30Z</updated>
    <published>2019-07-18T16:24:30Z</published>
    <title>Stack sorting with restricted stacks</title>
    <summary>  The (classical) problem of characterizing and enumerating permutations that
can be sorted using two stacks connected in series is still largely open. In
the present paper we address a related problem, in which we impose restrictions
both on the procedure and on the stacks. More precisely, we consider a greedy
algorithm where we perform the rightmost legal operation (here "rightmost"
refers to the usual representation of stack sorting problems). Moreover, the
first stack is required to be $\sigma$-avoiding, for some permutation $\sigma$,
meaning that, at each step, the elements maintained in the stack avoid the
pattern $\sigma$ when read from top to bottom. Since the set of permutations
which can be sorted by such a device (which we call $\sigma$-machine) is not
always a class, it would be interesting to understand when it happens. We will
prove that the set of $\sigma$-machines whose associated sortable permutations
are not a class is counted by Catalan numbers. Moreover, we will analyze two
specific $\sigma$-machines in full details (namely when $\sigma=321$ and
$\sigma=123$), providing for each of them a complete characterization and
enumeration of sortable permutations.
</summary>
    <author>
      <name>Giulio Cerbai</name>
    </author>
    <author>
      <name>Anders Claesson</name>
    </author>
    <author>
      <name>Luca Ferrari</name>
    </author>
    <link href="http://arxiv.org/abs/1907.08142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.08142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.07795">
    <id>http://arxiv.org/abs/1907.07795v1</id>
    <updated>2019-07-17T22:11:22Z</updated>
    <published>2019-07-17T22:11:22Z</published>
    <title>Efficient computation of the Jacobi symbol</title>
    <summary>  The family of left-to-right GCD algorithms reduces input numbers by
repeatedly subtracting the smaller number, or multiple of the smaller number,
from the larger number. This paper describes how to extend any such algorithm
to compute the Jacobi symbol, using a single table lookup per reduction. For
both quadratic time GCD algorithms (Euclid, Lehmer) and subquadratic algorithms
(Knuth, Sch\"onhage, M\"oller), the additional cost is linear, roughly one
table lookup per quotient in the quotient sequence. This method was used for
the 2010 rewrite of the Jacobi symbol computation in GMP.
</summary>
    <author>
      <name>Niels Möller</name>
    </author>
    <link href="http://arxiv.org/abs/1907.07795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.07795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11Y16" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.00192">
    <id>http://arxiv.org/abs/1907.00192v1</id>
    <updated>2019-06-29T12:09:18Z</updated>
    <published>2019-06-29T12:09:18Z</published>
    <title>Recurrence along directions in multidimensional words</title>
    <summary>  In this paper we introduce and study new notions of uniform recurrence in
multidimensional words. A $d$-dimensional word is called \emph{uniformly
recurrent} if for all $(s_1,\ldots,s_d)\in\N^d$ there exists $n\in\N$ such that
each block of size $(n,\ldots,n)$ contains the prefix of size
$(s_1,\ldots,s_d)$. We are interested in a modification of this property.
Namely, we ask that for each rational direction $(q_1,\ldots,q_d)$, each
rectangular prefix occurs along this direction in positions
$\ell(q_1,\ldots,q_d)$ with bounded gaps. Such words are called \emph{uniformly
recurrent along all directions}. We provide several constructions of
multidimensional words satisfying this condition, and more generally, a series
of four increasingly stronger conditions. In particular, we study the uniform
recurrence along directions of multidimentional rotation words and of fixed
points of square morphisms.
</summary>
    <author>
      <name>Émilie Charlier</name>
    </author>
    <author>
      <name>Svetlana Puzynina</name>
    </author>
    <author>
      <name>Élise Vandomme</name>
    </author>
    <link href="http://arxiv.org/abs/1907.00192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.08731">
    <id>http://arxiv.org/abs/1804.08731v2</id>
    <updated>2018-07-16T16:09:31Z</updated>
    <published>2018-04-23T20:40:40Z</published>
    <title>Longest Common Substring Made Fully Dynamic</title>
    <summary>  In the longest common substring (LCS) problem, we are given two strings $S$
and $T$, each of length at most $n$, and we are asked to find a longest string
occurring as a fragment of both $S$ and $T$. This is a classical and
well-studied problem in computer science with a known $\mathcal{O}(n)$-time
solution. In the fully dynamic version of the problem, edit operations are
allowed in either of the two strings, and we are asked to report an LCS after
each such operation. We present the first solution to this problem that
requires sublinear time per edit operation. In particular, we show how to
return an LCS in $\tilde{\mathcal{O}}(n^{2/3})$ time (or
$\tilde{\mathcal{O}}(\sqrt{n})$ time if edits are allowed in only one of the
two strings) after each operation using $\tilde{\mathcal{O}}(n)$ space.
  This line of research was recently initiated by the authors [SPIRE 2017] in a
somewhat restricted dynamic variant. An $\tilde{\mathcal{O}}(n)$-sized data
structure that returns an LCS of the two strings after a single edit operation
(that is reverted afterwards) in $\tilde{\mathcal{O}}(1)$ time was presented.
At CPM 2018, three papers studied analogously restricted dynamic variants of
problems on strings. We show that our techniques can be used to obtain fully
dynamic algorithms for several classical problems on strings, namely, computing
the longest repeat, the longest palindrome and the longest Lyndon substring of
a string. The only previously known sublinear-time dynamic algorithms for
problems on strings were obtained for maintaining a dynamic collection of
strings for comparison queries and for pattern matching with the most recent
advances made by Gawrychowski et al. [SODA 2018] and by Clifford et al. [STACS
2018].
</summary>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08731v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08731v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.00425">
    <id>http://arxiv.org/abs/1808.00425v1</id>
    <updated>2018-08-01T17:10:42Z</updated>
    <published>2018-08-01T17:10:42Z</published>
    <title>List Decoding with Double Samplers</title>
    <summary>  We develop the notion of "double samplers", first introduced by Dinur and
Kaufman [Proc. 58th FOCS, 2017], which are samplers with additional
combinatorial properties, and whose existence we prove using high dimensional
expanders.
  We show how double samplers give a generic way of amplifying distance in a
way that enables efficient list-decoding. There are many error correcting code
constructions that achieve large distance by starting with a base code $C$ with
moderate distance, and then amplifying the distance using a sampler, e.g., the
ABNNR code construction [IEEE Trans. Inform. Theory, 38(2):509--516, 1992.]. We
show that if the sampler is part of a larger double sampler then the
construction has an efficient list-decoding algorithm and the list decoding
algorithm is oblivious to the base code $C$ (i.e., it runs the unique decoder
for $C$ in a black box way).
  Our list-decoding algorithm works as follows: it uses a local voting scheme
from which it constructs a unique games constraint graph. The constraint graph
is an expander, so we can solve unique games efficiently. These solutions are
the output of the list decoder. This is a novel use of a unique games algorithm
as a subroutine in a decoding procedure, as opposed to the more common
situation in which unique games are used for demonstrating hardness results.
  Double samplers and high dimensional expanders are akin to pseudorandom
objects in their utility, but they greatly exceed random objects in their
combinatorial properties. We believe that these objects hold significant
potential for coding theoretic constructions and view this work as
demonstrating the power of double samplers in this context.
</summary>
    <author>
      <name>Irit Dinur</name>
    </author>
    <author>
      <name>Prahladh Harsha</name>
    </author>
    <author>
      <name>Tali Kaufman</name>
    </author>
    <author>
      <name>Inbal Livni Navon</name>
    </author>
    <author>
      <name>Amnon Ta Shma</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.03530">
    <id>http://arxiv.org/abs/1803.03530v1</id>
    <updated>2018-03-08T02:45:15Z</updated>
    <published>2018-03-08T02:45:15Z</published>
    <title>Synchronization Strings: Efficient and Fast Deterministic Constructions
  over Small Alphabets</title>
    <summary>  Synchronization strings are recently introduced by Haeupler and Shahrasbi
(STOC 2017) in the study of codes for correcting insertion and deletion errors
(insdel codes). They showed that for any parameter $\varepsilon>0$,
synchronization strings of arbitrary length exist over an alphabet whose size
depends only on $\varepsilon$. Specifically, they obtained an alphabet size of
$O(\varepsilon^{-4})$, which left an open question on where the minimal size of
such alphabets lies between $\Omega(\varepsilon^{-1})$ and
$O(\varepsilon^{-4})$. In this work, we partially bridge this gap by providing
an improved lower bound of $\Omega(\varepsilon^{-3/2})$, and an improved upper
bound of $O(\varepsilon^{-2})$. We also provide fast explicit constructions of
synchronization strings over small alphabets.
  Further, along the lines of previous work on similar combinatorial objects,
we study the extremal question of the smallest possible alphabet size over
which synchronization strings can exist for some constant $\varepsilon &lt; 1$. We
show that one can construct $\varepsilon$-synchronization strings over
alphabets of size four while no such string exists over binary alphabets. This
reduces the extremal question to whether synchronization strings exist over
ternary alphabets.
</summary>
    <author>
      <name>Kuan Cheng</name>
    </author>
    <author>
      <name>Bernhard Haeupler</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Amirbehshad Shahrasbi</name>
    </author>
    <author>
      <name>Ke Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages. arXiv admin note: substantial text overlap with
  arXiv:1710.07356</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.03530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.08343">
    <id>http://arxiv.org/abs/1904.08343v1</id>
    <updated>2019-04-17T16:12:54Z</updated>
    <published>2019-04-17T16:12:54Z</published>
    <title>The power word problem</title>
    <summary>  In this work we introduce a new succinct variant of the word problem in a
finitely generated group $G$, which we call the power word problem: the input
word may contain powers $p^x$, where $p$ is a finite word over generators of
$G$ and $x$ is a binary encoded integer. The power word problem is a
restriction of the compressed word problem, where the input word is represented
by a straight-line program (i.e., an algebraic circuit over $G$). The main
result of the paper states that the power word problem for a finitely generated
free group $F$ is AC$^0$-Turing-reducible to the word problem for $F$.
Moreover, the following hardness result is shown: For a wreath product $G \wr
\mathbb{Z}$, where $G$ is either free of rank at least two or finite
non-solvable, the power word problem is complete for coNP. This contrasts with
the situation where $G$ is abelian: then the power word problem is shown to be
in TC$^0$.
</summary>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Armin Weiß</name>
    </author>
    <link href="http://arxiv.org/abs/1904.08343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.11423">
    <id>http://arxiv.org/abs/1906.11423v1</id>
    <updated>2019-06-27T03:33:23Z</updated>
    <published>2019-06-27T03:33:23Z</published>
    <title>Vector Programming Using Generative Recursion</title>
    <summary>  Vector programming is an important topic in many Introduction to Computer
Science courses. Despite the importance of vectors, learning vector programming
is a source of frustration for many students. Much of the frustration is rooted
in discovering the source of bugs that are manifested as out-of-bounds
indexing. The problem is that such bugs are, sometimes, rooted in incorrectly
computing an index. Other times, however, these errors are rooted in mistaken
reasoning about how to correctly process a vector. Unfortunately, either way,
all too often beginners are left adrift to resolve indexing errors on their
own. This article extends the work done on vector programming using vector
intervals and structural recursion to using generative recursion. As for
problems solved using structural recursion, vector intervals provide beginners
with a useful framework for designing code that properly indexes vectors. This
article presents the methodology and concrete examples that others may use to
build their own CS1 modules involving vector programming using any programming
language.
</summary>
    <author>
      <name>Marco T. Morazán</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Seton Hall University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.295.3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.295.3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TFPIE 2018, arXiv:1906.10757. arXiv admin note: text
  overlap with arXiv:1805.05124</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 295, 2019, pp. 35-51</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.11423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.11423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.01600">
    <id>http://arxiv.org/abs/1907.01600v1</id>
    <updated>2019-07-02T19:45:34Z</updated>
    <published>2019-07-02T19:45:34Z</published>
    <title>Approximate Similarity Search Under Edit Distance Using
  Locality-Sensitive Hashing</title>
    <summary>  Edit distance similarity search, also called approximate pattern matching, is
a fundamental problem with widespread applications. The goal of the problem is
to preprocess n strings of length d to quickly answer queries q of the form: if
there is a database string within edit distance r of q, return a database
string within edit distance cr of q. A data structure solving this problem is
analyzed using two criteria: the amount of extra space used in preprocessing,
and the expected time to answer a query.
  Previous approaches to this problem have either used trie-based methods,
which give exact solutions at the cost of expensive queries, or embeddings,
which only work for large (superconstant) values of c.
  In this work we achieve the first bounds for any approximation factor c, via
a simple and easy-to-implement hash function. This gives a running time of
$\tilde{O}(d3^rn^{1/c})$, with space $\tilde{O}(3^r n^{1 + 1/c} + dn)$. We show
how to apply these ideas to the closely-related Approximate Nearest Neighbor
problem for edit distance, obtaining similar time bounds.
</summary>
    <author>
      <name>Samuel McCauley</name>
    </author>
    <link href="http://arxiv.org/abs/1907.01600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.01631">
    <id>http://arxiv.org/abs/1907.01631v1</id>
    <updated>2019-07-02T20:55:47Z</updated>
    <published>2019-07-02T20:55:47Z</published>
    <title>Cache-Friendly Search Trees; or, In Which Everything Beats std::set</title>
    <summary>  While a lot of work in theoretical computer science has gone into optimizing
the runtime and space usage of data structures, such work very often neglects a
very important component of modern computers: the cache. In doing so, very
often, data structures are developed that achieve theoretically-good runtimes
but are slow in practice due to a large number of cache misses. In 1999, Frigo
et al. introduced the notion of a cache-oblivious algorithm: an algorithm that
uses the cache to its advantage, regardless of the size or structure of said
cache. Since then, various authors have designed cache-oblivious algorithms and
data structures for problems from matrix multiplication to array sorting. We
focus in this work on cache-oblivious search trees; i.e. implementing an
ordered dictionary in a cache-friendly manner. We will start by presenting an
overview of cache-oblivious data structures, especially cache-oblivious search
trees. We then give practical results using these cache-oblivious structures on
modern-day machinery, comparing them to the standard std::set and other
cache-friendly dictionaries such as B-trees.
</summary>
    <author>
      <name>Jeffrey Barratt</name>
    </author>
    <author>
      <name>Brian Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1907.01631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.01815">
    <id>http://arxiv.org/abs/1907.01815v2</id>
    <updated>2020-01-13T12:29:55Z</updated>
    <published>2019-07-03T09:35:53Z</published>
    <title>Circular Pattern Matching with $k$ Mismatches</title>
    <summary>  The $k$-mismatch problem consists in computing the Hamming distance between a
pattern $P$ of length $m$ and every length-$m$ substring of a text $T$ of
length $n$, if this distance is no more than $k$. In many real-world
applications, any cyclic rotation of $P$ is a relevant pattern, and thus one is
interested in computing the minimal distance of every length-$m$ substring of
$T$ and any cyclic rotation of $P$. This is the circular pattern matching with
$k$ mismatches ($k$-CPM) problem. A multitude of papers have been devoted to
solving this problem but, to the best of our knowledge, only average-case upper
bounds are known. In this paper, we present the first non-trivial worst-case
upper bounds for the $k$-CPM problem. Specifically, we show an $O(nk)$-time
algorithm and an $O(n+\frac{n}{m}\,k^4)$-time algorithm. The latter algorithm
applies in an extended way a technique that was very recently developed for the
$k$-mismatch problem [Bringmann et al., SODA 2019].
  A preliminary version of this work appeared at FCT 2019. In this version we
improve the time complexity of the main algorithm from $O(n+\frac{n}{m}\,k^5)$
to $O(n+\frac{n}{m}\,k^4)$.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszyński</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper from FCT 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.01815v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01815v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.02308">
    <id>http://arxiv.org/abs/1907.02308v1</id>
    <updated>2019-07-04T09:56:34Z</updated>
    <published>2019-07-04T09:56:34Z</published>
    <title>The Alternating BWT: an algorithmic perspective</title>
    <summary>  The Burrows-Wheeler Transform (BWT) is a word transformation introduced in
1994 for Data Compression. It has become a fundamental tool for designing
self-indexing data structures, with important applications in several area in
science and engineering. The Alternating Burrows-Wheeler Transform (ABWT) is
another transformation recently introduced in [Gessel et al. 2012] and studied
in the field of Combinatorics on Words. It is analogous to the BWT, except that
it uses an alternating lexicographical order instead of the usual one. Building
on results in [Giancarlo et al. 2018], where we have shown that BWT and ABWT
are part of a larger class of reversible transformations, here we provide a
combinatorial and algorithmic study of the novel transform ABWT. We establish a
deep analogy between BWT and ABWT by proving they are the only ones in the
above mentioned class to be rank-invertible, a novel notion guaranteeing
efficient invertibility. In addition, we show that the backward-search
procedure can be efficiently generalized to the ABWT; this result implies that
also the ABWT can be used as a basis for efficient compressed full text
indices. Finally, we prove that the ABWT can be efficiently computed by using a
combination of the Difference Cover suffix sorting algorithm
[K\"{a}rkk\"{a}inen et al., 2006] with a linear time algorithm for finding the
minimal cyclic rotation of a word with respect to the alternating
lexicographical order.
</summary>
    <author>
      <name>Raffaele Giancarlo</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Antonio Restivo</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <link href="http://arxiv.org/abs/1907.02308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.06273">
    <id>http://arxiv.org/abs/1811.06273v1</id>
    <updated>2018-11-15T10:12:09Z</updated>
    <published>2018-11-15T10:12:09Z</published>
    <title>On Infinite Prefix Normal Words</title>
    <summary>  Prefix normal words are binary words that have no factor with more $1$s than
the prefix of the same length. Finite prefix normal words were introduced in
[Fici and Lipt\'ak, DLT 2011]. In this paper, we study infinite prefix normal
words and explore their relationship to some known classes of infinite binary
words. In particular, we establish a connection between prefix normal words and
Sturmian words, between prefix normal words and abelian complexity, and between
prefix normality and lexicographic order.
</summary>
    <author>
      <name>Ferdinando Cicalese</name>
    </author>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 4 figures, accepted at SOFSEM 2019 (45th International
  Conference on Current Trends in Theory and Practice of Computer Science,
  Nov\'y Smokovec, Slovakia, January 27-30, 2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.06273v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06273v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.03235">
    <id>http://arxiv.org/abs/1907.03235v3</id>
    <updated>2019-12-03T12:05:34Z</updated>
    <published>2019-07-07T07:33:13Z</published>
    <title>Bidirectional Text Compression in External Memory</title>
    <summary>  Bidirectional compression algorithms work by substituting repeated substrings
by references that, unlike in the famous LZ77-scheme, can point to either
direction. We present such an algorithm that is particularly suited for an
external memory implementation. We evaluate it experimentally on large data
sets of size up to 128 GiB (using only 16 GiB of RAM) and show that it is
significantly faster than all known LZ77 compressors, while producing a roughly
similar number of factors. We also introduce an external memory decompressor
for texts compressed with any uni- or bidirectional compression scheme.
</summary>
    <author>
      <name>Patrick Dinklage</name>
    </author>
    <author>
      <name>Jonas Ellert</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Manuel Penschuck</name>
    </author>
    <link href="http://arxiv.org/abs/1907.03235v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.03235v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.04752">
    <id>http://arxiv.org/abs/1907.04752v1</id>
    <updated>2019-07-10T14:29:22Z</updated>
    <published>2019-07-10T14:29:22Z</published>
    <title>Sparse Regular Expression Matching</title>
    <summary>  We present the first algorithm for regular expression matching that can take
advantage of sparsity in the input instance. Our main result is a new algorithm
that solves regular expression matching in $O\left(\Delta \log \log
\frac{nm}{\Delta} + n + m\right)$ time, where $m$ is the number of positions in
the regular expression, $n$ is the length of the string, and $\Delta$ is the
\emph{density} of the instance, defined as the total number of active states in
a simulation of the position automaton. This measure is a lower bound on the
total number of active states in simulations of all classic polynomial sized
finite automata. Our bound improves the best known bounds for regular
expression matching by almost a linear factor in the density of the problem.
The key component in the result is a novel linear space representation of the
position automaton that supports state-set transition computation in
near-linear time in the size of the input and output state sets.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <link href="http://arxiv.org/abs/1907.04752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.04660">
    <id>http://arxiv.org/abs/1907.04660v1</id>
    <updated>2019-07-10T12:21:12Z</updated>
    <published>2019-07-10T12:21:12Z</published>
    <title>String Attractors and Combinatorics on Words</title>
    <summary>  The notion of \emph{string attractor} has recently been introduced in
[Prezza, 2017] and studied in [Kempa and Prezza, 2018] to provide a unifying
framework for known dictionary-based compressors. A string attractor for a word
$w=w[1]w[2]\cdots w[n]$ is a subset $\Gamma$ of the positions $\{1,\ldots,n\}$,
such that all distinct factors of $w$ have an occurrence crossing at least one
of the elements of $\Gamma$. While finding the smallest string attractor for a
word is a NP-complete problem, it has been proved in [Kempa and Prezza, 2018]
that dictionary compressors can be interpreted as algorithms approximating the
smallest string attractor for a given word.
  In this paper we explore the notion of string attractor from a combinatorial
point of view, by focusing on several families of finite words. The results
presented in the paper suggest that the notion of string attractor can be used
to define new tools to investigate combinatorial properties of the words.
</summary>
    <author>
      <name>Sabrina Mantaci</name>
    </author>
    <author>
      <name>Antonio Restivo</name>
    </author>
    <author>
      <name>Giuseppe Romana</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <link href="http://arxiv.org/abs/1907.04660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.04405">
    <id>http://arxiv.org/abs/1907.04405v1</id>
    <updated>2019-07-09T20:42:46Z</updated>
    <published>2019-07-09T20:42:46Z</published>
    <title>$L_p$ Pattern Matching in a Stream</title>
    <summary>  We consider the problem of computing distance between a pattern of length $n$
and all $n$-length subwords of a text in the streaming model.
  In the streaming setting, only the Hamming distance ($L_0$) has been studied.
It is known that computing the Hamming distance between a pattern and a
streaming text exactly requires $\Omega(n)$ space. Therefore, to develop
sublinear-space solutions, one must relax their requirements. One possibility
to do so is to compute only the distances bounded by a threshold $k$,
see~[SODA'19, Clifford, Kociumaka, Porat]. The motivation for this variant of
this problem is that we are interested in subwords of the text that are similar
to the pattern, i.e. in subwords such that the distance between them and the
pattern is relatively small.
  On the other hand, the main application of the streaming setting is
processing large-scale data, such as biological data. Recent advances in
hardware technology allow generating such data at a very high speed, but
unfortunately, the produced data may contain about 10\% of noise~[Biol.
Direct.'07, Klebanov and Yakovlev]. To analyse such data, it is not sufficient
to consider small distances only. A possible workaround for this issue is
$(1\pm\varepsilon)$-approximation. This line of research was initiated in
[ICALP'16, Clifford and Starikovskaya] who gave a
$(1\pm\varepsilon)$-approximation algorithm with space
$\widetilde{O}(\varepsilon^{-5}\sqrt{n})$.
  In this work, we show a suite of new streaming algorithms for computing the
Hamming, $L_1$, $L_2$ and general $L_p$ ($0 &lt; p \le 2$) distances between the
pattern and the text. Our results significantly extend over the previous result
in this setting. In particular, for the Hamming distance case and for the $L_p$
distance when $0 &lt; p \le 1$ we show a streaming algorithm that uses
$\widetilde{O}(\varepsilon^{-2}\sqrt{n})$ space for polynomial-size alphabets.
</summary>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <author>
      <name>Michal Svagerka</name>
    </author>
    <author>
      <name>Przemysław Uznański</name>
    </author>
    <link href="http://arxiv.org/abs/1907.04405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.05486">
    <id>http://arxiv.org/abs/1906.05486v1</id>
    <updated>2019-06-13T05:24:44Z</updated>
    <published>2019-06-13T05:24:44Z</published>
    <title>On Longest Common Property Preserved Substring Queries</title>
    <summary>  We revisit the problem of longest common property preserving substring
queries introduced by~Ayad et al. (SPIRE 2018, arXiv 2018). We consider a
generalized and unified on-line setting, where we are given a set $X$ of $k$
strings of total length $n$ that can be pre-processed so that, given a query
string $y$ and a positive integer $k'\leq k$, we can determine the longest
substring of $y$ that satisfies some specific property and is common to at
least $k'$ strings in $X$. Ayad et al. considered the longest square-free
substring in an on-line setting and the longest periodic and palindromic
substring in an off-line setting. In this paper, we give efficient solutions in
the on-line setting for finding the longest common square, periodic,
palindromic, and Lyndon substrings. More precisely, we show that $X$ can be
pre-processed in $O(n)$ time resulting in a data structure of $O(n)$ size that
answers queries in $O(|y|\log\sigma)$ time and $O(1)$ working space, where
$\sigma$ is the size of the alphabet, and the common substring must be a
square, a periodic substring, a palindrome, or a Lyndon word.
</summary>
    <author>
      <name>Kazuki Kai</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">minor change from version submitted to SPIRE 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.05384">
    <id>http://arxiv.org/abs/1906.05384v1</id>
    <updated>2019-06-12T21:28:51Z</updated>
    <published>2019-06-12T21:28:51Z</published>
    <title>Loop Programming Practices that Simplify Quicksort Implementations</title>
    <summary>  Quicksort algorithm with Hoare's partition scheme is traditionally
implemented with nested loops. In this article, we present loop programming and
refactoring techniques that lead to simplified implementation for Hoare's
quicksort algorithm consisting of a single loop. We believe that the techniques
are beneficial for general programming and may be used for the discovery of
more novel algorithms.
</summary>
    <author>
      <name>Shoupu Wan</name>
    </author>
    <link href="http://arxiv.org/abs/1906.05384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.06015">
    <id>http://arxiv.org/abs/1906.06015v1</id>
    <updated>2019-06-14T04:31:12Z</updated>
    <published>2019-06-14T04:31:12Z</published>
    <title>Dynamic Path-Decomposed Tries</title>
    <summary>  A keyword dictionary is an associative array whose keys are strings. Recent
applications handling massive keyword dictionaries in main memory have a need
for a space-efficient implementation. When limited to static applications,
there are a number of highly-compressed keyword dictionaries based on the
advancements of practical succinct data structures. However, as most succinct
data structures are only efficient in the static case, it is still difficult to
implement a keyword dictionary that is space efficient and dynamic. In this
article, we propose such a keyword dictionary. Our main idea is to embrace the
path decomposition technique, which was proposed for constructing
cache-friendly tries. To store the path-decomposed trie in small memory, we
design data structures based on recent compact hash trie representations.
Exhaustive experiments on real-world datasets reveal that our dynamic keyword
dictionary needs up to 68% less space than the existing smallest ones.
</summary>
    <author>
      <name>Shunsuke Kanda</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <author>
      <name>Kazuhiro Morita</name>
    </author>
    <author>
      <name>Masao Fuketa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.06015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.06015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.06965">
    <id>http://arxiv.org/abs/1906.06965v2</id>
    <updated>2019-07-29T14:45:41Z</updated>
    <published>2019-06-17T11:38:42Z</published>
    <title>Matching Patterns with Variables</title>
    <summary>  A pattern p (i.e., a string of variables and terminals) matches a word w, if
w can be obtained by uniformly replacing the variables of p by terminal words.
The respective matching problem, i.e., deciding whether or not a given pattern
matches a given word, is generally NP-complete, but can be solved in
polynomial-time for classes of patterns with restricted structure. In this
paper we overview a series of recent results related to efficient matching for
patterns with variables, as well as a series of extensions of this problem.
</summary>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Markus L. Schmid</name>
    </author>
    <link href="http://arxiv.org/abs/1906.06965v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.06965v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.07874">
    <id>http://arxiv.org/abs/1906.07874v1</id>
    <updated>2019-06-19T01:45:10Z</updated>
    <published>2019-06-19T01:45:10Z</published>
    <title>Space Efficient Algorithms for Breadth-Depth Search</title>
    <summary>  Continuing the recent trend, in this article we design several
space-efficient algorithms for two well-known graph search methods. Both these
search methods share the same name {\it breadth-depth search} (henceforth {\sf
BDS}), although they work entirely in different fashion. The classical
implementation for these graph search methods takes $O(m+n)$ time and $O(n \lg
n)$ bits of space in the standard word RAM model (with word size being
$\Theta(\lg n)$ bits), where $m$ and $n$ denotes the number of edges and
vertices of the input graph respectively. Our goal here is to beat the space
bound of the classical implementations, and design $o(n \lg n)$ space
algorithms for these search methods by paying little to no penalty in the
running time. Note that our space bounds (i.e., with $o(n \lg n)$ bits of
space) do not even allow us to explicitly store the required information to
implement the classical algorithms, yet our algorithms visits and reports all
the vertices of the input graph in correct order.
</summary>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Anish Mukherjee</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, This work will appear in FCT 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.07874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.07874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.07871">
    <id>http://arxiv.org/abs/1906.07871v1</id>
    <updated>2019-06-19T01:34:47Z</updated>
    <published>2019-06-19T01:34:47Z</published>
    <title>Indexing Graph Search Trees and Applications</title>
    <summary>  We consider the problem of compactly representing the Depth First Search
(DFS) tree of a given undirected or directed graph having $n$ vertices and $m$
edges while supporting various DFS related queries efficiently in the RAM with
logarithmic word size. We study this problem in two well-known models: {\it
indexing} and {\it encoding} models. While most of these queries can be
supported easily in constant time using $O(n \lg n)$ bits\footnote{We use $\lg$
to denote logarithm to the base $2$.} of extra space, our goal here is, more
specifically, to beat this trivial $O(n \lg n)$ bit space bound, yet not
compromise too much on the running time of these queries. In the {\it indexing}
model, the space bound of our solution involves the quantity $m$, hence, we
obtain different bounds for sparse and dense graphs respectively. In the {\it
encoding} model, we first give a space lower bound, followed by an almost
optimal data structure with extremely fast query time. Central to our algorithm
is a partitioning of the DFS tree into connected subtrees, and a compact way to
store these connections. Finally, we also apply these techniques to compactly
index the shortest path structure, biconnectivity structures among others.
</summary>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, Preliminary version of this paper will appear in MFCS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.07871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.07871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.09732">
    <id>http://arxiv.org/abs/1906.09732v1</id>
    <updated>2019-06-24T05:33:41Z</updated>
    <published>2019-06-24T05:33:41Z</published>
    <title>Dynamic Palindrome Detection</title>
    <summary>  Lately, there is a growing interest in dynamic string matching problems.
Specifically, the dynamic Longest Common Factor problem has been researched and
some interesting results has been reached. In this paper we examine another
classic string problem in a dynamic setting - finding the longest palindrome
substring of a given string. We show that the longest palindrome can be
maintained in poly-logarithmic time per symbol edit.
</summary>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Itai Boneh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1806.02718 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.09732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.09732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.10535">
    <id>http://arxiv.org/abs/1906.10535v2</id>
    <updated>2019-09-13T10:19:31Z</updated>
    <published>2019-06-25T13:56:56Z</published>
    <title>Pseudo-solutions of word equations</title>
    <summary>  We present a framework which allows a uniform approach to the recently
introduced concept of pseudo-repetitions on words in the morphic case. This
framework is at the same time more general and simpler. We introduce the
concept of a pseudo-solution and a pseudo-rank of an equation. In particular,
this allows to prove that if a classical equation forces periodicity then it
also forces pseudo-periodicity. Consequently, there is no need to investigate
generalizations of important equations one by one.
</summary>
    <author>
      <name>Štěpán Holub</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">small corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.10535v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10535v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.11030">
    <id>http://arxiv.org/abs/1906.11030v2</id>
    <updated>2019-12-28T13:33:02Z</updated>
    <published>2019-06-26T12:34:14Z</published>
    <title>Combinatorial Algorithms for String Sanitization</title>
    <summary>  String data are often disseminated to support applications such as
location-based service provision or DNA sequence analysis. This dissemination,
however, may expose sensitive patterns that model confidential knowledge. In
this paper, we consider the problem of sanitizing a string by concealing the
occurrences of sensitive patterns, while maintaining data utility, in two
settings that are relevant to many common string processing tasks.
  In the first setting, we aim to generate the minimal-length string that
preserves the order of appearance and frequency of all non-sensitive patterns.
Such a string allows accurately performing tasks based on the sequential nature
and pattern frequencies of the string. To construct such a string, we propose a
time-optimal algorithm, TFS-ALGO. We also propose another time-optimal
algorithm, PFS-ALGO, which preserves a partial order of appearance of
non-sensitive patterns but produces a much shorter string that can be analyzed
more efficiently. The strings produced by either of these algorithms are
constructed by concatenating non-sensitive parts of the input string. However,
it is possible to detect the sensitive patterns by ``reversing'' the
concatenation operations. In response, we propose a heuristic, MCSR-ALGO, which
replaces letters in the strings output by the algorithms with carefully
selected letters, so that sensitive patterns are not reinstated, implausible
patterns are not introduced, and occurrences of spurious patterns are
prevented. In the second setting, we aim to generate a string that is at
minimal edit distance from the original string, in addition to preserving the
order of appearance and frequency of all non-sensitive patterns. To construct
such a string, we propose an algorithm, ETFS-ALGO, based on solving specific
instances of approximate regular expression matching.
</summary>
    <author>
      <name>Giulia Bernardini</name>
    </author>
    <author>
      <name>Huiping Chen</name>
    </author>
    <author>
      <name>Alessio Conte</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Grigorios Loukides</name>
    </author>
    <author>
      <name>Nadia Pisanti</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <author>
      <name>Michelle Sweering</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper accepted to ECML/PKDD 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.11030v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.11030v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.11062">
    <id>http://arxiv.org/abs/1906.11062v1</id>
    <updated>2019-06-24T18:57:57Z</updated>
    <published>2019-06-24T18:57:57Z</published>
    <title>Survey of Information Encoding Techniques for DNA</title>
    <summary>  Key to DNA storage is encoding the information to a sequence of nucleotides
before it can be synthesised for storage. Definition of such an encoding or
mapping must adhere to multiple design restrictions. First, not all possible
sequences of nucleotides can be synthesised. Homopolymers, e.g., sequences of
the same nucleotide, of a length of more than two, for example, cannot be
synthesised without potential errors. Similarly, the G-C content of the
resulting sequences should be higher than 50\%. Second, given that synthesis is
expensive, the encoding must map as many bits as possible to one nucleotide.
Third, the synthesis (as well as the sequencing) is error prone, leading to
substitutions, deletions and insertions. An encoding must therefore be designed
to be resilient to errors through error correction codes or replication.
Fourth, for the purpose of computation and selective retrieval, encodings
should result in substantially different sequences across all data, even for
very similar data. In the following we discuss the history and evolution of
encodings.
</summary>
    <author>
      <name>Thomas Heinis</name>
    </author>
    <link href="http://arxiv.org/abs/1906.11062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.11062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.00809">
    <id>http://arxiv.org/abs/1906.00809v1</id>
    <updated>2019-06-03T13:45:43Z</updated>
    <published>2019-06-03T13:45:43Z</published>
    <title>Rpair: Rescaling RePair with Rsync</title>
    <summary>  Data compression is a powerful tool for managing massive but repetitive
datasets, especially schemes such as grammar-based compression that support
computation over the data without decompressing it. In the best case such a
scheme takes a dataset so big that it must be stored on disk and shrinks it
enough that it can be stored and processed in internal memory. Even then,
however, the scheme is essentially useless unless it can be built on the
original dataset reasonably quickly while keeping the dataset on disk. In this
paper we show how we can preprocess such datasets with context-triggered
piecewise hashing such that afterwards we can apply RePair and other
grammar-based compressors more easily. We first give our algorithm, then show
how a variant of it can be used to approximate the LZ77 parse, then leverage
that to prove theoretical bounds on compression, and finally give experimental
evidence that our approach is competitive in practice.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <link href="http://arxiv.org/abs/1906.00809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.00563">
    <id>http://arxiv.org/abs/1906.00563v1</id>
    <updated>2019-06-03T04:17:38Z</updated>
    <published>2019-06-03T04:17:38Z</published>
    <title>Direct Linear Time Construction of Parameterized Suffix and LCP Arrays
  for Constant Alphabets</title>
    <summary>  We present the first worst-case linear time algorithm that directly computes
the parameterized suffix and LCP arrays for constant sized alphabets. Previous
algorithms either required quadratic time or the parameterized suffix tree to
be built first. More formally, for a string over static alphabet $\Sigma$ and
parameterized alphabet $\Pi$, our algorithm runs in $O(n\pi)$ time and $O(n)$
words of space, where $\pi$ is the number of distinct symbols of $\Pi$ in the
string.
</summary>
    <author>
      <name>Noriki Fujisato</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to SPIRE 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.00563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.01346">
    <id>http://arxiv.org/abs/1906.01346v1</id>
    <updated>2019-06-04T11:07:29Z</updated>
    <published>2019-06-04T11:07:29Z</published>
    <title>Characteristic Parameters and Special Trapezoidal Words</title>
    <summary>  Following earlier work by Aldo de Luca and others, we study trapezoidal words
and their prefixes, with respect to their characteristic parameters $K$ and $R$
(length of shortest unrepeated suffix, and shortest length without right
special factors, respectively), as well as their symmetric versions $H$ and
$L$. We consider the distinction between closed (i.e., periodic-like) and open
prefixes, and between Sturmian and non-Sturmian ones. Our main results
characterize right special and strictly bispecial trapezoidal words, as done by
de Luca and Mignosi for Sturmian words.
</summary>
    <author>
      <name>Alma D'Aniello</name>
    </author>
    <author>
      <name>Alessandro De Luca</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages; to appear in LNCS proceedings for WORDS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.01346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.00665">
    <id>http://arxiv.org/abs/1906.00665v1</id>
    <updated>2019-06-03T09:41:57Z</updated>
    <published>2019-06-03T09:41:57Z</published>
    <title>Every nonnegative real number is an abelian critical exponent</title>
    <summary>  The abelian critical exponent of an infinite word $w$ is defined as the
maximum ratio between the exponent and the period of an abelian power occurring
in $w$. It was shown by Fici et al. that the set of finite abelian critical
exponents of Sturmian words coincides with the Lagrange spectrum. This spectrum
contains every large enough positive real number. We construct words whose
abelian critical exponents fill the remaining gaps, that is, we prove that for
each nonnegative real number $\theta$ there exists an infinite word having
abelian critical exponent $\theta$. We also extend this result to the
$k$-abelian setting.
</summary>
    <author>
      <name>Jarkko Peltomäki</name>
    </author>
    <author>
      <name>Markus A. Whiteland</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-28796-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-28796-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 12th International Conference, WORDS, Lecture
  Notes in Computer Science Vol. 11682, pp. 275-285 (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.00665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.03689">
    <id>http://arxiv.org/abs/1906.03689v1</id>
    <updated>2019-06-09T18:50:25Z</updated>
    <published>2019-06-09T18:50:25Z</published>
    <title>Borders, Palindrome Prefixes, and Square Prefixes</title>
    <summary>  We show that the number of length-$n$ words over a $k$-letter alphabet having
no even palindromic prefix is the same as the number of length-$n$ unbordered
words, by constructing an explicit bijection between the two sets. A similar
result holds for those words having no odd palindromic prefix, again by
constructing a certain bijection. Using known results on borders, it follows
that the number of length-$n$ words having no even (resp., odd) palindromic
prefix is asymptotically $\gamma_k \cdot k^n$ for some positive constant
$\gamma_k$. We obtain an analogous result for words having no nontrivial
palindromic prefix. Finally, we obtain similar results for words having no
square prefix, thus proving a 2013 conjecture of Chaffin, Linderman, Sloane,
and Wilks.
</summary>
    <author>
      <name>Daniel Gabric</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/1906.03689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.04346">
    <id>http://arxiv.org/abs/1607.04346v2</id>
    <updated>2016-11-14T02:20:36Z</updated>
    <published>2016-07-15T00:16:58Z</published>
    <title>Space-Efficient Construction of Compressed Indexes in Deterministic
  Linear Time</title>
    <summary>  We show that the compressed suffix array and the compressed suffix tree of a
string $T$ can be built in $O(n)$ deterministic time using $O(n\log\sigma)$
bits of space, where $n$ is the string length and $\sigma$ is the alphabet
size. Previously described deterministic algorithms either run in time that
depends on the alphabet size or need $\omega(n\log \sigma)$ bits of working
space. Our result has immediate applications to other problems, such as
yielding the first linear-time LZ77 and LZ78 parsing algorithms that use $O(n
\log\sigma)$ bits.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper to appear at SODA 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04346v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04346v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.09120">
    <id>http://arxiv.org/abs/1812.09120v1</id>
    <updated>2018-12-21T13:52:54Z</updated>
    <published>2018-12-21T13:52:54Z</published>
    <title>Lower bounds for text indexing with mismatches and differences</title>
    <summary>  In this paper we study lower bounds for the fundamental problem of text
indexing with mismatches and differences. In this problem we are given a long
string of length $n$, the "text", and the task is to preprocess it into a data
structure such that given a query string $Q$, one can quickly identify
substrings that are within Hamming or edit distance at most $k$ from $Q$. This
problem is at the core of various problems arising in biology and text
processing. While exact text indexing allows linear-size data structures with
linear query time, text indexing with $k$ mismatches (or $k$ differences) seems
to be much harder: All known data structures have exponential dependency on $k$
either in the space, or in the time bound. We provide conditional and
pointer-machine lower bounds that make a step toward explaining this
phenomenon. We start by demonstrating lower bounds for $k = \Theta(\log n)$. We
show that assuming the Strong Exponential Time Hypothesis, any data structure
for text indexing that can be constructed in polynomial time cannot have
$\mathcal{O}(n^{1-\delta})$ query time, for any $\delta>0$. This bound also
extends to the setting where we only ask for $(1+\varepsilon)$-approximate
solutions for text indexing. However, in many applications the value of $k$ is
rather small, and one might hope that for small~$k$ we can develop more
efficient solutions. We show that this would require a radically new approach
as using the current methods one cannot avoid exponential dependency on $k$
either in the space, or in the time bound for all even $\frac{8}{\sqrt{3}}
\sqrt{\log n} \le k = o(\log n)$. Our lower bounds also apply to the dictionary
look-up problem, where instead of a text one is given a set of strings.
</summary>
    <author>
      <name>Vincent Cohen-Addad</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Feuilloley</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIF</arxiv:affiliation>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DI-ENS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SODA 2019, Jan 2019, San Diego, United States</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.09120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.09120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.04897">
    <id>http://arxiv.org/abs/1906.04897v1</id>
    <updated>2019-05-20T01:53:05Z</updated>
    <published>2019-05-20T01:53:05Z</published>
    <title>Prefix Block-Interchanges on Binary and Ternary Strings</title>
    <summary>  The genome rearrangement problem computes the minimum number of operations
that are required to sort all elements of a permutation. A block-interchange
operation exchanges two blocks of a permutation which are not necessarily
adjacent and in a prefix block-interchange, one block is always the prefix of
that permutation. In this paper, we focus on applying prefix block-interchanges
on binary and ternary strings. We present upper bounds to group and sort a
given binary/ternary string. We also provide upper bounds for a different
version of the block-interchange operation which we refer to as the `restricted
prefix block-interchange'. We observe that our obtained upper bound for
restricted prefix block-interchange operations on binary strings is better than
that of other genome rearrangement operations to group fully normalized binary
strings. Consequently, we provide a linear-time algorithm to solve the problem
of grouping binary normalized strings by restricted prefix block-interchanges.
We also provide a polynomial time algorithm to group normalized ternary strings
by prefix block-interchange operations. Finally, we provide a classification
for ternary strings based on the required number of prefix block-interchange
operations.
</summary>
    <author>
      <name>Md. Khaledur Rahman</name>
    </author>
    <author>
      <name>M. Sohel Rahman</name>
    </author>
    <link href="http://arxiv.org/abs/1906.04897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.04897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.05208">
    <id>http://arxiv.org/abs/1906.05208v1</id>
    <updated>2019-06-12T15:24:07Z</updated>
    <published>2019-06-12T15:24:07Z</published>
    <title>Sorted Top-k in Rounds</title>
    <summary>  We consider the sorted top-$k$ problem whose goal is to recover the top-$k$
items with the correct order out of $n$ items using pairwise comparisons. In
many applications, multiple rounds of interaction can be costly. We restrict
our attention to algorithms with a constant number of rounds $r$ and try to
minimize the sample complexity, i.e. the number of comparisons.
  When the comparisons are noiseless, we characterize how the optimal sample
complexity depends on the number of rounds (up to a polylogarithmic factor for
general $r$ and up to a constant factor for $r=1$ or 2). In particular, the
sample complexity is $\Theta(n^2)$ for $r=1$, $\Theta(n\sqrt{k} + n^{4/3})$ for
$r=2$ and $\tilde{\Theta}\left(n^{2/r} k^{(r-1)/r} + n\right)$ for $r \geq 3$.
  We extend our results of sorted top-$k$ to the noisy case where each
comparison is correct with probability $2/3$. When $r=1$ or 2, we show that the
sample complexity gets an extra $\Theta(\log(k))$ factor when we transition
from the noiseless case to the noisy case.
  We also prove new results for top-$k$ and sorting in the noisy case. We
believe our techniques can be generally useful for understanding the trade-off
between round complexities and sample complexities of rank aggregation
problems.
</summary>
    <author>
      <name>Mark Braverman</name>
    </author>
    <author>
      <name>Jieming Mao</name>
    </author>
    <author>
      <name>Yuval Peres</name>
    </author>
    <link href="http://arxiv.org/abs/1906.05208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1906.05266">
    <id>http://arxiv.org/abs/1906.05266v1</id>
    <updated>2019-06-12T17:47:17Z</updated>
    <published>2019-06-12T17:47:17Z</published>
    <title>The Tandem Duplication Distance is NP-hard</title>
    <summary>  In computational biology, tandem duplication is an important biological
phenomenon which can occur either at the genome or at the DNA level. A tandem
duplication takes a copy of a genome segment and inserts it right after the
segment - this can be represented as the string operation $AXB \Rightarrow
AXXB$. For example, Tandem exon duplications have been found in many species
such as human, fly or worm, and have been largely studied in computational
biology. The Tandem Duplication (TD) distance problem we investigate in this
paper is defined as follows: given two strings $S$ and $T$ over the same
alphabet, compute the smallest sequence of tandem duplications required to
convert $S$ to $T$. The natural question of whether the TD distance can be
computed in polynomial time was posed in 2004 by Leupold et al. and had
remained open, despite the fact that tandem duplications have received much
attention ever since. In this paper, we prove that this problem is NP-hard. We
further show that this hardness holds even if all characters of $S$ are
distinct. This is known as the exemplar TD distance, which is of special
relevance in bioinformatics. One of the tools we develop for the reduction is a
new problem called the Cost-Effective Subgraph, for which we obtain
W[1]-hardness results that might be of independent interest. We finally show
that computing the exemplar TD distance between $S$ and $T$ is fixed-parameter
tractable. Our results open the door to many other questions, and we conclude
with several open problems.
</summary>
    <author>
      <name>Manuel Lafond</name>
    </author>
    <author>
      <name>Binhai Zhu</name>
    </author>
    <author>
      <name>Peng Zou</name>
    </author>
    <link href="http://arxiv.org/abs/1906.05266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.07455">
    <id>http://arxiv.org/abs/1905.07455v3</id>
    <updated>2019-10-23T14:56:50Z</updated>
    <published>2019-05-16T12:39:10Z</published>
    <title>Speeding up the Karatsuba algorithm</title>
    <summary>  This paper describes an $\sim {\cal O}(n)$ pre-compute technique to speed up
the Karatsuba algorithm for multiplying two numbers.
</summary>
    <author>
      <name>Satish Ramakrishna</name>
    </author>
    <author>
      <name>Kamesh Aiyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.07455v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07455v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.08563">
    <id>http://arxiv.org/abs/1905.08563v2</id>
    <updated>2019-10-09T17:22:17Z</updated>
    <published>2019-05-21T11:44:49Z</published>
    <title>Memory lower bounds for deterministic self-stabilization</title>
    <summary>  In the context of self-stabilization, a \emph{silent} algorithm guarantees
that the register of every node does not change once the algorithm has
stabilized. At the end of the 90's, Dolev et al. [Acta Inf. '99] showed that,
for finding the centers of a graph, for electing a leader, or for constructing
a spanning tree, every silent algorithm must use a memory of $\Omega(\log n)$
bits per register in $n$-node networks. Similarly, Korman et al. [Dist. Comp.
'07] proved, using the notion of proof-labeling-scheme, that, for constructing
a minimum-weight spanning trees (MST), every silent algorithm must use a memory
of $\Omega(\log^2n)$ bits per register. It follows that requiring the algorithm
to be silent has a cost in terms of memory space, while, in the context of
self-stabilization, where every node constantly checks the states of its
neighbors, the silence property can be of limited practical interest. In fact,
it is known that relaxing this requirement results in algorithms with smaller
space-complexity.
  In this paper, we are aiming at measuring how much gain in terms of memory
can be expected by using arbitrary self-stabilizing algorithms, not necessarily
silent. To our knowledge, the only known lower bound on the memory requirement
for general algorithms, also established at the end of the 90's, is due to
Beauquier et al.~[PODC '99] who proved that registers of constant size are not
sufficient for leader election algorithms. We improve this result by
establishing a tight lower bound of $\Theta(\log \Delta+\log \log n)$ bits per
register for self-stabilizing algorithms solving $(\Delta+1)$-coloring or
constructing a spanning tree in networks of maximum degree~$\Delta$. The lower
bound $\Omega(\log \log n)$ bits per register also holds for leader election.
</summary>
    <author>
      <name>Lélia Blin</name>
    </author>
    <author>
      <name>Laurent Feuilloley</name>
    </author>
    <author>
      <name>Gabriel Le Bouder</name>
    </author>
    <link href="http://arxiv.org/abs/1905.08563v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08563v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.08974">
    <id>http://arxiv.org/abs/1905.08974v1</id>
    <updated>2019-05-22T06:15:31Z</updated>
    <published>2019-05-22T06:15:31Z</published>
    <title>Cartesian Tree Matching and Indexing</title>
    <summary>  We introduce a new metric of match, called Cartesian tree matching, which
means that two strings match if they have the same Cartesian trees. Based on
Cartesian tree matching, we define single pattern matching for a text of length
n and a pattern of length m, and multiple pattern matching for a text of length
n and k patterns of total length m. We present an O(n+m) time algorithm for
single pattern matching, and an O((n+m) log k) deterministic time or O(n+m)
randomized time algorithm for multiple pattern matching. We also define an
index data structure called Cartesian suffix tree, and present an O(n)
randomized time algorithm to build the Cartesian suffix tree. Our efficient
algorithms for Cartesian tree matching use a representation of the Cartesian
tree, called the parent-distance representation.
</summary>
    <author>
      <name>Sung Gwan Park</name>
    </author>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Kunsoo Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures, Submitted to CPM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.08974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.08977">
    <id>http://arxiv.org/abs/1905.08977v1</id>
    <updated>2019-05-22T06:39:48Z</updated>
    <published>2019-05-22T06:39:48Z</published>
    <title>A Memory-Efficient Sketch Method for Estimating High Similarities in
  Streaming Sets</title>
    <summary>  Estimating set similarity and detecting highly similar sets are fundamental
problems in areas such as databases, machine learning, and information
retrieval. MinHash is a well-known technique for approximating Jaccard
similarity of sets and has been successfully used for many applications such as
similarity search and large scale learning. Its two compressed versions, b-bit
MinHash and Odd Sketch, can significantly reduce the memory usage of the
original MinHash method, especially for estimating high similarities (i.e.,
similarities around 1). Although MinHash can be applied to static sets as well
as streaming sets, of which elements are given in a streaming fashion and
cardinality is unknown or even infinite, unfortunately, b-bit MinHash and Odd
Sketch fail to deal with streaming data. To solve this problem, we design a
memory efficient sketch method, MaxLogHash, to accurately estimate Jaccard
similarities in streaming sets. Compared to MinHash, our method uses smaller
sized registers (each register consists of less than 7 bits) to build a compact
sketch for each set. We also provide a simple yet accurate estimator for
inferring Jaccard similarity from MaxLogHash sketches. In addition, we derive
formulas for bounding the estimation error and determine the smallest necessary
memory usage (i.e., the number of registers used for a MaxLogHash sketch) for
the desired accuracy. We conduct experiments on a variety of datasets, and
experimental results show that our method MaxLogHash is about 5 times more
memory efficient than MinHash with the same accuracy and computational cost for
estimating high similarities.
</summary>
    <author>
      <name>Pinghui Wang</name>
    </author>
    <author>
      <name>Yiyan Qi</name>
    </author>
    <author>
      <name>Yuanming Zhang</name>
    </author>
    <author>
      <name>Qiaozhu Zhai</name>
    </author>
    <author>
      <name>Chenxu Wang</name>
    </author>
    <author>
      <name>John C. S. Lui</name>
    </author>
    <author>
      <name>Xiaohong Guan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3292500.3330825</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3292500.3330825" rel="related"/>
    <link href="http://arxiv.org/abs/1905.08977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.09656">
    <id>http://arxiv.org/abs/1905.09656v1</id>
    <updated>2019-05-23T13:50:56Z</updated>
    <published>2019-05-23T13:50:56Z</published>
    <title>On the Average Case of MergeInsertion</title>
    <summary>  MergeInsertion, also known as the Ford-Johnson algorithm, is a sorting
algorithm which, up to today, for many input sizes achieves the best known
upper bound on the number of comparisons. Indeed, it gets extremely close to
the information-theoretic lower bound. While the worst-case behavior is well
understood, only little is known about the average case.
  This work takes a closer look at the average case behavior. In particular, we
establish an upper bound of $n \log n - 1.4005n + o(n)$ comparisons. We also
give an exact description of the probability distribution of the length of the
chain a given element is inserted into and use it to approximate the average
number of comparisons numerically. Moreover, we compute the exact average
number of comparisons for $n$ up to 148.
  Furthermore, we experimentally explore the impact of different decision trees
for binary insertion. To conclude, we conduct experiments showing that a
slightly different insertion order leads to a better average case and we
compare the algorithm to the recent combination with (1,2)-Insertionsort by
Iwama and Teruyama.
</summary>
    <author>
      <name>Florian Stober</name>
    </author>
    <author>
      <name>Armin Weiß</name>
    </author>
    <link href="http://arxiv.org/abs/1905.09656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.09656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1108.0866">
    <id>http://arxiv.org/abs/1108.0866v1</id>
    <updated>2011-08-03T15:24:49Z</updated>
    <published>2011-08-03T15:24:49Z</published>
    <title>Towards Optimal Sorting of 16 Elements</title>
    <summary>  One of the fundamental problem in the theory of sorting is to find the
pessimistic number of comparisons sufficient to sort a given number of
elements. Currently 16 is the lowest number of elements for which we do not
know the exact value. We know that 46 comparisons suffices and that 44 do not.
There is an open question if 45 comparisons are sufficient. We present an
attempt to resolve that problem by performing an exhaustive computer search. We
also present an algorithm for counting linear extensions which substantially
speeds up computations.
</summary>
    <author>
      <name>Marcin Peczarski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures, 2 tables. First submitted to IWOCA 2010, 21st
  International Workshop on Combinatorial Algorithms. Submission was rejected</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Acta Universitatis Sapientiae, Informatica, 4(2) (2012) 215-224</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1108.0866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.0866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.12987">
    <id>http://arxiv.org/abs/1905.12987v2</id>
    <updated>2019-07-26T19:34:44Z</updated>
    <published>2019-05-30T12:01:00Z</published>
    <title>Inducing the Lyndon Array</title>
    <summary>  In this paper we propose a variant of the induced suffix sorting algorithm by
Nong (TOIS, 2013) that computes simultaneously the Lyndon array and the suffix
array of a text in $O(n)$ time using $\sigma + O(1)$ words of working space,
where $n$ is the length of the text and $\sigma$ is the alphabet size. Our
result improves the previous best space requirement for linear time computation
of the Lyndon array. In fact, all the known linear algorithms for Lyndon array
computation use suffix sorting as a preprocessing step and use $O(n)$ words of
working space in addition to the Lyndon array and suffix array. Experimental
results with real and synthetic datasets show that our algorithm is not only
space-efficient but also fast in practice.
</summary>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Sabrina Mantaci</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <author>
      <name>Guilherme P. Telles</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SPIRE'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.12987v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.12987v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.12854">
    <id>http://arxiv.org/abs/1905.12854v3</id>
    <updated>2019-09-15T05:28:41Z</updated>
    <published>2019-05-30T05:07:09Z</published>
    <title>Compact Data Structures for Shortest Unique Substring Queries</title>
    <summary>  Given a string T of length n, a substring u = T[i.. j] of T is called a
shortest unique substring (SUS) for an interval [s, t] if (a) u occurs exactly
once in T, (b) u contains the interval [s, t] (i.e. i \leq s \leq t \leq j),
and (c) every substring v of T with |v| &lt; |u| containing [s, t] occurs at least
twice in T. Given a query interval [s, t] \subset [1, n], the interval SUS
problem is to output all the SUSs for the interval [s, t]. In this article, we
propose a 4n + o(n) bits data structure answering an interval SUS query in
output-sensitive O(occ) time, where occ is the number of returned SUSs.
Additionally, we focus on the point SUS problem, which is the interval SUS
problem for s = t. Here, we propose a \lceil (log2 3 + 1)n \rceil + o(n) bits
data structure answering a point SUS query in the same output-sensitive time.
</summary>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1905.12854v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.12854v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.13064">
    <id>http://arxiv.org/abs/1905.13064v4</id>
    <updated>2019-06-09T19:51:19Z</updated>
    <published>2019-05-30T14:12:03Z</published>
    <title>The Bloom Clock</title>
    <summary>  The bloom clock is a space-efficient, probabilistic data structure designed
to determine the partial order of events in highly distributed systems. The
bloom clock, like the vector clock, can autonomously detect causality
violations by comparing its logical timestamps. Unlike the vector clock, the
space complexity of the bloom clock does not depend on the number of nodes in a
system. Instead it depends on a set of chosen parameters that determine its
confidence interval, i.e. false positive rate. To reduce the space complexity
from which the vector clock suffers, the bloom clock uses a 'moving window' in
which the partial order of events can be inferred with high confidence. If two
clocks are not comparable, the bloom clock can always deduce it, i.e. false
negatives are not possible. If two clocks are comparable, the bloom clock can
calculate the confidence of that statement, i.e. it can compute the false
positive rate between comparable pairs of clocks. By choosing an acceptable
threshold for the false positive rate, the bloom clock can properly compare the
order of its timestamps, with that of other nodes in a highly accurate and
space efficient way.
</summary>
    <author>
      <name>Lum Ramabaja</name>
    </author>
    <link href="http://arxiv.org/abs/1905.13064v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.13064v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.07224">
    <id>http://arxiv.org/abs/1905.07224v1</id>
    <updated>2019-05-17T12:24:17Z</updated>
    <published>2019-05-17T12:24:17Z</published>
    <title>Parallel decompression of gzip-compressed files and random access to DNA
  sequences</title>
    <summary>  Decompressing a file made by the gzip program at an arbitrary location is in
principle impossible, due to the nature of the DEFLATE compression algorithm.
Consequently, no existing program can take advantage of parallelism to rapidly
decompress large gzip-compressed files. This is an unsatisfactory bottleneck,
especially for the analysis of large sequencing data experiments. Here we
propose a parallel algorithm and an implementation, pugz, that performs fast
and exact decompression of any text file. We show that pugz is an order of
magnitude faster than gunzip, and 5x faster than a highly-optimized sequential
implementation (libdeflate). We also study the related problem of random access
to compressed data. We give simple models and experimental results that shed
light on the structure of gzip-compressed files containing DNA sequences.
Preliminary results show that random access to sequences within a
gzip-compressed FASTQ file is almost always feasible at low compression levels,
yet is approximate at higher compression levels.
</summary>
    <author>
      <name>Maël Kerbiriou</name>
    </author>
    <author>
      <name>Rayan Chikhi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HiCOMB'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.07224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.12135">
    <id>http://arxiv.org/abs/1904.12135v1</id>
    <updated>2019-04-27T09:31:32Z</updated>
    <published>2019-04-27T09:31:32Z</published>
    <title>About Fibonacci trees. I</title>
    <summary>  In this first paper, we look at the following question: are the properties of
the Fibonacci tree still true if we consider a finitely generated tree by the
same rules but rooted at a black node? The direct answer is no, but new
properties arise, a bit more complex than in the case of a tree rooted at a
white node, but still of interest.
</summary>
    <author>
      <name>Maurice Margenstern</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.12135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.12135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.00369">
    <id>http://arxiv.org/abs/1905.00369v3</id>
    <updated>2019-11-15T11:46:12Z</updated>
    <published>2019-05-01T16:33:11Z</published>
    <title>Fast hashing with Strong Concentration Bounds</title>
    <summary>  Previous work on tabulation hashing by P{\v a}tra{\c s}cu and Thorup from
STOC'11 on simple tabulation and from SODA'13 on twisted tabulation offered
Chernoff-style concentration bounds on hash based sums, e.g., the number of
balls/keys hashing to a given bin, but under some quite severe restrictions on
the expected values of these sums. The basic idea in tabulation hashing is to
view a key as consisting of $c=O(1)$ characters, e.g., a 64-bit key as $c=8$
characters of 8-bits. The character domain $\Sigma$ should be small enough that
character tables of size $|\Sigma|$ fit in fast cache. The schemes then use
$O(1)$ tables of this size, so the space of tabulation hashing is
$O(|\Sigma|)$. However, the concentration bounds by P{\v a}tra{\c s}cu and
Thorup only apply if the expected sums are $\ll |\Sigma|$.
  To see the problem, consider the very simple case where we use tabulation
hashing to throw $n$ balls into $m$ bins and want to analyse the number of
balls in a given bin. With their concentration bounds, we are fine if $n=m$,
for then the expected value is $1$. However, if $m=2$, as when tossing $n$
unbiased coins, the expected value $n/2$ is $\gg |\Sigma|$ for large data sets,
e.g., data sets that do not fit in fast cache.
  To handle expectations that go beyond the limits of our small space, we need
a much more advanced analysis of simple tabulation, plus a new tabulation
technique that we call \emph{tabulation-permutation} hashing which is at most
twice as slow as simple tabulation. No other hashing scheme of comparable speed
offers similar Chernoff-style concentration bounds.
</summary>
    <author>
      <name>Anders Aamand</name>
    </author>
    <author>
      <name>Jakob B. T. Knudsen</name>
    </author>
    <author>
      <name>Mathias B. T. Knudsen</name>
    </author>
    <author>
      <name>Peter M. R. Rasmussen</name>
    </author>
    <author>
      <name>Mikkel Thorup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.00369v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.00369v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.00163">
    <id>http://arxiv.org/abs/1905.00163v1</id>
    <updated>2019-05-01T02:22:14Z</updated>
    <published>2019-05-01T02:22:14Z</published>
    <title>Separate Chaining Meets Compact Hashing</title>
    <summary>  While separate chaining is a common strategy for resolving collisions in a
hash table taught in most textbooks, compact hashing is a less common technique
for saving space when hashing integers whose domain is relatively small with
respect to the problem size. It is widely believed that hash tables waste a
considerable amount of memory, as they either leave allocated space untouched
(open addressing) or store additional pointers (separate chaining). For the
former, Cleary introduced the compact hashing technique that stores only a part
of a key to save space. However, as can be seen by the line of research
focusing on compact hash tables with open addressing, there is additional
information, called displacement, required for restoring a key. There are
several representations of this displacement information with different space
and time trade-offs. In this article, we introduce a separate chaining hash
table that applies the compact hashing technique without the need for the
displacement information. Practical evaluations reveal that insertions in this
hash table are faster or use less space than all previously known compact hash
tables on modern computer architectures when storing sufficiently large
satellite data.
</summary>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of the local proceedings of the 173th SIGAL, 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.00163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.00163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.00118">
    <id>http://arxiv.org/abs/1905.00118v3</id>
    <updated>2020-02-26T02:21:40Z</updated>
    <published>2019-04-30T22:04:28Z</published>
    <title>Using Non-Linear Difference Equations to Study Quicksort Algorithms</title>
    <summary>  Using non-linear difference equations, combined with symbolic computations,
we make a detailed study of the running times of numerous variants of the
celebrated Quicksort algorithms, where we consider the variants of single-pivot
and multi-pivot Quicksort algorithms as discrete probability problems. With
non-linear difference equations, recurrence relations and experimental
mathematics techniques, explicit expressions for expectations, variances and
even higher moments of their numbers of comparisons and swaps can be obtained.
For some variants, Monte Carlo experiments are performed, the numerical results
are demonstrated and the scaled limiting distribution is also discussed.
</summary>
    <author>
      <name>Yukun Yao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.00118v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.00118v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.01254">
    <id>http://arxiv.org/abs/1905.01254v1</id>
    <updated>2019-05-03T16:19:31Z</updated>
    <published>2019-05-03T16:19:31Z</published>
    <title>RLE edit distance in near optimal time</title>
    <summary>  We show that the edit distance between two run-length encoded strings of
compressed lengths $m$ and $n$ respectively, can be computed in
$\mathcal{O}(mn\log(mn))$ time. This improves the previous record by a factor
of $\mathcal{O}(n/\log(mn))$. The running time of our algorithm is within
subpolynomial factors of being optimal, subject to the standard SETH-hardness
assumption. This effectively closes a line of algorithmic research first
started in 1993.
</summary>
    <author>
      <name>Raphaël Clifford</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Daniel P. Martin</name>
    </author>
    <author>
      <name>Przemysław Uznański</name>
    </author>
    <link href="http://arxiv.org/abs/1905.01254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.01340">
    <id>http://arxiv.org/abs/1905.01340v1</id>
    <updated>2019-05-03T18:40:09Z</updated>
    <published>2019-05-03T18:40:09Z</published>
    <title>Palindromic Ziv-Lempel and Crochemore Factorizations of $m$-Bonacci
  Infinite Words</title>
    <summary>  We introduce a variation of the Ziv-Lempel and Crochemore factorizations of
words by requiring each factor to be a palindrome. We compute these
factorizations for the Fibonacci word, and more generally, for all $m$-bonacci
words.
</summary>
    <author>
      <name>Marieh Jahannia</name>
    </author>
    <author>
      <name>Morteza Mohammad-noori</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <author>
      <name>Manon Stipulanti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.01340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.02589">
    <id>http://arxiv.org/abs/1905.02589v1</id>
    <updated>2019-05-07T14:01:52Z</updated>
    <published>2019-05-07T14:01:52Z</published>
    <title>Order-Preserving Pattern Matching Indeterminate Strings</title>
    <summary>  Given an indeterminate string pattern $p$ and an indeterminate string text
$t$, the problem of order-preserving pattern matching with character
uncertainties ($\mu$OPPM) is to find all substrings of $t$ that satisfy one of
the possible orderings defined by $p$. When the text and pattern are
determinate strings, we are in the presence of the well-studied exact
order-preserving pattern matching (OPPM) problem with diverse applications on
time series analysis. Despite its relevance, the exact OPPM problem suffers
from two major drawbacks: 1) the inability to deal with indetermination in the
text, thus preventing the analysis of noisy time series; and 2) the inability
to deal with indetermination in the pattern, thus imposing the strict
satisfaction of the orders among all pattern positions. This paper provides the
first polynomial algorithm to answer the $\mu$OPPM problem when indetermination
is observed on the pattern or text. Given two strings with length $m$ and
$O(r)$ uncertain characters per string position, we show that the $\mu$OPPM
problem can be solved in $O(mr\lg r)$ time when one string is indeterminate and
$r\in\mathbb{N}^+$. Mappings into satisfiability problems are provided when
indetermination is observed on both the pattern and the text, and results
concerning the general problem complexity are presented as well, with $\mu$OPPM
problem proved to be NP-hard in general.
</summary>
    <author>
      <name>Diogo Costa</name>
    </author>
    <author>
      <name>Luís M. S. Russo</name>
    </author>
    <author>
      <name>Rui Henriques</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Alexandre P. Francisco</name>
    </author>
    <link href="http://arxiv.org/abs/1905.02589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.02589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.02322">
    <id>http://arxiv.org/abs/1905.02322v1</id>
    <updated>2019-05-07T02:08:05Z</updated>
    <published>2019-05-07T02:08:05Z</published>
    <title>Orthogonal Range Reporting and Rectangle Stabbing for Fat Rectangles</title>
    <summary>  In this paper we study two geometric data structure problems in the special
case when input objects or queries are fat rectangles. We show that in this
case a significant improvement compared to the general case can be achieved.
  We describe data structures that answer two- and three-dimensional orthogonal
range reporting queries in the case when the query range is a \emph{fat}
rectangle. Our two-dimensional data structure uses $O(n)$ words and supports
queries in $O(\log\log U +k)$ time, where $n$ is the number of points in the
data structure, $U$ is the size of the universe and $k$ is the number of points
in the query range. Our three-dimensional data structure needs
$O(n\log^{\varepsilon}U)$ words of space and answers queries in $O(\log \log U
+ k)$ time. We also consider the rectangle stabbing problem on a set of
three-dimensional fat rectangles. Our data structure uses $O(n)$ space and
answers stabbing queries in $O(\log U\log\log U +k)$ time.
</summary>
    <author>
      <name>Timothy M. Chan</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Michiel Smid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of a WADS'19 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.02322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.02322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.02298">
    <id>http://arxiv.org/abs/1905.02298v1</id>
    <updated>2019-05-07T00:26:49Z</updated>
    <published>2019-05-07T00:26:49Z</published>
    <title>Even Faster Elastic-Degenerate String Matching via Fast Matrix
  Multiplication</title>
    <summary>  An elastic-degenerate (ED) string is a sequence of $n$ sets of strings of
total length $N$, which was recently proposed to model a set of similar
sequences. The ED string matching (EDSM) problem is to find all occurrences of
a pattern of length $m$ in an ED text. The EDSM problem has recently received
some attention in the combinatorial pattern matching community, and an
$\mathcal{O}(nm^{1.5}\sqrt{\log m} + N)$-time algorithm is known [Aoyama et
al., CPM 2018]. The standard assumption in the prior work on this question is
that $N$ is substantially larger than both $n$ and $m$, and thus we would like
to have a linear dependency on the former. Under this assumption, the natural
open problem is whether we can decrease the 1.5 exponent in the time
complexity, similarly as in the related (but, to the best of our knowledge, not
equivalent) word break problem [Backurs and Indyk, FOCS 2016].
  Our starting point is a conditional lower bound for the EDSM problem. We use
the popular combinatorial Boolean matrix multiplication (BMM) conjecture
stating that there is no truly subcubic combinatorial algorithm for BMM [Abboud
and Williams, FOCS 2014]. By designing an appropriate reduction we show that a
combinatorial algorithm solving the EDSM problem in
$\mathcal{O}(nm^{1.5-\epsilon} + N)$ time, for any $\epsilon>0$, refutes this
conjecture. Of course, the notion of combinatorial algorithms is not clearly
defined, so our reduction should be understood as an indication that decreasing
the exponent requires fast matrix multiplication.
  Two standard tools used in algorithms on strings are string periodicity and
fast Fourier transform. Our main technical contribution is that we successfully
combine these tools with fast matrix multiplication to design a
non-combinatorial $\mathcal{O}(nm^{1.381} + N)$-time algorithm for EDSM. To the
best of our knowledge, we are the first to do so.
</summary>
    <author>
      <name>Giulia Bernardini</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Nadia Pisanti</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of paper in ICALP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.02298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.02298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.06138">
    <id>http://arxiv.org/abs/1905.06138v2</id>
    <updated>2020-01-02T12:16:32Z</updated>
    <published>2019-05-15T12:33:32Z</published>
    <title>Abelian periods of factors of Sturmian words</title>
    <summary>  We study the abelian period sets of Sturmian words, which are codings of
irrational rotations on a one-dimensional torus. The main result states that
the minimum abelian period of a factor of a Sturmian word of angle $\alpha$
with continued fraction expansion $[0; a_1, a_2, \ldots]$ is either $tq_k$ with
$1 \leq t \leq a_{k+1}$ (a multiple of a denominator $q_k$ of a convergent of
$\alpha$) or $q_{k,\ell}$ (a denominator $q_{k,\ell}$ of a semiconvergent of
$\alpha$). This result generalizes a result of Fici et. al stating that the
abelian period set of the Fibonacci word is the set of Fibonacci numbers. A
characterization of the Fibonacci word in terms of its abelian period set is
obtained as a corollary.
</summary>
    <author>
      <name>Jarkko Peltomäki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.06138v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06138v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15, 11A55" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.05459">
    <id>http://arxiv.org/abs/1904.05459v2</id>
    <updated>2019-05-09T08:42:20Z</updated>
    <published>2019-04-10T21:52:07Z</published>
    <title>Constant factor approximations to edit distance on far input pairs in
  nearly linear time</title>
    <summary>  For any $T \geq 1$, there are constants $R=R(T) \geq 1$ and
$\zeta=\zeta(T)>0$ and a randomized algorithm that takes as input an integer
$n$ and two strings $x,y$ of length at most $n$, and runs in time
$O(n^{1+\frac{1}{T}})$ and outputs an upper bound $U$ on the edit distance
$ED(x,y)$ that with high probability, satisfies $U \leq
R(ED(x,y)+n^{1-\zeta})$. In particular, on any input with $ED(x,y) \geq
n^{1-\zeta}$ the algorithm outputs a constant factor approximation with high
probability.
  A similar result has been proven independently by Brakensiek and Rubinstein
(2019).
</summary>
    <author>
      <name>Michal Koucký</name>
    </author>
    <author>
      <name>Michael E. Saks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected typos. Revised argument in Section 4.9, results unchanged</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05459v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05459v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.05390">
    <id>http://arxiv.org/abs/1904.05390v2</id>
    <updated>2020-01-28T18:54:41Z</updated>
    <published>2019-04-10T18:55:56Z</published>
    <title>Constant-factor approximation of near-linear edit distance in
  near-linear time</title>
    <summary>  We show that the edit distance between two strings of length $n$ can be
computed within a factor of $f(\epsilon)$ in $n^{1+\epsilon}$ time as long as
the edit distance is at least $n^{1-\delta}$ for some $\delta(\epsilon) > 0$.
</summary>
    <author>
      <name>Joshua Brakensiek</name>
    </author>
    <author>
      <name>Aviad Rubinstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05390v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05390v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.05452">
    <id>http://arxiv.org/abs/1904.05452v1</id>
    <updated>2019-04-10T21:41:35Z</updated>
    <published>2019-04-10T21:41:35Z</published>
    <title>What Storage Access Privacy is Achievable with Small Overhead?</title>
    <summary>  Oblivious RAM (ORAM) and private information retrieval (PIR) are classic
cryptographic primitives used to hide the access pattern to data whose storage
has been outsourced to an untrusted server. Unfortunately, both primitives
require considerable overhead compared to plaintext access. For large-scale
storage infrastructure with highly frequent access requests, the degradation in
response time and the exorbitant increase in resource costs incurred by either
ORAM or PIR prevent their usage. In an ideal scenario, a privacy-preserving
storage protocols with small overhead would be implemented for these heavily
trafficked storage systems to avoid negatively impacting either performance
and/or costs. In this work, we study the problem of the best $\mathit{storage\
access\ privacy}$ that is achievable with only $\mathit{small\ overhead}$ over
plaintext access.
  To answer this question, we consider $\mathit{differential\ privacy\ access}$
which is a generalization of the $\mathit{oblivious\ access}$ security notion
that are considered by ORAM and PIR. Quite surprisingly, we present strong
evidence that constant overhead storage schemes may only be achieved with
privacy budgets of $\epsilon = \Omega(\log n)$. We present asymptotically
optimal constructions for differentially private variants of both ORAM and PIR
with privacy budgets $\epsilon = \Theta(\log n)$ with only $O(1)$ overhead. In
addition, we consider a more complex storage primitive called key-value storage
in which data is indexed by keys from a large universe (as opposed to
consecutive integers in ORAM and PIR). We present a differentially private
key-value storage scheme with $\epsilon = \Theta(\log n)$ and $O(\log\log n)$
overhead. This construction uses a new oblivious, two-choice hashing scheme
that may be of independent interest.
</summary>
    <author>
      <name>Sarvar Patel</name>
    </author>
    <author>
      <name>Giuseppe Persiano</name>
    </author>
    <author>
      <name>Kevin Yeo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at PODS'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.07467">
    <id>http://arxiv.org/abs/1904.07467v2</id>
    <updated>2019-11-16T07:37:22Z</updated>
    <published>2019-04-16T05:20:47Z</published>
    <title>Dynamic Packed Compact Tries Revisited</title>
    <summary>  Given a dynamic set $K$ of $k$ strings of total length $n$ whose characters
are drawn from an alphabet of size $\sigma$, a keyword dictionary is a data
structure built on $K$ that provides lookup, prefix search, and update
operations on $K$. Under the assumption that $\alpha = w/ \lg \sigma$
characters fit into a single machine word of $w$ bits, we propose a keyword
dictionary that represents $K$ in either $n \lg \sigma + \Theta(k \lg n)$ or
$|T| \lg \sigma + \Theta(k w)$ bits of space, where $|T|$ is the number of
nodes of a trie representing $K$. It supports all operations in $O(m / \alpha +
\lg \alpha)$ expected time on an input string of length $m$ in the word RAM
model. An exhaustive practical evaluation highlights the practical usefulness
of the proposed data structure, especially for prefix searches - one of the
most essential keyword dictionary operations.
</summary>
    <author>
      <name>Kazuya Tsuruta</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Shunsuke Kanda</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1904.07467v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07467v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.07619">
    <id>http://arxiv.org/abs/1904.07619v3</id>
    <updated>2020-02-27T09:28:13Z</updated>
    <published>2019-04-16T12:27:24Z</published>
    <title>Compressed Indexes for Fast Search of Semantic Data</title>
    <summary>  The sheer increase in volume of RDF data demands efficient solutions for the
triple indexing problem, that is devising a compressed data structure to
compactly represent RDF triples by guaranteeing, at the same time, fast pattern
matching operations. This problem lies at the heart of delivering good
practical performance for the resolution of complex SPARQL queries on large RDF
datasets. In this work, we propose a trie-based index layout to solve the
problem and introduce two novel techniques to reduce its space of
representation for improved effectiveness. The extensive experimental analysis
conducted over a wide range of publicly available real-world datasets, reveals
that our best space/time trade-off configuration substantially outperforms
existing solutions at the state-of-the-art, by taking 30-60% less space and
speeding up query execution by a factor of 2-81x.
</summary>
    <author>
      <name>Raffaele Perego</name>
    </author>
    <author>
      <name>Giulio Ermanno Pibiri</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TKDE.2020.2966609</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TKDE.2020.2966609" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in IEEE Transactions on Knowledge and Data Engineering
  (TKDE), 14 January 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.07619v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07619v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.10028">
    <id>http://arxiv.org/abs/1904.10028v1</id>
    <updated>2019-04-22T18:52:01Z</updated>
    <published>2019-04-22T18:52:01Z</published>
    <title>Repetitions in infinite palindrome-rich words</title>
    <summary>  Rich words are characterized by containing the maximum possible number of
distinct palindromes. Several characteristic properties of rich words have been
studied; yet the analysis of repetitions in rich words still involves some
interesting open problems. We address lower bounds on the repetition threshold
of infinite rich words over 2 and 3-letter alphabets, and construct a candidate
infinite rich word over the alphabet $\Sigma_2=\{0,1\}$ with a small critical
exponent of $2+\sqrt{2}/2$. This represents the first progress on an open
problem of Vesti from 2017.
</summary>
    <author>
      <name>Aseem Raj Baranwal</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-28796-2_7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-28796-2_7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.10028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.10028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.09157">
    <id>http://arxiv.org/abs/1904.09157v1</id>
    <updated>2019-04-19T11:53:45Z</updated>
    <published>2019-04-19T11:53:45Z</published>
    <title>New results on pseudosquare avoidance</title>
    <summary>  We start by considering binary words containing the minimum possible numbers
of squares and antisquares (where an antisquare is a word of the form $x
\overline{x}$), and we completely classify which possibilities can occur. We
consider avoiding $x p(x)$, where $p$ is any permutation of the underlying
alphabet, and $x t(x)$, where $t$ is any transformation of the underlying
alphabet. Finally, we prove the existence of an infinite binary word
simultaneously avoiding all occurrences of $x h(x)$ for every nonerasing
morphism $h$ and all sufficiently large words $x$.
</summary>
    <author>
      <name>Tim Ng</name>
    </author>
    <author>
      <name>Pascal Ochem</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/1904.09157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.09125">
    <id>http://arxiv.org/abs/1904.09125v2</id>
    <updated>2019-05-24T08:57:45Z</updated>
    <published>2019-04-19T09:09:49Z</published>
    <title>k-Spectra of weakly-c-Balanced Words</title>
    <summary>  A word $u$ is a scattered factor of $w$ if $u$ can be obtained from $w$ by
deleting some of its letters. That is, there exist the (potentially empty)
words $u_1,u_2,..., u_n$, and $v_0,v_1,..,v_n$ such that $u = u_1u_2...u_n$ and
$w = v_0u_1v_1u_2v_2...u_nv_n$. We consider the set of length-$k$ scattered
factors of a given word w, called here $k$-spectrum and denoted
$\ScatFact_k(w)$. We prove a series of properties of the sets $\ScatFact_k(w)$
for binary strictly balanced and, respectively, $c$-balanced words $w$, i.e.,
words over a two-letter alphabet where the number of occurrences of each letter
is the same, or, respectively, one letter has $c$-more occurrences than the
other. In particular, we consider the question which cardinalities $n=
|\ScatFact_k(w)|$ are obtainable, for a positive integer $k$, when $w$ is
either a strictly balanced binary word of length $2k$, or a $c$-balanced binary
word of length $2k-c$. We also consider the problem of reconstructing words
from their $k$-spectra.
</summary>
    <author>
      <name>Joel D. Day</name>
    </author>
    <author>
      <name>Pamela Fleischmann</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Dirk Nowotka</name>
    </author>
    <link href="http://arxiv.org/abs/1904.09125v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09125v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.13369">
    <id>http://arxiv.org/abs/1904.13369v2</id>
    <updated>2019-06-22T14:34:56Z</updated>
    <published>2019-04-30T17:04:08Z</published>
    <title>Constrained Orthogonal Segment Stabbing</title>
    <summary>  Let $S$ and $D$ each be a set of orthogonal line segments in the plane. A
line segment $s\in S$ \emph{stabs} a line segment $s'\in D$ if $s\cap
s'\neq\emptyset$. It is known that the problem of stabbing the line segments in
$D$ with the minimum number of line segments of $S$ is NP-hard. However, no
better than $O(\log |S\cup D|)$-approximation is known for the problem. In this
paper, we introduce a constrained version of this problem in which every
horizontal line segment of $S\cup D$ intersects a common vertical line. We
study several versions of the problem, depending on which line segments are
used for stabbing and which line segments must be stabbed. We obtain several
NP-hardness and constant approximation results for these versions. Our finding
implies, the problem remains NP-hard even under the extra assumption on input,
but small constant approximation algorithms can be designed.
</summary>
    <author>
      <name>Sayan Bandyapadhyay</name>
    </author>
    <author>
      <name>Saeed Mehrabi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear at CCCG 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.13369v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.13369v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1606.06617">
    <id>http://arxiv.org/abs/1606.06617v4</id>
    <updated>2017-10-10T08:23:45Z</updated>
    <published>2016-06-21T15:26:55Z</published>
    <title>A Self-Index on Block Trees</title>
    <summary>  The Block Tree is a recently proposed data structure that reaches compression
close to Lempel-Ziv while supporting efficient direct access to text
substrings. In this paper we show how a self-index can be built on top of a
Block Tree so that it provides efficient pattern searches while using space
proportional to that of the original data structure. More precisely, if a
Lempel-Ziv parse cuts a text of length $n$ into $z$ non-overlapping phrases,
then our index uses $O(z\log(n/z))$ words and finds the $occ$ occurrences of a
pattern of length $m$ in time $O(m\log n+occ\log^\epsilon n)$ for any constant
$\epsilon>0$.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 3 is the final SPIRE 2017 version. Version 4 corrects some
  errors and typos, the important one about what is inserted in the grid. It
  also improves the time when O(w) space can be used</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06617v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06617v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.12312">
    <id>http://arxiv.org/abs/1903.12312v1</id>
    <updated>2019-03-29T01:09:50Z</updated>
    <published>2019-03-29T01:09:50Z</published>
    <title>Data structures to represent sets of k-long DNA sequences</title>
    <summary>  The analysis of biological sequencing data has been one of the biggest
applications of string algorithms. The approaches used in many such
applications are based on the analysis of k-mers, which are short fixed-length
strings present in a dataset. While these approaches are rather diverse,
storing and querying k-mer sets has emerged as a shared underlying component.
Sets of k-mers have unique features and applications that, over the last ten
years, have resulted in many specialized approaches for their representation.
In this survey, we give a unified presentation and comparison of the data
structures that have been proposed to store and query k-mer sets. We hope this
survey will not only serve as a resource for researchers in the field but also
make the area more accessible to outsiders
</summary>
    <author>
      <name>Rayan Chikhi</name>
    </author>
    <author>
      <name>Jan Holub</name>
    </author>
    <author>
      <name>Paul Medvedev</name>
    </author>
    <link href="http://arxiv.org/abs/1903.12312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.12312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.12449">
    <id>http://arxiv.org/abs/1903.12449v1</id>
    <updated>2019-03-29T11:05:51Z</updated>
    <published>2019-03-29T11:05:51Z</published>
    <title>Multiplication method for factoring natural numbers</title>
    <summary>  We offer multiplication method for factoring big natural numbers which
extends the group of the Fermat's and Lehman's factorization algorithms and has
run-time complexity $O(n^{1/3})$. This paper is argued the finiteness of
proposed algorithm depending on the value of the factorizable number n. We
provide here comparative tests results of related algorithms on a large amount
of computational checks. We describe identified advantages of the proposed
algorithm over others. The possibilities of algorithm optimization for reducing
the complexity of factorization are also shown here.
</summary>
    <author>
      <name>Igor Nesiolovskiy</name>
    </author>
    <author>
      <name>Artem Nesiolovskiy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.12449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.12449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.12525">
    <id>http://arxiv.org/abs/1903.12525v1</id>
    <updated>2019-03-17T20:19:23Z</updated>
    <published>2019-03-17T20:19:23Z</published>
    <title>Shed More Light on Bloom Filter's Variants</title>
    <summary>  Bloom Filter is a probabilistic membership data structure and it is
excessively used data structure for membership query. Bloom Filter becomes the
predominant data structure in approximate membership filtering. Bloom Filter
extremely enhances the query response time, and the response time is very fast.
Bloom filter (BF) is used to detect whether an element belongs to a given set
or not. The Bloom Filter returns True Positive (TP), False Positive (FP), or
True Negative (TN). The Bloom Filter is widely adapted in numerous areas to
enhance the performance of a system. In this paper, we present a) in-depth
insight on the Bloom Filter,and b) the prominent variants of the Bloom Filters.
</summary>
    <author>
      <name>Ripon Patgiri</name>
    </author>
    <author>
      <name>Sabuzima Nayak</name>
    </author>
    <author>
      <name>Samir Kumar Borgohain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 Figures, 1 Table, Proceedings of the 2018 International
  Conference on Information and Knowledge Engineering (IKE'18), pp. 14-21</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2018 International Conference on Information
  and Knowledge Engineering (IKE'18), pp. 14-21, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.12525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.12525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1202.3311">
    <id>http://arxiv.org/abs/1202.3311v1</id>
    <updated>2012-02-15T14:13:02Z</updated>
    <published>2012-02-15T14:13:02Z</published>
    <title>Speeding-up $q$-gram mining on grammar-based compressed texts</title>
    <summary>  We present an efficient algorithm for calculating $q$-gram frequencies on
strings represented in compressed form, namely, as a straight line program
(SLP). Given an SLP $\mathcal{T}$ of size $n$ that represents string $T$, the
algorithm computes the occurrence frequencies of all $q$-grams in $T$, by
reducing the problem to the weighted $q$-gram frequencies problem on a
trie-like structure of size $m = |T|-\mathit{dup}(q,\mathcal{T})$, where
$\mathit{dup}(q,\mathcal{T})$ is a quantity that represents the amount of
redundancy that the SLP captures with respect to $q$-grams. The reduced problem
can be solved in linear time. Since $m = O(qn)$, the running time of our
algorithm is $O(\min\{|T|-\mathit{dup}(q,\mathcal{T}),qn\})$, improving our
previous $O(qn)$ algorithm when $q = \Omega(|T|/n)$.
</summary>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-31265-6_18</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-31265-6_18" rel="related"/>
    <link href="http://arxiv.org/abs/1202.3311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1304.5373">
    <id>http://arxiv.org/abs/1304.5373v2</id>
    <updated>2014-06-06T13:40:56Z</updated>
    <published>2013-04-19T11:11:36Z</published>
    <title>Compact q-gram Profiling of Compressed Strings</title>
    <summary>  We consider the problem of computing the q-gram profile of a string \str of
size $N$ compressed by a context-free grammar with $n$ production rules. We
present an algorithm that runs in $O(N-\alpha)$ expected time and uses
$O(n+q+\kq)$ space, where $N-\alpha\leq qn$ is the exact number of characters
decompressed by the algorithm and $\kq\leq N-\alpha$ is the number of distinct
q-grams in $\str$. This simultaneously matches the current best known time
bound and improves the best known space bound. Our space bound is
asymptotically optimal in the sense that any algorithm storing the grammar and
the q-gram profile must use $\Omega(n+q+\kq)$ space. To achieve this we
introduce the q-gram graph that space-efficiently captures the structure of a
string with respect to its q-grams, and show how to construct it from a
grammar.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Patrick Hagge Cording</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <link href="http://arxiv.org/abs/1304.5373v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.5373v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.02809">
    <id>http://arxiv.org/abs/1904.02809v2</id>
    <updated>2019-07-02T09:49:48Z</updated>
    <published>2019-04-04T22:20:12Z</published>
    <title>Proving tree algorithms for succinct data structures</title>
    <summary>  Succinct data structures give space-efficient representations of large
amounts of data without sacrificing performance. They rely one cleverly
designed data representations and algorithms. We present here the formalization
in Coq/SSReflect of two different tree-based succinct representations and their
accompanying algorithms. One is the Level-Order Unary Degree Sequence, which
encodes the structure of a tree in breadth-first order as a sequence of bits,
where access operations can be defined in terms of Rank and Select, which work
in constant time for static bit sequences. The other represents dynamic bit
sequences as binary balanced trees, where Rank and Select present a low
logarithmic overhead compared to their static versions, and with efficient
insertion and deletion. The two can be stacked to provide a dynamic
representation of dictionaries for instance. While both representations are
well-known, we believe this to be their first formalization and a needed step
towards provably-safe implementations of big data.
</summary>
    <author>
      <name>Reynald Affeldt</name>
    </author>
    <author>
      <name>Jacques Garrigue</name>
    </author>
    <author>
      <name>Xuanrui Qi</name>
    </author>
    <author>
      <name>Kazunari Tanaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 10th International Conference on Interactive Theorem
  Proving (ITP 2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.02809v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.02809v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.3.1; E.1; D.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.08162">
    <id>http://arxiv.org/abs/1811.08162v1</id>
    <updated>2018-11-20T10:12:55Z</updated>
    <published>2018-11-20T10:12:55Z</published>
    <title>DeepZip: Lossless Data Compression using Recurrent Neural Networks</title>
    <summary>  Sequential data is being generated at an unprecedented pace in various forms,
including text and genomic data. This creates the need for efficient
compression mechanisms to enable better storage, transmission and processing of
such data. To solve this problem, many of the existing compressors attempt to
learn models for the data and perform prediction-based compression. Since
neural networks are known as universal function approximators with the
capability to learn arbitrarily complex mappings, and in practice show
excellent performance in prediction tasks, we explore and devise methods to
compress sequential data using neural network predictors. We combine recurrent
neural network predictors with an arithmetic coder and losslessly compress a
variety of synthetic, text and genomic datasets. The proposed compressor
outperforms Gzip on the real datasets and achieves near-optimal compression for
the synthetic datasets. The results also help understand why and where neural
networks are good alternatives for traditional finite context models
</summary>
    <author>
      <name>Mohit Goyal</name>
    </author>
    <author>
      <name>Kedar Tatwawadi</name>
    </author>
    <author>
      <name>Shubham Chandak</name>
    </author>
    <author>
      <name>Idoia Ochoa</name>
    </author>
    <link href="http://arxiv.org/abs/1811.08162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.08162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.04228">
    <id>http://arxiv.org/abs/1904.04228v2</id>
    <updated>2019-05-05T21:52:51Z</updated>
    <published>2019-04-08T17:56:21Z</published>
    <title>String Synchronizing Sets: Sublinear-Time BWT Construction and Optimal
  LCE Data Structure</title>
    <summary>  Burrows-Wheeler transform (BWT) is an invertible text transformation that,
given a text $T$ of length $n$, permutes its symbols according to the
lexicographic order of suffixes of $T$. BWT is one of the most heavily studied
algorithms in data compression with numerous applications in indexing, sequence
analysis, and bioinformatics. Its construction is a bottleneck in many
scenarios, and settling the complexity of this task is one of the most
important unsolved problems in sequence analysis that has remained open for 25
years. Given a binary string of length $n$, occupying $O(n/\log n)$ machine
words, the BWT construction algorithm due to Hon et al. (SIAM J. Comput., 2009)
runs in $O(n)$ time and $O(n/\log n)$ space. Recent advancements (Belazzougui,
STOC 2014, and Munro et al., SODA 2017) focus on removing the alphabet-size
dependency in the time complexity, but they still require $\Omega(n)$ time.
  In this paper, we propose the first algorithm that breaks the $O(n)$-time
barrier for BWT construction. Given a binary string of length $n$, our
procedure builds the Burrows-Wheeler transform in $O(n/\sqrt{\log n})$ time and
$O(n/\log n)$ space. We complement this result with a conditional lower bound
proving that any further progress in the time complexity of BWT construction
would yield faster algorithms for the very well studied problem of counting
inversions: it would improve the state-of-the-art $O(m\sqrt{\log m})$-time
solution by Chan and P\v{a}tra\c{s}cu (SODA 2010). Our algorithm is based on a
novel concept of string synchronizing sets, which is of independent interest.
As one of the applications, we show that this technique lets us design a data
structure of the optimal size $O(n/\log n)$ that answers Longest Common
Extension queries (LCE queries) in $O(1)$ time and, furthermore, can be
deterministically constructed in the optimal $O(n/\log n)$ time.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper accepted to STOC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.04228v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.04228v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.04513">
    <id>http://arxiv.org/abs/1904.04513v3</id>
    <updated>2019-07-02T10:26:27Z</updated>
    <published>2019-04-09T08:05:42Z</published>
    <title>Suffix Trees, DAWGs and CDAWGs for Forward and Backward Tries</title>
    <summary>  The suffix tree, DAWG, and CDAWG are fundamental indexing structures of a
string, with a number of applications in bioinformatics, information retrieval,
data mining, etc. An edge-labeled rooted tree (trie) is a natural
generalization of a string, which can also be seen as a compact representation
of a set of strings. Breslauer [TCS 191(1-2): 131-144, 1998] proposed the
suffix tree for a backward trie, where the strings in the trie are read in the
leaf-to-root direction. In contrast to a backward trie, we call a usual trie as
a forward trie. Despite a few follow-up works after Breslauer's paper, indexing
forward/backward tries is not well understood yet. In this paper, we show a
full perspective on the sizes of indexing structures such as suffix trees,
DAWGs, and CDAWGs for forward and backward tries. In particular, we show that
the size of the DAWG for a forward trie with $n$ nodes is $\Omega(\sigma n)$,
where $\sigma$ is the number of distinct characters in the trie. This becomes
$\Omega(n^2)$ for a large alphabet. Still, we show that there is a compact
$O(n)$-space representation of the DAWG for a forward trie over any alphabet,
and present an $O(n)$-time and space algorithm to construct such a
representation of the DAWG for a given forward trie.
</summary>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <link href="http://arxiv.org/abs/1904.04513v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.04513v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1001.1565">
    <id>http://arxiv.org/abs/1001.1565v3</id>
    <updated>2013-10-29T08:44:10Z</updated>
    <published>2010-01-11T20:29:41Z</published>
    <title>Random Access to Grammar Compressed Strings</title>
    <summary>  Grammar based compression, where one replaces a long string by a small
context-free grammar that generates the string, is a simple and powerful
paradigm that captures many popular compression schemes. In this paper, we
present a novel grammar representation that allows efficient random access to
any character or substring without decompressing the string.
  Let $S$ be a string of length $N$ compressed into a context-free grammar
$\mathcal{S}$ of size $n$. We present two representations of $\mathcal{S}$
achieving $O(\log N)$ random access time, and either $O(n\cdot \alpha_k(n))$
construction time and space on the pointer machine model, or $O(n)$
construction time and space on the RAM. Here, $\alpha_k(n)$ is the inverse of
the $k^{th}$ row of Ackermann's function. Our representations also efficiently
support decompression of any substring in $S$: we can decompress any substring
of length $m$ in the same complexity as a single random access query and
additional $O(m)$ time. Combining these results with fast algorithms for
uncompressed approximate string matching leads to several efficient algorithms
for approximate string matching on grammar-compressed strings without
decompression. For instance, we can find all approximate occurrences of a
pattern $P$ with at most $k$ errors in time $O(n(\min\{|P|k, k^4 + |P|\} + \log
N) + occ)$, where $occ$ is the number of occurrences of $P$ in $S$. Finally, we
generalize our results to navigation and other operations on grammar-compressed
ordered trees.
  All of the above bounds significantly improve the currently best known
results. To achieve these bounds, we introduce several new techniques and data
structures of independent interest, including a predecessor data structure, two
"biased" weighted ancestor data structures, and a compact representation of
heavy paths in grammars.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version in SODA 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.1565v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.1565v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1203.1080">
    <id>http://arxiv.org/abs/1203.1080v2</id>
    <updated>2012-05-03T15:45:08Z</updated>
    <published>2012-03-06T00:52:14Z</published>
    <title>Data Structure Lower Bounds on Random Access to Grammar-Compressed
  Strings</title>
    <summary>  In this paper we investigate the problem of building a static data structure
that represents a string s using space close to its compressed size, and allows
fast access to individual characters of s. This type of structures was
investigated by the recent paper of Bille et al. Let n be the size of a
context-free grammar that derives a unique string s of length L. (Note that L
might be exponential in n.) Bille et al. showed a data structure that uses
space O(n) and allows to query for the i-th character of s using running time
O(log L). Their data structure works on a word RAM with a word size of logL
bits. Here we prove that for such data structures, if the space is poly(n),
then the query time must be at least (log L)^{1-\epsilon}/log S where S is the
space used, for any constant eps>0. As a function of n, our lower bound is
\Omega(n^{1/2-\epsilon}). Our proof holds in the cell-probe model with a word
size of log L bits, so in particular it holds in the word RAM model. We show
that no lower bound significantly better than n^{1/2-\epsilon} can be achieved
in the cell-probe model, since there is a data structure in the cell-probe
model that uses O(n) space and achieves O(\sqrt{n log n}) query time. The "bad"
setting of parameters occurs roughly when L=2^{\sqrt{n}}. We also prove a lower
bound for the case of not-as-compressible strings, where, say,
L=n^{1+\epsilon}. For this case, we prove that if the space is n polylog(n),
then the query time must be at least \Omega(log n/loglog n).
  The proof works by reduction to communication complexity, namely to the LSD
problem, recently employed by Patrascu and others. We prove lower bounds also
for the case of LZ-compression and Burrows-Wheeler (BWT) compression. All of
our lower bounds hold even when the strings are over an alphabet of size 2 and
hold even for randomized data structures with 2-sided error.
</summary>
    <author>
      <name>Shiteng Chen</name>
    </author>
    <author>
      <name>Elad Verbin</name>
    </author>
    <author>
      <name>Wei Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to ICALP 2012, with strengthened results included</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.1080v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.1080v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1207.4607">
    <id>http://arxiv.org/abs/1207.4607v1</id>
    <updated>2012-07-19T10:28:56Z</updated>
    <published>2012-07-19T10:28:56Z</published>
    <title>Efficient LZ78 factorization of grammar compressed text</title>
    <summary>  We present an efficient algorithm for computing the LZ78 factorization of a
text, where the text is represented as a straight line program (SLP), which is
a context free grammar in the Chomsky normal form that generates a single
string. Given an SLP of size $n$ representing a text $S$ of length $N$, our
algorithm computes the LZ78 factorization of $T$ in $O(n\sqrt{N}+m\log N)$ time
and $O(n\sqrt{N}+m)$ space, where $m$ is the number of resulting LZ78 factors.
We also show how to improve the algorithm so that the $n\sqrt{N}$ term in the
time and space complexities becomes either $nL$, where $L$ is the length of the
longest LZ78 factor, or $(N - \alpha)$ where $\alpha \geq 0$ is a quantity
which depends on the amount of redundancy that the SLP captures with respect to
substrings of $S$ of a certain length. Since $m = O(N/\log_\sigma N)$ where
$\sigma$ is the alphabet size, the latter is asymptotically at least as fast as
a linear time algorithm which runs on the uncompressed string when $\sigma$ is
constant, and can be more efficient when the text is compressible, i.e. when
$m$ and $n$ are small.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-34109-0_10</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-34109-0_10" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SPIRE 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.4607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.03561">
    <id>http://arxiv.org/abs/1808.03561v1</id>
    <updated>2018-08-10T14:26:29Z</updated>
    <published>2018-08-10T14:26:29Z</published>
    <title>Finding a Small Number of Colourful Components</title>
    <summary>  A partition $(V_1,\ldots,V_k)$ of the vertex set of a graph $G$ with a (not
necessarily proper) colouring $c$ is colourful if no two vertices in any $V_i$
have the same colour and every set $V_i$ induces a connected graph. The
COLOURFUL PARTITION problem is to decide whether a coloured graph $(G,c)$ has a
colourful partition of size at most $k$. This problem is closely related to the
COLOURFUL COMPONENTS problem, which is to decide whether a graph can be
modified into a graph whose connected components form a colourful partition by
deleting at most $p$ edges. Nevertheless we show that COLOURFUL PARTITION and
COLOURFUL COMPONENTS may have different complexities for restricted instances.
We tighten known NP-hardness results for both problems and in addition we prove
new hardness and tractability results for COLOURFUL PARTITION. Using these
results we complete our paper with a thorough parameterized study of COLOURFUL
PARTITION.
</summary>
    <author>
      <name>Laurent Bulteau</name>
    </author>
    <author>
      <name>Konrad K. Dabrowski</name>
    </author>
    <author>
      <name>Guillaume Fertin</name>
    </author>
    <author>
      <name>Matthew Johnson</name>
    </author>
    <author>
      <name>Daniel Paulusma</name>
    </author>
    <author>
      <name>Stephane Vialette</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0910.3123">
    <id>http://arxiv.org/abs/0910.3123v2</id>
    <updated>2010-02-19T09:19:42Z</updated>
    <published>2009-10-16T13:50:12Z</published>
    <title>Wee LCP</title>
    <summary>  We prove that longest common prefix (LCP) information can be stored in much
less space than previously known. More precisely, we show that in the presence
of the text and the suffix array, o(n) additional bits are sufficient to answer
LCP-queries asymptotically in the same time that is needed to retrieve an entry
from the suffix array. This yields the smallest compressed suffix tree with
sub-logarithmic navigation time.
</summary>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <link href="http://arxiv.org/abs/0910.3123v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.3123v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.10081">
    <id>http://arxiv.org/abs/1903.10081v8</id>
    <updated>2019-10-05T16:47:14Z</updated>
    <published>2019-03-24T23:38:27Z</published>
    <title>Determining satisfiability of 3-SAT in polynomial time</title>
    <summary>  In this paper, we provide a polynomial time (and space), algorithm that
determines satisfiability of 3-SAT. The complexity analysis for the algorithm
takes into account no efficiency and yet provides a low enough bound, that
efficient versions are practical with respect to today's hardware. We accompany
this paper with a serial version of the algorithm without non-trivial
efficiencies (link: polynomial3sat.org).
</summary>
    <author>
      <name>Ortho Flint</name>
    </author>
    <author>
      <name>Asanka Wickramasinghe</name>
    </author>
    <author>
      <name>Jason Brasse</name>
    </author>
    <author>
      <name>Christopher Fowler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Versions 1-7 are missing cases in the theorem. Version 8 is complete</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.10081v8" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10081v8" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.10583">
    <id>http://arxiv.org/abs/1903.10583v1</id>
    <updated>2019-03-25T20:21:17Z</updated>
    <published>2019-03-25T20:21:17Z</published>
    <title>Algorithms to compute the Burrows-Wheeler Similarity Distribution</title>
    <summary>  The Burrows-Wheeler transform (BWT) is a well studied text transformation
widely used in data compression and text indexing. The BWT of two strings can
also provide similarity measures between them, based on the observation that
the more their symbols are intermixed in the transformation, the more the
strings are similar. In this article we present two new algorithms to compute
similarity measures based on the BWT for string collections. In particular, we
present practical and theoretical improvements to the computation of the
Burrows-Wheeler similarity distribution for all pairs of strings in a
collection. Our algorithms take advantage of the BWT computed for the
concatenation of all strings, and use compressed data structures that allow
reducing the running time with a small memory footprint, as shown by a set of
experiments with real and artificial datasets.
</summary>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Guilherme P. Telles</name>
    </author>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Liang Zhao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2019.03.012</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2019.03.012" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to TCS</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.10583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0908.0239">
    <id>http://arxiv.org/abs/0908.0239v1</id>
    <updated>2009-08-03T13:10:48Z</updated>
    <published>2009-08-03T13:10:48Z</published>
    <title>On Bijective Variants of the Burrows-Wheeler Transform</title>
    <summary>  The sort transform (ST) is a modification of the Burrows-Wheeler transform
(BWT). Both transformations map an arbitrary word of length n to a pair
consisting of a word of length n and an index between 1 and n. The BWT sorts
all rotation conjugates of the input word, whereas the ST of order k only uses
the first k letters for sorting all such conjugates. If two conjugates start
with the same prefix of length k, then the indices of the rotations are used
for tie-breaking. Both transforms output the sequence of the last letters of
the sorted list and the index of the input within the sorted list. In this
paper, we discuss a bijective variant of the BWT (due to Scott), proving its
correctness and relations to other results due to Gessel and Reutenauer (1993)
and Crochemore, Desarmenien, and Perrin (2005). Further, we present a novel
bijective variant of the ST.
</summary>
    <author>
      <name>Manfred Kufleitner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, presented at the Prague Stringology Conference 2009 (PSC
  2009)</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.0239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1410.3438">
    <id>http://arxiv.org/abs/1410.3438v2</id>
    <updated>2015-06-29T13:25:45Z</updated>
    <published>2014-10-13T19:02:40Z</published>
    <title>Efficient and Compact Representations of Prefix Codes</title>
    <summary>  Most of the attention in statistical compression is given to the space used
by the compressed sequence, a problem completely solved with optimal prefix
codes. However, in many applications, the storage space used to represent the
prefix code itself can be an issue. In this paper we introduce and compare
several techniques to store prefix codes. Let $N$ be the sequence length and
$n$ be the alphabet size. Then a naive storage of an optimal prefix code uses
$O(n\log n)$ bits. Our first technique shows how to use $O(n\log\log(N/n))$
bits to store the optimal prefix code. Then we introduce an approximate
technique that, for any $0&lt;\epsilon&lt;1/2$, takes $O(n \log \log (1 / \epsilon))$
bits to store a prefix code with average codeword length within an additive
$\epsilon$ of the minimum. Finally, a second approximation takes, for any
constant $c > 1$, $O(n^{1 / c} \log n)$ bits to store a prefix code with
average codeword length at most $c$ times the minimum. In all cases, our data
structures allow encoding and decoding of any symbol in $O(1)$ time. We
experimentally compare our new techniques with the state of the art, showing
that we achieve 6--8-fold space reductions, at the price of a slower encoding
(2.5--8 times slower) and decoding (12--24 times slower). The approximations
further reduce this space and improve the time significantly, up to recovering
the speed of classical implementations, for a moderate penalty in the average
code length. As a byproduct, we compare various heuristic, approximate, and
optimal algorithms to generate length-restricted codes, showing that the
optimal ones are clearly superior and practical enough to be implemented.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Alberto Ordóñez</name>
    </author>
    <link href="http://arxiv.org/abs/1410.3438v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.3438v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1412.0967">
    <id>http://arxiv.org/abs/1412.0967v1</id>
    <updated>2014-12-02T16:37:37Z</updated>
    <published>2014-12-02T16:37:37Z</published>
    <title>Queries on LZ-Bounded Encodings</title>
    <summary>  We describe a data structure that stores a string $S$ in space similar to
that of its Lempel-Ziv encoding and efficiently supports access, rank and
select queries. These queries are fundamental for implementing succinct and
compressed data structures, such as compressed trees and graphs. We show that
our data structure can be built in a scalable manner and is both small and fast
in practice compared to other data structures supporting such queries.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Alberto Ordóñez</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/1412.0967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.0967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1606.05031">
    <id>http://arxiv.org/abs/1606.05031v1</id>
    <updated>2016-06-16T02:55:39Z</updated>
    <published>2016-06-16T02:55:39Z</published>
    <title>Scalable Partial Least Squares Regression on Grammar-Compressed Data
  Matrices</title>
    <summary>  With massive high-dimensional data now commonplace in research and industry,
there is a strong and growing demand for more scalable computational techniques
for data analysis and knowledge discovery. Key to turning these data into
knowledge is the ability to learn statistical models with high
interpretability. Current methods for learning statistical models either
produce models that are not interpretable or have prohibitive computational
costs when applied to massive data. In this paper we address this need by
presenting a scalable algorithm for partial least squares regression (PLS),
which we call compression-based PLS (cPLS), to learn predictive linear models
with a high interpretability from massive high-dimensional data. We propose a
novel grammar-compressed representation of data matrices that supports fast row
and column access while the data matrix is in a compressed form. The original
data matrix is grammar-compressed and then the linear model in PLS is learned
on the compressed data matrix, which results in a significant reduction in
working space, greatly improving scalability. We experimentally test cPLS on
its ability to learn linear models for classification, regression and feature
extraction with various massive high-dimensional data, and show that cPLS
performs superiorly in terms of prediction accuracy, computational efficiency,
and interpretability.
</summary>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <author>
      <name>Hiroto Saigo</name>
    </author>
    <author>
      <name>Yoshihiro Yamanishi</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be appeared in the Proceedings of KDD'16</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1611.01479">
    <id>http://arxiv.org/abs/1611.01479v1</id>
    <updated>2016-11-04T18:25:21Z</updated>
    <published>2016-11-04T18:25:21Z</published>
    <title>Space-Efficient Re-Pair Compression</title>
    <summary>  Re-Pair is an effective grammar-based compression scheme achieving strong
compression rates in practice. Let $n$, $\sigma$, and $d$ be the text length,
alphabet size, and dictionary size of the final grammar, respectively. In their
original paper, the authors show how to compute the Re-Pair grammar in expected
linear time and $5n + 4\sigma^2 + 4d + \sqrt{n}$ words of working space on top
of the text. In this work, we propose two algorithms improving on the space of
their original solution. Our model assumes a memory word of $\lceil\log_2
n\rceil$ bits and a re-writable input text composed by $n$ such words. Our
first algorithm runs in expected $\mathcal O(n/\epsilon)$ time and uses
$(1+\epsilon)n +\sqrt n$ words of space on top of the text for any parameter
$0&lt;\epsilon \leq 1$ chosen in advance. Our second algorithm runs in expected
$\mathcal O(n\log n)$ time and improves the space to $n +\sqrt n$ words.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1611.01479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1704.08558">
    <id>http://arxiv.org/abs/1704.08558v1</id>
    <updated>2017-04-27T13:28:45Z</updated>
    <published>2017-04-27T13:28:45Z</published>
    <title>Practical and Effective Re-Pair Compression</title>
    <summary>  Re-Pair is an efficient grammar compressor that operates by recursively
replacing high-frequency character pairs with new grammar symbols. The most
space-efficient linear-time algorithm computing Re-Pair uses
$(1+\epsilon)n+\sqrt n$ words on top of the re-writable text (of length $n$ and
stored in $n$ words), for any constant $\epsilon>0$; in practice however, this
solution uses complex sub-procedures preventing it from being practical. In
this paper, we present an implementation of the above-mentioned result making
use of more practical solutions; our tool further improves the working space to
$(1.5+\epsilon)n$ words (text included), for some small constant $\epsilon$. As
a second contribution, we focus on compact representations of the output
grammar. The lower bound for storing a grammar with $d$ rules is
$\log(d!)+2d\approx d\log d+0.557 d$ bits, and the most efficient encoding
algorithm in the literature uses at most $d\log d + 2d$ bits and runs in
$\mathcal O(d^{1.5})$ time. We describe a linear-time heuristic maximizing the
compressibility of the output Re-Pair grammar. On real datasets, our grammar
encoding uses---on average---only $2.8\%$ more bits than the
information-theoretic minimum. In half of the tested cases, our compressor
improves the output size of 7-Zip with maximum compression rate turned on.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1704.08558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.09538">
    <id>http://arxiv.org/abs/1705.09538v2</id>
    <updated>2017-07-25T11:15:37Z</updated>
    <published>2017-05-26T11:33:05Z</published>
    <title>On Two LZ78-style Grammars: Compression Bounds and Compressed-Space
  Computation</title>
    <summary>  We investigate two closely related LZ78-based compression schemes: LZMW (an
old scheme by Miller and Wegman) and LZD (a recent variant by Goto et al.).
Both LZD and LZMW naturally produce a grammar for a string of length $n$; we
show that the size of this grammar can be larger than the size of the smallest
grammar by a factor $\Omega(n^{\frac{1}3})$ but is always within a factor
$O((\frac{n}{\log n})^{\frac{2}{3}})$. In addition, we show that the standard
algorithms using $\Theta(z)$ working space to construct the LZD and LZMW
parsings, where $z$ is the size of the parsing, work in $\Omega(n^{\frac{5}4})$
time in the worst case. We then describe a new Las Vegas LZD/LZMW parsing
algorithm that uses $O (z \log n)$ space and $O(n + z \log^2 n)$ time w.h.p..
</summary>
    <author>
      <name>Golnaz Badkobeh</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, accepted to SPIRE 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.09538v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09538v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.07967">
    <id>http://arxiv.org/abs/1903.07967v1</id>
    <updated>2019-03-19T12:45:58Z</updated>
    <published>2019-03-19T12:45:58Z</published>
    <title>A New Lower Bound for Semigroup Orthogonal Range Searching</title>
    <summary>  We report the first improvement in the space-time trade-off of lower bounds
for the orthogonal range searching problem in the semigroup model, since
Chazelle's result from 1990. This is one of the very fundamental problems in
range searching with a long history. Previously, Andrew Yao's influential
result had shown that the problem is already non-trivial in one
dimension~\cite{Yao-1Dlb}: using $m$ units of space, the query time $Q(n)$ must
be $\Omega( \alpha(m,n) + \frac{n}{m-n+1})$ where $\alpha(\cdot,\cdot)$ is the
inverse Ackermann's function, a very slowly growing function.
  In $d$ dimensions, Bernard Chazelle~\cite{Chazelle.LB.II} proved that the
query time must be $Q(n) = \Omega( (\log_\beta n)^{d-1})$ where $\beta = 2m/n$.
Chazelle's lower bound is known to be tight for when space consumption is
`high' i.e., $m = \Omega(n \log^{d+\varepsilon}n)$. We have two main results.
The first is a lower bound that shows Chazelle's lower bound was not tight for
`low space': we prove that we must have $m (n) = \Omega(n (\log n \log\log
n)^{d-1})$. Our lower bound does not close the gap to the existing data
structures, however, our second result is that our analysis is tight. Thus, we
believe the gap is in fact natural since lower bounds are proven for idempotent
semigroups while the data structures are built for general semigroups and thus
they cannot assume (and use) the properties of an idempotent semigroup. As a
result, we believe to close the gap one must study lower bounds for
non-idempotent semigroups or building data structures for idempotent
semigroups. We develope significantly new ideas for both of our results that
could be useful in pursuing either of these directions.
</summary>
    <author>
      <name>Peyman Afshani</name>
    </author>
    <link href="http://arxiv.org/abs/1903.07967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.07775">
    <id>http://arxiv.org/abs/1903.07775v1</id>
    <updated>2019-03-19T00:04:32Z</updated>
    <published>2019-03-19T00:04:32Z</published>
    <title>QuickSort: Improved right-tail asymptotics for the limiting
  distribution, and large deviations</title>
    <summary>  We substantially refine asymptotic logarithmic upper bounds produced by
Svante Janson (2015) on the right tail of the limiting QuickSort distribution
function $F$ and by Fill and Hung (2018) on the right tails of the
corresponding density $f$ and of the absolute derivatives of $f$ of each order.
For example, we establish an upper bound on $\log[1 - F(x)]$ that matches
conjectured asymptotics of Knessl and Szpankowski (1999) through terms of order
$(\log x)^2$; the corresponding order for the Janson (2015) bound is the lead
order, $x \log x$.
  Using the refined asymptotic bounds on $F$, we derive right-tail large
deviation (LD) results for the distribution of the number of comparisons
required by QuickSort that substantially sharpen the two-sided LD results of
McDiarmid and Hayward (1996).
</summary>
    <author>
      <name>James Allen Fill</name>
    </author>
    <author>
      <name>Wei-Chun Hung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages; submitted for publication in January, 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.07775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P10 (Primary) 60E05, 60C05 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1304.0528">
    <id>http://arxiv.org/abs/1304.0528v1</id>
    <updated>2013-04-02T04:41:33Z</updated>
    <published>2013-04-02T04:41:33Z</published>
    <title>Efficient repeat finding via suffix arrays</title>
    <summary>  We solve the problem of finding interspersed maximal repeats using a suffix
array construction. As it is well known, all the functionality of suffix trees
can be handled by suffix arrays, gaining practicality. Our solution improves
the suffix tree based approaches for the repeat finding problem, being
particularly well suited for very large inputs. We prove the corrrectness and
complexity of the algorithms.
</summary>
    <author>
      <name>Veronica Becher</name>
    </author>
    <author>
      <name>Alejandro Deymonnaz</name>
    </author>
    <author>
      <name>Pablo Ariel Heiber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bioinformatics, 25(14):1746-1753, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.0528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.0528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.10382">
    <id>http://arxiv.org/abs/1705.10382v4</id>
    <updated>2017-07-11T18:21:03Z</updated>
    <published>2017-05-29T20:24:07Z</published>
    <title>Optimal-Time Text Indexing in BWT-runs Bounded Space</title>
    <summary>  Indexing highly repetitive texts --- such as genomic databases, software
repositories and versioned text collections --- has become an important problem
since the turn of the millennium. A relevant compressibility measure for
repetitive texts is $r$, the number of runs in their Burrows-Wheeler Transform
(BWT). One of the earliest indexes for repetitive collections, the Run-Length
FM-index, used $O(r)$ space and was able to efficiently count the number of
occurrences of a pattern of length $m$ in the text (in loglogarithmic time per
pattern symbol, with current techniques). However, it was unable to locate the
positions of those occurrences efficiently within a space bounded in terms of
$r$. Since then, a number of other indexes with space bounded by other measures
of repetitiveness --- the number of phrases in the Lempel-Ziv parse, the size
of the smallest grammar generating the text, the size of the smallest automaton
recognizing the text factors --- have been proposed for efficiently locating,
but not directly counting, the occurrences of a pattern. In this paper we close
this long-standing problem, showing how to extend the Run-Length FM-index so
that it can locate the $occ$ occurrences efficiently within $O(r)$ space (in
loglogarithmic time each), and reaching optimal time $O(m+occ)$ within
$O(r\log(n/r))$ space, on a RAM machine of $w=\Omega(\log n)$ bits. Within
$O(r\log (n/r))$ space, our index can also count in optimal time $O(m)$.
Raising the space to $O(r w\log_\sigma(n/r))$, we support count and locate in
$O(m\log(\sigma)/w)$ and $O(m\log(\sigma)/w+occ)$ time, which is optimal in the
packed setting and had not been obtained before in compressed space. We also
describe a structure using $O(r\log(n/r))$ space that replaces the text and
extracts any text substring of length $\ell$ in almost-optimal time
$O(\log(n/r)+\ell\log(\sigma)/w)$. (...continues...)
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1705.10382v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10382v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.02517">
    <id>http://arxiv.org/abs/1809.02517v2</id>
    <updated>2019-01-22T12:10:23Z</updated>
    <published>2018-09-07T14:54:53Z</published>
    <title>Streaming dictionary matching with mismatches</title>
    <summary>  In the $k$-mismatch problem we are given a pattern of length $m$ and a text
and must find all locations where the Hamming distance between the pattern and
the text is at most $k$. A series of recent breakthroughs have resulted in an
ultra-efficient streaming algorithm for this problem that requires only $O(k
\log \frac{m}{k})$ space [Clifford, Kociumaka, Porat, SODA 2019]. In this work,
we consider a strictly harder problem called dictionary matching with $k$
mismatches, where we are given a dictionary of $d$ patterns of lengths $\le m$
and must find all their $k$-mismatch occurrences in the text, and show the
first streaming algorithm for it. The algorithm uses $O(k d \log^k d \,
\mathrm{polylog} \, m)$ space and processes each position of the text in $O(k
\log^{k} d \, \mathrm{polylog} \, m + occ)$ time, where $occ$ is the number of
$k$-mismatch occurrences of the patterns that end at this position. The
algorithm is randomised and outputs correct answers with high probability.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <link href="http://arxiv.org/abs/1809.02517v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.02517v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1702.07577">
    <id>http://arxiv.org/abs/1702.07577v1</id>
    <updated>2017-02-24T13:41:29Z</updated>
    <published>2017-02-24T13:41:29Z</published>
    <title>Compression with the tudocomp Framework</title>
    <summary>  We present a framework facilitating the implementation and comparison of text
compression algorithms. We evaluate its features by a case study on two novel
compression algorithms based on the Lempel-Ziv compression schemes that perform
well on highly repetitive texts.
</summary>
    <author>
      <name>Patrick Dinklage</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Marvin Löbel</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <link href="http://arxiv.org/abs/1702.07577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.03560">
    <id>http://arxiv.org/abs/1903.03560v1</id>
    <updated>2019-03-08T17:16:26Z</updated>
    <published>2019-03-08T17:16:26Z</published>
    <title>Belga B-trees</title>
    <summary>  We revisit self-adjusting external memory tree data structures, which combine
the optimal (and practical) worst-case I/O performances of B-trees, while
adapting to the online distribution of queries. Our approach is analogous to
undergoing efforts in the BST model, where Tango Trees (Demaine et al. 2007)
were shown to be $O(\log\log N)$-competitive with the runtime of the best
offline binary search tree on every sequence of searches. Here we formalize the
B-Tree model as a natural generalization of the BST model. We prove lower
bounds for the B-Tree model, and introduce a B-Tree model data structure, the
Belga B-tree, that executes any sequence of searches within a $O(\log \log N)$
factor of the best offline B-tree model algorithm, provided $B=\log^{O(1)}N$.
We also show how to transform any static BST into a static B-tree which is
faster by a $\Theta(\log B)$ factor; the transformation is randomized and we
show that randomization is necessary to obtain any significant speedup.
</summary>
    <author>
      <name>Erik D. Demaine</name>
    </author>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Grigorios Koumoutsos</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <link href="http://arxiv.org/abs/1903.03560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.03560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.04214">
    <id>http://arxiv.org/abs/1903.04214v3</id>
    <updated>2020-02-07T15:48:24Z</updated>
    <published>2019-03-11T11:00:38Z</published>
    <title>How far away must forced letters be so that squares are still avoidable?</title>
    <summary>  We describe a new non-constructive technique to show that squares are
avoidable by an infinite word even if we force some letters from the alphabet
to appear at certain occurrences. We show that as long as forced positions are
at distance at least 19 (resp. 3, resp. 2) from each other then we can avoid
squares over 3 letters (resp. 4 letters, resp. 6 or more letters). We can also
deduce exponential lower bounds on the number of solutions. For our main
Theorem to be applicable, we need to check the existence of some languages and
we explain how to verify that they exist with a computer. We hope that this
technique could be applied to other avoidability questions where the good
approach seems to be non-constructive (e.g., the Thue-list coloring number of
the infinite path).
</summary>
    <author>
      <name>Matthieu Rosenfeld</name>
    </author>
    <link href="http://arxiv.org/abs/1903.04214v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.04214v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.04936">
    <id>http://arxiv.org/abs/1903.04936v1</id>
    <updated>2019-03-12T14:05:07Z</updated>
    <published>2019-03-12T14:05:07Z</published>
    <title>The k-d tree data structure and a proof for neighborhood computation in
  expected logarithmic time</title>
    <summary>  For practical applications, any neighborhood concept imposed on a finite
point set P is not of any use if it cannot be computed efficiently. Thus, in
this paper, we give an introduction to the data structure of k-d trees, first
presented by Friedman, Bentley, and Finkel in 1977. After a short introduction
to the data structure (Section 1), we turn to the proof of efficiency by
Friedman and his colleagues (Section 2). The main contribution of this paper is
the translation of the proof of Freedman, Bentley, and Finkel into modern terms
and the elaboration of the proof.
</summary>
    <author>
      <name>Martin Skrodzki</name>
    </author>
    <link href="http://arxiv.org/abs/1903.04936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.04936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.05442">
    <id>http://arxiv.org/abs/1903.05442v2</id>
    <updated>2019-12-18T17:06:43Z</updated>
    <published>2019-03-13T12:20:37Z</published>
    <title>Maximal State Complexity and Generalized de Bruijn Words</title>
    <summary>  We compute the exact maximum state complexity for the language consisting of
$m$ words of length $N$, and characterize languages achieving the maximum. We
also consider a special case, namely languages $C(w)$ consisting of the
conjugates of a single word $w$. The words for which the maximum state
complexity of $C(w)$ is achieved turn out to be a natural generalization of de
Bruijn words. We show that generalized de Bruijn words exist for each length
and consider the number of them.
</summary>
    <author>
      <name>Daniel Gabric</name>
    </author>
    <author>
      <name>Štěpán Holub</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected and extended version</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.05442v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.05442v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.06570">
    <id>http://arxiv.org/abs/1903.06570v1</id>
    <updated>2019-03-15T14:26:59Z</updated>
    <published>2019-03-15T14:26:59Z</published>
    <title>scaleBF: A High Scalable Membership Filter using 3D Bloom Filter</title>
    <summary>  Bloom Filter is extensively deployed data structure in various applications
and research domain since its inception. Bloom Filter is able to reduce the
space consumption in an order of magnitude. Thus, Bloom Filter is used to keep
information of a very large scale data. There are numerous variants of Bloom
Filters available, however, scalability is a serious dilemma of Bloom Filter
for years. To solve this dilemma, there are also diverse variants of Bloom
Filter. However, the time complexity and space complexity become the key issue
again. In this paper, we present a novel Bloom Filter to address the
scalability issue without compromising the performance, called scaleBF. scaleBF
deploys many 3D Bloom Filter to filter the set of items. In this paper, we
theoretically compare the contemporary Bloom Filter for scalability and scaleBF
outperforms in terms of time complexity.
</summary>
    <author>
      <name>Ripon Patgiri</name>
    </author>
    <author>
      <name>Sabuzima Nayak</name>
    </author>
    <author>
      <name>Samir Kumar Borgohain</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14569/IJACSA.2018.091277</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14569/IJACSA.2018.091277" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, 3 Figures, 1 Table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Science and
  Applications(IJACSA), Volume 9 Issue 12, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.06570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.06570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.06290">
    <id>http://arxiv.org/abs/1903.06290v2</id>
    <updated>2020-03-24T02:30:35Z</updated>
    <published>2019-03-14T22:46:55Z</published>
    <title>Fast Algorithms for the Shortest Unique Palindromic Substring Problem on
  Run-Length Encoded Strings</title>
    <summary>  For a string $S$, a palindromic substring $S[i..j]$ is said to be a
\emph{shortest unique palindromic substring} ($\mathit{SUPS}$) for an interval
$[s, t]$ in $S$, if $S[i..j]$ occurs exactly once in $S$, the interval $[i, j]$
contains $[s, t]$, and every palindromic substring containing $[s, t]$ which is
shorter than $S[i..j]$ occurs at least twice in $S$. In this paper, we study
the problem of answering $\mathit{SUPS}$ queries on run-length encoded strings.
We show how to preprocess a given run-length encoded string $\mathit{RLE}_{S}$
of size $m$ in $O(m)$ space and $O(m \log \sigma_{\mathit{RLE}_{S}} + m
\sqrt{\log m / \log\log m})$ time so that all $\mathit{SUPSs}$ for any
subsequent query interval can be answered in $O(\sqrt{\log m / \log\log m} +
\alpha)$ time, where $\alpha$ is the number of outputs, and
$\sigma_{\mathit{RLE}_{S}}$ is the number of distinct runs of
$\mathit{RLE}_{S}$. Additionaly, we consider a variant of the SUPS problem
where a query interval is also given in a run-length encoded form. For this
variant of the problem, we present two alternative algorithms with faster
queries. The first one answers queries in $O(\sqrt{\log\log m /\log\log\log m}
+ \alpha)$ time and can be built in $O(m \log \sigma_{\mathit{RLE}_{S}} + m
\sqrt{\log m / \log\log m})$ time, and the second one answers queries in
$O(\log \log m + \alpha)$ time and can be built in $O(m \log
\sigma_{\mathit{RLE}_{S}})$ time. Both of these data structures require $O(m)$
space.
</summary>
    <author>
      <name>Kiichi Watanabe</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1903.06290v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.06290v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.06289">
    <id>http://arxiv.org/abs/1903.06289v1</id>
    <updated>2019-03-14T22:45:34Z</updated>
    <published>2019-03-14T22:45:34Z</published>
    <title>The Parameterized Position Heap of a Trie</title>
    <summary>  Let $\Sigma$ and $\Pi$ be disjoint alphabets of respective size $\sigma$ and
$\pi$. Two strings over $\Sigma \cup \Pi$ of equal length are said to
parameterized match (p-match) if there is a bijection $f:\Sigma \cup \Pi
\rightarrow \Sigma \cup \Pi$ such that (1) $f$ is identity on $\Sigma$ and (2)
$f$ maps the characters of one string to those of the other string so that the
two strings become identical. We consider the p-matching problem on a
(reversed) trie $\mathcal{T}$ and a string pattern $P$ such that every path
that p-matches $P$ has to be reported. Let $N$ be the size of the given trie
$\mathcal{T}$. In this paper, we propose the parameterized position heap for
$\mathcal{T}$ that occupies $O(N)$ space and supports p-matching queries in
$O(m \log (\sigma + \pi) + m \pi + \mathit{pocc}))$ time, where $m$ is the
length of a query pattern $P$ and $\mathit{pocc}$ is the number of paths in
$\mathcal{T}$ to report. We also present an algorithm which constructs the
parameterized position heap for a given trie $\mathcal{T}$ in $O(N (\sigma +
\pi))$ time and working space.
</summary>
    <author>
      <name>Noriki Fujisato</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1903.06289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.06289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.09779">
    <id>http://arxiv.org/abs/1705.09779v2</id>
    <updated>2017-07-27T07:22:13Z</updated>
    <published>2017-05-27T07:24:44Z</published>
    <title>Linear-size CDAWG: new repetition-aware indexing and grammar compression</title>
    <summary>  In this paper, we propose a novel approach to combine \emph{compact directed
acyclic word graphs} (CDAWGs) and grammar-based compression. This leads us to
an efficient self-index, called Linear-size CDAWGs (L-CDAWGs), which can be
represented with $O(\tilde e_T \log n)$ bits of space allowing for $O(\log
n)$-time random and $O(1)$-time sequential accesses to edge labels, and $O(m
\log \sigma + occ)$-time pattern matching. Here, $\tilde e_T$ is the number of
all extensions of maximal repeats in $T$, $n$ and $m$ are respectively the
lengths of the text $T$ and a given pattern, $\sigma$ is the alphabet size, and
$occ$ is the number of occurrences of the pattern in $T$. The repetitiveness
measure $\tilde e_T$ is known to be much smaller than the text length $n$ for
highly repetitive text. For constant alphabets, our L-CDAWGs achieve $O(m +
occ)$ pattern matching time with $O(e_T^r \log n)$ bits of space, which
improves the pattern matching time of Belazzougui et al.'s run-length
BWT-CDAWGs by a factor of $\log \log n$, with the same space complexity. Here,
$e_T^r$ is the number of right extensions of maximal repeats in $T$. As a
byproduct, our result gives a way of constructing an SLP of size $O(\tilde
e_T)$ for a given text $T$ in $O(n + \tilde e_T \log \sigma)$ time.
</summary>
    <author>
      <name>Takuya Takagi</name>
    </author>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Yuta Fujishige</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hiroki Arimura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.09779v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09779v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.01720">
    <id>http://arxiv.org/abs/1705.01720v1</id>
    <updated>2017-05-04T07:11:47Z</updated>
    <published>2017-05-04T07:11:47Z</published>
    <title>Near-optimal linear decision trees for k-SUM and related problems</title>
    <summary>  We construct near optimal linear decision trees for a variety of decision
problems in combinatorics and discrete geometry. For example, for any constant
$k$, we construct linear decision trees that solve the $k$-SUM problem on $n$
elements using $O(n \log^2 n)$ linear queries. Moreover, the queries we use are
comparison queries, which compare the sums of two $k$-subsets; when viewed as
linear queries, comparison queries are $2k$-sparse and have only $\{-1,0,1\}$
coefficients. We give similar constructions for sorting sumsets $A+B$ and for
solving the SUBSET-SUM problem, both with optimal number of queries, up to
poly-logarithmic terms.
  Our constructions are based on the notion of "inference dimension", recently
introduced by the authors in the context of active classification with
comparison queries. This can be viewed as another contribution to the fruitful
link between machine learning and discrete geometry, which goes back to the
discovery of the VC dimension.
</summary>
    <author>
      <name>Daniel M. Kane</name>
    </author>
    <author>
      <name>Shachar Lovett</name>
    </author>
    <author>
      <name>Shay Moran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 paged, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1007.5406">
    <id>http://arxiv.org/abs/1007.5406v1</id>
    <updated>2010-07-30T10:14:21Z</updated>
    <published>2010-07-30T10:14:21Z</published>
    <title>Tree structure compression with RePair</title>
    <summary>  In this work we introduce a new linear time compression algorithm, called
"Re-pair for Trees", which compresses ranked ordered trees using linear
straight-line context-free tree grammars. Such grammars generalize
straight-line context-free string grammars and allow basic tree operations,
like traversal along edges, to be executed without prior decompression. Our
algorithm can be considered as a generalization of the "Re-pair" algorithm
developed by N. Jesper Larsson and Alistair Moffat in 2000. The latter
algorithm is a dictionary-based compression algorithm for strings. We also
introduce a succinct coding which is specialized in further compressing the
grammars generated by our algorithm. This is accomplished without loosing the
ability do directly execute queries on this compressed representation of the
input tree. Finally, we compare the grammars and output files generated by a
prototype of the Re-pair for Trees algorithm with those of similar compression
algorithms. The obtained results show that that our algorithm outperforms its
competitors in terms of compression ratio, runtime and memory usage.
</summary>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Sebastian Maneth</name>
    </author>
    <author>
      <name>Roy Mennicke</name>
    </author>
    <link href="http://arxiv.org/abs/1007.5406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.5406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P15 (Primary), 68P30 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.08809">
    <id>http://arxiv.org/abs/1902.08809v2</id>
    <updated>2019-04-16T15:52:03Z</updated>
    <published>2019-02-23T16:24:20Z</published>
    <title>Faster and simpler algorithms for finding large patterns in permutations</title>
    <summary>  Permutation patterns and pattern avoidance have been intensively studied in
combinatorics and computer science, going back at least to the seminal work of
Knuth on stack-sorting (1968). Perhaps the most natural algorithmic question in
this area is deciding whether a given permutation of length $n$ contains a
given pattern of length $k$.
  In this work we give two new algorithms for this well-studied problem, one
whose running time is $n^{0.44k+o(k)}$, and one whose running time is the
better of $O(1.6181^n)$ and $n^{k/2+o(k)}$. These results improve the earlier
best bounds of Ahal and Rabinovich (2000), and Bruner and Lackner (2012), and
are the fastest algorithms for the problem when $k = \Omega(\log n)$. When $k =
o(\log n)$, the parameterized algorithm of Guillemot and Marx (2013) dominates.
  Our second algorithm uses polynomial space and is significantly simpler than
all previous approaches with comparable running times, including an
$n^{k/2+o(k)}$ algorithm proposed by Guillemot and Marx. Our approach can be
summarized as follows: "for every matching of the even-valued entries of the
pattern, try to match all odd-valued entries left-to-right". For the special
case of patterns that are Jordan-permutations, we show an improved,
subexponential running time.
</summary>
    <author>
      <name>László Kozma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The second analysis of Algorithm~S was mistaken. The corrected bound
  is 1.618^n instead of 1.414^n</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.08809v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.08809v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.09228">
    <id>http://arxiv.org/abs/1902.09228v1</id>
    <updated>2019-02-25T12:29:11Z</updated>
    <published>2019-02-25T12:29:11Z</published>
    <title>Succinct Data Structures for Families of Interval Graphs</title>
    <summary>  We consider the problem of designing succinct data structures for interval
graphs with $n$ vertices while supporting degree, adjacency, neighborhood and
shortest path queries in optimal time in the $\Theta(\log n)$-bit word RAM
model. The degree query reports the number of incident edges to a given vertex
in constant time, the adjacency query returns true if there is an edge between
two vertices in constant time, the neighborhood query reports the set of all
adjacent vertices in time proportional to the degree of the queried vertex, and
the shortest path query returns a shortest path in time proportional to its
length, thus the running times of these queries are optimal. Towards showing
succinctness, we first show that at least $n\log{n} - 2n\log\log n - O(n)$ bits
are necessary to represent any unlabeled interval graph $G$ with $n$ vertices,
answering an open problem of Yang and Pippenger [Proc. Amer. Math. Soc. 2017].
This is augmented by a data structure of size $n\log{n} +O(n)$ bits while
supporting not only the aforementioned queries optimally but also capable of
executing various combinatorial algorithms (like proper coloring, maximum
independent set etc.) on the input interval graph efficiently. Finally, we
extend our ideas to other variants of interval graphs, for example, proper/unit
interval graphs, k-proper and k-improper interval graphs, and circular-arc
graphs, and design succinct/compact data structures for these graph classes as
well along with supporting queries on them efficiently.
</summary>
    <author>
      <name>Hüseyin Acan</name>
    </author>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Seungbum Jo</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <link href="http://arxiv.org/abs/1902.09228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.09228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.10812">
    <id>http://arxiv.org/abs/1902.10812v1</id>
    <updated>2019-02-27T22:27:12Z</updated>
    <published>2019-02-27T22:27:12Z</published>
    <title>Padovan heaps</title>
    <summary>  We analyze priority queues of Fibonacci family. The paper is inspired by
Violation heap [1], where A. Elmasry saves one pointer in representation of
Fibonacci heap nodes while achieving the same amortized bounds as Fibonacci
heaps [2] of M. L. Fredman and R. E. Tarjan. Unfortunately author forces the
heaps to be wide, what goes against optimal heap principles. Our goal is to
achieve the same result, but with much narrower heaps. We follow the principle
of superexpensive comparison so we try to remember results of all comparisons
and never compare elements that cannot be minimal. We delay comparisons as long
as possible. Actually I have always want to share superexpensive comparison
principle ideas, discovery of Padovan heaps allowed me to do so. Of course
saving one pointer is not that big goal, but I hope the presented reasoning and
amortized analysis of the resulting heaps is worth a publication.
</summary>
    <author>
      <name>Vladan Majerech</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.10812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.10812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.10995">
    <id>http://arxiv.org/abs/1902.10995v2</id>
    <updated>2019-12-05T08:57:20Z</updated>
    <published>2019-02-28T10:29:35Z</published>
    <title>Fast Concurrent Data Sketches</title>
    <summary>  Data sketches are approximate succinct summaries of long streams. They are
widely used for processing massive amounts of data and answering statistical
queries about it in real-time. Existing libraries producing sketches are very
fast, but do not allow parallelism for creating sketches using multiple threads
or querying them while they are being built. We present a generic approach to
parallelising data sketches efficiently, while bounding the error that such
parallelism introduces. Utilising relaxed semantics and the notion of strong
linearisability we prove our algorithm's correctness and analyse the error it
induces in two specific sketches. Our implementation achieves high scalability
while keeping the error small.
</summary>
    <author>
      <name>Arik Rinberg</name>
    </author>
    <author>
      <name>Alexander Spiegelman</name>
    </author>
    <author>
      <name>Edward Bortnikov</name>
    </author>
    <author>
      <name>Eshcar Hillel</name>
    </author>
    <author>
      <name>Idit Keidar</name>
    </author>
    <author>
      <name>Lee Rhodes</name>
    </author>
    <author>
      <name>Hadar Serviansky</name>
    </author>
    <link href="http://arxiv.org/abs/1902.10995v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.10995v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.01387">
    <id>http://arxiv.org/abs/1903.01387v1</id>
    <updated>2019-03-04T17:30:51Z</updated>
    <published>2019-03-04T17:30:51Z</published>
    <title>A Simple Solution to the Level-Ancestor Problem</title>
    <summary>  A Level Ancestory query LA($u$, $d$) asks for the the ancestor of the node
$u$ at a depth $d$. We present a simple solution, which pre-processes the tree
in $O(n)$ time with $O(n)$ extra space, and answers the queries in $O(\log\
{n})$ time. Though other optimal algorithms exist, this is a simple enough
solution that could be taught and implemented easily.
</summary>
    <author>
      <name>Gaurav Menghani</name>
    </author>
    <author>
      <name>Dhruv Matani</name>
    </author>
    <link href="http://arxiv.org/abs/1903.01387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.01909">
    <id>http://arxiv.org/abs/1903.01909v1</id>
    <updated>2019-03-05T15:50:07Z</updated>
    <published>2019-03-05T15:50:07Z</published>
    <title>Lempel-Ziv-like Parsing in Small Space</title>
    <summary>  Lempel-Ziv (LZ77 or, briefly, LZ) is one of the most effective and
widely-used compressors for repetitive texts. However, the existing efficient
methods computing the exact LZ parsing have to use linear or close to linear
space to index the input text during the construction of the parsing, which is
prohibitive for long inputs. An alternative is Relative Lempel-Ziv (RLZ), which
indexes only a fixed reference sequence, whose size can be controlled. Deriving
the reference sequence by sampling the text yields reasonable compression
ratios for RLZ, but performance is not always competitive with that of LZ and
depends heavily on the similarity of the reference to the text. In this paper
we introduce ReLZ, a technique that uses RLZ as a preprocessor to approximate
the LZ parsing using little memory. RLZ is first used to produce a sequence of
phrases, and these are regarded as metasymbols that are input to LZ for a
second-level parsing on a (most often) drastically shorter sequence. This
parsing is finally translated into one on the original sequence.
  We analyze the new scheme and prove that, like LZ, it achieves the $k$th
order empirical entropy compression $n H_k + o(n\log\sigma)$ with $k =
o(\log_\sigma n)$, where $n$ is the input length and $\sigma$ is the alphabet
size. In fact, we prove this entropy bound not only for ReLZ but for a wide
class of LZ-like encodings. Then, we establish a lower bound on ReLZ
approximation ratio showing that the number of phrases in it can be
$\Omega(\log n)$ times larger than the number of phrases in LZ. Our experiments
show that ReLZ is orders of magnitude faster than other alternatives to compute
the (exact or approximate) LZ parsing, at the reasonable price of an
approximation factor below $2.0$ in practice, and sometimes below $1.05$, to
the size of LZ.
</summary>
    <author>
      <name>Daniel Valenzuela</name>
    </author>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.01909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.01465">
    <id>http://arxiv.org/abs/1903.01465v1</id>
    <updated>2019-03-04T18:34:01Z</updated>
    <published>2019-03-04T18:34:01Z</published>
    <title>Lightweight merging of compressed indices based on BWT variants</title>
    <summary>  In this paper we propose a flexible and lightweight technique for merging
compressed indices based on variants of Burrows-Wheeler transform (BWT), thus
addressing the need for algorithms that compute compressed indices over large
collections using a limited amount of working memory. Merge procedures make it
possible to use an incremental strategy for building large indices based on
merging indices for progressively larger subcollections.
  Starting with a known lightweight algorithm for merging BWTs [Holt and
McMillan, Bionformatics 2014], we show how to modify it in order to merge, or
compute from scratch, also the Longest Common Prefix (LCP) array. We then
expand our technique for merging compressed tries and circular/permuterm
compressed indices, two compressed data structures for which there were
hitherto no known merging algorithms.
</summary>
    <author>
      <name>Lavinia Egidi</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages. A preliminary version appeared in Proc. SPIRE 2017,
  Springer Verlag LNCS 10508. arXiv admin note: text overlap with
  arXiv:1609.04618</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.01465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.02533">
    <id>http://arxiv.org/abs/1903.02533v1</id>
    <updated>2019-03-06T18:13:33Z</updated>
    <published>2019-03-06T18:13:33Z</published>
    <title>Entropy Trees and Range-Minimum Queries In Optimal Average-Case Space</title>
    <summary>  The range-minimum query (RMQ) problem is a fundamental data structuring task
with numerous applications. Despite the fact that succinct solutions with
worst-case optimal $2n+o(n)$ bits of space and constant query time are known,
it has been unknown whether such a data structure can be made adaptive to the
reduced entropy of random inputs (Davoodi et al. 2014). We construct a succinct
data structure with the optimal $1.736n+o(n)$ bits of space on average for
random RMQ instances, settling this open problem.
  Our solution relies on a compressed data structure for binary trees that is
of independent interest. It can store a (static) binary search tree generated
by random insertions in asymptotically optimal expected space and supports many
queries in constant time. Using an instance-optimal encoding of subtrees, we
furthermore obtain a "hyper-succinct" data structure for binary trees that
improves upon the ultra-succinct representation of Jansson, Sadakane and Sung
(2012).
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <link href="http://arxiv.org/abs/1903.02533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.02533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.03003">
    <id>http://arxiv.org/abs/1903.03003v4</id>
    <updated>2020-02-18T14:31:53Z</updated>
    <published>2019-03-07T15:46:20Z</published>
    <title>Fast Exact Dynamic Time Warping on Run-Length Encoded Time Series</title>
    <summary>  Dynamic Time Warping (DTW) is a well-known similarity measure for time
series. The standard dynamic programming approach to compute the DTW distance
of two length-$n$ time series, however, requires $O(n^2)$ time, which is often
too slow for real-world applications. Therefore, many heuristics have been
proposed to speed up the DTW computation. These are often based on lower
bounding techniques, approximating the DTW distance, or considering special
input data such as binary or piecewise constant time series. In this paper, we
present a first exact algorithm to compute the DTW distance of two run-length
encoded time series whose running time only depends on the encoding lengths of
the inputs. The worst-case running time is cubic in the encoding length. In
experiments we show that our algorithm is indeed fast for time series with
short encoding lengths.
</summary>
    <author>
      <name>Vincent Froese</name>
    </author>
    <author>
      <name>Brijnesh Jain</name>
    </author>
    <author>
      <name>Maciej Rymar</name>
    </author>
    <link href="http://arxiv.org/abs/1903.03003v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.03003v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.02645">
    <id>http://arxiv.org/abs/1903.02645v1</id>
    <updated>2019-03-06T22:56:24Z</updated>
    <published>2019-03-06T22:56:24Z</published>
    <title>Encoding 3SUM</title>
    <summary>  We consider the following problem: given three sets of real numbers, output a
word-RAM data structure from which we can efficiently recover the sign of the
sum of any triple of numbers, one in each set. This is similar to a previous
work by some of the authors to encode the order type of a finite set of points.
While this previous work showed that it was possible to achieve slightly
subquadratic space and logarithmic query time, we show here that for the
simpler 3SUM problem, one can achieve an encoding that takes
$\tilde{O}(N^{\frac 32})$ space for inputs sets of size $N$ and allows constant
time queries in the word-RAM.
</summary>
    <author>
      <name>Sergio Cabello</name>
    </author>
    <author>
      <name>Jean Cardinal</name>
    </author>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <author>
      <name>Pat Morin</name>
    </author>
    <author>
      <name>Aurélien Ooms</name>
    </author>
    <link href="http://arxiv.org/abs/1903.02645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.02645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.04785">
    <id>http://arxiv.org/abs/1902.04785v1</id>
    <updated>2019-02-13T08:45:51Z</updated>
    <published>2019-02-13T08:45:51Z</published>
    <title>Constructing Antidictionaries in Output-Sensitive Space</title>
    <summary>  A word $x$ that is absent from a word $y$ is called minimal if all its proper
factors occur in $y$. Given a collection of $k$ words $y_1,y_2,\ldots,y_k$ over
an alphabet $\Sigma$, we are asked to compute the set
$\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{k}}$ of minimal absent words of length at
most $\ell$ of word $y=y_1\#y_2\#\ldots\#y_k$, $\#\notin\Sigma$. In data
compression, this corresponds to computing the antidictionary of $k$ documents.
In bioinformatics, it corresponds to computing words that are absent from a
genome of $k$ chromosomes. This computation generally requires $\Omega(n)$
space for $n=|y|$ using any of the plenty available $\mathcal{O}(n)$-time
algorithms. This is because an $\Omega(n)$-sized text index is constructed over
$y$ which can be impractical for large $n$. We do the identical computation
incrementally using output-sensitive space. This goal is reasonable when
$||\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{N}}||=o(n)$, for all $N\in[1,k]$. For
instance, in the human genome, $n \approx 3\times 10^9$ but
$||\mathrm{M}^{12}_{y_{1}\#\ldots\#y_{k}}|| \approx 10^6$. We consider a
constant-sized alphabet for stating our results. We show that all
$\mathrm{M}^{\ell}_{y_{1}},\ldots,\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{k}}$ can
be computed in
$\mathcal{O}(kn+\sum^{k}_{N=1}||\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{N}}||)$
total time using $\mathcal{O}(\mathrm{MaxIn}+\mathrm{MaxOut})$ space, where
$\mathrm{MaxIn}$ is the length of the longest word in $\{y_1,\ldots,y_{k}\}$
and
$\mathrm{MaxOut}=\max\{||\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{N}}||:N\in[1,k]\}$.
Proof-of-concept experimental results are also provided confirming our
theoretical findings and justifying our contribution.
</summary>
    <author>
      <name>Lorraine A. K. Ayad</name>
    </author>
    <author>
      <name>Golnaz Badkobeh</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Alice Héliou</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version accepted to DCC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.04785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.05224">
    <id>http://arxiv.org/abs/1902.05224v1</id>
    <updated>2019-02-14T05:09:57Z</updated>
    <published>2019-02-14T05:09:57Z</published>
    <title>Conversion from RLBWT to LZ77</title>
    <summary>  Converting a compressed format of a string into another compressed format
without an explicit decompression is one of the central research topics in
string processing. We discuss the problem of converting the run-length
Burrows-Wheeler Transform (RLBWT) of a string to Lempel-Ziv 77 (LZ77) phrases
of the reversed string. The first results with Policriti and Prezza's
conversion algorithm [Algorithmica 2018] were $O(n \log r)$ time and $O(r)$
working space for length of the string $n$, number of runs $r$ in the RLBWT,
and number of LZ77 phrases $z$. Recent results with Kempa's conversion
algorithm [SODA 2019] are $O(n / \log n + r \log^{9} n + z \log^{9} n)$ time
and $O(n / \log_{\sigma} n + r \log^{8} n)$ working space for the alphabet size
$\sigma$ of the RLBWT. In this paper, we present a new conversion algorithm by
improving Policriti and Prezza's conversion algorithm where dynamic data
structures for general purpose are used. We argue that these dynamic data
structures can be replaced and present new data structures for faster
conversion. The time and working space of our conversion algorithm with new
data structures are $O(n \min \{ \log \log n, \sqrt{\frac{\log r}{\log\log r}}
\})$ and $O(r)$, respectively.
</summary>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/1902.05224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.05224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.05166">
    <id>http://arxiv.org/abs/1902.05166v1</id>
    <updated>2019-02-13T23:47:23Z</updated>
    <published>2019-02-13T23:47:23Z</published>
    <title>Space-Efficient Data Structures for Lattices</title>
    <summary>  A lattice is a partially-ordered set in which every pair of elements has a
unique meet (greatest lower bound) and join (least upper bound). We present new
data structures for lattices that are simple, efficient, and nearly optimal in
terms of space complexity.
  Our first data structure can answer partial order queries in constant time
and find the meet or join of two elements in $O(n^{3/4})$ time, where $n$ is
the number of elements in the lattice. It occupies $O(n^{3/2}\log n)$ bits of
space, which is only a $\Theta(\log n)$ factor from the $\Theta(n^{3/2})$-bit
lower bound for storing lattices. The preprocessing time is $O(n^2)$.
  This structure admits a simple space-time tradeoff so that, for any $c \in
[\frac{1}{2}, 1]$, the data structure supports meet and join queries in
$O(n^{1-c/2})$ time, occupies $O(n^{1+c}\log n)$ bits of space, and can be
constructed in $O(n^2 + n^{1+3c/2})$ time.
  Our second data structure uses $O(n^{3/2}\log n)$ bits of space and supports
meet and join in $O(d \frac{\log n}{\log d})$ time, where $d$ is the maximum
degree of any element in the transitive reduction graph of the lattice. This
structure is much faster for lattices with low-degree elements.
  This paper also identifies an error in a long-standing solution to the
problem of representing lattices. We discuss the issue with this previous work.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Bryce Sandlund</name>
    </author>
    <author>
      <name>Corwin Sinnamon</name>
    </author>
    <link href="http://arxiv.org/abs/1902.05166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.05166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.01280">
    <id>http://arxiv.org/abs/1902.01280v1</id>
    <updated>2019-02-04T16:15:38Z</updated>
    <published>2019-02-04T16:15:38Z</published>
    <title>A New Class of Searchable and Provably Highly Compressible String
  Transformations</title>
    <summary>  The Burrows-Wheeler Transform is a string transformation that plays a
fundamental role for the design of self-indexing compressed data structures.
Over the years, researchers have successfully extended this transformation
outside the domains of strings. However, efforts to find non-trivial
alternatives of the original, now 25 years old, Burrows-Wheeler string
transformation have met limited success. In this paper we bring new lymph to
this area by introducing a whole new family of transformations that have all
the myriad virtues of the BWT: they can be computed and inverted in linear
time, they produce provably highly compressible strings, and they support
linear time pattern search directly on the transformed string. This new family
is a special case of a more general class of transformations based on context
adaptive alphabet orderings, a concept introduced here. This more general class
includes also the Alternating BWT, another invertible string transforms
recently introduced in connection with a generalization of Lyndon words.
</summary>
    <author>
      <name>Raffaele Giancarlo</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <link href="http://arxiv.org/abs/1902.01280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.05540">
    <id>http://arxiv.org/abs/1902.05540v1</id>
    <updated>2019-02-14T18:47:03Z</updated>
    <published>2019-02-14T18:47:03Z</published>
    <title>On long words avoiding Zimin patterns</title>
    <summary>  A pattern is encountered in a word if some infix of the word is the image of
the pattern under some non-erasing morphism. A pattern $p$ is unavoidable if,
over every finite alphabet, every sufficiently long word encounters $p$. A
theorem by Zimin and independently by Bean, Ehrenfeucht and McNulty states that
a pattern over $n$ distinct variables is unavoidable if, and only if, $p$
itself is encountered in the $n$-th Zimin pattern. Given an alphabet size $k$,
we study the minimal length $f(n,k)$ such that every word of length $f(n,k)$
encounters the $n$-th Zimin pattern. It is known that $f$ is upper-bounded by a
tower of exponentials. Our main result states that $f(n,k)$ is lower-bounded by
a tower of $n-3$ exponentials, even for $k=2$. To the best of our knowledge,
this improves upon a previously best-known doubly-exponential lower bound. As a
further result, we prove a doubly-exponential upper bound for encountering
Zimin patterns in the abelian sense.
</summary>
    <author>
      <name>Arnaud Carayol</name>
    </author>
    <author>
      <name>Stefan Göller</name>
    </author>
    <link href="http://arxiv.org/abs/1902.05540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.05540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.05651">
    <id>http://arxiv.org/abs/1902.05651v1</id>
    <updated>2019-02-15T00:19:24Z</updated>
    <published>2019-02-15T00:19:24Z</published>
    <title>Finite test sets for morphisms which are square-free on some of Thue's
  square-free ternary words</title>
    <summary>  Let $S$ be one of $\{aba,bcb\}$ and $\{aba, aca\}$, and let $w$ be an
infinite square-free word over $\Sigma=\{a,b,c\}$ with no factor in $S$.
Suppose that $f:\Sigma\rightarrow T^*$ is a non-erasing morphism. Word $f(w)$
is square-free if and only if $f$ is square-free on factors of $w$ of length 8
or less.
</summary>
    <author>
      <name>James D. Currie</name>
    </author>
    <link href="http://arxiv.org/abs/1902.05651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.05651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.06864">
    <id>http://arxiv.org/abs/1902.06864v3</id>
    <updated>2020-01-29T19:20:08Z</updated>
    <published>2019-02-19T02:32:32Z</published>
    <title>A sub-quadratic algorithm for the longest common increasing subsequence
  problem</title>
    <summary>  The Longest Common Increasing Subsequence problem (LCIS) is a natural variant
of the celebrated Longest Common Subsequence (LCS) problem. For LCIS, as well
as for LCS, there is an $O(n^2)$-time algorithm and a SETH-based conditional
lower bound of $O(n^{2-\varepsilon})$. For LCS, there is also the
Masek-Paterson $O(n^2 / \log{n})$-time algorithm, which does not seem to adapt
to LCIS in any obvious way. Hence, a natural question arises: does any
(slightly) sub-quadratic algorithm exist for the Longest Common Increasing
Subsequence problem? We answer this question positively, presenting a $O(n^2 /
\log^a{n})$-time algorithm for $a = \frac{1}{6}-o(1)$. The algorithm is not
based on memorizing small chunks of data (often used for logarithmic speedups,
including the "Four Russians Trick" in LCS), but rather utilizes a new
technique, bounding the number of significant symbol matches between the two
sequences.
</summary>
    <author>
      <name>Lech Duraj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages; STACS 2020 version -- heavily corrected from the previous
  one, might actually be readable</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.06864v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.06864v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.07599">
    <id>http://arxiv.org/abs/1902.07599v1</id>
    <updated>2019-02-20T15:34:06Z</updated>
    <published>2019-02-20T15:34:06Z</published>
    <title>Fast, Small, and Simple Document Listing on Repetitive Text Collections</title>
    <summary>  Document listing on string collections is the task of finding all documents
where a pattern appears. It is regarded as the most fundamental document
retrieval problem, and is useful in various applications. Many of the
fastest-growing string collections are composed of very similar documents, such
as versioned code and document collections, genome repositories, etc. Plain
pattern-matching indexes designed for repetitive text collections achieve
orders-of-magnitude reductions in space. Instead, there are not many analogous
indexes for document retrieval. In this paper we present a simple document
listing index for repetitive string collections of total length $n$ that lists
the $ndoc$ distinct documents where a pattern of length $m$ appears in time
$\mathcal{O}(m+ndoc \cdot \log n)$. We exploit the repetitiveness of the
document array (i.e., the suffix array coarsened to document identifiers) to
grammar-compress it while precomputing the answers to nonterminals, and store
them in grammar-compressed form as well. Our experimental results show that our
index sharply outperforms existing alternatives in the space/time tradeoff map.
</summary>
    <author>
      <name>Dustin Cobas</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1902.07599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.07599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.07353">
    <id>http://arxiv.org/abs/1902.07353v1</id>
    <updated>2019-02-19T23:58:11Z</updated>
    <published>2019-02-19T23:58:11Z</published>
    <title>In oder Aus</title>
    <summary>  Bloom filters are data structures used to determine set membership of
elements, with applications from string matching to networking and security
problems. These structures are favored because of their reduced memory
consumption and fast wallclock and asymptotic time bounds. Generally, Bloom
filters maintain constant membership query time, making them very fast in their
niche. However, they are limited in their lack of a removal operation, as well
as by their probabilistic nature. In this paper, we discuss various iterations
of and alternatives to the generic Bloom filter that have been researched and
implemented to overcome their inherent limitations. Bloom filters, especially
when used in conjunction with other data structures, are still powerful and
efficient data structures; we further discuss their use in industy and research
to optimize resource utilization.
</summary>
    <author>
      <name>Ethan Madison</name>
    </author>
    <author>
      <name>Zachary Zipper</name>
    </author>
    <link href="http://arxiv.org/abs/1902.07353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.07353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.08744">
    <id>http://arxiv.org/abs/1902.08744v2</id>
    <updated>2020-01-21T15:30:50Z</updated>
    <published>2019-02-23T06:10:34Z</published>
    <title>On Greedy Algorithms for Binary de Bruijn Sequences</title>
    <summary>  We propose a general greedy algorithm for binary de Bruijn sequences, called
Generalized Prefer-Opposite (GPO) Algorithm, and its modifications. By
identifying specific feedback functions and initial states, we demonstrate that
most previously-known greedy algorithms that generate binary de Bruijn
sequences are particular cases of our new algorithm.
</summary>
    <author>
      <name>Zuling Chang</name>
    </author>
    <author>
      <name>Martianus Frederic Ezerman</name>
    </author>
    <author>
      <name>Adamas Aqsa Fahreza</name>
    </author>
    <link href="http://arxiv.org/abs/1902.08744v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.08744v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.01088">
    <id>http://arxiv.org/abs/1902.01088v4</id>
    <updated>2019-07-09T07:53:59Z</updated>
    <published>2019-02-04T09:00:36Z</published>
    <title>Regular Languages meet Prefix Sorting</title>
    <summary>  Indexing strings via prefix (or suffix) sorting is, arguably, one of the most
successful algorithmic techniques developed in the last decades. Can indexing
be extended to languages? The main contribution of this paper is to initiate
the study of the sub-class of regular languages accepted by an automaton whose
states can be prefix-sorted. Starting from the recent notion of Wheeler graph
[Gagie et al., TCS 2017]-which extends naturally the concept of prefix sorting
to labeled graphs-we investigate the properties of Wheeler languages, that is,
regular languages admitting an accepting Wheeler finite automaton.
Interestingly, we characterize this family as the natural extension of regular
languages endowed with the co-lexicographic ordering: when sorted, the strings
belonging to a Wheeler language are partitioned into a finite number of
co-lexicographic intervals, each formed by elements from a single Myhill-Nerode
equivalence class. Moreover: (i) We show that every Wheeler NFA (WNFA) with $n$
states admits an equivalent Wheeler DFA (WDFA) with at most $2n-1-|\Sigma|$
states that can be computed in $O(n^3)$ time. This is in sharp contrast with
general NFAs. (ii) We describe a quadratic algorithm to prefix-sort a proper
superset of the WDFAs, a $O(n\log n)$-time online algorithm to sort acyclic
WDFAs, and an optimal linear-time offline algorithm to sort general WDFAs. By
contribution (i), our algorithms can also be used to index any WNFA at the
moderate price of doubling the automaton's size. (iii) We provide a
minimization theorem that characterizes the smallest WDFA recognizing the same
language of any input WDFA. The corresponding constructive algorithm runs in
optimal linear time in the acyclic case, and in $O(n\log n)$ time in the
general case. (iv) We show how to compute the smallest WDFA equivalent to any
acyclic DFA in nearly-optimal time.
</summary>
    <author>
      <name>Jarno Alanko</name>
    </author>
    <author>
      <name>Giovanna D'Agostino</name>
    </author>
    <author>
      <name>Alberto Policriti</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">added minimization theorems; uploaded submitted version; New version
  with new results (W-MH theorem, linear determinization), added author:
  Giovanna D'Agostino</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.01088v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01088v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.02187">
    <id>http://arxiv.org/abs/1902.02187v2</id>
    <updated>2019-09-20T11:09:45Z</updated>
    <published>2019-02-06T14:01:45Z</published>
    <title>Top Tree Compression of Tries</title>
    <summary>  We present a compressed representation of tries based on top tree compression
[ICALP 2013] that works on a standard, comparison-based, pointer machine model
of computation and supports efficient prefix search queries. Namely, we show
how to preprocess a set of strings of total length $n$ over an alphabet of size
$\sigma$ into a compressed data structure of worst-case optimal size
$O(n/\log_\sigma n)$ that given a pattern string $P$ of length $m$ determines
if $P$ is a prefix of one of the strings in time $O(\min(m\log \sigma,m + \log
n))$. We show that this query time is in fact optimal regardless of the size of
the data structure.
  Existing solutions either use $\Omega(n)$ space or rely on word RAM
techniques, such as tabulation, hashing, address arithmetic, or word-level
parallelism, and hence do not work on a pointer machine. Our result is the
first solution on a pointer machine that achieves worst-case $o(n)$ space.
Along the way, we develop several interesting data structures that work on a
pointer machine and are of independent interest. These include an optimal data
structures for random access to a grammar-compressed string and an optimal data
structure for a variant of the level ancestor problem.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract appeared at ISAAC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.02187v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02187v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.02499">
    <id>http://arxiv.org/abs/1902.02499v4</id>
    <updated>2019-03-02T03:02:40Z</updated>
    <published>2019-02-07T07:21:11Z</published>
    <title>A fast algorithm for constructing balanced binary search trees</title>
    <summary>  We suggest a new non-recursive algorithm for constructing a binary search
tree given an array of numbers. The algorithm has $O(N)$ time and $O(1)$ memory
complexity if the given array of $N$ numbers is sorted. The resulting tree is
of minimal height and can be transformed to a complete binary search tree
(retaining minimal height) with $O(\log N)$ time and $O(1)$ memory.
  The algorithm allows simple and effective parallelization.
</summary>
    <author>
      <name>Pavel S. Ruzankin</name>
    </author>
    <link href="http://arxiv.org/abs/1902.02499v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02499v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W01, 68W10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.02889">
    <id>http://arxiv.org/abs/1902.02889v5</id>
    <updated>2019-07-26T19:05:01Z</updated>
    <published>2019-02-07T23:56:50Z</published>
    <title>Space-efficient merging of succinct de Bruijn graphs</title>
    <summary>  We propose a new algorithm for merging succinct representations of de Bruijn
graphs introduced in [Bowe et al. WABI 2012]. Our algorithm is based on the
lightweight BWT merging approach by Holt and McMillan [Bionformatics 2014,
ACM-BCB 2014]. Our algorithm has the same asymptotic cost of the state of the
art tool for the same problem presented by Muggli et al. [bioRxiv 2017,
Bioinformatics 2019], but it uses less than half of its working space. A novel
important feature of our algorithm, not found in any of the existing tools, is
that it can compute the Variable Order succinct representation of the union
graph within the same asymptotic time/space bounds.
</summary>
    <author>
      <name>Lavinia Egidi</name>
    </author>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SPIRE'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.02889v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02889v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.03274">
    <id>http://arxiv.org/abs/1902.03274v1</id>
    <updated>2019-02-08T20:08:03Z</updated>
    <published>2019-02-08T20:08:03Z</published>
    <title>Faster Repetition-Aware Compressed Suffix Trees based on Block Trees</title>
    <summary>  Suffix trees are a fundamental data structure in stringology, but their space
usage, though linear, is an important problem for its applications. We design
and implement a new compressed suffix tree targeted to highly repetitive texts,
such as large genomic collections of the same species. Our suffix tree tree
builds on Block Trees, a recent Lempel-Ziv-bounded data structure that captures
the repetitiveness of its input. We use Block Trees to compress the topology of
the suffix tree, and augment the Block Tree nodes with data that speeds up
suffix tree navigation.
  Our compressed suffix tree is slightly larger than previous repetition-aware
suffix trees based on grammars, but outperforms them in time, often by orders
of magnitude. The component that represents the tree topology achieves a speed
comparable to that of general-purpose compressed trees, while using 2.3--10
times less space, and might be of interest in other scenarios.
</summary>
    <author>
      <name>Manuel Cáceres</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1902.03274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.4; H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.03568">
    <id>http://arxiv.org/abs/1902.03568v4</id>
    <updated>2019-10-01T11:58:08Z</updated>
    <published>2019-02-10T10:30:46Z</published>
    <title>Balancing Straight-Line Programs</title>
    <summary>  It is shown that a context-free grammar of size $m$ that produces a single
string $w$ (such a grammar is also called a string straight-line program) can
be transformed in linear time into a context-free grammar for $w$ of size
$\mathcal{O}(m)$, whose unique derivation tree has depth $\mathcal{O}(\log
|w|)$. This solves an open problem in the area of grammar-based compression.
Similar results are shown for two formalism for grammar-based tree compression:
top dags and forest straight-line programs. These balancing results are all
deduced from a single meta theorem stating that the depth of an algebraic
circuit over an algebra with a certain finite base property can be reduced to
$\mathcal{O}(\log n)$ with the cost of a constant multiplicative size increase.
Here, $n$ refers to the size of the unfolding (or unravelling) of the circuit.
</summary>
    <author>
      <name>Moses Ganardi</name>
    </author>
    <author>
      <name>Artur Jeż</name>
    </author>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extended abstract of this paper appears in the Proceedings of FOCS
  2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.03568v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03568v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68W32" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.03534">
    <id>http://arxiv.org/abs/1902.03534v1</id>
    <updated>2019-02-10T04:10:34Z</updated>
    <published>2019-02-10T04:10:34Z</published>
    <title>Set Cover in Sub-linear Time</title>
    <summary>  We study the classic set cover problem from the perspective of sub-linear
algorithms. Given access to a collection of $m$ sets over $n$ elements in the
query model, we show that sub-linear algorithms derived from existing
techniques have almost tight query complexities.
  On one hand, first we show an adaptation of the streaming algorithm presented
in Har-Peled et al. [2016] to the sub-linear query model, that returns an
$\alpha$-approximate cover using $\tilde{O}(m(n/k)^{1/(\alpha-1)} + nk)$
queries to the input, where $k$ denotes the value of a minimum set cover. We
then complement this upper bound by proving that for lower values of $k$, the
required number of queries is $\tilde{\Omega}(m(n/k)^{1/(2\alpha)})$, even for
estimating the optimal cover size. Moreover, we prove that even checking
whether a given collection of sets covers all the elements would require
$\Omega(nk)$ queries. These two lower bounds provide strong evidence that the
upper bound is almost tight for certain values of the parameter $k$.
  On the other hand, we show that this bound is not optimal for larger values
of the parameter $k$, as there exists a $(1+\varepsilon)$-approximation
algorithm with $\tilde{O}(mn/k\varepsilon^2)$ queries. We show that this bound
is essentially tight for sufficiently small constant $\varepsilon$, by
establishing a lower bound of $\tilde{\Omega}(mn/k)$ query complexity.
</summary>
    <author>
      <name>Piotr Indyk</name>
    </author>
    <author>
      <name>Sepideh Mahabadi</name>
    </author>
    <author>
      <name>Ronitt Rubinfeld</name>
    </author>
    <author>
      <name>Ali Vakilian</name>
    </author>
    <author>
      <name>Anak Yodpinyanee</name>
    </author>
    <link href="http://arxiv.org/abs/1902.03534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.03560">
    <id>http://arxiv.org/abs/1902.03560v1</id>
    <updated>2019-02-10T09:43:38Z</updated>
    <published>2019-02-10T09:43:38Z</published>
    <title>On the Complexity of Exact Pattern Matching in Graphs: Determinism and
  Zig-Zag Matching</title>
    <summary>  Exact pattern matching in labeled graphs is the problem of searching paths of
a graph $G=(V,E)$ that spell the same string as the given pattern $P[1..m]$.
This basic problem can be found at the heart of more complex operations on
variation graphs in computational biology, query operations in graph databases,
and analysis of heterogeneous networks, where the nodes of some paths must
match a sequence of labels or types. In our recent work we described a
conditional lower bound stating that the exact pattern matching problem in
labeled graphs cannot be solved in less than quadratic time, namely, $O(|E|^{1
- \epsilon} \, m)$ time or $O(|E| \, m^{1 - \epsilon})$ time for any constant
$\epsilon>0$, unless the Strong Exponential Time Hypothesis (SETH) is false.
The result holds even if node labels and pattern $P$ are drawn from a binary
alphabet, and $G$ is restricted to undirected graphs of maximum degree three or
directed acyclic graphs of maximum sum of indegree and outdegree three. It was
left open what happens on undirected graphs of maximum degree two, i.e., when
the pattern can have a zig-zag match in a (cyclic) bidirectional string. Also,
the reduction created a non-determistic directed acyclic graph, and it was left
open if determinism would make the problem easier. In this work, we show
through the Orthogonal Vectors hypothesis (OV) that the same conditional lower
bound holds even for these restricted cases.
</summary>
    <author>
      <name>Massimo Equi</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Alexandru I. Tomescu</name>
    </author>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Further developments on our previous work: arXiv:1901.05264</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.03560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.1; F.2.2; G.2.2; H.2.3; H.2.8; H.3.3; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.03285">
    <id>http://arxiv.org/abs/1902.03285v1</id>
    <updated>2019-02-08T20:42:07Z</updated>
    <published>2019-02-08T20:42:07Z</published>
    <title>Fast Sequence Segmentation using Log-Linear Models</title>
    <summary>  Sequence segmentation is a well-studied problem, where given a sequence of
elements, an integer K, and some measure of homogeneity, the task is to split
the sequence into K contiguous segments that are maximally homogeneous. A
classic approach to find the optimal solution is by using a dynamic program.
Unfortunately, the execution time of this program is quadratic with respect to
the length of the input sequence. This makes the algorithm slow for a sequence
of non-trivial length. In this paper we study segmentations whose measure of
goodness is based on log-linear models, a rich family that contains many of the
standard distributions. We present a theoretical result allowing us to prune
many suboptimal segmentations. Using this result, we modify the standard
dynamic program for one-dimensional log-linear models, and by doing so reduce
the computational time. We demonstrate empirically, that this approach can
significantly reduce the computational burden of finding the optimal
segmentation.
</summary>
    <author>
      <name>Nikolaj Tatti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10618-012-0301-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10618-012-0301-y" rel="related"/>
    <link href="http://arxiv.org/abs/1902.03285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.04427">
    <id>http://arxiv.org/abs/1902.04427v2</id>
    <updated>2019-05-29T09:59:23Z</updated>
    <published>2019-02-12T15:05:59Z</published>
    <title>Compressed Range Minimum Queries</title>
    <summary>  Given a string $S$ of $n$ integers in $[0,\sigma)$, a range minimum query
RMQ$(i, j)$ asks for the index of the smallest integer in $S[i \dots j]$. It is
well known that the problem can be solved with a succinct data structure of
size $2n + o(n)$ and constant query-time. In this paper we show how to
preprocess $S$ into a compressed representation that allows fast range minimum
queries. This allows for sublinear size data structures with logarithmic query
time. The most natural approach is to use string compression and construct a
data structure for answering range minimum queries directly on the compressed
string. We investigate this approach in the context of grammar compression. We
then consider an alternative approach. Instead of compressing $S$ using string
compression, we compress the Cartesian tree of $S$ using tree compression. We
show that this approach can be exponentially better than the former, is never
worse by more than an $O(\sigma)$ factor (i.e. for constant alphabets it is
never asymptotically worse), and can in fact be worse by an $\Omega(\sigma)$
factor.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Seungbum Jo</name>
    </author>
    <author>
      <name>Shay Mozes</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <link href="http://arxiv.org/abs/1902.04427v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04427v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.10453">
    <id>http://arxiv.org/abs/1901.10453v2</id>
    <updated>2019-11-29T13:15:51Z</updated>
    <published>2019-01-29T18:57:00Z</published>
    <title>Simulating the DNA String Graph in Succinct Space</title>
    <summary>  Converting a set of sequencing reads into a lossless compact data structure
that encodes all the relevant biological information is a major challenge. The
classical approaches are to build the string graph or the de Bruijn graph. Each
has advantages over the other depending on the application. Still, the ideal
setting would be to have an index of the reads that is easy to build and can be
adapted to any type of biological analysis. In this paper, we propose a new
data structure we call rBOSS, which gets close to that ideal. Our rBOSS is a de
Bruijn graph in practice, but it simulates any length up to k and can compute
overlaps of size at least m between the labels of the nodes, with k and m being
parameters. If we choose the parameter k equal to the size of the reads, then
we can simulate a complete string graph. As most BWT-based structures, rBOSS is
unidirectional, but it exploits the property of the DNA reverse complements to
simulate bi-directionality with some time-space trade-offs. We implemented a
genome assembler on top of rBOSS to demonstrate its usefulness. Our
experimental results show that using k = 100, rBOSS can assemble 185 MB of
reads in less than 15 minutes and using 110 MB in total. It produces contigs of
mean sizes over 10,000, which is twice the size obtained by using a pure de
Bruijn graph of fixed length k.
</summary>
    <author>
      <name>Diego Díaz-Domínguez</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.10453v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10453v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3, E.1, G.2.2" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3; E.1; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.10165">
    <id>http://arxiv.org/abs/1901.10165v2</id>
    <updated>2019-06-09T14:08:49Z</updated>
    <published>2019-01-29T08:33:09Z</published>
    <title>Fully-functional bidirectional Burrows-Wheeler indexes</title>
    <summary>  Given a string $T$ on an alphabet of size $\sigma$, we describe a
bidirectional Burrows-Wheeler index that takes $O(|T|\log{\sigma})$ bits of
space, and that supports the addition \emph{and removal} of one character, on
the left or right side of any substring of $T$, in constant time. Previously
known data structures that used the same space allowed constant-time addition
to any substring of $T$, but they could support removal only from specific
substrings of $T$. We also describe an index that supports bidirectional
addition and removal in $O(\log{\log{|T|}})$ time, and that occupies a number
of words proportional to the number of left and right extensions of the maximal
repeats of $T$. We use such fully-functional indexes to implement
bidirectional, frequency-aware, variable-order de Bruijn graphs in small space,
with no upper bound on their order, and supporting natural criteria for
increasing and decreasing the order during traversal.
</summary>
    <author>
      <name>Fabio Cunial</name>
    </author>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <link href="http://arxiv.org/abs/1901.10165v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10165v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.10045">
    <id>http://arxiv.org/abs/1901.10045v3</id>
    <updated>2019-04-10T08:45:20Z</updated>
    <published>2019-01-29T00:14:32Z</published>
    <title>Online Algorithms for Constructing Linear-size Suffix Trie</title>
    <summary>  The suffix trees are fundamental data structures for various kinds of string
processing. The suffix tree of a string $T$ of length $n$ has $O(n)$ nodes and
edges, and the string label of each edge is encoded by a pair of positions in
$T$. Thus, even after the tree is built, the input text $T$ needs to be kept
stored and random access to $T$ is still needed. The linear-size suffix tries
(LSTs), proposed by Crochemore et al. [Linear-size suffix tries, TCS
638:171-178, 2016], are a `stand-alone' alternative to the suffix trees.
Namely, the LST of a string $T$ of length $n$ occupies $O(n)$ total space, and
supports pattern matching and other tasks in the same efficiency as the suffix
tree without the need to store the input text $T$. Crochemore et al. proposed
an offline algorithm which transforms the suffix tree of $T$ into the LST of
$T$ in $O(n \log \sigma)$ time and $O(n)$ space, where $\sigma$ is the alphabet
size. In this paper, we present two types of online algorithms which `directly'
construct the LST, from right to left, and from left to right, without
constructing the suffix tree as an intermediate structure. Both algorithms
construct the LST incrementally when a new symbol is read, and do not access to
the previously read symbols. The right-to-left construction algorithm works in
$O(n \log \sigma)$ time and $O(n)$ space and the left-to-right construction
algorithm works in $O(n (\log \sigma + \log n / \log \log n))$ time and $O(n)$
space. The main feature of our algorithms is that the input text does not need
to be stored.
</summary>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Takuya Takagi</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.10045v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10045v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.10744">
    <id>http://arxiv.org/abs/1901.10744v3</id>
    <updated>2019-02-13T11:36:43Z</updated>
    <published>2019-01-30T10:17:52Z</published>
    <title>A study for Image compression using Re-Pair algorithm</title>
    <summary>  The compression is an important topic in computer science which allows we to
storage more amount of data on our data storage. There are several techniques
to compress any file. In this manuscript will be described the most important
algorithm to compress images such as JPEG and it will be compared with another
method to retrieve good reason to not use this method on images. So to compress
the text the most encoding technique known is the Huffman Encoding which it
will be explained in exhaustive way. In this manuscript will showed how to
compute a text compression method on images in particular the method and the
reason to choice a determinate image format against the other. The method
studied and analyzed in this manuscript is the Re-Pair algorithm which is
purely for grammatical context to be compress. At the and it will be showed the
good result of this application.
</summary>
    <author>
      <name>Pasquale De Luca</name>
    </author>
    <author>
      <name>Vincenzo Maria Russiello</name>
    </author>
    <author>
      <name>Raffaele Ciro Sannino</name>
    </author>
    <author>
      <name>Lorenzo Valente</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.10744v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10744v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.10722">
    <id>http://arxiv.org/abs/1901.10722v1</id>
    <updated>2019-01-30T09:31:31Z</updated>
    <published>2019-01-30T09:31:31Z</published>
    <title>Faster queries for longest substring palindrome after block edit</title>
    <summary>  Palindromes are important objects in strings which have been extensively
studied from combinatorial, algorithmic, and bioinformatics points of views.
Manacher [J. ACM 1975] proposed a seminal algorithm that computes the longest
substring palindromes (LSPals) of a given string in O(n) time, where n is the
length of the string. In this paper, we consider the problem of finding the
LSPal after the string is edited. We present an algorithm that uses O(n) time
and space for preprocessing, and answers the length of the LSPals in O(\ell +
\log \log n) time, after a substring in T is replaced by a string of arbitrary
length \ell. This outperforms the query algorithm proposed in our previous work
[CPM 2018] that uses O(\ell + \log n) time for each query.
</summary>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1901.10722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.10633">
    <id>http://arxiv.org/abs/1901.10633v2</id>
    <updated>2019-01-31T05:05:45Z</updated>
    <published>2019-01-30T01:26:07Z</published>
    <title>Computing runs on a trie</title>
    <summary>  A maximal repetition, or run, in a string, is a periodically maximal
substring whose smallest period is at most half the length of the substring. In
this paper, we consider runs that correspond to a path on a trie, or in other
words, on a rooted edge-labeled tree where the endpoints of the path must be a
descendant/ancestor of the other. For a trie with $n$ edges, we show that the
number of runs is less than $n$. We also show an $O(n\sqrt{\log n}\log \log n)$
time and $O(n)$ space algorithm for counting and finding the shallower endpoint
of all runs. We further show an $O(n\sqrt{\log n}\log^2\log n)$ time and $O(n)$
space algorithm for finding both endpoints of all runs.
</summary>
    <author>
      <name>Ryo Sugahara</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">corrections and improvements to version submitted to CPM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.10633v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10633v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.11305">
    <id>http://arxiv.org/abs/1901.11305v1</id>
    <updated>2019-01-31T11:15:04Z</updated>
    <published>2019-01-31T11:15:04Z</published>
    <title>Quasi-Linear-Time Algorithm for Longest Common Circular Factor</title>
    <summary>  We introduce the Longest Common Circular Factor (LCCF) problem in which,
given strings $S$ and $T$ of length $n$, we are to compute the longest factor
of $S$ whose cyclic shift occurs as a factor of $T$. It is a new similarity
measure, an extension of the classic Longest Common Factor. We show how to
solve the LCCF problem in $O(n \log^5 n)$ time.
</summary>
    <author>
      <name>Mai Alzamel</name>
    </author>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszyński</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <link href="http://arxiv.org/abs/1901.11305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.11305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.08833">
    <id>http://arxiv.org/abs/1810.08833v2</id>
    <updated>2019-05-29T05:10:14Z</updated>
    <published>2018-10-20T17:54:37Z</published>
    <title>MinJoin: Efficient Edit Similarity Joins via Local Hash Minima</title>
    <summary>  We study the problem of computing similarity joins under edit distance on a
set of strings. Edit similarity joins is a fundamental problem in databases,
data mining and bioinformatics. It finds important applications in data
cleaning and integration, collaborative filtering, genome sequence assembly,
etc. This problem has attracted significant attention in the past two decades.
However, all previous algorithms either cannot scale well to long strings and
large similarity thresholds, or suffer from imperfect accuracy.
  In this paper we propose a new algorithm for edit similarity joins using a
novel string partition based approach. We show mathematically that with high
probability our algorithm achieves a perfect accuracy, and runs in linear time
plus a data-dependent verification step. Experiments on real world datasets
show that our algorithm significantly outperforms the state-of-the-art
algorithms for edit similarity joins, and achieves perfect accuracy on all the
datasets that we have tested.
</summary>
    <author>
      <name>Haoyu Zhang</name>
    </author>
    <author>
      <name>Qin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to KDD 2019, full version, 22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.08833v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.08833v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.00257">
    <id>http://arxiv.org/abs/1902.00257v1</id>
    <updated>2019-02-01T10:06:52Z</updated>
    <published>2019-02-01T10:06:52Z</published>
    <title>An efficient sorting algorithm - Ultimate Heapsort(UHS)</title>
    <summary>  Motivated by the development of computer theory, the sorting algorithm is
emerging in an endless stream. Inspired by decrease and conquer method, we
propose a brand new sorting algorithmUltimately Heapsort. The algorithm
consists of two parts: building a heap and adjusting a heap. Through the
asymptotic analysis and experimental analysis of the algorithm, the time
complexity of our algorithm can reach O(nlogn) under any condition. Moreover,
its space complexity is only O(1). It can be seen that our algorithm is
superior to all previous algorithms.
</summary>
    <author>
      <name>Feiyang Chen</name>
    </author>
    <author>
      <name>Nan Chen</name>
    </author>
    <author>
      <name>Hanyang Mao</name>
    </author>
    <author>
      <name>Hanlin Hu</name>
    </author>
    <link href="http://arxiv.org/abs/1902.00257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.00257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1902.00216">
    <id>http://arxiv.org/abs/1902.00216v3</id>
    <updated>2019-09-04T04:37:56Z</updated>
    <published>2019-02-01T08:20:26Z</published>
    <title>An Extension of Linear-size Suffix Tries for Parameterized Strings</title>
    <summary>  In this paper, we propose a new indexing structure for parameterized strings
which we call PLSTs, by generalizing linear-size suffix tries for ordinary
strings. Two parameterized strings are said to match if there is a bijection on
the symbol set that makes the two coincide. PLSTs are applicable to the
parameterized pattern matching problem, which is to decide whether the input
parameterized text has a substring that matches the input parameterized
pattern. The size of PLSTs is linear in the text size, with which our algorithm
solves the parameterized pattern matching problem in linear time in the pattern
size. PLSTs can be seen as a compacted version of parameterized suffix tries
and a combination of linear-size suffix tries and parameterized suffix trees.
We experimentally show that PLSTs are more space efficient than parameterized
suffix trees for highly repetitive strings.
</summary>
    <author>
      <name>Katsuhito Nakashima</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.00216v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.00216v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.04068">
    <id>http://arxiv.org/abs/1901.04068v1</id>
    <updated>2019-01-13T21:28:35Z</updated>
    <published>2019-01-13T21:28:35Z</published>
    <title>Longest Common Subsequence on Weighted Sequences</title>
    <summary>  We consider the general problem of the Longest Common Subsequence (LCS) on
weighted sequences. Weighted sequences are an extension of classical strings,
where in each position every letter of the alphabet may occur with some
probability. In this paper we provide faster algorithms and prove a series of
hardness results for more general variants of the problem. In particular, we
provide an NP-Completeness result on the general variant of the problem instead
of the log-probability version used in earlier papers, already for alphabets of
size 2. Furthermore, we design an EPTAS for bounded alphabets, which is also an
improved, compared to previous results, PTAS for unbounded alphabets. These are
in a sense optimal, since it is known that there is no FPTAS for bounded
alphabets, while we prove that there is no EPTAS for unbounded alphabets.
Finally, we provide a matching conditional (under the Exponential Time
Hypothesis) lower bound for any PTAS. As a side note, we prove that it is
sufficient to work with only one threshold in the general variant of the
problem.
</summary>
    <author>
      <name>Evangelos Kipouridis</name>
    </author>
    <author>
      <name>Kostas Tsichlas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.04068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.04068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.03783">
    <id>http://arxiv.org/abs/1901.03783v1</id>
    <updated>2019-01-12T02:36:21Z</updated>
    <published>2019-01-12T02:36:21Z</published>
    <title>On Huang and Wong's Algorithm for Generalized Binary Split Trees</title>
    <summary>  Huang and Wong [5] proposed a polynomial-time dynamic-programming algorithm
for computing optimal generalized binary split trees. We show that their
algorithm is incorrect. Thus, it remains open whether such trees can be
computed in polynomial time. Spuler [11, 12] proposed modifying Huang and
Wong's algorithm to obtain an algorithm for a different problem: computing
optimal two-way-comparison search trees. We show that the dynamic program
underlying Spuler's algorithm is not valid, in that it does not satisfy the
necessary optimal-substructure property and its proposed recurrence relation is
incorrect. It remains unknown whether the algorithm is guaranteed to compute a
correct overall solution.
</summary>
    <author>
      <name>Marek Chrobak</name>
    </author>
    <author>
      <name>Mordecai Golin</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Neal E. Young</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal version of some results from arXiv:1505.00357</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.03783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.03783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P10, 68P30, 68W25, 94A45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; G.1.6; G.2.2; H.3.1; I.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.04358">
    <id>http://arxiv.org/abs/1901.04358v1</id>
    <updated>2019-01-14T15:08:16Z</updated>
    <published>2019-01-14T15:08:16Z</published>
    <title>Quotient Hash Tables - Efficiently Detecting Duplicates in Streaming
  Data</title>
    <summary>  This article presents the Quotient Hash Table (QHT) a new data structure for
duplicate detection in unbounded streams. QHTs stem from a corrected analysis
of streaming quotient filters (SQFs), resulting in a 33\% reduction in memory
usage for equal performance. We provide a new and thorough analysis of both
algorithms, with results of interest to other existing constructions.
  We also introduce an optimised version of our new data structure dubbed
Queued QHT with Duplicates (QQHTD).
  Finally we discuss the effect of adversarial inputs for hash-based duplicate
filters similar to QHT.
</summary>
    <author>
      <name>Rémi Géraud</name>
    </author>
    <author>
      <name>Marius Lombard-Platet</name>
    </author>
    <author>
      <name>David Naccache</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3297280.3297335</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3297280.3297335" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Shorter version was accepted at SIGAPP SAC '19</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.04358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.04358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.04911">
    <id>http://arxiv.org/abs/1901.04911v1</id>
    <updated>2019-01-15T16:24:07Z</updated>
    <published>2019-01-15T16:24:07Z</published>
    <title>Unconstrained Church-Turing thesis cannot possibly be true</title>
    <summary>  The Church-Turing thesis asserts that if a partial strings-to-strings
function is effectively computable then it is computable by a Turing machine.
  In the 1930s, when Church and Turing worked on their versions of the thesis,
there was a robust notion of algorithm. These traditional algorithms are known
also as classical or sequential. In the original thesis, effectively computable
meant computable by an effective classical algorithm. Based on an earlier
axiomatization of classical algorithms, the original thesis was proven in 2008.
  Since the 1930s, the notion of algorithm has changed dramatically. New
species of algorithms have been and are being introduced. We argue that the
generalization of the original thesis, where effectively computable means
computable by an effective algorithm of any species, cannot possibly be true.
</summary>
    <author>
      <name>Yuri Gurevich</name>
    </author>
    <link href="http://arxiv.org/abs/1901.04911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.04911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.05226">
    <id>http://arxiv.org/abs/1901.05226v3</id>
    <updated>2019-01-22T17:00:22Z</updated>
    <published>2019-01-16T10:53:33Z</published>
    <title>Space-Efficient Computation of the LCP Array from the Burrows-Wheeler
  Transform</title>
    <summary>  We show that the Longest Common Prefix Array of a text collection of total
size n on alphabet [1, {\sigma}] can be computed from the Burrows-Wheeler
transformed collection in O(n log {\sigma}) time using o(n log {\sigma}) bits
of working space on top of the input and output. Our result improves (on small
alphabets) and generalizes (to string collections) the previous solution from
Beller et al., which required O(n) bits of extra working space. We also show
how to merge the BWTs of two collections of total size n within the same time
and space bounds. The procedure at the core of our algorithms can be used to
enumerate suffix tree intervals in succinct space from the BWT, which is of
independent interest. An engineered implementation of our first algorithm on
DNA alphabet induces the LCP of a large (16 GiB) collection of short (100
bases) reads at a rate of 2.92 megabases per second using in total 1.5 Bytes
per base in RAM. Our second algorithm merges the BWTs of two short-reads
collections of 8 GiB each at a rate of 1.7 megabases per second and uses 0.625
Bytes per base in RAM. An extension of this algorithm that computes also the
LCP array of the merged collection processes the data at a rate of 1.48
megabases per second and uses 1.625 Bytes per base in RAM.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <link href="http://arxiv.org/abs/1901.05226v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05226v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.05264">
    <id>http://arxiv.org/abs/1901.05264v2</id>
    <updated>2019-02-08T16:33:56Z</updated>
    <published>2019-01-16T12:59:07Z</published>
    <title>On the Complexity of Exact Pattern Matching in Graphs: Binary Strings
  and Bounded Degree</title>
    <summary>  Exact pattern matching in labeled graphs is the problem of searching paths of
a graph $G=(V,E)$ that spell the same string as the pattern $P[1..m]$. This
basic problem can be found at the heart of more complex operations on variation
graphs in computational biology, of query operations in graph databases, and of
analysis operations in heterogeneous networks, where the nodes of some paths
must match a sequence of labels or types. We describe a simple conditional
lower bound that, for any constant $\epsilon>0$, an $O(|E|^{1 - \epsilon} \,
m)$-time or an $O(|E| \, m^{1 - \epsilon})$-time algorithm for exact pattern
matching on graphs, with node labels and patterns drawn from a binary alphabet,
cannot be achieved unless the Strong Exponential Time Hypothesis (SETH) is
false. The result holds even if restricted to undirected graphs of maximum
degree three or directed acyclic graphs of maximum sum of indegree and
outdegree three. Although a conditional lower bound of this kind can be somehow
derived from previous results (Backurs and Indyk, FOCS'16), we give a direct
reduction from SETH for dissemination purposes, as the result might interest
researchers from several areas, such as computational biology, graph database,
and graph mining, as mentioned before. Indeed, as approximate pattern matching
on graphs can be solved in $O(|E|\,m)$ time, exact and approximate matching are
thus equally hard (quadratic time) on graphs under the SETH assumption. In
comparison, the same problems restricted to strings have linear time vs
quadratic time solutions, respectively, where the latter ones have a matching
SETH lower bound on computing the edit distance of two strings (Backurs and
Indyk, STOC'15).
</summary>
    <author>
      <name>Massimo Equi</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <link href="http://arxiv.org/abs/1901.05264v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05264v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.1; F.2.2; G.2.2; H.2.3; H.2.8; H.3.3; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.06493">
    <id>http://arxiv.org/abs/1901.06493v1</id>
    <updated>2019-01-19T09:53:29Z</updated>
    <published>2019-01-19T09:53:29Z</published>
    <title>Dynamic Partition Bloom Filters: A Bounded False Positive Solution For
  Dynamic Set Membership (Extended Abstract)</title>
    <summary>  Dynamic Bloom filters (DBF) were proposed by Guo et. al. in 2010 to tackle
the situation where the size of the set to be stored compactly is not known in
advance or can change during the course of the application. We propose a novel
competitor to DBF with the following important property that DBF is not able to
achieve: our structure is able to maintain a bound on the false positive rate
for the set membership query across all possible sizes of sets that are stored
in it. The new data structure we propose is a dynamic structure that we call
Dynamic Partition Bloom filter (DPBF). DPBF is based on our novel concept of a
Bloom partition tree which is a tree structure with standard Bloom filters at
the leaves. DPBF is superior to standard Bloom filters because it can
efficiently handle a large number of unions and intersections of sets of
different sizes while controlling the false positive rate. This makes DPBF the
first structure to do so to the best of our knowledge. We provide theoretical
bounds comparing the false positive probability of DPBF to DBF.
</summary>
    <author>
      <name>Sidharth Negi</name>
    </author>
    <author>
      <name>Ameya Dubey</name>
    </author>
    <author>
      <name>Amitabha Bagchi</name>
    </author>
    <author>
      <name>Manish Yadav</name>
    </author>
    <author>
      <name>Nishant Yadav</name>
    </author>
    <author>
      <name>Jeetu Raj</name>
    </author>
    <link href="http://arxiv.org/abs/1901.06493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.06493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.07502">
    <id>http://arxiv.org/abs/1901.07502v1</id>
    <updated>2019-01-22T18:24:32Z</updated>
    <published>2019-01-22T18:24:32Z</published>
    <title>Palindromic Subsequences in Finite Words</title>
    <summary>  In 1999 Lyngs{\o} and Pedersen proposed a conjecture stating that every
binary circular word of length $n$ with equal number of zeros and ones has an
antipalindromic linear subsequence of length at least $\frac{2}{3}n$. No
progress over a trivial $\frac{1}{2}n$ bound has been achieved since then. We
suggest a palindromic counterpart to this conjecture and provide a non-trivial
infinite series of circular words which prove the upper bound of $\frac{2}{3}n$
for both conjectures at the same time. The construction also works for words
over an alphabet of size $k$ and gives rise to a generalization of the
conjecture by Lyngs{\o} and Pedersen. Moreover, we discuss some possible
strengthenings and weakenings of the named conjectures. We also propose two
similar conjectures for linear words and provide some evidences for them.
</summary>
    <author>
      <name>Clemens Müllner</name>
    </author>
    <author>
      <name>Andrew Ryzhikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to LATA 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.07502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.07502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.07809">
    <id>http://arxiv.org/abs/1901.07809v1</id>
    <updated>2019-01-23T10:42:25Z</updated>
    <published>2019-01-23T10:42:25Z</published>
    <title>A randomized strategy in the mirror game</title>
    <summary>  Alice and Bob take turns (with Alice playing first) in declaring numbers from
the set $[1,2N]$. If a player declares a number that was previously declared,
that player looses and the other player wins. If all numbers are declared
without repetition, the outcome is a tie. If both players have unbounded memory
and play optimally, then the game will be tied. Garg and Schneider [ITCS 2019]
showed that if Alice has unbounded memory, then Bob can secure a tie with $\log
N$ memory, whereas if Bob has unbounded memory, then Alice needs memory linear
in $N$ in order to secure a tie.
  Garg and Schneider also considered an {\em auxiliary matching} model in which
Alice gets as an additional input a random matching $M$ over the numbers
$[1,2N]$, and storing this input does not count towards the memory used by
Alice. They showed that is this model there is a strategy for Alice that ties
with probability at least $1 - \frac{1}{N}$, and uses only $O(\sqrt{N} (\log
N)^2)$ memory. We show how to modify Alice's strategy so that it uses only
$O((\log N)^3)$ space.
</summary>
    <author>
      <name>Uriel Feige</name>
    </author>
    <link href="http://arxiv.org/abs/1901.07809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.07809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.08544">
    <id>http://arxiv.org/abs/1901.08544v3</id>
    <updated>2020-02-14T19:22:44Z</updated>
    <published>2019-01-24T18:07:59Z</published>
    <title>Learning Space Partitions for Nearest Neighbor Search</title>
    <summary>  Space partitions of $\mathbb{R}^d$ underlie a vast and important class of
fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical
work on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,
Waingarten STOC 2018, FOCS 2018], we develop a new framework for building space
partitions reducing the problem to \emph{balanced graph partitioning} followed
by \emph{supervised classification.} We instantiate this general approach with
the KaHIP graph partitioner [Sanders, Schulz SEA 2013] and neural networks,
respectively, to obtain a new partitioning procedure called Neural
Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for
NNS, our experiments show that the partitions obtained by Neural LSH
consistently outperform partitions found by quantization-based and tree-based
methods.
</summary>
    <author>
      <name>Yihe Dong</name>
    </author>
    <author>
      <name>Piotr Indyk</name>
    </author>
    <author>
      <name>Ilya Razenshteyn</name>
    </author>
    <author>
      <name>Tal Wagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.08544v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.08544v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.10974">
    <id>http://arxiv.org/abs/1812.10974v1</id>
    <updated>2018-12-28T12:24:18Z</updated>
    <published>2018-12-28T12:24:18Z</published>
    <title>A Grammar-based Compressed Representation of 3D Trajectories</title>
    <summary>  Much research has been published about trajectory management on the ground or
at the sea, but compression or indexing of flight trajectories have usually
been less explored. However, air traffic management is a challenge because
airspace is becoming more and more congested, and large flight data collections
must be preserved and exploited for varied purposes. This paper proposes
3DGraCT, a new method for representing these flight trajectories. It extends
the GraCT compact data structure to cope with a third dimension (altitude),
while retaining its space/time complexities. 3DGraCT improves space
requirements of traditional spatio-temporal data structures by two orders of
magnitude, being competitive for the considered types of queries, even leading
the comparison for a particular one.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Adrián Gómez-Brandón</name>
    </author>
    <author>
      <name>Miguel A. Martínez-Prieto</name>
    </author>
    <author>
      <name>José R. Paramá</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-00479-8_9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-00479-8_9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">String Processing and Information Retrieval: 25th International
  Symposium, SPIRE 2018, Lima, Peru, October 9-11, 2018, Proceedings. Springer
  International Publishing. pp 102-116. ISBN: 9783030004781</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.10974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.10974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.11244">
    <id>http://arxiv.org/abs/1812.11244v1</id>
    <updated>2018-12-28T23:09:59Z</updated>
    <published>2018-12-28T23:09:59Z</published>
    <title>Using Compressed Suffix-Arrays for a Compact Representation of
  Temporal-Graphs</title>
    <summary>  Temporal graphs represent binary relationships that change along time. They
can model the dynamism of, for example, social and communication networks.
Temporal graphs are defined as sets of contacts that are edges tagged with the
temporal intervals when they are active. This work explores the use of the
Compressed Suffix Array (CSA), a well-known compact and self-indexed data
structure in the area of text indexing, to represent large temporal graphs. The
new structure, called Temporal Graph CSA (TGCSA), is experimentally compared
with the most competitive compact data structures in the state-of-the-art,
namely, EDGELOG and CET. The experimental results show that TGCSA obtains a
good space-time trade-off. It uses a reasonable space and is efficient for
solving complex temporal queries. Furthermore, TGCSA has wider expressive
capabilities than EDGELOG and CET, because it is able to represent temporal
graphs where contacts on an edge can temporally overlap.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Diego Caro</name>
    </author>
    <author>
      <name>Antonio Fariña</name>
    </author>
    <author>
      <name>M. Andrea Rodriguez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2018.07.023</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2018.07.023" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, Information Sciences</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Sciences Volume 465, October 2018, Pages 459-483</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.11244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.11244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.11249">
    <id>http://arxiv.org/abs/1812.11249v1</id>
    <updated>2018-12-28T23:57:14Z</updated>
    <published>2018-12-28T23:57:14Z</published>
    <title>A Compact Representation for Trips over Networks built on self-indexes</title>
    <summary>  Representing the movements of objects (trips) over a network in a compact way
while retaining the capability of exploiting such data effectively is an
important challenge of real applications. We present a new Compact Trip
Representation (CTR) that handles the spatio-temporal data associated with
users' trips over transportation networks. Depending on the network and types
of queries, nodes in the network can represent intersections, stops, or even
street segments.
  CTR represents separately sequences of nodes and the time instants when users
traverse these nodes. The spatial component is handled with a data structure
based on the well-known Compressed Suffix Array (CSA), which provides both a
compact representation and interesting indexing capabilities. The temporal
component is self-indexed with either a Hu-Tucker-shaped Wavelet-tree or a
Wavelet Matrix that solve range-interval queries efficiently. We show how CTR
can solve relevant counting-based spatial, temporal, and spatio-temporal
queries over large sets of trips. Experimental results show the space
requirements (around 50-70% of the space needed by a compact non-indexed
baseline) and query efficiency (most queries are solved in the range of 1-1000
microseconds) of CTR.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Antonio Fariña</name>
    </author>
    <author>
      <name>Daniil Galaktionov</name>
    </author>
    <author>
      <name>M. Andrea Rodriguez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.is.2018.06.010</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.is.2018.06.010" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Systems, Volume 78, November 2018, Pages 1-22</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.11249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.11249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.00718">
    <id>http://arxiv.org/abs/1901.00718v1</id>
    <updated>2019-01-03T13:58:15Z</updated>
    <published>2019-01-03T13:58:15Z</published>
    <title>Mergeable Dictionaries With Shifts</title>
    <summary>  We revisit the mergeable dictionaries with shift problem, where the goal is
to maintain a family of sets subject to search, split, merge, make-set, and
shift operations. The search, split, and make-set operations are the usual
well-known textbook operations. The merge operation merges two sets and the
shift operation adds or subtracts an integer from all elements in a set. Note
that unlike the join operation on standard balanced search tree structures,
such as AVL trees or 2-4 trees, the merge operation has no restriction on the
key space of the input sets and supports merging arbitrarily interleaved sets.
This problem is a key component in searching Lempel-Ziv compressed texts, in
the mergeable trees problem, and in the union-split-find problem.
  We present the first solution achieving O(log U) amortized time for all
operations, where {1, 2, ..., U} is the universe of the sets. This bound is
optimal when the size of the universe is polynomially bounded by the sum of the
sizes of the sets. Our solution is simple and based on a novel extension of
biased search trees.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Mikko Berggren Etienne</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <link href="http://arxiv.org/abs/1901.00718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.00718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.01665">
    <id>http://arxiv.org/abs/1901.01665v1</id>
    <updated>2019-01-07T04:48:34Z</updated>
    <published>2019-01-07T04:48:34Z</published>
    <title>Communication cost of consensus for nodes with limited memory</title>
    <summary>  Motivated by applications in blockchains and sensor networks, we consider a
model of $n$ nodes trying to reach consensus on their majority bit. Each node
$i$ is assigned a bit at time zero, and is a finite automaton with $m$ bits of
memory (i.e., $2^m$ states) and a Poisson clock. When the clock of $i$ rings,
$i$ can choose to communicate, and is then matched to a uniformly chosen node
$j$. The nodes $j$ and $i$ may update their states based on the state of the
other node. Previous work has focused on minimizing the time to consensus and
the probability of error, while our goal is minimizing the number of
communications. We show that when $m>3 \log\log\log(n)$, consensus can be
reached at linear communication cost, but this is impossible if
$m&lt;\log\log\log(n)$. We also study a synchronous variant of the model, where
our upper and lower bounds on $m$ for achieving linear communication cost are
$2\log\log\log(n)$ and $\log\log\log(n)$, respectively. A key step is to
distinguish when nodes can become aware of knowing the majority bit and stop
communicating. We show that this is impossible if their memory is too low.
</summary>
    <author>
      <name>Giulia Fanti</name>
    </author>
    <author>
      <name>Nina Holden</name>
    </author>
    <author>
      <name>Yuval Peres</name>
    </author>
    <author>
      <name>Gireeja Ranade</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">62 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.01665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.01665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.01825">
    <id>http://arxiv.org/abs/1901.01825v2</id>
    <updated>2019-01-11T08:26:58Z</updated>
    <published>2019-01-07T14:21:01Z</published>
    <title>Multiple Set Matching and Pre-Filtering with Bloom Multifilters</title>
    <summary>  Bloom filter is a space-efficient probabilistic data structure for checking
elements' membership in a set. Given multiple sets, however, a standard Bloom
filter is not sufficient when looking for the items to which an element or a
set of input elements belong to. In this article, we solve multiple set
matching problem by proposing two efficient Bloom Multifilters called Bloom
Matrix and Bloom Vector. Both of them are space efficient and answer queries
with a set of identifiers for multiple set matching problems. We show that the
space efficiency can be optimized further according to the distribution of
labels among multiple sets: Uniform and Zipf. While both of them are space
efficient, Bloom Vector can efficiently exploit Zipf distribution of data for
further space reduction. Our results also highlight that basic ADD and LOOKUP
operations on Bloom Matrix are faster than on Bloom Vector. However, Bloom
Matrix does not meet the theoretical false positive rate of less than $10^{-2}$
for LOOKUP operations if the represented data or the labels are not uniformly
distributed among the multiple sets. Consequently, we introduce \textit{Bloom
Test} which uses Bloom Matrix as the pre-filter structure to determine which
structure is suitable for improved performance with an arbitrary input dataset.
</summary>
    <author>
      <name>Francesco Concas</name>
    </author>
    <author>
      <name>Pengfei Xu</name>
    </author>
    <author>
      <name>Mohammad A. Hoque</name>
    </author>
    <author>
      <name>Jiaheng Lu</name>
    </author>
    <author>
      <name>Sasu Tarkoma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 11 figures, Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.01825v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.01825v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.01926">
    <id>http://arxiv.org/abs/1901.01926v1</id>
    <updated>2019-01-07T17:14:25Z</updated>
    <published>2019-01-07T17:14:25Z</published>
    <title>An in-place, subquadratic algorithm for permutation inversion</title>
    <summary>  We assume the permutation $\pi$ is given by an $n$-element array in which the
$i$-th element denotes the value $\pi(i)$. Constructing its inverse in-place
(i.e. using $O(\log{n})$ bits of additional memory) can be achieved in linear
time with a simple algorithm. Limiting the numbers that can be stored in our
array to the range $[1...n]$ still allows a straightforward $O(n^2)$ time
solution. The time complexity can be improved using randomization, but this
only improves the expected, not the pessimistic running time. We present a
deterministic algorithm that runs in $O(n^{3/2})$ time.
</summary>
    <author>
      <name>Grzegorz Guśpiel</name>
    </author>
    <link href="http://arxiv.org/abs/1901.01926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.01926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.01944">
    <id>http://arxiv.org/abs/1901.01944v1</id>
    <updated>2019-01-07T17:52:23Z</updated>
    <published>2019-01-07T17:52:23Z</published>
    <title>A Compact Representation of Raster Time Series</title>
    <summary>  The raster model is widely used in Geographic Information Systems to
represent data that vary continuously in space, such as temperatures,
precipitations, elevation, among other spatial attributes. In applications like
weather forecast systems, not just a single raster, but a sequence of rasters
covering the same region at different timestamps, known as a raster time
series, needs to be stored and queried. Compact data structures have proven
successful to provide space-efficient representations of rasters with query
capabilities. Hence, a naive approach to save space is to use such a
representation for each raster in a time series. However, in this paper we show
that it is possible to take advantage of the temporal locality that exists in a
raster time series to reduce the space necessary to store it while keeping
competitive query times for several types of queries.
</summary>
    <author>
      <name>Nataly Cruces</name>
    </author>
    <author>
      <name>Diego Seco</name>
    </author>
    <author>
      <name>Gilberto Gutiérrez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Data Compression Conference (DCC 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1901.01944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.01944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.03155">
    <id>http://arxiv.org/abs/1901.03155v2</id>
    <updated>2019-01-17T05:22:50Z</updated>
    <published>2019-01-10T13:41:19Z</published>
    <title>Entropy Bounds for Grammar-Based Tree Compressors</title>
    <summary>  The definition of $k^{th}$-order empirical entropy of strings is extended to
node labelled binary trees. A suitable binary encoding of tree straight-line
programs (that have been used for grammar-based tree compression before) is
shown to yield binary tree encodings of size bounded by the $k^{th}$-order
empirical entropy plus some lower order terms. This generalizes recent results
for grammar-based string compression to grammar-based tree compression.
</summary>
    <author>
      <name>Danny Hucke</name>
    </author>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Louisa Seelbach Benkner</name>
    </author>
    <link href="http://arxiv.org/abs/1901.03155v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.03155v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P30" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1901.03689">
    <id>http://arxiv.org/abs/1901.03689v1</id>
    <updated>2019-01-11T18:50:21Z</updated>
    <published>2019-01-11T18:50:21Z</published>
    <title>Depth First Search in the Semi-streaming Model</title>
    <summary>  Depth first search (DFS) tree is a fundamental data structure for solving
various graph problems. The classical DFS algorithm requires $O(m+n)$ time for
a graph having $n$ vertices and $m$ edges. In the streaming model, an algorithm
is allowed several passes (preferably single) over the input graph having a
restriction on the size of local space used.
  Trivially, a DFS tree can be computed using a single pass using $O(m)$ space.
In the semi-streaming model allowing $O(n)$ space, it can be computed in $O(n)$
passes, where each pass adds one vertex to the DFS tree. However, it remains an
open problem to compute a DFS tree using $o(n)$ passes using $o(m)$ space even
in any relaxed streaming environment.
  We present the first semi-streaming algorithms that compute a DFS tree of an
undirected graph in $o(n)$ passes using $o(m)$ space. We first describe an
extremely simple algorithm that requires at most $\lceil n/k\rceil$ passes
using $O(nk)$ space, where $k$ is any positive integer. We then improve this
algorithm by using more involved techniques to reduce the number of passes to
$\lceil h/k\rceil$ under similar space constraints, where $h$ is the height of
the computed DFS tree. In particular, this algorithm improves the bounds for
the case where the computed DFS tree is shallow (having $o(n)$ height).
Moreover, this algorithm is presented as a framework that allows the
flexibility of using any algorithm to maintain a DFS tree of a stored sparser
subgraph as a black box, which may be of independent interest. Both these
algorithms essentially demonstrate the existence of a trade-off between the
space and number of passes required for computing a DFS tree. Furthermore, we
evaluate these algorithms experimentally which reveals their exceptional
performance in practice. For both random and real graphs, they require merely a
few passes even when allowed just $O(n)$ space.
</summary>
    <author>
      <name>Shahbaz Khan</name>
    </author>
    <author>
      <name>Shashank K. Mehta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 6 Figures, STACS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.03689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.03689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; G.2.2; G.4; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.02570">
    <id>http://arxiv.org/abs/1812.02570v2</id>
    <updated>2019-06-26T18:14:23Z</updated>
    <published>2018-12-06T14:51:25Z</published>
    <title>ALLSAT compressed with wildcards: Partitionings and face-numbers of
  simplicial complexes</title>
    <summary>  Given the facets of a finite simplicial complex, we use wildcards to
enumerate its faces in compressed fashion. Our algorithm, coded in high-level
Mathematica code, compares favorably to the hardwired Mathematica command
BooleanConvert (=exclusive sums of products). As to running time, depending on
the particular problem instance, either method can excel. When our method wins
it may not just beat BooleanConvert but also SatisfiabilityCount (although the
latter is only asked to c o u n t). Independent of running time, our
compression rate is always higher.
</summary>
    <author>
      <name>Marcel Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.02570v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.02570v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.04261">
    <id>http://arxiv.org/abs/1812.04261v1</id>
    <updated>2018-12-11T08:17:52Z</updated>
    <published>2018-12-11T08:17:52Z</published>
    <title>LZRR: LZ77 Parsing with Right Reference</title>
    <summary>  Lossless data compression has been widely studied in computer science. One of
the most widely used lossless data compressions is Lempel-Zip(LZ) 77 parsing,
which achieves a high compression ratio. Bidirectional (a.k.a. macro) parsing
is a lossless data compression and computes a sequence of phrases copied from
another substring (target phrase) on either the left or the right position in
an input string. Gagie et al.(LATIN 2018) recently showed that a large gap
exists between the number of smallest bidirectional phrases of a given string
and that of LZ77 phrases. In addition, finding the smallest bidirectional parse
of a given text is NP-complete. Several variants of bidirectional parsing have
been proposed thus far, but no prior work for bidirectional parsing has
achieved high compression that is smaller than that of LZ77 phrasing for any
string. In this paper, we present the first practical bidirectional parsing
named LZ77 parsing with right reference (LZRR), in which the number of LZRR
phrases is theoretically guaranteed to be smaller than the number of LZ77
phrases. Experimental results using benchmark strings show the number of LZRR
phrases is approximately five percent smaller than that of LZ77 phrases.
</summary>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/1812.04261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.04261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1712.04886">
    <id>http://arxiv.org/abs/1712.04886v8</id>
    <updated>2019-05-20T01:00:47Z</updated>
    <published>2017-12-13T17:56:24Z</published>
    <title>Optimal Construction of Compressed Indexes for Highly Repetitive Texts</title>
    <summary>  We propose algorithms that, given the input string of length $n$ over integer
alphabet of size $\sigma$, construct the Burrows-Wheeler transform (BWT), the
permuted longest-common-prefix (PLCP) array, and the LZ77 parsing in
$O(n/\log_{\sigma}n+r\,{\rm polylog}\,n)$ time and working space, where $r$ is
the number of runs in the BWT of the input. These are the essential components
of many compressed indexes such as compressed suffix tree, FM-index, and
grammar and LZ77-based indexes, but also find numerous applications in sequence
analysis and data compression. The value of $r$ is a common measure of
repetitiveness that is significantly smaller than $n$ if the string is highly
repetitive. Since just accessing every symbol of the string requires
$\Omega(n/\log_{\sigma}n)$ time, the presented algorithms are time and space
optimal for inputs satisfying the assumption $n/r\in\Omega({\rm polylog}\,n)$
on the repetitiveness. For such inputs our result improves upon the currently
fastest general algorithms of Belazzougui (STOC 2014) and Munro et al. (SODA
2017) which run in $O(n)$ time and use $O(n/\log_{\sigma} n)$ working space. We
also show how to use our techniques to obtain optimal solutions on highly
repetitive data for other fundamental string processing problems such as:
Lyndon factorization, construction of run-length compressed suffix arrays, and
some classical "textbook" problems such as computing the longest substring
occurring at least some fixed number of times.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <link href="http://arxiv.org/abs/1712.04886v8" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04886v8" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.05306">
    <id>http://arxiv.org/abs/1812.05306v1</id>
    <updated>2018-12-13T08:04:09Z</updated>
    <published>2018-12-13T08:04:09Z</published>
    <title>Optimal Algorithm for Profiling Dynamic Arrays with Finite Values</title>
    <summary>  How can one quickly answer the most and top popular objects at any time,
given a large log stream in a system of billions of users? It is equivalent to
find the mode and top-frequent elements in a dynamic array corresponding to the
log stream. However, most existing work either restrain the dynamic array
within a sliding window, or do not take advantages of only one element can be
added or removed in a log stream. Therefore, we propose a profiling algorithm,
named S-Profile, which is of $O(1)$ time complexity for every updating of the
dynamic array, and optimal in terms of computational complexity. With the
profiling results, answering the queries on the statistics of dynamic array
becomes trivial and fast. With the experiments of various settings of dynamic
arrays, our accurate S-Profile algorithm outperforms the well-known methods,
showing at least 2X speedup to the heap based approach and 13X or larger
speedup to the balanced tree based approach.
</summary>
    <author>
      <name>Dingcheng Yang</name>
    </author>
    <author>
      <name>Wenjian Yu</name>
    </author>
    <author>
      <name>Junhui Deng</name>
    </author>
    <author>
      <name>Shenghua Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.05306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.05306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.06177">
    <id>http://arxiv.org/abs/1812.06177v5</id>
    <updated>2020-03-03T02:54:28Z</updated>
    <published>2018-12-14T21:40:09Z</published>
    <title>Simple Concurrent Labeling Algorithms for Connected Components</title>
    <summary>  We study a class of simple algorithms for concurrently computing the
connected components of an $n$-vertex, $m$-edge graph. Our algorithms are easy
to implement in either the COMBINING CRCW PRAM or the MPC computing model. For
two related algorithms in this class, we obtain $\Theta(\lg n)$ step and
$\Theta(m \lg n)$ work bounds. For two others, we obtain $O(\lg^2 n)$ step and
$O(m \lg^2 n)$ work bounds, which are tight for one of them. All our algorithms
are simpler than related algorithms in the literature. We also point out some
gaps and errors in the analysis of previous algorithms. Our results show that
even a basic problem like connected components still has secrets to reveal.
</summary>
    <author>
      <name>S. Cliff Liu</name>
    </author>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.06177v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.06177v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.06778">
    <id>http://arxiv.org/abs/1812.06778v3</id>
    <updated>2018-12-24T20:40:40Z</updated>
    <published>2018-12-13T22:40:34Z</published>
    <title>Minuet: A method to solve Sudoku puzzles by hand</title>
    <summary>  This paper presents a systematic method to solve difficult 9 x 9 Sudoku
puzzles by hand. While computer algorithms exist to solve these puzzles, these
algorithms are not good for human's to use because they involve too many steps
and require too much memory. For humans, all one can find in the literature are
individual tricks, which used together in ad hoc ways can be used to solve some
puzzles--but not all. To the author's knowledge, a systematic procedure made up
of well-defined steps that can be carried out by hand and solve all puzzles has
not been devised. This paper proposes one such technique--the "minuet" method.
It is based on a new system of markings and a new way of simplifying the
puzzles that can be easily carried out by hand--or by computer. The author has
solved hundreds of puzzles of the most difficult kind ("evil" in Sudoku's
parlance) and never found one that could not be solved. The average time to
solve one of these puzzles is slightly over 1 hour. It is conjectured that this
method can solve all well-posed 9 x 9 puzzles. The method's distinguishing
feature is a "minuet" strategy that is applied when the puzzle cannot be
further simplified with basic tricks. The strategy consists in concurrently
developing two potential solutions, sometimes alone and sometimes in concert as
if they were dancing a minuet.
</summary>
    <author>
      <name>Carlos F. Daganzo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; Working Paper, University of California, Berkeley</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.06778v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.06778v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.07030">
    <id>http://arxiv.org/abs/1812.07030v1</id>
    <updated>2018-12-17T19:59:44Z</updated>
    <published>2018-12-17T19:59:44Z</published>
    <title>A Fast Combination of AES Encryption and LZ4 Compression Algorithms</title>
    <summary>  From a long time ago, beside encryption of data and making it secure,
compression packing it was also important that could make transmission of data
faster. In the past years need for improvement of encryption and compression
for a fast and easy transmission is more necessary. In this paper, a new method
for combination of LZ4 combination and AES encryption algorithms for a fast and
easy packing, securing and compressing of data is presented. Choose of these
two algorithms was for some special features of them about aim of this paper.
This paper also is introducing a method for Parallelism of compression and
encryption in a special way for improvement of speed and security of data.
</summary>
    <author>
      <name>Saber Malekzadeh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.33644.56960</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.33644.56960" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the 3rd International Conference on applied research in
  Computer Science and Information Technology. in Farsi</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.07030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.07030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.08101">
    <id>http://arxiv.org/abs/1812.08101v1</id>
    <updated>2018-12-19T17:22:32Z</updated>
    <published>2018-12-19T17:22:32Z</published>
    <title>Efficient Representation and Counting of Antipower Factors in Words</title>
    <summary>  A $k$-antipower (for $k \ge 2$) is a concatenation of $k$ pairwise distinct
words of the same length. The study of antipower factors of a word was
initiated by Fici et al. (ICALP 2016) and first algorithms for computing
antipower factors were presented by Badkobeh et al. (Inf. Process. Lett.,
2018). We address two open problems posed by Badkobeh et al. Our main results
are algorithms for counting and reporting factors of a word which are
$k$-antipowers. They work in $\mathcal{O}(nk \log k)$ time and $\mathcal{O}(nk
\log k + C)$ time, respectively, where $C$ is the number of reported factors.
For $k=o(\sqrt{n/\log n})$, this improves the time complexity of
$\mathcal{O}(n^2/k)$ of the solution by Badkobeh et al. Our main algorithmic
tools are runs and gapped repeats. We also present an improved data structure
that checks, for a given factor of a word and an integer $k$, if the factor is
a $k$-antipower.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszyński</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper accepted to LATA 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.08101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.08101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.09094">
    <id>http://arxiv.org/abs/1812.09094v3</id>
    <updated>2019-11-02T18:17:50Z</updated>
    <published>2018-12-21T13:03:06Z</published>
    <title>A Simple Algorithm for Computing the Document Array</title>
    <summary>  We present a simple algorithm for computing the document array given a string
collection and its suffix array as input. Our algorithm runs in linear time
using constant additional space for strings from constant alphabets.
</summary>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <link href="http://arxiv.org/abs/1812.09094v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.09094v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.10950">
    <id>http://arxiv.org/abs/1812.10950v2</id>
    <updated>2019-02-15T14:16:45Z</updated>
    <published>2018-12-28T10:43:12Z</published>
    <title>Fast Breadth-First Search in Still Less Space</title>
    <summary>  It is shown that a breadth-first search in a directed or undirected graph
with $n$ vertices and $m$ edges can be carried out in $O(n+m)$ time with
$n\log_2 3+O((\log n)^2)$ bits of working memory.
</summary>
    <author>
      <name>Torben Hagerup</name>
    </author>
    <link href="http://arxiv.org/abs/1812.10950v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.10950v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.02457">
    <id>http://arxiv.org/abs/1811.02457v2</id>
    <updated>2019-05-29T12:39:32Z</updated>
    <published>2018-11-06T16:08:32Z</published>
    <title>Tunneling on Wheeler Graphs</title>
    <summary>  The Burrows-Wheeler Transform (BWT) is an important technique both in data
compression and in the design of compact indexing data structures. It has been
generalized from single strings to collections of strings and some classes of
labeled directed graphs, such as tries and de Bruijn graphs. The BWTs of
repetitive datasets are often compressible using run-length compression, but
recently Baier (CPM 2018) described how they could be even further compressed
using an idea he called tunneling. In this paper we show that tunneled BWTs can
still be used for indexing and extend tunneling to the BWTs of Wheeler graphs,
a framework that includes all the generalizations mentioned above.
</summary>
    <author>
      <name>Jarno Alanko</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Louisa Seelbach Benkner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 Pages, 1 figure. This research has received funding from the
  European Union's Horizon 2020 research and innovation programme under the
  Marie Sk{\l}odowska-Curie Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.02457v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.02457v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.02177">
    <id>http://arxiv.org/abs/1811.02177v1</id>
    <updated>2018-11-06T06:05:14Z</updated>
    <published>2018-11-06T06:05:14Z</published>
    <title>The entropy of lies: playing twenty questions with a liar</title>
    <summary>  `Twenty questions' is a guessing game played by two players: Bob thinks of an
integer between $1$ and $n$, and Alice's goal is to recover it using a minimal
number of Yes/No questions. Shannon's entropy has a natural interpretation in
this context. It characterizes the average number of questions used by an
optimal strategy in the distributional variant of the game: let $\mu$ be a
distribution over $[n]$, then the average number of questions used by an
optimal strategy that recovers $x\sim \mu$ is between $H(\mu)$ and $H(\mu)+1$.
We consider an extension of this game where at most $k$ questions can be
answered falsely. We extend the classical result by showing that an optimal
strategy uses roughly $H(\mu) + k H_2(\mu)$ questions, where $H_2(\mu) = \sum_x
\mu(x)\log\log\frac{1}{\mu(x)}$. This also generalizes a result by Rivest et
al. for the uniform distribution. Moreover, we design near optimal strategies
that only use comparison queries of the form `$x \leq c$?' for $c\in[n]$. The
usage of comparison queries lends itself naturally to the context of sorting,
where we derive sorting algorithms in the presence of adversarial noise.
</summary>
    <author>
      <name>Yuval Dagan</name>
    </author>
    <author>
      <name>Yuval Filmus</name>
    </author>
    <author>
      <name>Daniel Kane</name>
    </author>
    <author>
      <name>Shay Moran</name>
    </author>
    <link href="http://arxiv.org/abs/1811.02177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.02177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.02725">
    <id>http://arxiv.org/abs/1811.02725v3</id>
    <updated>2019-02-13T23:40:39Z</updated>
    <published>2018-11-07T02:11:39Z</published>
    <title>Static Data Structure Lower Bounds Imply Rigidity</title>
    <summary>  We show that static data structure lower bounds in the group (linear) model
imply semi-explicit lower bounds on matrix rigidity. In particular, we prove
that an explicit lower bound of $t \geq \omega(\log^2 n)$ on the cell-probe
complexity of linear data structures in the group model, even against
arbitrarily small linear space $(s= (1+\varepsilon)n)$, would already imply a
semi-explicit ($\bf P^{NP}\rm$) construction of rigid matrices with
significantly better parameters than the current state of art (Alon, Panigrahy
and Yekhanin, 2009). Our results further assert that polynomial ($t\geq
n^{\delta}$) data structure lower bounds against near-optimal space, would
imply super-linear circuit lower bounds for log-depth linear circuits (a
four-decade open question). In the succinct space regime $(s=n+o(n))$, we show
that any improvement on current cell-probe lower bounds in the linear model
would also imply new rigidity bounds. Our results rely on a new connection
between the "inner" and "outer" dimensions of a matrix (Paturi and Pudlak,
2006), and on a new reduction from worst-case to average-case rigidity, which
is of independent interest.
</summary>
    <author>
      <name>Zeev Dvir</name>
    </author>
    <author>
      <name>Alexander Golovnev</name>
    </author>
    <author>
      <name>Omri Weinstein</name>
    </author>
    <link href="http://arxiv.org/abs/1811.02725v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.02725v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.04596">
    <id>http://arxiv.org/abs/1811.04596v2</id>
    <updated>2019-02-18T05:29:30Z</updated>
    <published>2018-11-12T08:11:54Z</published>
    <title>MR-RePair: Grammar Compression based on Maximal Repeats</title>
    <summary>  We analyze the grammar generation algorithm of the RePair compression
algorithm and show the relation between a grammar generated by RePair and
maximal repeats. We reveal that RePair replaces step by step the most frequent
pairs within the corresponding most frequent maximal repeats. Then, we design a
novel variant of RePair, called MR-RePair, which substitutes the most frequent
maximal repeats at once instead of substituting the most frequent pairs
consecutively. We implemented MR-RePair and compared the size of the grammar
generated by MR-RePair to that by RePair on several text corpus. Our
experiments show that MR-RePair generates more compact grammars than RePair
does, especially for highly repetitive texts.
</summary>
    <author>
      <name>Isamu Furuya</name>
    </author>
    <author>
      <name>Takuya Takagi</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Takuya Kida</name>
    </author>
    <link href="http://arxiv.org/abs/1811.04596v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04596v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.04300">
    <id>http://arxiv.org/abs/1811.04300v1</id>
    <updated>2018-11-10T20:06:19Z</updated>
    <published>2018-11-10T20:06:19Z</published>
    <title>Efficiently Approximating Edit Distance Between Pseudorandom Strings</title>
    <summary>  We present an algorithm for approximating the edit distance
$\operatorname{ed}(x, y)$ between two strings $x$ and $y$ in time parameterized
by the degree to which one of the strings $x$ satisfies a natural
pseudorandomness property. The pseudorandomness model is asymmetric in that no
requirements are placed on the second string $y$, which may be constructed by
an adversary with full knowledge of $x$.
  We say that $x$ is \emph{$(p, B)$-pseudorandom} if all pairs $a$ and $b$ of
disjoint $B$-letter substrings of $x$ satisfy $\operatorname{ed}(a, b) \ge pB$.
Given parameters $p$ and $B$, our algorithm computes the edit distance between
a $(p, B)$-pseudorandom string $x$ and an arbitrary string $y$ within a factor
of $O(1/p)$ in time $\tilde{O}(nB)$, with high probability.
  Our algorithm is robust in the sense that it can handle a small portion of
$x$ being adversarial (i.e., not satisfying the pseudorandomness property). In
this case, the algorithm incurs an additive approximation error proportional to
the fraction of $x$ which behaves maliciously.
  The asymmetry of our pseudorandomness model has particular appeal for the
case where $x$ is a \emph{source string}, meaning that $\operatorname{ed}(x,
y)$ will be computed for many strings $y$. Suppose that one wishes to achieve
an $O(\alpha)$-approximation for each $\operatorname{ed}(x, y)$ computation,
and that $B$ is the smallest block-size for which the string $x$ is $(1/\alpha,
B)$-pseudorandom. We show that without knowing $B$ beforehand, $x$ may be
preprocessed in time $\tilde{O}(n^{1.5}\sqrt{B})$, so that all future
computations of the form $\operatorname{ed}(x, y)$ may be
$O(\alpha)$-approximated in time $\tilde{O}(nB)$. Furthermore, for the special
case where only a single $\operatorname{ed}(x, y)$ computation will be
performed, we show how to achieve an $O(\alpha)$-approximation in time
$\tilde{O}(n^{4/3}B^{2/3})$.
</summary>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SODA 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.04300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.06933">
    <id>http://arxiv.org/abs/1811.06933v1</id>
    <updated>2018-11-16T17:33:22Z</updated>
    <published>2018-11-16T17:33:22Z</published>
    <title>Efficient Construction of a Complete Index for Pan-Genomics Read
  Alignment</title>
    <summary>  While short read aligners, which predominantly use the FM-index, are able to
easily index one or a few human genomes, they do not scale well to indexing
databases containing thousands of genomes. To understand why, it helps to
examine the main components of the FM-index in more detail, which is a rank
data structure over the Burrows-Wheeler Transform (BWT) of the string that will
allow us to find the interval in the string's suffix array (SA) containing
pointers to starting positions of occurrences of a given pattern; second, a
sample of the SA that --- when used with the rank data structure --- allows us
access the SA. The rank data structure can be kept small even for large genomic
databases, by run-length compressing the BWT, but until recently there was no
means known to keep the SA sample small without greatly slowing down access to
the SA. Now that Gagie et al. (SODA 2018) have defined an SA sample that takes
about the same space as the run-length compressed BWT --- we have the design
for efficient FM-indexes of genomic databases but are faced with the problem of
building them. In 2018 we showed how to build the BWT of large genomic
databases efficiently (WABI 2018) but the problem of building Gagie et al.'s SA
sample efficiently was left open. We compare our approach to state-of-the-art
methods for constructing the SA sample, and demonstrate that it is the fastest
and most space-efficient method on highly repetitive genomic databases. Lastly,
we apply our method for indexing partial and whole human genomes, and show that
it improves over Bowtie with respect to both memory and time.
</summary>
    <author>
      <name>Alan Kuhnle</name>
    </author>
    <author>
      <name>Taher Mun</name>
    </author>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Ben Langmead</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <link href="http://arxiv.org/abs/1811.06933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.10498">
    <id>http://arxiv.org/abs/1811.10498v1</id>
    <updated>2018-11-26T16:45:30Z</updated>
    <published>2018-11-26T16:45:30Z</published>
    <title>An optimized Parallel Failure-less Aho-Corasick algorithm for DNA
  sequence matching</title>
    <summary>  The Aho-Corasick algorithm is multiple patterns searching algorithm running
sequentially in various applications like network intrusion detection and
bioinformatics for finding several input strings within a given large input
string. The parallel version of the Aho-Corasick algorithm is called as
Parallel Failure-less Aho-Corasick algorithm because it doesn't need failure
links like in the original Aho-Corasick algorithm. In this research, we
implemented an application specific parallel failureless Aho-Corasick algorithm
to the general purpose graphics processing unit by applying several cache
optimization techniques for matching DNA sequences. Our parallel Aho-Corasick
algorithm shows better performance than the available parallel Aho-Corasick
algorithm library due to its simplicity and optimized cache memory usage of
graphics processing units for matching DNA sequences.
</summary>
    <author>
      <name>Vajira Thambawita</name>
    </author>
    <author>
      <name>Roshan G. Ragel</name>
    </author>
    <author>
      <name>Dhammike Elkaduwe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICIAFS.2016.7946533</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICIAFS.2016.7946533" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, 4 tables, 5 graphs, 2016 IEEE International
  Conference on Information and Automation for Sustainability (ICIAfS)</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.10498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.10498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.12779">
    <id>http://arxiv.org/abs/1811.12779v6</id>
    <updated>2019-09-04T18:48:37Z</updated>
    <published>2018-11-30T13:16:24Z</published>
    <title>Optimal-Time Dictionary-Compressed Indexes</title>
    <summary>  We describe the first self-indexes able to count and locate pattern
occurrences in optimal time within a space bounded by the size of the most
popular dictionary compressors. To achieve this result we combine several
recent findings, including \emph{string attractors} --- new combinatorial
objects encompassing most known compressibility measures for highly repetitive
texts ---, and grammars based on \emph{locally-consistent parsing}.
  More in detail, let $\gamma$ be the size of the smallest attractor for a text
$T$ of length $n$. The measure $\gamma$ is an (asymptotic) lower bound to the
size of dictionary compressors based on Lempel--Ziv, context-free grammars, and
many others. The smallest known text representations in terms of attractors use
space $O(\gamma\log(n/\gamma))$, and our lightest indexes work within the same
asymptotic space. Let $\epsilon>0$ be a suitably small constant fixed at
construction time, $m$ be the pattern length, and $occ$ be the number of its
text occurrences. Our index counts pattern occurrences in
$O(m+\log^{2+\epsilon}n)$ time, and locates them in $O(m+(occ+1)\log^\epsilon
n)$ time. These times already outperform those of most dictionary-compressed
indexes, while obtaining the least asymptotic space for any index searching
within $O((m+occ)\,\textrm{polylog}\,n)$ time. Further, by increasing the space
to $O(\gamma\log(n/\gamma)\log^\epsilon n)$, we reduce the locating time to the
optimal $O(m+occ)$, and within $O(\gamma\log(n/\gamma)\log n)$ space we can
also count in optimal $O(m)$ time. No dictionary-compressed index had obtained
this time before. All our indexes can be constructed in $O(n)$ space and
$O(n\log n)$ expected time.
  As a byproduct of independent interest...
</summary>
    <author>
      <name>Anders Roy Christiansen</name>
    </author>
    <author>
      <name>Mikko Berggren Ettienne</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1811.12779v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.12779v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.00359">
    <id>http://arxiv.org/abs/1812.00359v2</id>
    <updated>2020-01-01T10:10:44Z</updated>
    <published>2018-12-02T09:11:20Z</published>
    <title>Locally Consistent Parsing for Text Indexing in Small Space</title>
    <summary>  We consider two closely related problems of text indexing in a sub-linear
working space. The first problem is the Sparse Suffix Tree (SST) construction
of a set of suffixes $B$ using only $O(|B|)$ words of space. The second problem
is the Longest Common Extension (LCE) problem, where for some parameter
$1\le\tau\le n$, the goal is to construct a data structure that uses $O(\frac
{n}{\tau})$ words of space and can compute the longest common prefix length of
any pair of suffixes. We show how to use ideas based on the Locally Consistent
Parsing technique, that was introduced by Sahinalp and Vishkin [STOC '94], in
some non-trivial ways in order to improve the known results for the above
problems. We introduce new Las-Vegas and deterministic algorithms for both
problems.
  We introduce the first Las-Vegas SST construction algorithm that takes $O(n)$
time. This is an improvement over the last result of Gawrychowski and Kociumaka
[SODA '17] who obtained $O(n)$ time for Monte-Carlo algorithm, and
$O(n\sqrt{\log |B|})$ time for Las-Vegas algorithm. In addition, we introduce a
randomized Las-Vegas construction for an LCE data structure that can be
constructed in linear time and answers queries in $O(\tau)$ time.
  For the deterministic algorithms, we introduce an SST construction algorithm
that takes $O(n\log \frac{n}{|B|})$ time (for $|B|=\Omega(\log n)$). This is
the first almost linear time, $O(n\cdot poly\log{n})$, deterministic SST
construction algorithm, where all previous algorithms take at least
$\Omega\left(\min\{n|B|,\frac{n^2}{|B|}\}\right)$ time. For the LCE problem, we
introduce a data structure that answers LCE queries in $O(\tau\sqrt{\log^*n})$
time, with $O(n\log\tau)$ construction time (for $\tau=O(\frac{n}{\log n})$).
This data structure improves both query time and construction time upon the
results of Tanimura et al. [CPM '16].
</summary>
    <author>
      <name>Or Birenzwige</name>
    </author>
    <author>
      <name>Shay Golan</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract to appear is SODA 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.00359v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.00359v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1812.01844">
    <id>http://arxiv.org/abs/1812.01844v1</id>
    <updated>2018-12-05T07:41:53Z</updated>
    <published>2018-12-05T07:41:53Z</published>
    <title>Improving Similarity Search with High-dimensional Locality-sensitive
  Hashing</title>
    <summary>  We propose a new class of data-independent locality-sensitive hashing (LSH)
algorithms based on the fruit fly olfactory circuit. The fundamental difference
of this approach is that, instead of assigning hashes as dense points in a low
dimensional space, hashes are assigned in a high dimensional space, which
enhances their separability. We show theoretically and empirically that this
new family of hash functions is locality-sensitive and preserves rank
similarity for inputs in any `p space. We then analyze different variations on
this strategy and show empirically that they outperform existing LSH methods
for nearest-neighbors search on six benchmark datasets. Finally, we propose a
multi-probe version of our algorithm that achieves higher performance for the
same query time, or conversely, that maintains performance of prior approaches
while taking significantly less indexing time and memory. Overall, our approach
leverages the advantages of separability provided by high-dimensional spaces,
while still remaining computationally efficient
</summary>
    <author>
      <name>Jaiyam Sharma</name>
    </author>
    <author>
      <name>Saket Navlakha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.01844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.01844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.12388">
    <id>http://arxiv.org/abs/1810.12388v1</id>
    <updated>2018-10-29T20:16:28Z</updated>
    <published>2018-10-29T20:16:28Z</published>
    <title>Distinct Sampling on Streaming Data with Near-Duplicates</title>
    <summary>  In this paper we study how to perform distinct sampling in the streaming
model where data contain near-duplicates. The goal of distinct sampling is to
return a distinct element uniformly at random from the universe of elements,
given that all the near-duplicates are treated as the same element. We also
extend the result to the sliding window cases in which we are only interested
in the most recent items. We present algorithms with provable theoretical
guarantees for datasets in the Euclidean space, and also verify their
effectiveness via an extensive set of experiments.
</summary>
    <author>
      <name>Jiecao Chen</name>
    </author>
    <author>
      <name>Qin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to PODS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.12388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.12388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.13187">
    <id>http://arxiv.org/abs/1810.13187v1</id>
    <updated>2018-10-31T09:53:50Z</updated>
    <published>2018-10-31T09:53:50Z</published>
    <title>Non-Empty Bins with Simple Tabulation Hashing</title>
    <summary>  We consider the hashing of a set $X\subseteq U$ with $|X|=m$ using a simple
tabulation hash function $h:U\to [n]=\{0,\dots,n-1\}$ and analyse the number of
non-empty bins, that is, the size of $h(X)$. We show that the expected size of
$h(X)$ matches that with fully random hashing to within low-order terms. We
also provide concentration bounds. The number of non-empty bins is a
fundamental measure in the balls and bins paradigm, and it is critical in
applications such as Bloom filters and Filter hashing. For example, normally
Bloom filters are proportioned for a desired low false-positive probability
assuming fully random hashing (see \url{en.wikipedia.org/wiki/Bloom_filter}).
Our results imply that if we implement the hashing with simple tabulation, we
obtain the same low false-positive probability for any possible input.
</summary>
    <author>
      <name>Anders Aamand</name>
    </author>
    <author>
      <name>Mikkel Thorup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at SODA'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.13187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.13187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.00833">
    <id>http://arxiv.org/abs/1811.00833v1</id>
    <updated>2018-11-02T12:18:23Z</updated>
    <published>2018-11-02T12:18:23Z</published>
    <title>Worst-Case Efficient Sorting with QuickMergesort</title>
    <summary>  The two most prominent solutions for the sorting problem are Quicksort and
Mergesort. While Quicksort is very fast on average, Mergesort additionally
gives worst-case guarantees, but needs extra space for a linear number of
elements. Worst-case efficient in-place sorting, however, remains a challenge:
the standard solution, Heapsort, suffers from a bad cache behavior and is also
not overly fast for in-cache instances.
  In this work we present median-of-medians QuickMergesort (MoMQuickMergesort),
a new variant of QuickMergesort, which combines Quicksort with Mergesort
allowing the latter to be implemented in place. Our new variant applies the
median-of-medians algorithm for selecting pivots in order to circumvent the
quadratic worst case. Indeed, we show that it uses at most $n \log n + 1.6n$
comparisons for $n$ large enough.
  We experimentally confirm the theoretical estimates and show that the new
algorithm outperforms Heapsort by far and is only around 10% slower than
Introsort (std::sort implementation of stdlibc++), which has a rather poor
guarantee for the worst case. We also simulate the worst case, which is only
around 10% slower than the average case. In particular, the new algorithm is a
natural candidate to replace Heapsort as a worst-case stopper in Introsort.
</summary>
    <author>
      <name>Stefan Edelkamp</name>
    </author>
    <author>
      <name>Armin Weiß</name>
    </author>
    <link href="http://arxiv.org/abs/1811.00833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.00833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.07422">
    <id>http://arxiv.org/abs/1810.07422v2</id>
    <updated>2019-08-07T14:13:28Z</updated>
    <published>2018-10-17T08:12:01Z</published>
    <title>Fast and Longest Rollercoasters</title>
    <summary>  For $k\geq 3$, a k-rollercoaster is a sequence of numbers whose every maximal
contiguous subsequence, that is increasing or decreasing, has length at least
$k$; $3$-rollercoasters are called simply rollercoasters. Given a sequence of
distinct numbers, we are interested in computing its maximum-length (not
necessarily contiguous) subsequence that is a $k$-rollercoaster. Biedl et al.
[ICALP 2018] have shown that each sequence of $n$ distinct real numbers
contains a rollercoaster of length at least $\lceil n/2\rceil$ for $n>7$, and
that a longest rollercoaster contained in such a sequence can be computed in
$O(n\log n)$-time. They have also shown that every sequence of $n\geq
(k-1)^2+1$ distinct real numbers contains a $k$-rollercoaster of length at
least $\frac{n}{2(k-1)}-\frac{3k}{2}$, and gave an $O(nk\log n)$-time algorithm
computing a longest $k$-rollercoaster in a sequence of length $n$.
  In this paper, we give an $O(nk^2)$-time algorithm computing the length of a
longest $k$-rollercoaster contained in a sequence of $n$ distinct real numbers;
hence, for constant $k$, our algorithm computes the length of a longest
$k$-rollercoaster in optimal linear time. The algorithm can be easily adapted
to output the respective $k$-rollercoaster. In particular, this improves the
results of Biedl et al. [ICALP 2018], by showing that a longest rollercoaster
can be computed in optimal linear time. We also present an algorithm computing
the length of a longest $k$-rollercoaster in $O(n \log^2 n)$-time, that is,
subquadratic even for large values of $k\leq n$. Again, the rollercoaster can
be easily retrieved. Finally, we show an $\Omega(n \log k)$ lower bound for the
number of comparisons in any comparison-based algorithm computing the length of
a longest $k$-rollercoaster.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Radosław Serafin</name>
    </author>
    <link href="http://arxiv.org/abs/1810.07422v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.07422v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.01472">
    <id>http://arxiv.org/abs/1811.01472v1</id>
    <updated>2018-11-05T01:24:30Z</updated>
    <published>2018-11-05T01:24:30Z</published>
    <title>RePair in Compressed Space and Time</title>
    <summary>  Given a string $T$ of length $N$, the goal of grammar compression is to
construct a small context-free grammar generating only $T$. Among existing
grammar compression methods, RePair (recursive paring) [Larsson and Moffat,
1999] is notable for achieving good compression ratios in practice. Although
the original paper already achieved a time-optimal algorithm to compute the
RePair grammar RePair($T$) in expected $O(N)$ time, the study to reduce its
working space is still active so that it is applicable to large-scale data. In
this paper, we propose the first RePair algorithm working in compressed space,
i.e., potentially $o(N)$ space for highly compressible texts. The key idea is
to give a new way to restructure an arbitrary grammar $S$ for $T$ into
RePair($T$) in compressed space and time. Based on the recompression technique,
we propose an algorithm for RePair($T$) in $O(\min(N, nm \log N))$ space and
expected $O(\min(N, nm \log N) m)$ time or $O(\min(N, nm \log N) \log \log N)$
time, where $n$ is the size of $S$ and $m$ is the number of variables in
RePair($T$). We implemented our algorithm running in $O(\min(N, nm \log N) m)$
time and show it can actually run in compressed space. We also present a new
approach to reduce the peak memory usage of existing RePair algorithms
combining with our algorithms, and show that the new approach outperforms, both
in computation time and space, the most space efficient linear-time RePair
implementation to date.
</summary>
    <author>
      <name>Kensuke Sakai</name>
    </author>
    <author>
      <name>Tatsuya Ohno</name>
    </author>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <link href="http://arxiv.org/abs/1811.01472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.01259">
    <id>http://arxiv.org/abs/1811.01259v1</id>
    <updated>2018-11-03T18:00:29Z</updated>
    <published>2018-11-03T18:00:29Z</published>
    <title>QuickXsort - A Fast Sorting Scheme in Theory and Practice</title>
    <summary>  QuickXsort is a highly efficient in-place sequential sorting scheme that
mixes Hoare's Quicksort algorithm with X, where X can be chosen from a wider
range of other known sorting algorithms, like Heapsort, Insertionsort and
Mergesort. Its major advantage is that QuickXsort can be in-place even if X is
not. In this work we provide general transfer theorems expressing the number of
comparisons of QuickXsort in terms of the number of comparisons of X. More
specifically, if pivots are chosen as medians of (not too fast) growing size
samples, the average number of comparisons of QuickXsort and X differ only by
$o(n)$-terms. For median-of-$k$ pivot selection for some constant $k$, the
difference is a linear term whose coefficient we compute precisely. For
instance, median-of-three QuickMergesort uses at most $n \lg n - 0.8358n +
O(\log n)$ comparisons.
  Furthermore, we examine the possibility of sorting base cases with some other
algorithm using even less comparisons. By doing so the average-case number of
comparisons can be reduced down to $n \lg n- 1.4106n + o(n)$ for a remaining
gap of only $0.0321n$ comparisons to the known lower bound (while using only
$O(\log n)$ additional space and $O(n \log n)$ time overall).
  Implementations of these sorting strategies show that the algorithms
challenge well-established library implementations like Musser's Introsort.
</summary>
    <author>
      <name>Stefan Edelkamp</name>
    </author>
    <author>
      <name>Armin Weiß</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <link href="http://arxiv.org/abs/1811.01259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.01248">
    <id>http://arxiv.org/abs/1811.01248v2</id>
    <updated>2019-03-31T11:33:54Z</updated>
    <published>2018-11-03T17:06:18Z</published>
    <title>Compressed Multiple Pattern Matching</title>
    <summary>  Given $d$ strings over the alphabet $\{0,1,\ldots,\sigma{-}1\}$, the
classical Aho--Corasick data structure allows us to find all $occ$ occurrences
of the strings in any text $T$ in $O(|T| + occ)$ time using $O(m\log m)$ bits
of space, where $m$ is the number of edges in the trie containing the strings.
Fix any constant $\varepsilon \in (0, 2)$. We describe a compressed solution
for the problem that, provided $\sigma \le m^\delta$ for a constant $\delta &lt;
1$, works in $O(|T| \frac{1}{\varepsilon} \log\frac{1}{\varepsilon} + occ)$
time, which is $O(|T| + occ)$ since $\varepsilon$ is constant, and occupies
$mH_k + 1.443 m + \varepsilon m + O(d\log\frac{m}{d})$ bits of space, for all
$0 \le k \le \max\{0,\alpha\log_\sigma m - 2\}$ simultaneously, where $\alpha
\in (0,1)$ is an arbitrary constant and $H_k$ is the $k$th-order empirical
entropy of the trie. Hence, we reduce the $3.443m$ term in the space bounds of
previously best succinct solutions to $(1.443 + \varepsilon)m$, thus solving an
open problem posed by Belazzougui. Further, we notice that $L =
\log\binom{\sigma (m+1)}{m} - O(\log(\sigma m))$ is a worst-case space lower
bound for any solution of the problem and, for $d = o(m)$ and constant
$\varepsilon$, our approach allows to achieve $L + \varepsilon m$ bits of
space, which gives an evidence that, for $d = o(m)$, the space of our data
structure is theoretically optimal up to the $\varepsilon m$ additive term and
it is hardly possible to eliminate the term $1.443m$. In addition, we refine
the space analysis of previous works by proposing a more appropriate definition
for $H_k$. We also simplify the construction for practice adapting the fixed
block compression boosting technique, then implement our data structure, and
conduct a number of experiments showing that it is comparable to the state of
the art in terms of time and is superior in space.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Nikita Sivukhin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.01248v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01248v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.01226">
    <id>http://arxiv.org/abs/1811.01226v1</id>
    <updated>2018-11-03T14:21:14Z</updated>
    <published>2018-11-03T14:21:14Z</published>
    <title>Multidimensional segment trees can do range queries and updates in
  logarithmic time</title>
    <summary>  Updating and querying on a range is a classical algorithmic problem with a
multitude of applications. The Segment Tree data structure is particularly
notable in handling the range query and update operations. A Segment Segment
Tree divides the range into disjoint segments and merges them together to
perform range queries and range updates elegantly. Although this data structure
is remarkably potent for 1-dimensional problems, it falls short in higher
dimensions. Lazy Propagation enables the operations to be computed in $O(logn)$
time in single dimension. However, the concept of lazy propagation could not be
translated to higher dimensional cases, which imposes a time complexity of
$O(n^{k-1} \; logn)$ for operations on $k$-dimensional data. In this paper, we
have made an attempt to emulate the idea of lazy propagation differently so
that it can be applied for 2-dimensional cases. Moreover, the proposed
modification is capable of performing any general aggregate function similar to
the original Segment Tree, and can also be extended to even higher dimensions.
Our proposed algorithm manages to perform range queries and updates in
$O(\log^2 n)$ time for a 2-dimensional problem, which becomes $O(\log^d n)$ for
a $d$-dimensional situation.
</summary>
    <author>
      <name>Nabil Ibtehaz</name>
    </author>
    <author>
      <name>M. Kaykobad</name>
    </author>
    <author>
      <name>M. Sohel Rahman</name>
    </author>
    <link href="http://arxiv.org/abs/1811.01226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.01209">
    <id>http://arxiv.org/abs/1811.01209v3</id>
    <updated>2018-12-21T18:00:33Z</updated>
    <published>2018-11-03T13:09:57Z</published>
    <title>Optimal Rank and Select Queries on Dictionary-Compressed Text</title>
    <summary>  We study the problem of supporting queries on a string $S$ of length $n$
within a space bounded by the size $\gamma$ of a string attractor for $S$.
Recent works showed that random access on $S$ can be supported in optimal
$O(\log(n/\gamma)/\log\log n)$ time within $O\left (\gamma\ \rm{polylog}\ n
\right)$ space. In this paper, we extend this result to \emph{rank} and
\emph{select} queries and provide lower bounds matching our upper bounds on
alphabets of polylogarithmic size. Our solutions are given in the form of a
space-time trade-off that is more general than the one previously known for
grammars and that improves existing bounds on LZ77-compressed text by a
$\log\log n$ time-factor in \emph{select} queries. We also provide matching
lower and upper bounds for \emph{partial sum} and \emph{predecessor} queries
within attractor-bounded space, and extend our lower bounds to encompass
navigation of dictionary-compressed tree representations.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">improved select bound with reduction to psum. Added lower bounds on
  trees</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.01209v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01209v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.02078">
    <id>http://arxiv.org/abs/1811.02078v2</id>
    <updated>2019-04-05T02:47:30Z</updated>
    <published>2018-11-05T23:05:04Z</published>
    <title>Optimal Succinct Rank Data Structure via Approximate Nonnegative Tensor
  Decomposition</title>
    <summary>  Given an $n$-bit array $A$, the succinct rank data structure problem asks to
construct a data structure using space $n+r$ bits for $r\ll n$, supporting rank
queries of form $\mathtt{rank}(x)=\sum_{i=0}^{x-1} A[i]$. In this paper, we
design a new succinct rank data structure with $r=n/(\log
n)^{\Omega(t)}+n^{1-c}$ and query time $O(t)$ for some constant $c>0$,
improving the previous best-known by Patrascu [Pat08], which has
$r=n/(\frac{\log n}{t})^{\Omega(t)}+\tilde{O}(n^{3/4})$ bits of redundancy. For
$r>n^{1-c}$, our space-time tradeoff matches the cell-probe lower bound by
Patrascu and Viola [PV10], which asserts that $r$ must be at least $n/(\log
n)^{O(t)}$. Moreover, one can avoid an $n^{1-c}$-bit lookup table when the data
structure is implemented in the cell-probe model, achieving $r=\lceil n/(\log
n)^{\Omega(t)}\rceil$. It matches the lower bound for the full range of
parameters.
  En route to our new data structure design, we establish an interesting
connection between succinct data structures and approximate nonnegative tensor
decomposition. Our connection shows that for specific problems, to construct a
space-efficient data structure, it suffices to approximate a particular tensor
by a sum of (few) nonnegative rank-$1$ tensors. For the rank problem, we
explicitly construct such an approximation, which yields an explicit
construction of the data structure.
</summary>
    <author>
      <name>Huacheng Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary version of this paper will appear in STOC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.02078v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.02078v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.05303">
    <id>http://arxiv.org/abs/1810.05303v1</id>
    <updated>2018-10-12T01:05:11Z</updated>
    <published>2018-10-12T01:05:11Z</published>
    <title>Parallelism in Randomized Incremental Algorithms</title>
    <summary>  In this paper we show that many sequential randomized incremental algorithms
are in fact parallel. We consider algorithms for several problems including
Delaunay triangulation, linear programming, closest pair, smallest enclosing
disk, least-element lists, and strongly connected components.
  We analyze the dependences between iterations in an algorithm, and show that
the dependence structure is shallow with high probability, or that by violating
some dependences the structure is shallow and the work is not increased
significantly. We identify three types of algorithms based on their dependences
and present a framework for analyzing each type. Using the framework gives
work-efficient polylogarithmic-depth parallel algorithms for most of the
problems that we study.
  This paper shows the first incremental Delaunay triangulation algorithm with
optimal work and polylogarithmic depth, which is an open problem for over 30
years. This result is important since most implementations of parallel Delaunay
triangulation use the incremental approach. Our results also improve bounds on
strongly connected components and least-elements lists, and significantly
simplify parallel algorithms for several problems.
</summary>
    <author>
      <name>Guy E. Blelloch</name>
    </author>
    <author>
      <name>Yan Gu</name>
    </author>
    <author>
      <name>Julian Shun</name>
    </author>
    <author>
      <name>Yihan Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1810.05303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.05303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.05313">
    <id>http://arxiv.org/abs/1810.05313v2</id>
    <updated>2018-10-26T01:30:24Z</updated>
    <published>2018-10-12T01:40:57Z</published>
    <title>Xorshift1024*, Xorshift1024+, Xorshift128+ and Xoroshiro128+ Fail
  Statistical Tests for Linearity</title>
    <summary>  L'Ecuyer &amp; Simard's Big Crush statistical test suite has revealed statistical
flaws in many popular random number generators including Marsaglia's Xorshift
generators. Vigna recently proposed some 64-bit variations on the Xorshift
scheme that are further scrambled (i.e., Xorshift1024*, Xorshift1024+,
Xorshift128+, Xoroshiro128+). Unlike their unscrambled counterparts, they pass
Big Crush when interleaving blocks of 32 bits for each 64-bit word (most
significant, least significant, most significant, least significant, etc.). We
report that these scrambled generators systematically fail Big
Crush---specifically the linear-complexity and matrix-rank tests that detect
linearity---when taking the 32 lowest-order bits in reverse order from each
64-bit word.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Melissa E. O'Neill</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cam.2018.10.019</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cam.2018.10.019" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational and Applied Mathematics 350, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.05313v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.05313v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.05753">
    <id>http://arxiv.org/abs/1810.05753v1</id>
    <updated>2018-10-12T23:01:24Z</updated>
    <published>2018-10-12T23:01:24Z</published>
    <title>Relative compression of trajectories</title>
    <summary>  We present RCT, a new compact data structure to represent trajectories of
objects. It is based on a relative compression technique called Relative
Lempel-Ziv (RLZ), which compresses sequences by applying an LZ77 encoding with
respect to an artificial reference. Combined with $O(z)$-sized data structures
on the sequence of phrases that allows to solve trajectory and spatio-temporal
queries efficiently. We plan that RCT improves in compression and time
performance the previous compressed representations in the state of the art.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Adrián Gómez-Brandón</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>José R. Paramá</name>
    </author>
    <link href="http://arxiv.org/abs/1810.05753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.05753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.07259">
    <id>http://arxiv.org/abs/1810.07259v1</id>
    <updated>2018-10-16T20:27:46Z</updated>
    <published>2018-10-16T20:27:46Z</published>
    <title>Nearly Optimal Space Efficient Algorithm for Depth First Search</title>
    <summary>  We design a space-efficient algorithm for performing depth-first search
traversal(DFS) of a graph in $O(m+n\log^* n)$ time using $O(n)$ bits of space.
While a normal DFS algorithm results in a DFS-tree (in case the graph is
connected), our space bounds do not permit us even to store such a tree.
However, our algorithm correctly outputs all edges of the DFS-tree.
  The previous best algorithm (which used $O(n)$ working space) took $O(m \log
n)$ time (Asano, Izumi, Kiyomi, Konagaya, Ono, Otachi, Schweitzer, Tarui,
Uehara (ISAAC 2014) and Elmasry, Hagerup, Krammer (STACS 2015)). The main open
question left behind in this area was to design faster algorithm for DFS using
$O(n)$ bits of space. Our algorithm answers this open question as it has a
nearly optimal running time (as the DFS takes $O(m+n)$ time even if there is no
space restriction).
</summary>
    <author>
      <name>Jayesh Choudhari</name>
    </author>
    <author>
      <name>Manoj Gupta</name>
    </author>
    <author>
      <name>Shivdutt Sharma</name>
    </author>
    <link href="http://arxiv.org/abs/1810.07259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.07259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.10635">
    <id>http://arxiv.org/abs/1810.10635v1</id>
    <updated>2018-10-24T21:43:32Z</updated>
    <published>2018-10-24T21:43:32Z</published>
    <title>Lower Bounds for Oblivious Data Structures</title>
    <summary>  An oblivious data structure is a data structure where the memory access
patterns reveals no information about the operations performed on it. Such data
structures were introduced by Wang et al. [ACM SIGSAC'14] and are intended for
situations where one wishes to store the data structure at an untrusted server.
One way to obtain an oblivious data structure is simply to run a classic data
structure on an oblivious RAM (ORAM). Until very recently, this resulted in an
overhead of $\omega(\lg n)$ for the most natural setting of parameters.
Moreover, a recent lower bound for ORAMs by Larsen and Nielsen [CRYPTO'18] show
that they always incur an overhead of at least $\Omega(\lg n)$ if used in a
black box manner. To circumvent the $\omega(\lg n)$ overhead, researchers have
instead studied classic data structure problems more directly and have obtained
efficient solutions for many such problems such as stacks, queues, deques,
priority queues and search trees. However, none of these data structures
process operations faster than $\Theta(\lg n)$, leaving open the question of
whether even faster solutions exist. In this paper, we rule out this
possibility by proving $\Omega(\lg n)$ lower bounds for oblivious stacks,
queues, deques, priority queues and search trees.
</summary>
    <author>
      <name>Riko Jacob</name>
    </author>
    <author>
      <name>Kasper Green Larsen</name>
    </author>
    <author>
      <name>Jesper Buus Nielsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at SODA'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.10635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.10635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.11308">
    <id>http://arxiv.org/abs/1810.11308v1</id>
    <updated>2018-10-26T13:17:20Z</updated>
    <published>2018-10-26T13:17:20Z</published>
    <title>Sub-O(log n) Out-of-Order Sliding-Window Aggregation</title>
    <summary>  Sliding-window aggregation summarizes the most recent information in a data
stream. Users specify how that summary is computed, usually as an associative
binary operator because this is the most general known form for which it is
possible to avoid naively scanning every window. For strictly in-order
arrivals, there are algorithms with $O(1)$ time per window change assuming
associative operators. Meanwhile, it is common in practice for streams to have
data arriving slightly out of order, for instance, due to clock drifts or
communication delays. Unfortunately, for out-of-order streams, one has to
resort to latency-prone buffering or pay $O(\log n)$ time per insert or evict,
where $n$ is the window size.
  This paper presents the design, analysis, and implementation of FiBA, a novel
sliding-window aggregation algorithm with an amortized upper bound of $O(\log
d)$ time per insert or evict, where $d$ is the distance of the inserted or
evicted value to the closer end of the window. This means $O(1)$ time for
in-order arrivals and nearly $O(1)$ time for slightly out-of-order arrivals,
with a smooth transition towards $O(\log n)$ as $d$ approaches $n$. We also
prove a matching lower bound on running time, showing optimality. Our algorithm
is as general as the prior state-of-the-art: it requires associativity, but not
invertibility nor commutativity. At the heart of the algorithm is a careful
combination of finger-searching techniques, lazy rebalancing, and
position-aware partial aggregates. We further show how to answer range queries
that aggregate subwindows for window sharing. Finally, our experimental
evaluation shows that FiBA performs well in practice and supports the
theoretical findings.
</summary>
    <author>
      <name>Kanat Tangwongsan</name>
    </author>
    <author>
      <name>Martin Hirzel</name>
    </author>
    <author>
      <name>Scott Schneider</name>
    </author>
    <link href="http://arxiv.org/abs/1810.11308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.11308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.11262">
    <id>http://arxiv.org/abs/1810.11262v1</id>
    <updated>2018-10-26T10:56:24Z</updated>
    <published>2018-10-26T10:56:24Z</published>
    <title>Some comments on the structure of the best known networks sorting 16
  elements</title>
    <summary>  We propose an explanation of the structure of the best known sorting networks
for 16 elements with respect to the complexity and to the depth due to Green
and van Voorhis.
</summary>
    <author>
      <name>Igor S. Sergeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7+7 pages, 6 figures (in English and Russian)</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.11262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.11262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.11863">
    <id>http://arxiv.org/abs/1810.11863v2</id>
    <updated>2019-04-09T23:37:33Z</updated>
    <published>2018-10-28T19:21:33Z</published>
    <title>Near-Linear Time Insertion-Deletion Codes and
  (1+$\varepsilon$)-Approximating Edit Distance via Indexing</title>
    <summary>  We introduce fast-decodable indexing schemes for edit distance which can be
used to speed up edit distance computations to near-linear time if one of the
strings is indexed by an indexing string $I$. In particular, for every length
$n$ and every $\varepsilon >0$, one can in near linear time construct a string
$I \in \Sigma'^n$ with $|\Sigma'| = O_{\varepsilon}(1)$, such that, indexing
any string $S \in \Sigma^n$, symbol-by-symbol, with $I$ results in a string $S'
\in \Sigma''^n$ where $\Sigma'' = \Sigma \times \Sigma'$ for which edit
distance computations are easy, i.e., one can compute a
$(1+\varepsilon)$-approximation of the edit distance between $S'$ and any other
string in $O(n \text{poly}(\log n))$ time.
  Our indexing schemes can be used to improve the decoding complexity of
state-of-the-art error correcting codes for insertions and deletions. In
particular, they lead to near-linear time decoding algorithms for the
insertion-deletion codes of [Haeupler, Shahrasbi; STOC `17] and faster decoding
algorithms for list-decodable insertion-deletion codes of [Haeupler, Shahrasbi,
Sudan; ICALP `18]. Interestingly, the latter codes are a crucial ingredient in
the construction of fast-decodable indexing schemes.
</summary>
    <author>
      <name>Bernhard Haeupler</name>
    </author>
    <author>
      <name>Aviad Rubinstein</name>
    </author>
    <author>
      <name>Amirbehshad Shahrasbi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3313276.3316371</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3313276.3316371" rel="related"/>
    <link href="http://arxiv.org/abs/1810.11863v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.11863v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.12047">
    <id>http://arxiv.org/abs/1810.12047v1</id>
    <updated>2018-10-29T10:37:21Z</updated>
    <published>2018-10-29T10:37:21Z</published>
    <title>Simple and Fast BlockQuicksort using Lomuto's Partitioning Scheme</title>
    <summary>  This paper presents simple variants of the BlockQuicksort algorithm described
by Edelkamp and Weiss (ESA 2016). The simplification is achieved by using
Lomuto's partitioning scheme instead of Hoare's crossing pointer technique to
partition the input. To achieve a robust sorting algorithm that works well on
many different input types, the paper introduces a novel two-pivot variant of
Lomuto's partitioning scheme. A surprisingly simple twist to the generic
two-pivot quicksort approach makes the algorithm robust. The paper provides an
analysis of the theoretical properties of the proposed algorithms and compares
them to their competitors. The analysis shows that Lomuto-based approaches
incur a higher average sorting cost than the Hoare-based approach of
BlockQuicksort. Moreover, the analysis is particularly useful to reason about
pivot choices that suit the two-pivot approach. An extensive experimental study
shows that, despite their worse theoretical behavior, the simpler variants
perform as well as the original version of BlockQuicksort.
</summary>
    <author>
      <name>Martin Aumüller</name>
    </author>
    <author>
      <name>Nikolaj Hass</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ALENEX 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.12047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.12047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.12322">
    <id>http://arxiv.org/abs/1810.12322v1</id>
    <updated>2018-10-29T18:02:20Z</updated>
    <published>2018-10-29T18:02:20Z</published>
    <title>Sesquickselect: One and a half pivots for cache-efficient selection</title>
    <summary>  Because of unmatched improvements in CPU performance, memory transfers have
become a bottleneck of program execution. As discovered in recent years, this
also affects sorting in internal memory. Since partitioning around several
pivots reduces overall memory transfers, we have seen renewed interest in
multiway Quicksort. Here, we analyze in how far multiway partitioning helps in
Quickselect.
  We compute the expected number of comparisons and scanned elements
(approximating memory transfers) for a generic class of (non-adaptive) multiway
Quickselect and show that three or more pivots are not helpful, but two pivots
are. Moreover, we consider "adaptive" variants which choose partitioning and
pivot-selection methods in each recursive step from a finite set of
alternatives depending on the current (relative) sought rank. We show that
"Sesquickselect", a new Quickselect variant that uses either one or two pivots,
makes better use of small samples w.r.t. memory transfers than other
Quickselect variants.
</summary>
    <author>
      <name>Conrado Martínez</name>
    </author>
    <author>
      <name>Markus Nebel</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/1.9781611975505.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/1.9781611975505.6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">appears in ANALCO 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.12322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.12322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.01238">
    <id>http://arxiv.org/abs/1810.01238v1</id>
    <updated>2018-10-02T13:42:21Z</updated>
    <published>2018-10-02T13:42:21Z</published>
    <title>Sketching, Streaming, and Fine-Grained Complexity of (Weighted) LCS</title>
    <summary>  We study sketching and streaming algorithms for the Longest Common
Subsequence problem (LCS) on strings of small alphabet size $|\Sigma|$. For the
problem of deciding whether the LCS of strings $x,y$ has length at least $L$,
we obtain a sketch size and streaming space usage of $\mathcal{O}(L^{|\Sigma| -
1} \log L)$.
  We also prove matching unconditional lower bounds.
  As an application, we study a variant of LCS where each alphabet symbol is
equipped with a weight that is given as input, and the task is to compute a
common subsequence of maximum total weight. Using our sketching algorithm, we
obtain an $\mathcal{O}(\textrm{min}\{nm, n + m^{{\lvert \Sigma
\rvert}}\})$-time algorithm for this problem, on strings $x,y$ of length $n,m$,
with $n \ge m$. We prove optimality of this running time up to lower order
factors, assuming the Strong Exponential Time Hypothesis.
</summary>
    <author>
      <name>Karl Bringmann</name>
    </author>
    <author>
      <name>Bhaskar Ray Chaudhury</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in FSTTCS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.01238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Computer Science" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.01676">
    <id>http://arxiv.org/abs/1810.01676v3</id>
    <updated>2019-07-23T07:48:46Z</updated>
    <published>2018-10-03T10:38:59Z</published>
    <title>Approximating Approximate Pattern Matching</title>
    <summary>  Given a text $T$ of length $n$ and a pattern $P$ of length $m$, the
approximate pattern matching problem asks for computation of a particular
\emph{distance} function between $P$ and every $m$-substring of $T$. We
consider a $(1\pm\varepsilon)$ multiplicative approximation variant of this
problem, for $\ell_p$ distance function. In this paper, we describe two
$(1+\varepsilon)$-approximate algorithms with a runtime of
$\widetilde{O}(\frac{n}{\varepsilon})$ for all (constant) non-negative values
of $p$. For constant $p \ge 1$ we show a deterministic
$(1+\varepsilon)$-approximation algorithm. Previously, such run time was known
only for the case of $\ell_1$ distance, by Gawrychowski and Uzna\'nski [ICALP
2018] and only with a randomized algorithm. For constant $0 \le p \le 1$ we
show a randomized algorithm for the $\ell_p$, thereby providing a smooth
tradeoff between algorithms of Kopelowitz and Porat [FOCS~2015, SOSA~2018] for
Hamming distance (case of $p=0$) and of Gawrychowski and Uzna\'nski for
$\ell_1$ distance.
</summary>
    <author>
      <name>Jan Studený</name>
    </author>
    <author>
      <name>Przemysław Uznański</name>
    </author>
    <link href="http://arxiv.org/abs/1810.01676v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01676v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.01726">
    <id>http://arxiv.org/abs/1810.01726v2</id>
    <updated>2019-03-27T13:35:14Z</updated>
    <published>2018-10-03T13:21:14Z</published>
    <title>Fault Tolerant and Fully Dynamic DFS in Undirected Graphs: Simple Yet
  Efficient</title>
    <summary>  We present an algorithm for a fault tolerant Depth First Search (DFS) Tree in
an undirected graph. This algorithm is drastically simpler than the current
state-of-the-art algorithms for this problem, uses optimal space and optimal
preprocessing time, and still achieves better time complexity. This algorithm
also leads to a better time complexity for maintaining a DFS tree in a fully
dynamic environment.
</summary>
    <author>
      <name>Surender Baswana</name>
    </author>
    <author>
      <name>Shiv Kumar Gupta</name>
    </author>
    <author>
      <name>Ayush Tulsyan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.01726v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01726v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.01785">
    <id>http://arxiv.org/abs/1810.01785v1</id>
    <updated>2018-10-03T15:07:51Z</updated>
    <published>2018-10-03T15:07:51Z</published>
    <title>Weighted dynamic finger in binary search trees</title>
    <summary>  It is shown that the online binary search tree data structure GreedyASS
performs asymptotically as well on a sufficiently long sequence of searches as
any static binary search tree where each search begins from the previous search
(rather than the root). This bound is known to be equivalent to assigning each
item $i$ in the search tree a positive weight $w_i$ and bounding the search
cost of an item in the search sequence $s_1,\ldots,s_m$ by $$O\left(1+ \log
\frac{\displaystyle \sum_{\min(s_{i-1},s_i) \leq x \leq
\max(s_{i-1},s_i)}w_x}{\displaystyle \min(w_{s_i},w_{s_{i-1}})} \right)$$
amortized. This result is the strongest finger-type bound to be proven for
binary search trees. By setting the weights to be equal, one observes that our
bound implies the dynamic finger bound. Compared to the previous proof of the
dynamic finger bound for Splay trees, our result is significantly shorter,
stronger, simpler, and has reasonable constants.
</summary>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An earlier version of this work appeared in the Proceedings of the
  Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.01785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.02270">
    <id>http://arxiv.org/abs/1810.02270v1</id>
    <updated>2018-10-04T15:11:47Z</updated>
    <published>2018-10-04T15:11:47Z</published>
    <title>Compound Binary Search Tree and Algorithms</title>
    <summary>  The Binary Search Tree (BST) is average in computer science which supports a
compact data structure in memory and oneself even conducts a row of quick
algorithms, by which people often apply it in dynamical circumstance. Besides
these edges, it is also with weakness on its own structure specially with poor
performance at worst case. In this paper, we will develop this data structure
into a synthesis to show a series of novel features residing in. Of that, there
are new methods invented for raising the performance and efficiency
nevertheless some existing ones in logarithm or linear time.
</summary>
    <author>
      <name>Yong Tan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages with words into 6300</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.02270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.02270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05C05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.7; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.02099">
    <id>http://arxiv.org/abs/1810.02099v1</id>
    <updated>2018-10-04T08:39:06Z</updated>
    <published>2018-10-04T08:39:06Z</published>
    <title>Longest Property-Preserved Common Factor</title>
    <summary>  In this paper we introduce a new family of string processing problems. We are
given two or more strings and we are asked to compute a factor common to all
strings that preserves a specific property and has maximal length. Here we
consider three fundamental string properties: square-free factors, periodic
factors, and palindromic factors under three different settings, one per
property. In the first setting, we are given a string $x$ and we are asked to
construct a data structure over $x$ answering the following type of on-line
queries: given string $y$, find a longest square-free factor common to $x$ and
$y$. In the second setting, we are given $k$ strings and an integer $1 &lt; k'\leq
k$ and we are asked to find a longest periodic factor common to at least $k'$
strings. In the third setting, we are given two strings and we are asked to
find a longest palindromic factor common to the two strings. We present
linear-time solutions for all settings. We anticipate that our paradigm can be
extended to other string properties or settings.
</summary>
    <author>
      <name>Lorraine A. K Ayad</name>
    </author>
    <author>
      <name>Giulia Bernardini</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Nadia Pisanti</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of SPIRE 2018 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.02099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.02099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.02227">
    <id>http://arxiv.org/abs/1810.02227v1</id>
    <updated>2018-10-04T14:08:57Z</updated>
    <published>2018-10-04T14:08:57Z</published>
    <title>Randen - fast backtracking-resistant random generator with
  AES+Feistel+Reverie</title>
    <summary>  Algorithms that rely on a pseudorandom number generator often lose their
performance guarantees when adversaries can predict the behavior of the
generator. To protect non-cryptographic applications against such attacks, we
propose 'strong' pseudorandom generators characterized by two properties:
computationally indistinguishable from random and backtracking-resistant. Some
existing cryptographically secure generators also meet these criteria, but they
are too slow to be accepted for general-purpose use. We introduce a new
open-sourced generator called 'Randen' and show that it is 'strong' in addition
to outperforming Mersenne Twister, PCG, ChaCha8, ISAAC and Philox in real-world
benchmarks. This is made possible by hardware acceleration. Randen is an
instantiation of Reverie, a recently published robust sponge-like random
generator, with a new permutation built from an improved generalized Feistel
structure with 16 branches. We provide new bounds on active s-boxes for up to
24 rounds of this construction, made possible by a memory-efficient search
algorithm. Replacing existing generators with Randen can protect randomized
algorithms such as reservoir sampling from attack. The permutation may also be
useful for wide-block ciphers and hashing functions.
</summary>
    <author>
      <name>Jan Wassenberg</name>
    </author>
    <author>
      <name>Robert Obryk</name>
    </author>
    <author>
      <name>Jyrki Alakuijala</name>
    </author>
    <author>
      <name>Emmanuel Mogenet</name>
    </author>
    <link href="http://arxiv.org/abs/1810.02227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.02227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A60" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.03551">
    <id>http://arxiv.org/abs/1810.03551v2</id>
    <updated>2018-11-05T11:03:00Z</updated>
    <published>2018-10-08T16:10:17Z</published>
    <title>Approximate Online Pattern Matching in Sub-linear Time</title>
    <summary>  We consider the approximate pattern matching problem under edit distance. In
this problem we are given a pattern $P$ of length $w$ and a text $T$ of length
$n$ over some alphabet $\Sigma$, and a positive integer $k$. The goal is to
find all the positions $j$ in $T$ such that there is a substring of $T$ ending
at $j$ which has edit distance at most $k$ from the pattern $P$. Recall, the
edit distance between two strings is the minimum number of character
insertions, deletions, and substitutions required to transform one string into
the other. For a position $t$ in $\{1,...,n\}$, let $k_t$ be the smallest edit
distance between $P$ and any substring of $T$ ending at $t$. In this paper we
give a constant factor approximation to the sequence $k_1,k_2,...,k_{n}$. We
consider both offline and online settings.
  In the offline setting, where both $P$ and $T$ are available, we present an
algorithm that for all $t$ in $\{1,...,n\}$, computes the value of $k_t$
approximately within a constant factor. The worst case running time of our
algorithm is $O(n w^{3/4})$. As a consequence we break the $O(nw)$-time barrier
for this problem.
  In the online setting, we are given $P$ and then $T$ arrives one symbol at a
time. We design an algorithm that upon arrival of the $t$-th symbol of $T$
computes $k_t$ approximately within $O(1)$-multiplicative factor and
$w^{8/9}$-additive error. Our algorithm takes $O(w^{1-(7/54)})$ amortized time
per symbol arrival and takes $O(w^{1-(1/54)})$ additional space apart from
storing the pattern $P$.
  Both of our algorithms are randomized and produce correct answer with high
probability. To the best of our knowledge this is the first worst-case
sub-linear (in the length of the pattern) time and sub-linear succinct space
algorithm for online approximate pattern matching problem.
</summary>
    <author>
      <name>Diptarka Chakraborty</name>
    </author>
    <author>
      <name>Debarati Das</name>
    </author>
    <author>
      <name>Michal Koucky</name>
    </author>
    <link href="http://arxiv.org/abs/1810.03551v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.03551v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.03374">
    <id>http://arxiv.org/abs/1810.03374v1</id>
    <updated>2018-10-08T11:14:24Z</updated>
    <published>2018-10-08T11:14:24Z</published>
    <title>On the discrepancy of random low degree set systems</title>
    <summary>  Motivated by the celebrated Beck-Fiala conjecture, we consider the random
setting where there are $n$ elements and $m$ sets and each element lies in $t$
randomly chosen sets. In this setting, Ezra and Lovett showed an $O((t \log
t)^{1/2})$ discrepancy bound in the regime when $n \leq m$ and an $O(1)$ bound
when $n \gg m^t$.
  In this paper, we give a tight $O(\sqrt{t})$ bound for the entire range of
$n$ and $m$, under a mild assumption that $t = \Omega (\log \log m)^2$. The
result is based on two steps. First, applying the partial coloring method to
the case when $n = m \log^{O(1)} m$ and using the properties of the random set
system we show that the overall discrepancy incurred is at most $O(\sqrt{t})$.
Second, we reduce the general case to that of $n \leq m \log^{O(1)}m$ using LP
duality and a careful counting argument.
</summary>
    <author>
      <name>Nikhil Bansal</name>
    </author>
    <author>
      <name>Raghu Meka</name>
    </author>
    <link href="http://arxiv.org/abs/1810.03374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.03374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1810.03664">
    <id>http://arxiv.org/abs/1810.03664v1</id>
    <updated>2018-10-08T19:08:37Z</updated>
    <published>2018-10-08T19:08:37Z</published>
    <title>Approximating Edit Distance Within Constant Factor in Truly
  Sub-Quadratic Time</title>
    <summary>  Edit distance is a measure of similarity of two strings based on the minimum
number of character insertions, deletions, and substitutions required to
transform one string into the other. The edit distance can be computed exactly
using a dynamic programming algorithm that runs in quadratic time. Andoni,
Krauthgamer and Onak (2010) gave a nearly linear time algorithm that
approximates edit distance within approximation factor $\text{poly}(\log n)$.
  In this paper, we provide an algorithm with running time
$\tilde{O}(n^{2-2/7})$ that approximates the edit distance within a constant
factor.
</summary>
    <author>
      <name>Diptarka Chakraborty</name>
    </author>
    <author>
      <name>Debarati Das</name>
    </author>
    <author>
      <name>Elazar Goldenberg</name>
    </author>
    <author>
      <name>Michal Koucky</name>
    </author>
    <author>
      <name>Michael Saks</name>
    </author>
    <link href="http://arxiv.org/abs/1810.03664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.03664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.01759">
    <id>http://arxiv.org/abs/1809.01759v1</id>
    <updated>2018-09-05T22:52:51Z</updated>
    <published>2018-09-05T22:52:51Z</published>
    <title>Multi-finger binary search trees</title>
    <summary>  We study multi-finger binary search trees (BSTs), a far-reaching extension of
the classical BST model, with connections to the well-studied $k$-server
problem. Finger search is a popular technique for speeding up BST operations
when a query sequence has locality of reference. BSTs with multiple fingers can
exploit more general regularities in the input. In this paper we consider the
cost of serving a sequence of queries in an optimal (offline) BST with $k$
fingers, a powerful benchmark against which other algorithms can be measured.
  We show that the $k$-finger optimum can be matched by a standard dynamic BST
(having a single root-finger) with an $O(\log{k})$ factor overhead. This result
is tight for all $k$, improving the $O(k)$ factor implicit in earlier work.
Furthermore, we describe new online BSTs that match this bound up to a
$(\log{k})^{O(1)}$ factor. Previously only the "one-finger" special case was
known to hold for an online BST (Iacono, Langerman, 2016; Cole et al., 2000).
Splay trees, assuming their conjectured optimality (Sleator and Tarjan, 1983),
would have to match our bounds for all $k$.
  Our online algorithms are randomized and combine techniques developed for the
$k$-server problem with a multiplicative-weights scheme for learning tree
metrics. To our knowledge, this is the first time when tools developed for the
$k$-server problem are used in BSTs. As an application of our $k$-finger
results, we show that BSTs can efficiently serve queries that are close to some
recently accessed item. This is a (restricted) form of the unified property
(Iacono, 2001) that was previously not known to hold for any BST algorithm,
online or offline.
</summary>
    <author>
      <name>Parinya Chalermsook</name>
    </author>
    <author>
      <name>Mayank Goswami</name>
    </author>
    <author>
      <name>László Kozma</name>
    </author>
    <author>
      <name>Kurt Mehlhorn</name>
    </author>
    <author>
      <name>Thatchaphol Saranurak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at ISAAC 2018. Also extends (and supersedes parts of)
  arXiv:1603.04892, with possible text overlaps</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.03685">
    <id>http://arxiv.org/abs/1809.03685v2</id>
    <updated>2018-09-14T23:41:03Z</updated>
    <published>2018-09-11T05:24:43Z</published>
    <title>Massively Parallel Dynamic Programming on Trees</title>
    <summary>  Dynamic programming is a powerful technique that is, unfortunately, often
inherently sequential. That is, there exists no unified method to parallelize
algorithms that use dynamic programming. In this paper, we attempt to address
this issue in the Massively Parallel Computations (MPC) model which is a
popular abstraction of MapReduce-like paradigms. Our main result is an
algorithmic framework to adapt a large family of dynamic programs defined over
trees.
  We introduce two classes of graph problems that admit dynamic programming
solutions on trees. We refer to them as "(polylog)-expressible" and
"linear-expressible" problems. We show that both classes can be parallelized in
$O(\log n)$ rounds using a sublinear number of machines and a sublinear memory
per machine. To achieve this result, we introduce a series of techniques that
can be plugged together. To illustrate the generality of our framework, we
implement in $O(\log n)$ rounds of MPC, the dynamic programming solution of
graph problems such as minimum bisection, $k$-spanning tree, maximum
independent set, longest path, etc., when the input graph is a tree.
</summary>
    <author>
      <name>MohammadHossein Bateni</name>
    </author>
    <author>
      <name>Soheil Behnezhad</name>
    </author>
    <author>
      <name>Mahsa Derakhshan</name>
    </author>
    <author>
      <name>MohammadTaghi Hajiaghayi</name>
    </author>
    <author>
      <name>Vahab Mirrokni</name>
    </author>
    <link href="http://arxiv.org/abs/1809.03685v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.03685v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.05419">
    <id>http://arxiv.org/abs/1809.05419v1</id>
    <updated>2018-09-14T13:45:27Z</updated>
    <published>2018-09-14T13:45:27Z</published>
    <title>Approximate Query Processing over Static Sets and Sliding Windows</title>
    <summary>  Indexing of static and dynamic sets is fundamental to a large set of
applications such as information retrieval and caching. Denoting the
characteristic vector of the set by B, we consider the problem of encoding sets
and multisets to support approximate versions of the operations rank(i) (i.e.,
computing sum_{j &lt;= i}B[j]) and select(i) (i.e., finding min{p | rank(p) >= i})
queries. We study multiple types of approximations (allowing an error in the
query or the result) and present lower bounds and succinct data structures for
several variants of the problem. We also extend our model to sliding windows,
in which we process a stream of elements and compute suffix sums. This is a
generalization of the window summation problem that allows the user to specify
the window size at query time. Here, we provide an algorithm that supports
updates and queries in constant time while requiring just (1+o(1)) factor more
space than the fixed-window summation algorithms.
</summary>
    <author>
      <name>Ran Ben Basat</name>
    </author>
    <author>
      <name>Seungbum Jo</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <author>
      <name>Shubham Ugare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ISAAC 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.05419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.05419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.05844">
    <id>http://arxiv.org/abs/1809.05844v1</id>
    <updated>2018-09-16T09:43:53Z</updated>
    <published>2018-09-16T09:43:53Z</published>
    <title>Calculation of extended gcd by normalization</title>
    <summary>  We propose a new algorithm solving the extended gcd problem, which provides a
solution minimizing one of the two coordinates. The algorithm relies on
elementary arithmetic properties.
</summary>
    <author>
      <name>Marc Wolf</name>
    </author>
    <author>
      <name>François Wolf</name>
    </author>
    <author>
      <name>Corentin Le Coz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SCIREA Journal of Mathematics. Vol. 3, No. 3, 2018, pp. 118 - 131</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.05844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.05844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11D04" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.07067">
    <id>http://arxiv.org/abs/1809.07067v2</id>
    <updated>2018-09-20T14:01:39Z</updated>
    <published>2018-09-19T08:51:58Z</published>
    <title>Encoding two-dimensional range top-k queries revisited</title>
    <summary>  We consider the problem of encoding two-dimensional arrays, whose elements
come from a total order, for answering top-k queries. The aim is to obtain
encodings that use space close to the information-theoretic lower bound, which
can be constructed efficiently. For $2 \times n$ arrays, we first give upper
and lower bounds on space for answering sorted and unsorted 3-sided top-k
queries. For $m \times n$ arrays, with $m \le n$ and $k \le mn$, we obtain
$(m\lg{{(k+1)n \choose n}}+4nm(m-1)+o(n))$-bit encoding for answering sorted
4-sided top-k queries. This improves the $\min{(O(mn\lg{n}),m^2\lg{{(k+1)n
\choose n}} +m\lg{m}+o(n))}$-bit encoding of Jo et al. [CPM, 2016] when $m =
o(\lg{n})$. This is a consequence of a new encoding that encodes a $2 \times n$
array to support sorted 4-sided top-k queries on it using an additional $4n$
bits, in addition to the encodings to support the top-k queries on individual
rows. This new encoding is a non-trivial generalization of the encoding of Jo
et al. [CPM, 2016] that supports sorted 4-sided top-2 queries on it using an
additional $3n$ bits. We also give almost optimal space encodings for $3$-sided
top-k queries, and show lower bounds on encodings for $3$-sided and $4$-sided
top-k queries.
</summary>
    <author>
      <name>Seungbum Jo</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <link href="http://arxiv.org/abs/1809.07067v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.07067v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.07320">
    <id>http://arxiv.org/abs/1809.07320v3</id>
    <updated>2019-02-01T11:28:28Z</updated>
    <published>2018-09-19T15:58:53Z</published>
    <title>Relaxing Wheeler Graphs for Indexing Reads</title>
    <summary>  As industry standards for average-coverage rates increase, DNA readsets are
becoming more repetitive. The run-length compressed Burrows-Wheeler Transform
(RLBWT) is the basis for several powerful algorithms and data structures
designed to handle repetitive genetic datasets, but applying it directly to
readsets is problematic because end-of-string symbols break up runs and, worse,
the characters at the ends of the reads lack context and are thus scattered
throughout the BWT. In this paper we first propose storing the readset as a
Wheeler graph consisting of a set of paths, to avoid end-of-string symbols at
the cost of storing nodes' in- and out-degrees. We then propose rebuilding the
Wheeler graph as if each read were preceded by some imaginary context. This
requires us to relax the constraint that nodes with in-degree 0 in the graph
should appear first in the ordering showing that it is a Wheeler graph, and can
lead to false-positive pattern matches. Nevertheless, we first describe how to
support fast locating, which allows us to filter out false matches and return
all true matches, in time bounded in terms of the total number of matches. More
importantly, we then also show how to augment the RLBWT for the relaxed Wheeler
graph such that we can tell after what point a backward search will return only
false matches, and quickly return as a witness one true match if a backward
search yields any.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Garance Gourdel</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Jared Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1809.07320v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.07320v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.07661">
    <id>http://arxiv.org/abs/1809.07661v1</id>
    <updated>2018-09-20T14:57:04Z</updated>
    <published>2018-09-20T14:57:04Z</published>
    <title>Small Uncolored and Colored Choice Dictionaries</title>
    <summary>  A choice dictionary can be initialized with a parameter $n\in\mathbb{N}$ and
subsequently maintains an initially empty subset $S$ of $\{1,\ldots,n\}$ under
insertion, deletion, membership queries and an operation $\textit{choice}$ that
returns an arbitrary element of $S$. The choice dictionary is fundamental in
space-efficient computing and has numerous applications. The best previous
choice dictionary can be initialized with $n$ and $t\in\mathbb{N}$ and
subsequently executes all operations in $O(t)$ time and occupies
$n+O(n({t/w})^t+\log n)$ bits on a word RAM with a word length of
$w=\Omega(\log n)$ bits. We describe a new choice dictionary that executes all
operations in constant time and, in addition to the space needed to store the
integer $n$, occupies only $n+1$ bits, which is shown to be optimal if
$w=o(n)$.
  A generalization of the choice dictionary called a colored choice dictionary
is initialized with $c\in\mathbb{N}$ in addition to $n$ and subsequently
maintains a semipartition $(S_0,\ldots,S_{c-1})$ of $\{1,\ldots,n\}$ under the
operations $\textit{setcolor}(j,\ell)$, which moves $\ell$ from its current
subset to $S_j$, $\textit{color}(\ell)$, which returns the unique
$j\in\{0,\ldots,c-1\}$ with $\ell\in S_j$, and $\textit{choice}(j)$, which
returns an arbitrary element of $S_j$. We describe new colored choice
dictionaries that, if initialized with constant $c$, execute
$\textit{setcolor}$, $\textit{color}$ and $\textit{choice}$ in constant time
and occupy $n\log_2\!c+1$ bits plus the space needed to store $n$ if $c$ is a
power of 2, and at most $n\log_2\!c+n^\epsilon$ bits in general, for arbitrary
fixed $\epsilon>0$. We also study the possibility of iterating over the set $S$
or over $S_j$ for given $j\in\{0,\ldots,c-1\}$ and an application of this to
breadth-first search.
</summary>
    <author>
      <name>Torben Hagerup</name>
    </author>
    <link href="http://arxiv.org/abs/1809.07661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.07661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.08411">
    <id>http://arxiv.org/abs/1809.08411v3</id>
    <updated>2019-10-30T15:39:24Z</updated>
    <published>2018-09-22T08:50:24Z</published>
    <title>Adaptive Shivers Sort: An Alternative Sorting Algorithm</title>
    <summary>  We present one stable mergesort algorithm, called \Adaptive Shivers Sort,
that exploits the existence of monotonic runs for sorting efficiently partially
sorted data. We also prove that, although this algorithm is simple to
implement, its computational cost, in number of comparisons performed, is
optimal up to a small additive linear term.
</summary>
    <author>
      <name>Vincent Jugé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of the article published in the proceedings of the 31th
  Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.08411v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08411v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.08669">
    <id>http://arxiv.org/abs/1809.08669v2</id>
    <updated>2019-07-08T15:58:52Z</updated>
    <published>2018-09-23T20:13:14Z</published>
    <title>Collapsing Superstring Conjecture</title>
    <summary>  In the Shortest Common Superstring (SCS) problem, one is given a collection
of strings, and needs to find a shortest string containing each of them as a
substring. SCS admits $2\frac{11}{23}$-approximation in polynomial time (Mucha,
SODA'13). While this algorithm and its analysis are technically involved, the
30 years old Greedy Conjecture claims that the trivial and efficient Greedy
Algorithm gives a 2-approximation for SCS.
  We develop a graph-theoretic framework for studying approximation algorithms
for SCS. The framework is reminiscent of the classical 2-approximation for
Traveling Salesman: take two copies of an optimal solution, apply a trivial
edge-collapsing procedure, and get an approximate solution. In this framework,
we observe two surprising properties of SCS solutions, and we conjecture that
they hold for all input instances. The first conjecture, that we call
Collapsing Superstring conjecture, claims that there is an elementary way to
transform any solution repeated twice into the same graph $G$. This conjecture
would give an elementary 2-approximate algorithm for SCS. The second conjecture
claims that not only the resulting graph $G$ is the same for all solutions, but
that $G$ can be computed by an elementary greedy procedure called Greedy
Hierarchical Algorithm.
  While the second conjecture clearly implies the first one, perhaps
surprisingly we prove their equivalence. We support these equivalent
conjectures by giving a proof for the special case where all input strings have
length at most 3. We prove that the standard Greedy Conjecture implies Greedy
Hierarchical Conjecture, while the latter is sufficient for an efficient greedy
2-approximate approximation of SCS. Except for its (conjectured) good
approximation ratio, the Greedy Hierarchical Algorithm provably finds a
3.5-approximation.
</summary>
    <author>
      <name>Alexander Golovnev</name>
    </author>
    <author>
      <name>Alexander S. Kulikov</name>
    </author>
    <author>
      <name>Alexander Logunov</name>
    </author>
    <author>
      <name>Ivan Mihajlin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">visualization available at: http://compsciclub.ru/scs/</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.08669v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08669v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.09330">
    <id>http://arxiv.org/abs/1809.09330v2</id>
    <updated>2019-08-20T19:40:27Z</updated>
    <published>2018-09-25T05:47:44Z</published>
    <title>Improved Parallel Cache-Oblivious Algorithms for Dynamic Programming and
  Linear Algebra</title>
    <summary>  Emerging non-volatile main memory (NVRAM) technologies provide
byte-addressability, low idle power, and improved memory-density, and are
likely to be a key component in the future memory hierarchy. However, a
critical challenge in achieving high performance is in accounting for the
asymmetry that NVRAM writes can be significantly more expensive than NVRAM
reads.
  In this paper, we consider a large class of cache-oblivious algorithms for
dynamic programming (DP) and linear algebra, and try to reduce the writes in
the asymmetric setting while maintaining high parallelism. To achieve that, our
key approach is to show the correspondence between these problems and an
abstraction for their computation, which is referred to as the $k$-d grids.
Then by showing lower bound and new algorithms for computing $k$-d grids, we
show a list of improved cache-oblivious algorithms of many DP recurrences and
in linear algebra in the asymmetric setting, both sequentially and in parallel.
  Surprisingly, even without considering the read-write asymmetry (i.e.,
setting the write cost to be the same as the read cost in the algorithms), the
new algorithms improve the existing cache complexity of many problems. We
believe the reason is that the extra level of abstraction of $k$-d grids helps
us to better understand the complexity and difficulties of these problems. We
believe that the novelty of our framework is of interests and leads to many new
questions for future work.
</summary>
    <author>
      <name>Guy E. Blleloch</name>
    </author>
    <author>
      <name>Yan Gu</name>
    </author>
    <link href="http://arxiv.org/abs/1809.09330v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.09330v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.11580">
    <id>http://arxiv.org/abs/1807.11580v1</id>
    <updated>2018-07-27T01:37:45Z</updated>
    <published>2018-07-27T01:37:45Z</published>
    <title>Enumerating Cryptarithms Using Deterministic Finite Automata</title>
    <summary>  A cryptarithm is a mathematical puzzle where given an arithmetic equation
written with letters rather than numerals, a player must discover an assignment
of numerals on letters that makes the equation hold true. In this paper, we
propose a method to construct a DFA that accepts cryptarithms that admit
(unique) solutions for each base. We implemented the method and constructed a
DFA for bases $k \le 7$. Those DFAs can be used as complete catalogues of
cryptarithms,whose applications include enumeration of and counting the exact
numbers $G_k(n)$ of cryptarithm instances with $n$ digits that admit base-$k$
solutions. Moreover, explicit formulas for $G_2(n)$ and $G_3(n)$ are given.
</summary>
    <author>
      <name>Yuki Nozaki</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Takashi Horiyama</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.00674">
    <id>http://arxiv.org/abs/1808.00674v1</id>
    <updated>2018-08-02T05:53:17Z</updated>
    <published>2018-08-02T05:53:17Z</published>
    <title>Reconstructing Strings from Substrings: Optimal Randomized and
  Average-Case Algorithms</title>
    <summary>  The problem called "String reconstruction from substrings" is a mathematical
model of sequencing by hybridization that plays an important role in DNA
sequencing. In this problem, we are given a blackbox oracle holding an unknown
string ${\mathcal X}$ and are required to obtain (reconstruct) ${\mathcal X}$
through "substring queries" $Q(S)$. $Q(S)$ is given to the oracle with a string
$S$ and the answer of the oracle is Yes if ${\mathcal X}$ includes $S$ as a
substring and No otherwise. Our goal is to minimize the number of queries for
the reconstruction. In this paper, we deal with only binary strings for
${\mathcal X}$ whose length $n$ is given in advance by using a sequence of good
$S$'s. In 1995, Skiena and Sundaram first studied this problem and obtained an
algorithm whose query complexity is $n+O(\log n)$. Its information theoretic
lower bound is $n$, and they posed an obvious open question; if we can remove
the $O(\log n)$ additive term. No progress has been made until now. This paper
gives two partially positive answers to this open question. One is a randomized
algorithm whose query complexity is $n+O(1)$ with high probability and the
other is an average-case algorithm also having a query complexity of $n+O(1)$
on average. The $n$ lower bound is still true for both cases, and hence they
are optimal up to an additive constant.
</summary>
    <author>
      <name>Kazuo Iwama</name>
    </author>
    <author>
      <name>Junichi Teruyama</name>
    </author>
    <author>
      <name>Shuntaro Tsuyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.00963">
    <id>http://arxiv.org/abs/1808.00963v1</id>
    <updated>2018-08-02T16:38:49Z</updated>
    <published>2018-08-02T16:38:49Z</published>
    <title>Scalable String and Suffix Sorting: Algorithms, Techniques, and Tools</title>
    <summary>  This dissertation focuses on two fundamental sorting problems: string sorting
and suffix sorting. The first part considers parallel string sorting on
shared-memory multi-core machines, the second part external memory suffix
sorting using the induced sorting principle, and the third part distributed
external memory suffix sorting with a new distributed algorithmic big data
framework named Thrill.
</summary>
    <author>
      <name>Timo Bingmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5445/IR/1000085031</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5445/IR/1000085031" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">396 pages, dissertation, Karlsruher Instituts f\"ur Technologie
  (2018). arXiv admin note: text overlap with arXiv:1101.3448 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.01071">
    <id>http://arxiv.org/abs/1808.01071v1</id>
    <updated>2018-08-03T02:24:26Z</updated>
    <published>2018-08-03T02:24:26Z</published>
    <title>Right-to-left online construction of parameterized position heaps</title>
    <summary>  Two strings of equal length are said to parameterized match if there is a
bijection that maps the characters of one string to those of the other string,
so that two strings become identical. The parameterized pattern matching
problem is, given two strings $T$ and $P$, to find the occurrences of
substrings in $T$ that parameterized match $P$. Diptarama et al. [Position
Heaps for Parameterized Strings, CPM 2017] proposed an indexing data structure
called parameterized position heaps, and gave a left-to-right online
construction algorithm. In this paper, we present a right-to-left online
construction algorithm for parameterized position heaps. For a text string $T$
of length $n$ over two kinds of alphabets $\Sigma$ and $\Pi$ of respective size
$\sigma$ and $\pi$, our construction algorithm runs in $O(n \log(\sigma +
\pi))$ time with $O(n)$ space. Our right-to-left parameterized position heaps
support pattern matching queries in $O(m \log (\sigma + \pi) + m \pi +
\mathit{pocc}))$ time, where $m$ is the length of a query pattern $P$ and
$\mathit{pocc}$ is the number of occurrences to report. Our construction and
pattern matching algorithms are as efficient as Diptarama et al.'s algorithms.
</summary>
    <author>
      <name>Noriki Fujisato</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.03553">
    <id>http://arxiv.org/abs/1808.03553v1</id>
    <updated>2018-08-10T14:05:28Z</updated>
    <published>2018-08-10T14:05:28Z</published>
    <title>Dynamic all scores matrices for LCS score</title>
    <summary>  The problem of aligning two strings A,B in order to determine their
similarity is fundamental in the field of pattern matching. An important
concept in this domain is the "all scores matrix" that encodes the local
alignment comparison of two strings. Namely, let K denote the all scores matrix
containing the alignment score of every substring of B with A, and let J denote
the all scores matrix containing the alignment score of every suffix of B with
every prefix of A.
  In this paper we consider the problem of maintaining an all scores matrix
where the scoring function is the LCS score, while supporting single character
prepend and append operations to A and N. Our algorithms exploit the sparsity
parameters L=LCS(A,B) and Delta = |B|-L. For the matrix K we propose an
algorithm that supports incremental operations to both ends of A in O(Delta)
time. Whilst for the matrix J we propose an algorithm that supports a single
type of incremental operation, either a prepend operation to A or an append
operation to B, in O(L) time. This structure can also be extended to support
both operations simultaneously in O(L log log L) time.
</summary>
    <author>
      <name>Amir Carmel</name>
    </author>
    <author>
      <name>Dekel Tsur</name>
    </author>
    <author>
      <name>Michal Ziv-Ukelson</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.03307">
    <id>http://arxiv.org/abs/1808.03307v1</id>
    <updated>2018-08-09T19:06:40Z</updated>
    <published>2018-08-09T19:06:40Z</published>
    <title>Longest Increasing Subsequence under Persistent Comparison Errors</title>
    <summary>  We study the problem of computing a longest increasing subsequence in a
sequence $S$ of $n$ distinct elements in the presence of persistent comparison
errors. In this model, every comparison between two elements can return the
wrong result with some fixed (small) probability $ p $, and comparisons cannot
be repeated. Computing the longest increasing subsequence exactly is impossible
in this model, therefore, the objective is to identify a subsequence that (i)
is indeed increasing and (ii) has a length that approximates the length of the
longest increasing subsequence.
  We present asymptotically tight upper and lower bounds on both the
approximation factor and the running time. In particular, we present an
algorithm that computes an $O(\log n)$-approximation in time $O(n\log n)$, with
high probability. This approximation relies on the fact that that we can
approximately sort $n$ elements in $O(n\log n)$ time such that the maximum
dislocation of an element is at most $O(\log n)$. For the lower bounds, we
prove that (i) there is a set of sequences, such that on a sequence picked
randomly from this set every algorithm must return an $\Omega(\log
n)$-approximation with high probability, and (ii) any $O(\log n)$-approximation
algorithm for longest increasing subsequence requires $\Omega(n \log n)$
comparisons, even in the absence of errors.
</summary>
    <author>
      <name>Barbara Geissmann</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.03978">
    <id>http://arxiv.org/abs/1808.03978v2</id>
    <updated>2018-12-05T06:09:03Z</updated>
    <published>2018-08-12T18:16:59Z</published>
    <title>Local Decodability of the Burrows-Wheeler Transform</title>
    <summary>  The Burrows-Wheeler Transform (BWT) is among the most influential discoveries
in text compression and DNA storage. It is a reversible preprocessing step that
rearranges an $n$-letter string into runs of identical characters (by
exploiting context regularities), resulting in highly compressible strings, and
is the basis of the \texttt{bzip} compression program. Alas, the decoding
process of BWT is inherently sequential and requires $\Omega(n)$ time even to
retrieve a \emph{single} character.
  We study the succinct data structure problem of locally decoding short
substrings of a given text under its \emph{compressed} BWT, i.e., with small
additive redundancy $r$ over the \emph{Move-To-Front} (\texttt{bzip})
compression. The celebrated BWT-based FM-index (FOCS '00), as well as other
related literature, yield a trade-off of $r=\tilde{O}(n/\sqrt{t})$ bits, when a
single character is to be decoded in $O(t)$ time. We give a near-quadratic
improvement $r=\tilde{O}(n\lg(t)/t)$. As a by-product, we obtain an
\emph{exponential} (in $t$) improvement on the redundancy of the FM-index for
counting pattern-matches on compressed text. In the interesting regime where
the text compresses to $n^{1-o(1)}$ bits, these results provide an $\exp(t)$
\emph{overall} space reduction. For the local decoding problem of BWT, we also
prove an $\Omega(n/t^2)$ cell-probe lower bound for "symmetric" data
structures.
  We achieve our main result by designing a compressed partial-sums (Rank) data
structure over BWT. The key component is a \emph{locally-decodable}
Move-to-Front (MTF) code: with only $O(1)$ extra bits per block of length
$n^{\Omega(1)}$, the decoding time of a single character can be decreased from
$\Omega(n)$ to $O(\lg n)$. This result is of independent interest in
algorithmic information theory.
</summary>
    <author>
      <name>Sandip Sinha</name>
    </author>
    <author>
      <name>Omri Weinstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The following two technical typos were fixed: (1) On page 2,
  following Theorem 1, the decoding time of a contiguous substring of size
  $\ell$ was corrected from $O(t + \ell)$ to $O(t + \ell \cdot \lg t)$. (2) In
  the statement of Theorem 2, the query time to count occurrences of patterns
  of length $\ell$ was corrected to $O(t \ell)$, independent of the number of
  occurrences</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03978v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03978v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.03658">
    <id>http://arxiv.org/abs/1808.03658v1</id>
    <updated>2018-08-10T18:06:01Z</updated>
    <published>2018-08-10T18:06:01Z</published>
    <title>The effective entropy of next/previous larger/smaller value queries</title>
    <summary>  We study the problem of storing the minimum number of bits required to answer
next/previous larger/smaller value queries on an array $A$ of $n$ numbers,
without storing $A$. We show that these queries can be answered by storing at
most $3.701 n$ bits. Our result improves the result of Jo and Satti [TCS 2016]
that gives an upper bound of $4.088n$ bits for this problem.
</summary>
    <author>
      <name>Dekel Tsur</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03658v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03658v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.05879">
    <id>http://arxiv.org/abs/1808.05879v3</id>
    <updated>2018-12-18T22:27:26Z</updated>
    <published>2018-08-17T14:26:00Z</published>
    <title>Cardinality Estimators do not Preserve Privacy</title>
    <summary>  Cardinality estimators like HyperLogLog are sketching algorithms that
estimate the number of distinct elements in a large multiset. Their use in
privacy-sensitive contexts raises the question of whether they leak private
information. In particular, can they provide any privacy guarantees while
preserving their strong aggregation properties? We formulate an abstract notion
of cardinality estimators, that captures this aggregation requirement: one can
merge sketches without losing precision. We propose an attacker model and a
corresponding privacy definition, strictly weaker than differential privacy: we
assume that the attacker has no prior knowledge of the data. We then show that
if a cardinality estimator satisfies this definition, then it cannot have a
reasonable level of accuracy. We prove similar results for weaker versions of
our definition, and analyze the privacy of existing algorithms, showing that
their average privacy loss is significant, even for multisets with large
cardinalities. We conclude that strong aggregation requirements are
incompatible with any reasonable definition of privacy, and that cardinality
estimators should be considered as sensitive as raw data. We also propose risk
mitigation strategies for their real-world applications.
</summary>
    <author>
      <name>Damien Desfontaines</name>
    </author>
    <author>
      <name>Andreas Lochbihler</name>
    </author>
    <author>
      <name>David Basin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05879v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05879v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1809.02792">
    <id>http://arxiv.org/abs/1809.02792v2</id>
    <updated>2019-07-04T15:31:22Z</updated>
    <published>2018-09-08T12:15:58Z</published>
    <title>Fully-Functional Suffix Trees and Optimal Text Searching in BWT-runs
  Bounded Space</title>
    <summary>  Indexing highly repetitive texts - such as genomic databases, software
repositories and versioned text collections - has become an important problem
since the turn of the millennium. A relevant compressibility measure for
repetitive texts is r, the number of runs in their Burrows-Wheeler Transforms
(BWTs). One of the earliest indexes for repetitive collections, the Run-Length
FM-index, used O(r) space and was able to efficiently count the number of
occurrences of a pattern of length m in the text (in loglogarithmic time per
pattern symbol, with current techniques). However, it was unable to locate the
positions of those occurrences efficiently within a space bounded in terms of
r. In this paper we close this long-standing problem, showing how to extend the
Run-Length FM-index so that it can locate the occ occurrences efficiently
within O(r) space (in loglogarithmic time each), and reaching optimal time, O(m
+ occ), within O(r log log w ({\sigma} + n/r)) space, for a text of length n
over an alphabet of size {\sigma} on a RAM machine with words of w =
{\Omega}(log n) bits. Within that space, our index can also count in optimal
time, O(m). Multiplying the space by O(w/ log {\sigma}), we support count and
locate in O(dm log({\sigma})/we) and O(dm log({\sigma})/we + occ) time, which
is optimal in the packed setting and had not been obtained before in compressed
space. We also describe a structure using O(r log(n/r)) space that replaces the
text and extracts any text substring of length ` in almost-optimal time
O(log(n/r) + ` log({\sigma})/w). Within that space, we similarly provide direct
access to suffix array, inverse suffix array, and longest common prefix array
cells, and extend these capabilities to full suffix tree functionality,
typically in O(log(n/r)) time per operation.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted version; optimal count and locate in smaller space: O(r log
  log_w(n/r + sigma))</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.02792v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.02792v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.03827">
    <id>http://arxiv.org/abs/1807.03827v1</id>
    <updated>2018-07-10T19:00:45Z</updated>
    <published>2018-07-10T19:00:45Z</published>
    <title>Improved Time and Space Bounds for Dynamic Range Mode</title>
    <summary>  Given an array A of $n$ elements, we wish to support queries for the most
frequent and least frequent element in a subrange $[l, r]$ of $A$. We also wish
to support updates that change a particular element at index $i$ or insert/
delete an element at index $i$. For the range mode problem, our data structure
supports all operations in $O(n^{2/3})$ deterministic time using only $O(n)$
space. This improves two results by Chan et al. \cite{C14}: a linear space data
structure supporting update and query operations in $\tilde{O}(n^{3/4})$ time
and an $O(n^{4/3})$ space data structure supporting update and query operations
in $\tilde{O}(n^{2/3})$ time. For the range least frequent problem, we address
two variations. In the first, we are allowed to answer with an element of $A$
that may not appear in the query range, and in the second, the returned element
must be present in the query range. For the first variation, we develop a data
structure that supports queries in $\tilde{O}(n^{2/3})$ time, updates in
$O(n^{2/3})$ time, and occupies $O(n)$ space. For the second variation, we
develop a Monte Carlo data structure that supports queries in $O(n^{2/3})$
time, updates in $\tilde{O}(n^{2/3})$ time, and occupies $\tilde{O}(n)$ space,
but requires that updates are made independently of the results of previous
queries. The Monte Carlo data structure is also capable of answering
$k$-frequency queries; that is, the problem of finding an element of given
frequency in the specified query range. Previously, no dynamic data structures
were known for least frequent element or $k$-frequency queries.
</summary>
    <author>
      <name>Hicham El-Zein</name>
    </author>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Bryce Sandlund</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.04613">
    <id>http://arxiv.org/abs/1807.04613v1</id>
    <updated>2018-07-12T14:00:35Z</updated>
    <published>2018-07-12T14:00:35Z</published>
    <title>Push-Down Trees: Optimal Self-Adjusting Complete Trees</title>
    <summary>  Since Sleator and Tarjan's seminal work on self-adjusting lists, heaps and
binary search trees, researchers have been fascinated by dynamic datastructures
and the questions related to their performance over time. This paper initiates
the study of another classic datastructure, self-adjusting (binary) Complete
Trees (CTs): trees which do not provide a simple search mechanism but allow to
efficiently access items given a global map. Our problem finds applications,
e.g., in the context of warehouse optimization or self-adjusting communication
networks which can adapt to the demand they serve. We show that self-adjusting
complete trees assume an interesting position between the complexity of
self-adjusting (unordered) lists and binary search trees. In particular, we
observe that in contrast to lists, a simple move-to-front strategy alone is
insufficient to achieve a constant competitive ratio. Rather, and similarly to
binary search trees, an additional (efficient) tree update rule is needed.
Intriguingly, while it is unknown whether the working set is a lower bound for
binary search trees, we show that this holds in our model. So while finding an
update rule is still an open problem for binary search trees, this paper shows
that there exists a simple, random update rule for complete trees. Our main
result is a dynamically optimal (i.e., constant competitive) self-adjusting CT
called Push-Down Tree, on expectation against an oblivious adversary. At the
heart of our approach lies a distributed algorithm called Random-Push: this
algorithm approximates a natural notion of Most Recently Used (MRU) tree
(essentially an approximate working set), by first performing move-to-front,
but then pushing less recently accessed items down the tree using a random
walk.
</summary>
    <author>
      <name>Chen Avin</name>
    </author>
    <author>
      <name>Kaushik Mondal</name>
    </author>
    <author>
      <name>Stefan Schmid</name>
    </author>
    <link href="http://arxiv.org/abs/1807.04613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.04682">
    <id>http://arxiv.org/abs/1807.04682v2</id>
    <updated>2018-07-13T12:28:09Z</updated>
    <published>2018-07-12T15:47:56Z</published>
    <title>Know When to Fold 'Em: Self-Assembly of Shapes by Folding in Oritatami</title>
    <summary>  An oritatami system (OS) is a theoretical model of self-assembly via
co-transcriptional folding. It consists of a growing chain of beads which can
form bonds with each other as they are transcribed. During the transcription
process, the $\delta$ most recently produced beads dynamically fold so as to
maximize the number of bonds formed, self-assemblying into a shape
incrementally. The parameter $\delta$ is called the delay and is related to the
transcription rate in nature.
  This article initiates the study of shape self-assembly using oritatami. A
shape is a connected set of points in the triangular lattice. We first show
that oritatami systems differ fundamentally from tile-assembly systems by
exhibiting a family of infinite shapes that can be tile-assembled but cannot be
folded by any OS. As it is NP-hard in general to determine whether there is an
OS that folds into (self-assembles) a given finite shape, we explore the
folding of upscaled versions of finite shapes. We show that any shape can be
folded from a constant size seed, at any scale n >= 3, by an OS with delay 1.
We also show that any shape can be folded at the smaller scale 2 by an OS with
unbounded delay. This leads us to investigate the influence of delay and to
prove that, for all {\delta} > 2, there are shapes that can be folded (at scale
1) with delay {\delta} but not with delay {\delta}'&lt;{\delta}. These results
serve as a foundation for the study of shape-building in this new model of
self-assembly, and have the potential to provide better understanding of
cotranscriptional folding in biology, as well as improved abilities of
experimentalists to design artificial systems that self-assemble via this
complex dynamical process.
</summary>
    <author>
      <name>Erik D. Demaine</name>
    </author>
    <author>
      <name>Jacob Hendricks</name>
    </author>
    <author>
      <name>Meagan Olsen</name>
    </author>
    <author>
      <name>Matthew J. Patitz</name>
    </author>
    <author>
      <name>Trent A. Rogers</name>
    </author>
    <author>
      <name>Nicolas Schabanel</name>
    </author>
    <author>
      <name>Shinnosuke Seki</name>
    </author>
    <author>
      <name>Hadley Thomas</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Short version published at DNA24, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.04682v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04682v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.05356">
    <id>http://arxiv.org/abs/1807.05356v1</id>
    <updated>2018-07-14T08:33:39Z</updated>
    <published>2018-07-14T08:33:39Z</published>
    <title>A Simple and Space Efficient Segment Tree Implementation</title>
    <summary>  The segment tree is an extremely versatile data structure. In this paper, a
new heap based implementation of segment trees is proposed. In such an
implementation of segment tree, the structural information associated with the
tree nodes can be removed completely. Some primary computational geometry
problems such as stabbing counting queries, measure of union of intervals, and
maximum clique size of Intervals are used to demonstrate the efficiency of the
new heap based segment tree implementation. Each interval in a set $S=\{I_1
,I_2 ,\cdots,I_n\}$ of $n$ intervals can be insert into or delete from the heap
based segment tree in $O(\log n)$ time. All the primary computational geometry
problems can be solved efficiently.
</summary>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Xiaodong Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.06359">
    <id>http://arxiv.org/abs/1807.06359v1</id>
    <updated>2018-07-17T11:44:45Z</updated>
    <published>2018-07-17T11:44:45Z</published>
    <title>Using statistical encoding to achieve tree succinctness never seen
  before</title>
    <summary>  We propose a new succinct representation of labeled trees which represents a
tree T using |T|H_k(T) number of bits (plus some smaller order terms), where
|T|H_k(T) denotes the k-th order (tree label) entropy, as defined by Ferragina
at al. 2005. Our representation employs a new, simple method of partitioning
the tree, which preserves both tree shape and node degrees. Previously, the
only representation that used |T|H_k(T) bits was based on XBWT, a
transformation that linearizes tree labels into a single string, combined with
compression boosting. The proposed representation is much simpler than the one
based on XBWT, which used additional linear space (bounded by 0.01n) hidden in
the "smaller order terms" notion, as an artifact of using zeroth order entropy
coder; our representation uses sublinear additional space (for reasonable
values of k and size of the label alphabet {\sigma}). The proposed
representation can be naturally extended to a succinct data structure for
trees, which uses |T|H_k(T) plus additional O(|T|k log_{\sigma}/ log_{\sigma}
|T| + |T| log log_{\sigma} |T|/ log_{\sigma} |T|) bits and supports all the
usual navigational queries in constant time. At the cost of increasing the
query time to O(log log |T|/ log |T|) we can further reduce the space
redundancy to O(|T| log log |T|/ log_{\sigma} |T|) bits, assuming k &lt;=
log_{\sigma} |T|. This is a major improvement over representation based on
XBWT: even though XBWT-based representation uses |T|H_k(T) bits, the space
needed for structure supporting navigational queries is much larger: (...)
</summary>
    <author>
      <name>Michał Gańczorz</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.07596">
    <id>http://arxiv.org/abs/1807.07596v1</id>
    <updated>2018-07-19T18:33:20Z</updated>
    <published>2018-07-19T18:33:20Z</published>
    <title>The colored longest common prefix array computed via sequential scans</title>
    <summary>  Due to the increased availability of large datasets of biological sequences,
the tools for sequence comparison are now relying on efficient alignment-free
approaches to a greater extent. Most of the alignment-free approaches require
the computation of statistics of the sequences in the dataset. Such
computations become impractical in internal memory when very large collections
of long sequences are considered. In this paper, we present a new conceptual
data structure, the colored longest common prefix array (cLCP), that allows to
efficiently tackle several problems with an alignment-free approach. In fact,
we show that such a data structure can be computed via sequential scans in
semi-external memory. By using cLCP, we propose an efficient lightweight
strategy to solve the multi-string Average Common Substring (ACS) problem, that
consists in the pairwise comparison of a single string against a collection of
$m$ strings simultaneously, in order to obtain $m$ ACS induced distances.
Experimental results confirm the effectiveness of our approach.
</summary>
    <author>
      <name>F. Garofalo</name>
    </author>
    <author>
      <name>G. Rosone</name>
    </author>
    <author>
      <name>M. Sciortino</name>
    </author>
    <author>
      <name>D. Verzotto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version of the paper that will be included in the SPIRE
  2018 proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.08777">
    <id>http://arxiv.org/abs/1807.08777v3</id>
    <updated>2019-10-31T17:41:30Z</updated>
    <published>2018-07-23T18:30:25Z</published>
    <title>Two Algorithms to Find Primes in Patterns</title>
    <summary>  Let $k\ge 1$ be an integer, and let $P= (f_1(x), \ldots, f_k(x) )$ be $k$
admissible linear polynomials over the integers, or \textit{the pattern}. We
present two algorithms that find all integers $x$ where $\max{ \{f_i(x) \} }
\le n$ and all the $f_i(x)$ are prime.
  Our first algorithm takes at most $O_P(n/(\log\log n)^k)$ arithmetic
operations using $O(k\sqrt{n})$ space.
  Our second algorithm takes slightly more time, $O_P(n/(\log \log n)^{k-1})$
arithmetic operations, but uses only $n^{1/c}$ space for a constant $c>2$. We
prove correctness unconditionally, but the running time relies on two unproven
but reasonable conjectures.
  We are unaware of any previous complexity results for this problem beyond the
use of a prime sieve. We also implemented several parallel versions of our
second algorithm to show it is viable in practice. In particular, we found some
new Cunningham chains of length 15, and we found all quadruplet primes up to
$10^{17}$.
</summary>
    <author>
      <name>Jonathan P. Sorenson</name>
    </author>
    <author>
      <name>Jonathan Webster</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08777v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08777v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11A11, 11Y11, 11Y16, 68Q25" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.10483">
    <id>http://arxiv.org/abs/1807.10483v1</id>
    <updated>2018-07-27T08:16:29Z</updated>
    <published>2018-07-27T08:16:29Z</published>
    <title>Faster Recovery of Approximate Periods over Edit Distance</title>
    <summary>  The approximate period recovery problem asks to compute all
$\textit{approximate word-periods}$ of a given word $S$ of length $n$: all
primitive words $P$ ($|P|=p$) which have a periodic extension at edit distance
smaller than $\tau_p$ from $S$, where $\tau_p = \lfloor
\frac{n}{(3.75+\epsilon)\cdot p} \rfloor$ for some $\epsilon>0$. Here, the set
of periodic extensions of $P$ consists of all finite prefixes of $P^\infty$.
  We improve the time complexity of the fastest known algorithm for this
problem of Amir et al. [Theor. Comput. Sci., 2018] from $O(n^{4/3})$ to $O(n
\log n)$. Our tool is a fast algorithm for Approximate Pattern Matching in
Periodic Text. We consider only verification for the period recovery problem
when the candidate approximate word-period $P$ is explicitly given up to cyclic
rotation; the algorithm of Amir et al. reduces the general problem in $O(n)$
time to a logarithmic number of such more specific instances.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszyński</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SPIRE 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.11328">
    <id>http://arxiv.org/abs/1807.11328v3</id>
    <updated>2019-02-15T14:01:33Z</updated>
    <published>2018-07-30T13:03:27Z</published>
    <title>Guidesort: Simpler Optimal Deterministic Sorting for the Parallel Disk
  Model</title>
    <summary>  A new algorithm, Guidesort, for sorting in the uniprocessor variant of the
parallel disk model (PDM) of Vitter and Shriver is presented. The algorithm is
deterministic and executes a number of (parallel) I/O operations that comes
within a constant factor $C$ of the optimum. The algorithm and its analysis are
simpler than those proposed in previous work, and the achievable constant
factor $C$ of essentially 3 appears to be smaller than for all other known
deterministic algorithms, at least for plausible parameter values.
</summary>
    <author>
      <name>Torben Hagerup</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11328v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11328v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.05942">
    <id>http://arxiv.org/abs/1806.05942v1</id>
    <updated>2018-06-15T13:14:08Z</updated>
    <published>2018-06-15T13:14:08Z</published>
    <title>Enhanced string factoring from alphabet orderings</title>
    <summary>  In this note we consider the concept of alphabet ordering in the context of
string factoring. We propose a greedy-type algorithm which produces Lyndon
factorizations with small numbers of factors along with a modification for
large numbers of factors. For the technique we introduce the Exponent Parikh
vector. Applications and research directions derived from circ-UMFFs are
discussed.
</summary>
    <author>
      <name>Amanda Clare</name>
    </author>
    <author>
      <name>Jacqueline W. Daykin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.06726">
    <id>http://arxiv.org/abs/1806.06726v4</id>
    <updated>2019-11-12T02:39:25Z</updated>
    <published>2018-06-18T14:22:07Z</published>
    <title>Zip Trees</title>
    <summary>  We introduce the zip tree, a form of randomized binary search tree that
integrates previous ideas into one practical, performant, and
pleasant-to-implement package. A zip tree is a binary search tree in which each
node has a numeric rank and the tree is (max)-heap-ordered with respect to
ranks, with rank ties broken in favor of smaller keys. Zip trees are
essentially treaps (Seidel and Aragon 1996), except that ranks are drawn from a
geometric distribution instead of a uniform distribution, and we allow rank
ties. These changes enable us to use fewer random bits per node. We perform
insertions and deletions by unmerging and merging paths ("unzipping" and
"zipping") rather than by doing rotations, which avoids some pointer changes
and improves efficiency. The methods of zipping and unzipping take inspiration
from previous top-down approaches to insertion and deletion (Stephenson 1980;
Mart\'inez and Roura 1998; Sprugnoli 1980). From a theoretical standpoint, this
work provides two main results. First, zip trees require only $O(\log \log n)$
bits (with high probability) to represent the largest rank in an $n$-node
binary search tree; previous data structures require $O(\log n)$ bits for the
largest rank. Second, zip trees are naturally isomorphic to skip lists (Pugh
1990), and simplify the mapping of (Dean and Jones 2007) between skip lists and
binary search trees.
</summary>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <author>
      <name>Caleb C. Levy</name>
    </author>
    <author>
      <name>Stephen Timmel</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06726v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06726v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.07598">
    <id>http://arxiv.org/abs/1806.07598v1</id>
    <updated>2018-06-20T07:57:10Z</updated>
    <published>2018-06-20T07:57:10Z</published>
    <title>A Faster External Memory Priority Queue with DecreaseKeys</title>
    <summary>  A priority queue is a fundamental data structure that maintains a dynamic set
of (key, priority)-pairs and supports Insert, Delete, ExtractMin and
DecreaseKey operations. In the external memory model, the current best priority
queue supports each operation in amortized $O(\frac{1}{B}\log \frac{N}{B})$
I/Os. If the DecreaseKey operation does not need to be supported, one can
design a more efficient data structure that supports the Insert, Delete and
ExtractMin operations in $O(\frac{1}{B}\log \frac{N}{B}/ \log \frac{M}{B})$
I/Os. A recent result shows that a degradation in performance is inevitable by
proving a lower bound of $\Omega(\frac{1}{B}\log B/\log\log N)$ I/Os for
priority queues with DecreaseKeys. In this paper we tighten the gap between the
lower bound and the upper bound by proposing a new priority queue which
supports the DecreaseKey operation and has an expected amortized I/O complexity
of $O(\frac{1}{B}\log \frac{N}{B}/\log\log N)$. Our result improves the
external memory priority queue with DecreaseKeys for the first time in over a
decade, and also gives the fastest external memory single source shortest path
algorithm.
</summary>
    <author>
      <name>Shunhua Jiang</name>
    </author>
    <author>
      <name>Kasper Green Larsen</name>
    </author>
    <link href="http://arxiv.org/abs/1806.07598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.08692">
    <id>http://arxiv.org/abs/1806.08692v1</id>
    <updated>2018-06-22T14:26:39Z</updated>
    <published>2018-06-22T14:26:39Z</published>
    <title>Improved bounds for multipass pairing heaps and path-balanced binary
  search trees</title>
    <summary>  We revisit multipass pairing heaps and path-balanced binary search trees
(BSTs), two classical algorithms for data structure maintenance. The pairing
heap is a simple and efficient "self-adjusting" heap, introduced in 1986 by
Fredman, Sedgewick, Sleator, and Tarjan. In the multipass variant (one of the
original pairing heap variants described by Fredman et al.) the minimum item is
extracted via repeated pairing rounds in which neighboring siblings are linked.
  Path-balanced BSTs, proposed by Sleator (Subramanian, 1996), are a natural
alternative to Splay trees (Sleator and Tarjan, 1983). In a path-balanced BST,
whenever an item is accessed, the search path leading to that item is
re-arranged into a balanced tree.
  Despite their simplicity, both algorithms turned out to be difficult to
analyse. Fredman et al. showed that operations in multipass pairing heaps take
amortized $O(\log{n} \cdot \log\log{n} / \log\log\log{n})$ time. For searching
in path-balanced BSTs, Balasubramanian and Raman showed in 1995 the same
amortized time bound of $O(\log{n} \cdot \log\log{n} / \log\log\log{n})$, using
a different argument.
  In this paper we show an explicit connection between the two algorithms and
improve the two bounds to $O\left(\log{n} \cdot 2^{\log^{\ast}{n}} \cdot
\log^{\ast}{n}\right)$, respectively $O\left(\log{n} \cdot 2^{\log^{\ast}{n}}
\cdot (\log^{\ast}{n})^2 \right)$, where $\log^{\ast}(\cdot)$ denotes the very
slowly growing iterated logarithm function. These are the first improvements in
more than three, resp. two decades, approaching in both cases the
information-theoretic lower bound of $\Omega(\log{n})$.
</summary>
    <author>
      <name>Dani Dorfman</name>
    </author>
    <author>
      <name>Haim Kaplan</name>
    </author>
    <author>
      <name>László Kozma</name>
    </author>
    <author>
      <name>Seth Pettie</name>
    </author>
    <author>
      <name>Uri Zwick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at ESA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.09646">
    <id>http://arxiv.org/abs/1806.09646v1</id>
    <updated>2018-06-25T18:02:05Z</updated>
    <published>2018-06-25T18:02:05Z</published>
    <title>Fast entropy-bounded string dictionary look-up with mismatches</title>
    <summary>  We revisit the fundamental problem of dictionary look-up with mismatches.
Given a set (dictionary) of $d$ strings of length $m$ and an integer $k$, we
must preprocess it into a data structure to answer the following queries: Given
a query string $Q$ of length $m$, find all strings in the dictionary that are
at Hamming distance at most $k$ from $Q$. Chan and Lewenstein (CPM 2015) showed
a data structure for $k = 1$ with optimal query time $O(m/w + occ)$, where $w$
is the size of a machine word and $occ$ is the size of the output. The data
structure occupies $O(w d \log^{1+\varepsilon} d)$ extra bits of space (beyond
the entropy-bounded space required to store the dictionary strings). In this
work we give a solution with similar bounds for a much wider range of values
$k$. Namely, we give a data structure that has $O(m/w + \log^k d + occ)$ query
time and uses $O(w d \log^k d)$ extra bits of space.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper accepted to MFCS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.10498">
    <id>http://arxiv.org/abs/1806.10498v1</id>
    <updated>2018-06-27T14:27:02Z</updated>
    <published>2018-06-27T14:27:02Z</published>
    <title>Dynamic Trees with Almost-Optimal Access Cost</title>
    <summary>  An optimal binary search tree for an access sequence on elements is a static
tree that minimizes the total search cost. Constructing perfectly optimal
binary search trees is expensive so the most efficient algorithms construct
almost optimal search trees. There exists a long literature of constructing
almost optimal search trees dynamically, i.e., when the access pattern is not
known in advance. All of these trees, e.g., splay trees and treaps, provide a
multiplicative approximation to the optimal search cost.
  In this paper we show how to maintain an almost optimal weighted binary
search tree under access operations and insertions of new elements where the
approximation is an additive constant. More technically, we maintain a tree in
which the depth of the leaf holding an element $e_i$ does not exceed
$\min(\log(W/w_i),\log n)+O(1)$ where $w_i$ is the number of times $e_i$ was
accessed and $W$ is the total length of the access sequence.
  Our techniques can also be used to encode a sequence of $m$ symbols with a
dynamic alphabetic code in $O(m)$ time so that the encoding length is bounded
by $m(H+O(1))$, where $H$ is the entropy of the sequence. This is the first
efficient algorithm for adaptive alphabetic coding that runs in constant time
per symbol.
</summary>
    <author>
      <name>Mordecai Golin</name>
    </author>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of an ESA'18 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.10261">
    <id>http://arxiv.org/abs/1806.10261v1</id>
    <updated>2018-06-27T00:58:24Z</updated>
    <published>2018-06-27T00:58:24Z</published>
    <title>BDDs Naturally Represent Boolean Functions, and ZDDs Naturally Represent
  Sets of Sets</title>
    <summary>  This paper studies a difference between Binary Decision Diagrams (BDDs) and
Zero-suppressed BDDs (ZDDs) from a conceptual point of view. It is commonly
understood that a BDD is a representation of a Boolean function, whereas a ZDD
is a representation of a set of sets. However, there is a one-to-one
correspondence between Boolean functions and sets of sets, and therefore we
could also regard a BDD as a representation of a set of sets, and similarly for
a ZDD and a Boolean function. The aim of this paper is to give an explanation
why the distinction between BDDs and ZDDs mentioned above is made despite the
existence of the one-to-one correspondence. To achieve this, we first observe
that Boolean functions and sets of sets are equipped with non-isomorphic
functor structures, and show that these functor structures are reflected in the
definitions of BDDs and ZDDs. This result can be stated formally as naturality
of certain maps. To the author's knowledge, this is the first formally stated
theorem that justifies the commonly accepted distinction between BDDs and ZDDs.
In addition, we show that this result extends to sentential decision diagrams
and their zero-suppressed variant.
</summary>
    <author>
      <name>Kensuke Kojima</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.0; E.1; F.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.10176">
    <id>http://arxiv.org/abs/1806.10176v1</id>
    <updated>2018-06-26T19:12:10Z</updated>
    <published>2018-06-26T19:12:10Z</published>
    <title>Practical Access to Dynamic Programming on Tree Decompositions</title>
    <summary>  Parameterized complexity theory has lead to a wide range of algorithmic
breakthroughs within the last decades, but the practicability of these methods
for real-world problems is still not well understood. We investigate the
practicability of one of the fundamental approaches of this field: dynamic
programming on tree decompositions. Indisputably, this is a key technique in
parameterized algorithms and modern algorithm design. Despite the enormous
impact of this approach in theory, it still has very little influence on
practical implementations. The reasons for this phenomenon are manifold. One of
them is the simple fact that such an implementation requires a long chain of
non-trivial tasks (as computing the decomposition, preparing it,...). We
provide an easy way to implement such dynamic programs that only requires the
definition of the update rules. With this interface, dynamic programs for
various problems, such as 3-coloring, can be implemented easily in about 100
lines of structured Java code.
  The theoretical foundation of the success of dynamic programming on tree
decompositions is well understood due to Courcelle's celebrated theorem, which
states that every MSO-definable problem can be efficiently solved if a tree
decomposition of small width is given. We seek to provide practical access to
this theorem as well, by presenting a lightweight model-checker for a small
fragment of MSO. This fragment is powerful enough to describe many natural
problems, and our model-checker turns out to be very competitive against
similar state-of-the-art tools.
</summary>
    <author>
      <name>Max Bannach</name>
    </author>
    <author>
      <name>Sebastian Berndt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ESA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.00112">
    <id>http://arxiv.org/abs/1807.00112v1</id>
    <updated>2018-06-30T02:33:15Z</updated>
    <published>2018-06-30T02:33:15Z</published>
    <title>Approximate Nearest Neighbors in Limited Space</title>
    <summary>  We consider the $(1+\epsilon)$-approximate nearest neighbor search problem:
given a set $X$ of $n$ points in a $d$-dimensional space, build a data
structure that, given any query point $y$, finds a point $x \in X$ whose
distance to $y$ is at most $(1+\epsilon) \min_{x \in X} \|x-y\|$ for an
accuracy parameter $\epsilon \in (0,1)$. Our main result is a data structure
that occupies only $O(\epsilon^{-2} n \log(n) \log(1/\epsilon))$ bits of space,
assuming all point coordinates are integers in the range $\{-n^{O(1)} \ldots
n^{O(1)}\}$, i.e., the coordinates have $O(\log n)$ bits of precision. This
improves over the best previously known space bound of $O(\epsilon^{-2} n
\log(n)^2)$, obtained via the randomized dimensionality reduction method of
Johnson and Lindenstrauss (1984). We also consider the more general problem of
estimating all distances from a collection of query points to all data points
$X$, and provide almost tight upper and lower bounds for the space complexity
of this problem.
</summary>
    <author>
      <name>Piotr Indyk</name>
    </author>
    <author>
      <name>Tal Wagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">COLT 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.01804">
    <id>http://arxiv.org/abs/1807.01804v2</id>
    <updated>2018-11-02T17:37:28Z</updated>
    <published>2018-07-04T22:49:10Z</published>
    <title>Optimal Ball Recycling</title>
    <summary>  Balls-and-bins games have been a wildly successful tool for modeling load
balancing problems. In this paper, we study a new scenario, which we call the
ball recycling game, defined as follows:
  Throw m balls into n bins i.i.d. according to a given probability
distribution p. Then, at each time step, pick a non-empty bin and recycle its
balls: take the balls from the selected bin and re-throw them according to p.
  This balls-and-bins game closely models memory-access heuristics in
databases. The goal is to have a bin-picking method that maximizes the
recycling rate, defined to be the expected number of balls recycled per step in
the stationary distribution. We study two natural strategies for ball
recycling: Fullest Bin, which greedily picks the bin with the maximum number of
balls, and Random Ball, which picks a ball at random and recycles its bin. We
show that for general p, Random Ball is constant-optimal, whereas Fullest Bin
can be pessimal. However, when p = u, the uniform distribution, Fullest Bin is
optimal to within an additive constant.
</summary>
    <author>
      <name>Michael A. Bender</name>
    </author>
    <author>
      <name>Jake Christensen</name>
    </author>
    <author>
      <name>Alex Conway</name>
    </author>
    <author>
      <name>Martín Farach-Colton</name>
    </author>
    <author>
      <name>Rob Johnson</name>
    </author>
    <author>
      <name>Meng-Tsung Tsai</name>
    </author>
    <link href="http://arxiv.org/abs/1807.01804v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01804v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.01217">
    <id>http://arxiv.org/abs/1806.01217v1</id>
    <updated>2018-06-04T16:59:15Z</updated>
    <published>2018-06-04T16:59:15Z</published>
    <title>Efficient Genomic Interval Queries Using Augmented Range Trees</title>
    <summary>  Efficient large-scale annotation of genomic intervals is essential for
personal genome interpretation in the realm of precision medicine. There are 13
possible relations between two intervals according to Allen's interval algebra.
Conventional interval trees are routinely used to identify the genomic
intervals satisfying a coarse relation with a query interval, but cannot
support efficient query for more refined relations such as all Allen's
relations. We design and implement a novel approach to address this unmet need.
Through rewriting Allen's interval relations, we transform an interval query to
a range query, then adapt and utilize the range trees for querying. We
implement two types of range trees: a basic 2-dimensional range tree (2D-RT)
and an augmented range tree with fractional cascading (RTFC) and compare them
with the conventional interval tree (IT). Theoretical analysis shows that RTFC
can achieve the best time complexity for interval queries regarding all Allen's
relations among the three trees. We also perform comparative experiments on the
efficiency of RTFC, 2D-RT and IT in querying noncoding element annotations in a
large collection of personal genomes. Our experimental results show that 2D-RT
is more efficient than IT for interval queries regarding most of Allen's
relations, RTFC is even more efficient than 2D-RT. The results demonstrate that
RTFC is an efficient data structure for querying large-scale datasets regarding
Allen's relations between genomic intervals, such as those required by
interpreting genome-wide variation in large populations.
</summary>
    <author>
      <name>Chengsheng Mao</name>
    </author>
    <author>
      <name>Alal Eran</name>
    </author>
    <author>
      <name>Yuan Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.00588">
    <id>http://arxiv.org/abs/1806.00588v1</id>
    <updated>2018-06-02T06:18:15Z</updated>
    <published>2018-06-02T06:18:15Z</published>
    <title>Fast Locality Sensitive Hashing for Beam Search on GPU</title>
    <summary>  We present a GPU-based Locality Sensitive Hashing (LSH) algorithm to speed up
beam search for sequence models. We utilize the winner-take-all (WTA) hash,
which is based on relative ranking order of hidden dimensions and thus
resilient to perturbations in numerical values. Our algorithm is designed by
fully considering the underling architecture of CUDA-enabled GPUs
(Algorithm/Architecture Co-design): 1) A parallel Cuckoo hash table is applied
for LSH code lookup (guaranteed O(1) lookup time); 2) Candidate lists are
shared across beams to maximize the parallelism; 3) Top frequent words are
merged into candidate lists to improve performance. Experiments on 4
large-scale neural machine translation models demonstrate that our algorithm
can achieve up to 4x speedup on softmax module, and 2x overall speedup without
hurting BLEU on GPU.
</summary>
    <author>
      <name>Xing Shi</name>
    </author>
    <author>
      <name>Shizhen Xu</name>
    </author>
    <author>
      <name>Kevin Knight</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.01799">
    <id>http://arxiv.org/abs/1806.01799v2</id>
    <updated>2019-04-27T17:00:27Z</updated>
    <published>2018-06-05T16:41:07Z</published>
    <title>Survey and Taxonomy of Lossless Graph Compression and Space-Efficient
  Graph Representations</title>
    <summary>  Various graphs such as web or social networks may contain up to trillions of
edges. Compressing such datasets can accelerate graph processing by reducing
the amount of I/O accesses and the pressure on the memory subsystem. Yet,
selecting a proper compression method is challenging as there exist a plethora
of techniques, algorithms, domains, and approaches in compressing graphs. To
facilitate this, we present a survey and taxonomy on lossless graph compression
that is the first, to the best of our knowledge, to exhaustively analyze this
domain. Moreover, our survey does not only categorize existing schemes, but
also explains key ideas, discusses formal underpinning in selected works, and
describes the space of the existing compression schemes using three dimensions:
areas of research (e.g., compressing web graphs), techniques (e.g., gap
encoding), and features (e.g., whether or not a given scheme targets dynamic
graphs). Our survey can be used as a guide to select the best lossless
compression scheme in a given setting.
</summary>
    <author>
      <name>Maciej Besta</name>
    </author>
    <author>
      <name>Torsten Hoefler</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01799v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01799v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.01804">
    <id>http://arxiv.org/abs/1806.01804v2</id>
    <updated>2018-09-06T20:29:19Z</updated>
    <published>2018-06-05T16:53:02Z</published>
    <title>Tree Path Majority Data Structures</title>
    <summary>  We present the first solution to $\tau$-majorities on tree paths. Given a
tree of $n$ nodes, each with a label from $[1..\sigma]$, and a fixed threshold
$0&lt;\tau&lt;1$, such a query gives two nodes $u$ and $v$ and asks for all the
labels that appear more than $\tau \cdot |P_{uv}|$ times in the path $P_{uv}$
from $u$ to $v$, where $|P_{uv}|$ denotes the number of nodes in $P_{uv}$. Note
that the answer to any query is of size up to $1/\tau$. On a $w$-bit RAM, we
obtain a linear-space data structure with $O((1/\tau)\log^* n \log\log_w
\sigma)$ query time. For any $\kappa > 1$, we can also build a structure that
uses $O(n\log^{[\kappa]} n)$ space, where $\log^{[\kappa]} n$ denotes the
function that applies logarithm $\kappa$ times to $n$, and answers queries in
time $O((1/\tau)\log\log_w \sigma)$. The construction time of both structures
is $O(n\log n)$. We also describe two succinct-space solutions with the same
query time of the linear-space structure. One uses $2nH + 4n + o(n)(H+1)$ bits,
where $H \le \lg\sigma$ is the entropy of the label distribution, and can be
built in $O(n\log n)$ time. The other uses $nH + O(n) + o(nH)$ bits and is
built in $O(n\log n)$ time w.h.p.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01804v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01804v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.02004">
    <id>http://arxiv.org/abs/1806.02004v1</id>
    <updated>2018-06-06T04:39:41Z</updated>
    <published>2018-06-06T04:39:41Z</published>
    <title>Another Proof of Cuckoo hashing with New Variants</title>
    <summary>  We show a new proof for the load of obtained by a Cuckoo Hashing data
structure. Our proof is arguably simpler than previous proofs and allows for
new generalizations. The proof first appeared in Pinkas et. al. \cite{PSWW19}
in the context of a protocol for private set intersection. We present it here
separately to improve its readability.
</summary>
    <author>
      <name>Udi Wieder</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.02718">
    <id>http://arxiv.org/abs/1806.02718v1</id>
    <updated>2018-06-07T15:03:00Z</updated>
    <published>2018-06-07T15:03:00Z</published>
    <title>Alignment-free sequence comparison using absent words</title>
    <summary>  Sequence comparison is a prerequisite to virtually all comparative genomic
analyses. It is often realised by sequence alignment techniques, which are
computationally expensive. This has led to increased research into
alignment-free techniques, which are based on measures referring to the
composition of sequences in terms of their constituent patterns. These
measures, such as $q$-gram distance, are usually computed in time linear with
respect to the length of the sequences. In this paper, we focus on the
complementary idea: how two sequences can be efficiently compared based on
information that does not occur in the sequences. A word is an {\em absent
word} of some sequence if it does not occur in the sequence. An absent word is
{\em minimal} if all its proper factors occur in the sequence. Here we present
the first linear-time and linear-space algorithm to compare two sequences by
considering {\em all} their minimal absent words. In the process, we present
results of combinatorial interest, and also extend the proposed techniques to
compare circular sequences. We also present an algorithm that, given a word $x$
of length $n$, computes the largest integer for which all factors of $x$ of
that length occur in some minimal absent word of $x$ in time and space
$\cO(n)$. Finally, we show that the known asymptotic upper bound on the number
of minimal absent words of a word is tight.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Robert Mercas</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of "Linear-Time Sequence Comparison Using Minimal
  Absent Words &amp; Applications" Proc. LATIN 2016, arxiv:1506.04917</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.02718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.03102">
    <id>http://arxiv.org/abs/1806.03102v1</id>
    <updated>2018-06-08T11:58:40Z</updated>
    <published>2018-06-08T11:58:40Z</published>
    <title>Compressed Communication Complexity of Longest Common Prefixes</title>
    <summary>  We consider the communication complexity of fundamental longest common prefix
(Lcp) problems. In the simplest version, two parties, Alice and Bob, each hold
a string, $A$ and $B$, and we want to determine the length of their longest
common prefix $l=\text{Lcp}(A,B)$ using as few rounds and bits of communication
as possible. We show that if the longest common prefix of $A$ and $B$ is
compressible, then we can significantly reduce the number of rounds compared to
the optimal uncompressed protocol, while achieving the same (or fewer) bits of
communication. Namely, if the longest common prefix has an LZ77 parse of $z$
phrases, only $O(\lg z)$ rounds and $O(\lg \ell)$ total communication is
necessary.
  We extend the result to the natural case when Bob holds a set of strings
$B_1, \ldots, B_k$, and the goal is to find the length of the maximal longest
prefix shared by $A$ and any of $B_1, \ldots, B_k$. Here, we give a protocol
with $O(\log z)$ rounds and $O(\lg z \lg k + \lg \ell)$ total communication.
  We present our result in the public-coin model of computation but by a
standard technique our results generalize to the private-coin model.
Furthermore, if we view the input strings as integers the problems are the
greater-than problem and the predecessor problem.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Mikko Berggreen Ettienne</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Eva Rotenberg</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.03611">
    <id>http://arxiv.org/abs/1806.03611v1</id>
    <updated>2018-06-10T08:29:30Z</updated>
    <published>2018-06-10T08:29:30Z</published>
    <title>CuCoTrack: Cuckoo Filter Based Connection Tracking</title>
    <summary>  This paper introduces CuCoTrack, a cuckoo hash based data structure designed
to efficiently implement connection tracking. The proposed scheme exploits the
fact that queries always match one existing connection to compress the 5-tuple
that identifies the connection. This reduces significantly the amount of memory
needed to store the connections and also the memory bandwidth needed for
lookups. CuCoTrack uses a dynamic fingerprint to avoid collisions thus ensuring
that queries are completed in at most two memory accesses and facilitating a
hardware implementation. The proposed scheme has been analyzed theoretically
and validated by simulation. The results show that using 16 bits for the
fingerprint is enough to avoid collisions in practical configurations.
</summary>
    <author>
      <name>Pedro Reviriego</name>
    </author>
    <author>
      <name>Salvatore Pontarelli</name>
    </author>
    <author>
      <name>Gil Levy</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.04277">
    <id>http://arxiv.org/abs/1806.04277v1</id>
    <updated>2018-06-12T00:22:04Z</updated>
    <published>2018-06-12T00:22:04Z</published>
    <title>Indexed Dynamic Programming to boost Edit Distance and LCSS Computation</title>
    <summary>  There are efficient dynamic programming solutions to the computation of the
Edit Distance from $S\in[1..\sigma]^n$ to $T\in[1..\sigma]^m$, for many natural
subsets of edit operations, typically in time within $O(nm)$ in the worst-case
over strings of respective lengths $n$ and $m$ (which is likely to be optimal),
and in time within $O(n{+}m)$ in some special cases (e.g. disjoint alphabets).
We describe how indexing the strings (in linear time), and using such an index
to refine the recurrence formulas underlying the dynamic programs, yield faster
algorithms in a variety of models, on a continuum of classes of instances of
intermediate difficulty between the worst and the best case, thus refining the
analysis beyond the worst case analysis. As a side result, we describe similar
properties for the computation of the Longest Common Sub Sequence $LCSS(S,T)$
between $S$ and $T$, since it is a particular case of Edit Distance, and we
discuss the application of similar algorithmic and analysis techniques for
other dynamic programming solutions. More formally, we propose a parameterized
analysis of the computational complexity of the Edit Distance for various set
of operators and of the Longest Common Sub Sequence in function of the area of
the dynamic program matrix relevant to the computation.
</summary>
    <author>
      <name>Jérémy Barbay</name>
    </author>
    <author>
      <name>Andrés Olivares</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.04890">
    <id>http://arxiv.org/abs/1806.04890v1</id>
    <updated>2018-06-13T08:30:00Z</updated>
    <published>2018-06-13T08:30:00Z</published>
    <title>$O(n \log n)$-time text compression by LZ-style longest first
  substitution</title>
    <summary>  Mauer et al. [A Lempel-Ziv-style Compression Method for Repetitive Texts, PSC
2017] proposed a hybrid text compression method called LZ-LFS which has both
features of Lempel-Ziv 77 factorization and longest first substitution. They
showed that LZ-LFS can achieve better compression ratio for repetitive texts,
compared to some state-of-the-art compression algorithms. The drawback of Mauer
et al.'s method is that their LZ-LFS compression algorithm takes $O(n^2)$ time
on an input string of length $n$. In this paper, we show a faster LZ-LFS
compression algorithm that works in $O(n \log n)$ time. We also propose a
simpler version of LZ-LFS that can be computed in $O(n)$ time.
</summary>
    <author>
      <name>Akihiro Nishi</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.08612">
    <id>http://arxiv.org/abs/1805.08612v3</id>
    <updated>2019-07-07T21:03:01Z</updated>
    <published>2018-05-22T14:27:38Z</published>
    <title>On the Worst-Case Complexity of TimSort</title>
    <summary>  TimSort is an intriguing sorting algorithm designed in 2002 for Python, whose
worst-case complexity was announced, but not proved until our recent preprint.
In fact, there are two slightly different versions of TimSort that are
currently implemented in Python and in Java respectively. We propose a
pedagogical and insightful proof that the Python version runs in
$\mathcal{O}(n\log n)$. The approach we use in the analysis also applies to the
Java version, although not without very involved technical details. As a
byproduct of our study, we uncover a bug in the Java implementation that can
cause the sorting method to fail during the execution. We also give a proof
that Python's TimSort running time is in $\mathcal{O}(n + n\log \rho)$, where
$\rho$ is the number of runs (i.e. maximal monotonic sequences), which is quite
a natural parameter here and part of the explanation for the good behavior of
TimSort on partially sorted inputs.
</summary>
    <author>
      <name>Nicolas Auger</name>
    </author>
    <author>
      <name>Vincent Jugé</name>
    </author>
    <author>
      <name>Cyril Nicaud</name>
    </author>
    <author>
      <name>Carine Pivoteau</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08612v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08612v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.08602">
    <id>http://arxiv.org/abs/1805.08602v1</id>
    <updated>2018-05-19T14:23:51Z</updated>
    <published>2018-05-19T14:23:51Z</published>
    <title>Orthogonal Point Location and Rectangle Stabbing Queries in 3-d</title>
    <summary>  In this work, we present a collection of new results on two fundamental
problems in geometric data structures: orthogonal point location and rectangle
stabbing.
  -We give the first linear-space data structure that supports 3-d point
location queries on $n$ disjoint axis-aligned boxes with optimal $O\left( \log
n\right)$ query time in the (arithmetic) pointer machine model. This improves
the previous $O\left( \log^{3/2} n \right)$ bound of Rahul [SODA 2015]. We
similarly obtain the first linear-space data structure in the I/O model with
optimal query cost, and also the first linear-space data structure in the word
RAM model with sub-logarithmic query time.
  -We give the first linear-space data structure that supports 3-d $4$-sided
and $5$-sided rectangle stabbing queries in optimal $O(\log_wn+k)$ time in the
word RAM model. We similarly obtain the first optimal data structure for the
closely related problem of 2-d top-$k$ rectangle stabbing in the word RAM
model, and also improved results for 3-d 6-sided rectangle stabbing.
  For point location, our solution is simpler than previous methods, and is
based on an interesting variant of the van Emde Boas recursion, applied in a
round-robin fashion over the dimensions, combined with bit-packing techniques.
For rectangle stabbing, our solution is a variant of Alstrup, Brodal, and
Rauhe's grid-based recursive technique (FOCS 2000), combined with a number of
new ideas.
</summary>
    <author>
      <name>Timothy M. Chan</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Saladi Rahul</name>
    </author>
    <author>
      <name>Konstantinos Tsakalidis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of the ICALP'18 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.08816">
    <id>http://arxiv.org/abs/1805.08816v1</id>
    <updated>2018-05-22T18:58:54Z</updated>
    <published>2018-05-22T18:58:54Z</published>
    <title>copMEM: Finding maximal exact matches via sampling both genomes</title>
    <summary>  Genome-to-genome comparisons require designating anchor points, which are
given by Maximum Exact Matches (MEMs) between their sequences. For large
genomes this is a challenging problem and the performance of existing
solutions, even in parallel regimes, is not quite satisfactory. We present a
new algorithm, copMEM, that allows to sparsely sample both input genomes, with
sampling steps being coprime. Despite being a single-threaded implementation,
copMEM computes all MEMs of minimum length 100 between the human and mouse
genomes in less than 2 minutes, using less than 10 GB of RAM memory.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Wojciech Bieniecki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The source code of copMEM is freely available at
  https://github.com/wbieniec/copmem. Contact: wbieniec@kis.p.lodz.pl,
  wbieniec@kis.p.lodz.pl</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.09423">
    <id>http://arxiv.org/abs/1805.09423v1</id>
    <updated>2018-05-23T21:00:47Z</updated>
    <published>2018-05-23T21:00:47Z</published>
    <title>Optimal Hashing in External Memory</title>
    <summary>  Hash tables are a ubiquitous class of dictionary data structures. However,
standard hash table implementations do not translate well into the external
memory model, because they do not incorporate locality for insertions.
  Iacono and Patracsu established an update/query tradeoff curve for external
hash tables: a hash table that performs insertions in $O(\lambda/B)$ amortized
IOs requires $\Omega(\log_\lambda N)$ expected IOs for queries, where $N$ is
the number of items that can be stored in the data structure, $B$ is the size
of a memory transfer, $M$ is the size of memory, and $\lambda$ is a tuning
parameter.
  They provide a hashing data structure that meets this curve for $\lambda$
that is $\Omega(\log\log M + \log_M N)$. Their data structure, which we call an
\defn{IP hash table}, is complicated and, to the best of our knowledge, has not
been implemented.
  In this paper, we present a new and much simpler optimal external memory hash
table, the \defn{Bundle of Arrays Hash Table} (BOA). BOAs are based on
size-tiered LSMs, a well-studied data structure, and are almost as easy to
implement. The BOA is optimal for a narrower range of $\lambda$. However, the
simplicity of BOAs allows them to be readily modified to achieve the following
results:
  \begin{itemize}
  \item A new external memory data structure, the \defn{Bundle of Trees Hash
Table} (BOT), that matches the performance of the IP hash table, while
retaining some of the simplicity of the BOAs.
  \item The \defn{cache-oblivious Bundle of Trees Hash Table} (COBOT), the
first cache-oblivious hash table. This data structure matches the optimality of
BOTs and IP hash tables over the same range of $\lambda$. \end{itemize}
</summary>
    <author>
      <name>Alex Conway</name>
    </author>
    <author>
      <name>Martin Farach-Colton</name>
    </author>
    <author>
      <name>Philip Shilane</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.10070">
    <id>http://arxiv.org/abs/1805.10070v1</id>
    <updated>2018-05-25T10:24:12Z</updated>
    <published>2018-05-25T10:24:12Z</published>
    <title>Strong link between BWT and XBW via Aho-Corasick automaton and
  applications to Run-Length Encoding</title>
    <summary>  The boom of genomic sequencing makes compression of set of sequences
inescapable. This underlies the need for multi-string indexing data structures
that helps compressing the data. The most prominent example of such data
structures is the Burrows-Wheeler Transform (BWT), a reversible permutation of
a text that improves its compressibility. A similar data structure, the
eXtended Burrows-Wheeler Transform (XBW), is able to index a tree labelled with
alphabet symbols. A link between a multi-string BWT and the Aho-Corasick
automaton has already been found and led to a way to build a XBW from a
multi-string BWT. We exhibit a stronger link between a multi-string BWT and a
XBW by using the order of the concatenation in the multi-string. This bijective
link has several applications: first, it allows to build one data structure
from the other; second, it enables one to compute an ordering of the input
strings that optimises a Run-Length measure (i.e., the compressibility) of the
BWT or of the XBW.
</summary>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Eric Rivals</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.10042">
    <id>http://arxiv.org/abs/1805.10042v1</id>
    <updated>2018-05-25T08:54:26Z</updated>
    <published>2018-05-25T08:54:26Z</published>
    <title>Algorithms for Anti-Powers in Strings</title>
    <summary>  A string $S[1,n]$ is a power (or tandem repeat) of order $k$ and period $n/k$
if it can decomposed into $k$ consecutive equal-length blocks of letters.
Powers and periods are fundamental to string processing, and algorithms for
their efficient computation have wide application and are heavily studied.
Recently, Fici et al. (Proc. ICALP 2016) defined an {\em anti-power} of order
$k$ to be a string composed of $k$ pairwise-distinct blocks of the same length
($n/k$, called {\em anti-period}). Anti-powers are a natural converse to
powers, and are objects of combinatorial interest in their own right. In this
paper we initiate the algorithmic study of anti-powers. Given a string $S$, we
describe an optimal algorithm for locating all substrings of $S$ that are
anti-powers of a specified order. The optimality of the algorithm follows form
a combinatorial lemma that provides a lower bound on the number of distinct
anti-powers of a given order: we prove that a string of length $n$ can contain
$\Theta(n^2/k)$ distinct anti-powers of order $k$.
</summary>
    <author>
      <name>Golnaz Badkobeh</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ipl.2018.05.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ipl.2018.05.003" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Informnation Processing Letters Volume 137, September
  2018, Pages 57-60</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.10042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.09924">
    <id>http://arxiv.org/abs/1805.09924v2</id>
    <updated>2018-07-01T07:39:26Z</updated>
    <published>2018-05-24T22:14:27Z</published>
    <title>Longest Unbordered Factor in Quasilinear Time</title>
    <summary>  A border u of a word w is a proper factor of w occurring both as a prefix and
as a suffix. The maximal unbordered factor of w is the longest factor of w
which does not have a border. Here an O(n log n)-time with high probability (or
O(n log n log^2 log n)-time deterministic) algorithm to compute the Longest
Unbordered Factor Array of w for general alphabets is presented, where n is the
length of w. This array specifies the length of the maximal unbordered factor
starting at each position of w. This is a major improvement on the running time
of the currently best worst-case algorithm working in O(n^{1.5} ) time for
integer alphabets [Gawrychowski et al., 2015].
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Ritu Kundu</name>
    </author>
    <author>
      <name>Manal Mohamed</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09924v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09924v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.11255">
    <id>http://arxiv.org/abs/1805.11255v1</id>
    <updated>2018-05-29T05:53:08Z</updated>
    <published>2018-05-29T05:53:08Z</published>
    <title>Succinct data structure for dynamic trees with faster queries</title>
    <summary>  Navarro and Sadakane [TALG 2014] gave a dynamic succinct data structure for
storing an ordinal tree. The structure supports tree queries in either $O(\log
n/\log\log n)$ or $O(\log n)$ time, and insertion or deletion of a single node
in $O(\log n)$ time. In this paper we improve the result of Navarro and
Sadakane by reducing the time complexities of some queries (e.g.\ degree and
level\_ancestor) from $O(\log n)$ to $O(\log n/\log\log n)$.
</summary>
    <author>
      <name>Dekel Tsur</name>
    </author>
    <link href="http://arxiv.org/abs/1805.11255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.11864">
    <id>http://arxiv.org/abs/1805.11864v1</id>
    <updated>2018-05-30T08:56:47Z</updated>
    <published>2018-05-30T08:56:47Z</published>
    <title>Space-Efficient DFS and Applications: Simpler, Leaner, Faster</title>
    <summary>  The problem of space-efficient depth-first search (DFS) is reconsidered. A
particularly simple and fast algorithm is presented that, on a directed or
undirected input graph $G=(V,E)$ with $n$ vertices and $m$ edges, carries out a
DFS in $O(n+m)$ time with $n+\sum_{v\in V_{\ge 3}}\lceil{\log_2(d_v-1)}\rceil
  +O(\log n)\le n+m+O(\log n)$ bits of working memory, where $d_v$ is the
(total) degree of $v$, for each $v\in V$, and $V_{\ge 3}=\{v\in V\mid d_v\ge
3\}$. A slightly more complicated variant of the algorithm works in the same
time with at most $n+({4/5})m+O(\log n)$ bits. It is also shown that a DFS can
be carried out in a graph with $n$ vertices and $m$ edges in $O(n+m\log^*\! n)$
time with $O(n)$ bits or in $O(n+m)$ time with either $O(n\log\log(4+{m/n}))$
bits or, for arbitrary integer $k\ge 1$, $O(n\log^{(k)}\! n)$ bits. These
results among them subsume or improve most earlier results on space-efficient
DFS. Some of the new time and space bounds are shown to extend to applications
of DFS such as the computation of cut vertices, bridges, biconnected components
and 2-edge-connected components in undirected graphs.
</summary>
    <author>
      <name>Torben Hagerup</name>
    </author>
    <link href="http://arxiv.org/abs/1805.11864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1806.00198">
    <id>http://arxiv.org/abs/1806.00198v3</id>
    <updated>2018-08-06T07:50:30Z</updated>
    <published>2018-06-01T05:22:55Z</published>
    <title>Block Palindromes: A New Generalization of Palindromes</title>
    <summary>  We study a new generalization of palindromes and gapped palindromes called
block palindromes. A block palindrome is a string that becomes a palindrome
when identical substrings are replaced with a distinct character. We
investigate several properties of block palindromes and in particular, study
substrings of a string which are block palindromes. In so doing, we introduce
the notion of a \emph{maximal block palindrome}, which leads to a compact
representation of all block palindromes that occur in a string. We also propose
an algorithm which enumerates all maximal block palindromes that appear in a
given string $T$ in $O(|T| + \|\mathit{MBP}(T)\|)$ time, where
$\|\mathit{MBP}(T)\|$ is the output size, which is optimal unless all the
maximal block palindromes can be represented in a more compact way.
</summary>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00198v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00198v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.03834">
    <id>http://arxiv.org/abs/1805.03834v2</id>
    <updated>2018-06-15T06:47:25Z</updated>
    <published>2018-05-10T06:05:26Z</published>
    <title>Haplotype-aware graph indexes</title>
    <summary>  The variation graph toolkit (VG) represents genetic variation as a graph.
Each path in the graph is a potential haplotype, though most paths are unlikely
recombinations of true haplotypes. We augment the VG model with haplotype
information to identify which paths are more likely to be correct. For this
purpose, we develop a scalable implementation of the graph extension of the
positional Burrows--Wheeler transform. We demonstrate the scalability of the
new implementation by indexing the 1000 Genomes Project haplotypes. We also
develop an algorithm for simplifying variation graphs for k-mer indexing
without losing any k-mers in the haplotypes.
</summary>
    <author>
      <name>Jouni Sirén</name>
    </author>
    <author>
      <name>Erik Garrison</name>
    </author>
    <author>
      <name>Adam M. Novak</name>
    </author>
    <author>
      <name>Benedict Paten</name>
    </author>
    <author>
      <name>Richard Durbin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to WABI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.03834v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03834v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.04154">
    <id>http://arxiv.org/abs/1805.04154v1</id>
    <updated>2018-05-10T20:00:42Z</updated>
    <published>2018-05-10T20:00:42Z</published>
    <title>Nearly-Optimal Mergesorts: Fast, Practical Sorting Methods That
  Optimally Adapt to Existing Runs</title>
    <summary>  We present two stable mergesort variants, "peeksort" and "powersort", that
exploit existing runs and find nearly-optimal merging orders with practically
negligible overhead. Previous methods either require substantial effort for
determining the merging order (Takaoka 2009; Barbay &amp; Navarro 2013) or do not
have a constant-factor optimal worst-case guarantee (Peters 2001; Auger, Nicaud
&amp; Pivoteau 2015; Buss &amp; Knop 2018). We demonstrate that our methods are
competitive in terms of running time with state-of-the-art implementations of
stable sorting methods.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/lipics.esa.2018.63</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/lipics.esa.2018.63" rel="related"/>
    <link href="http://arxiv.org/abs/1805.04154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.04272">
    <id>http://arxiv.org/abs/1805.04272v2</id>
    <updated>2018-08-15T16:24:39Z</updated>
    <published>2018-05-11T08:28:55Z</published>
    <title>An $O(N)$ Sorting Algorithm: Machine Learning Sort</title>
    <summary>  We propose an $O(N\cdot M)$ sorting algorithm by Machine Learning method,
which shows a huge potential sorting big data. This sorting algorithm can be
applied to parallel sorting and is suitable for GPU or TPU acceleration.
Furthermore, we discuss the application of this algorithm to sparse hash table.
</summary>
    <author>
      <name>Hanqing Zhao</name>
    </author>
    <author>
      <name>Yuehan Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.04272v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04272v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.04151">
    <id>http://arxiv.org/abs/1805.04151v1</id>
    <updated>2018-05-10T19:53:55Z</updated>
    <published>2018-05-10T19:53:55Z</published>
    <title>Beating Fredman-Komlós for perfect $k$-hashing</title>
    <summary>  We say a subset $C \subseteq \{1,2,\dots,k\}^n$ is a $k$-hash code (also
called $k$-separated) if for every subset of $k$ codewords from $C$, there
exists a coordinate where all these codewords have distinct values.
Understanding the largest possible rate (in bits), defined as $(\log_2 |C|)/n$,
of a $k$-hash code is a classical problem. It arises in two equivalent
contexts: (i) the smallest size possible for a perfect hash family that maps a
universe of $N$ elements into $\{1,2,\dots,k\}$, and (ii) the zero-error
capacity for decoding with lists of size less than $k$ for a certain
combinatorial channel.
  A general upper bound of $k!/k^{k-1}$ on the rate of a $k$-hash code (in the
limit of large $n$) was obtained by Fredman and Koml\'{o}s in 1984 for any $k
\geq 4$. While better bounds have been obtained for $k=4$, their original bound
has remained the best known for each $k \ge 5$. In this work, we obtain the
first improvement to the Fredman-Koml\'{o}s bound for every $k \ge 5$. While we
get explicit (numerical) bounds for $k=5,6$, for larger $k$ we only show that
the FK bound can be improved by a positive, but unspecified, amount. Under a
conjecture on the optimum value of a certain polynomial optimization problem
over the simplex, our methods allow an effective bound to be computed for every
$k$.
</summary>
    <author>
      <name>Venkatesan Guruswami</name>
    </author>
    <author>
      <name>Andrii Riazanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.04151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.05228">
    <id>http://arxiv.org/abs/1805.05228v1</id>
    <updated>2018-05-14T15:26:30Z</updated>
    <published>2018-05-14T15:26:30Z</published>
    <title>Assembling Omnitigs using Hidden-Order de Bruijn Graphs</title>
    <summary>  De novo DNA assembly is a fundamental task in Bioinformatics, and finding
Eulerian paths on de Bruijn graphs is one of the dominant approaches to it. In
most of the cases, there may be no one order for the de Bruijn graph that works
well for assembling all of the reads. For this reason, some de Bruijn-based
assemblers try assembling on several graphs of increasing order, in turn.
Boucher et al. (2015) went further and gave a representation making it possible
to navigate in the graph and change order on the fly, up to a maximum $K$, but
they can use up to $\lg K$ extra bits per edge because they use an LCP array.
In this paper, we replace the LCP array by a succinct representation of that
array's Cartesian tree, which takes only 2 extra bits per edge and still lets
us support interesting navigation operations efficiently. These operations are
not enough to let us easily extract unitigs and only unitigs from the graph but
they do let us extract a set of safe strings that contains all unitigs. Suppose
we are navigating in a variable-order de Bruijn graph representation, following
these rules: if there are no outgoing edges then we reduce the order, hoping
one appears; if there is exactly one outgoing edge then we take it (increasing
the current order, up to $K$); if there are two or more outgoing edges then we
stop. Then we traverse a (variable-order) path such that we cross edges only
when we have no choice or, equivalently, we generate a string appending
characters only when we have no choice. It follows that the strings we extract
are safe. Our experiments show we extract a set of strings more informative
than the unitigs, while using a reasonable amount of memory.
</summary>
    <author>
      <name>Diego Díaz-Domínguez</name>
    </author>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <link href="http://arxiv.org/abs/1805.05228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.05787">
    <id>http://arxiv.org/abs/1805.05787v3</id>
    <updated>2018-07-11T11:27:04Z</updated>
    <published>2018-05-15T14:14:20Z</published>
    <title>Parallel Working-Set Search Structures</title>
    <summary>  In this paper we present two versions of a parallel working-set map on p
processors that supports searches, insertions and deletions. In both versions,
the total work of all operations when the map has size at least p is bounded by
the working-set bound, i.e., the cost of an item depends on how recently it was
accessed (for some linearization): accessing an item in the map with recency r
takes O(1+log r) work. In the simpler version each map operation has O((log
p)^2+log n) span (where n is the maximum size of the map). In the pipelined
version each map operation on an item with recency r has O((log p)^2+log r)
span. (Operations in parallel may have overlapping span; span is additive only
for operations in sequence.)
  Both data structures are designed to be used by a dynamic multithreading
parallel program that at each step executes a unit-time instruction or makes a
data structure call. To achieve the stated bounds, the pipelined data structure
requires a weak-priority scheduler, which supports a limited form of 2-level
prioritization. At the end we explain how the results translate to practical
implementations using work-stealing schedulers.
  To the best of our knowledge, this is the first parallel implementation of a
self-adjusting search structure where the cost of an operation adapts to the
access sequence. A corollary of the working-set bound is that it achieves work
static optimality: the total work is bounded by the access costs in an optimal
static search tree.
</summary>
    <author>
      <name>Kunal Agrawal</name>
    </author>
    <author>
      <name>Seth Gilbert</name>
    </author>
    <author>
      <name>Wei Quan Lim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3210377.3210390</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3210377.3210390" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Authors' version of a paper accepted to SPAA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.05787v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05787v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.05592">
    <id>http://arxiv.org/abs/1805.05592v2</id>
    <updated>2018-07-11T10:08:57Z</updated>
    <published>2018-05-15T07:02:20Z</published>
    <title>Parallel Write-Efficient Algorithms and Data Structures for
  Computational Geometry</title>
    <summary>  In this paper, we design parallel write-efficient geometric algorithms that
perform asymptotically fewer writes than standard algorithms for the same
problem. This is motivated by emerging non-volatile memory technologies with
read performance being close to that of random access memory but writes being
significantly more expensive in terms of energy and latency. We design
algorithms for planar Delaunay triangulation, $k$-d trees, and static and
dynamic augmented trees. Our algorithms are designed in the recently introduced
Asymmetric Nested-Parallel Model, which captures the parallel setting in which
there is a small symmetric memory where reads and writes are unit cost as well
as a large asymmetric memory where writes are $\omega$ times more expensive
than reads. In designing these algorithms, we introduce several techniques for
obtaining write-efficiency, including DAG tracing, prefix doubling,
reconstruction-based rebalancing and $\alpha$-labeling, which we believe will
be useful for designing other parallel write-efficient algorithms.
</summary>
    <author>
      <name>Guy E. Blelloch</name>
    </author>
    <author>
      <name>Yan Gu</name>
    </author>
    <author>
      <name>Yihan Sun</name>
    </author>
    <author>
      <name>Julian Shun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3210377.3210380</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3210377.3210380" rel="related"/>
    <link href="http://arxiv.org/abs/1805.05592v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05592v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.06177">
    <id>http://arxiv.org/abs/1805.06177v1</id>
    <updated>2018-05-16T07:56:49Z</updated>
    <published>2018-05-16T07:56:49Z</published>
    <title>On Computing Average Common Substring Over Run Length Encoded Sequences</title>
    <summary>  The Average Common Substring (ACS) is a popular alignment-free distance
measure for phylogeny reconstruction. The ACS can be computed in O(n) space and
time, where n=x+y is the input size. The compressed string matching is the
study of string matching problems with the following twist: the input data is
in a compressed format and the underling task must be performed with little or
no decompression. In this paper, we revisit the ACS problem under this paradigm
where the input sequences are given in their run-length encoded format. We
present an algorithm to compute ACS(X,Y) in O(Nlog N) time using O(N) space,
where N is the total length of sequences after run-length encoding.
</summary>
    <author>
      <name>Sahar Hooshmand</name>
    </author>
    <author>
      <name>Neda Tavakoli</name>
    </author>
    <author>
      <name>Paniz Abedin</name>
    </author>
    <author>
      <name>Sharma V. Thankachan</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.06821">
    <id>http://arxiv.org/abs/1805.06821v1</id>
    <updated>2018-05-17T15:28:49Z</updated>
    <published>2018-05-17T15:28:49Z</published>
    <title>External memory BWT and LCP computation for sequence collections with
  applications</title>
    <summary>  We propose an external memory algorithm for the computation of the BWT and
LCP array for a collection of sequences. Our algorithm takes the amount of
available memory as an input parameter, and tries to make the best use of it by
splitting the input collection into subcollections sufficiently small that it
can compute their BWT in RAM using an optimal linear time algorithm. Next, it
merges the partial BWTs in external memory and in the process it also computes
the LCP values. We prove that our algorithm performs O(n AveLcp) sequential
I/Os, where n is the total length of the collection, and AveLcp is the average
Longest Common Prefix of the collection. This bound is an improvement over the
known algorithms for the same task. The experimental results show that our
algorithm outperforms the current best algorithm for collections of sequences
with different lengths and for collections with relatively small average
Longest Common Prefix.
  In the second part of the paper, we show that our algorithm can be modified
to output two additional arrays that, used with the BWT and LCP arrays, provide
simple, scan based, external memory algorithms for three well known problems in
bioinformatics: the computation of maximal repeats, the all pairs suffix-prefix
overlaps, and the construction of succinct de Bruijn graphs. To our knowledge,
there are no other known external memory algorithms for these problems.
</summary>
    <author>
      <name>Lavinia Egidi</name>
    </author>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Guilherme P. Telles</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.06869">
    <id>http://arxiv.org/abs/1805.06869v2</id>
    <updated>2018-10-26T12:01:18Z</updated>
    <published>2018-05-17T17:16:07Z</published>
    <title>Revisiting the tree edit distance and its backtracing: A tutorial</title>
    <summary>  Almost 30 years ago, Zhang and Shasha (1989) published a seminal paper
describing an efficient dynamic programming algorithm computing the tree edit
distance, that is, the minimum number of node deletions, insertions, and
replacements that are necessary to transform one tree into another. Since then,
the tree edit distance has been widely applied, for example in biology and
intelligent tutoring systems. However, the original paper of Zhang and Shasha
can be challenging to read for newcomers and it does not describe how to
efficiently infer the optimal edit script. In this contribution, we provide a
comprehensive tutorial to the tree edit distance algorithm of Zhang and Shasha.
We further prove metric properties of the tree edit distance, and describe
efficient algorithms to infer the cheapest edit script, as well as a summary of
all cheapest edit scripts between two trees.
</summary>
    <author>
      <name>Benjamin Paaßen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supplementary material for the ICML 2018 paper: Tree Edit Distance
  Learning via Adaptive Symbol Embeddings</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06869v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06869v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.08285">
    <id>http://arxiv.org/abs/1804.08285v1</id>
    <updated>2018-04-23T08:44:22Z</updated>
    <published>2018-04-23T08:44:22Z</published>
    <title>Succinct Oblivious RAM</title>
    <summary>  Reducing the database space overhead is critical in big-data processing. In
this paper, we revisit oblivious RAM (ORAM) using big-data standard for the
database space overhead.
  ORAM is a cryptographic primitive that enables users to perform arbitrary
database accesses without revealing the access pattern to the server. It is
particularly important today since cloud services become increasingly common
making it necessary to protect users' private information from database access
pattern analyses. Previous ORAM studies focused mostly on reducing the access
overhead. Consequently, the access overhead of the state-of-the-art ORAM
constructions is almost at practical levels in certain application scenarios
such as secure processors. On the other hand, most existing ORAM constructions
require $(1+\Theta(1))n$ (say, $10n$) bits of server space where $n$ is the
database size. Though such space complexity is often considered to be
"optimal", overhead such as $10 \times$ is prohibitive for big-data
applications in practice.
  We propose ORAM constructions that take only $(1+o(1))n$ bits of server space
while maintaining state-of-the-art performance in terms of the access overhead
and the user space. We also give non-asymptotic analyses and simulation results
which indicate that the proposed ORAM constructions are practically effective.
</summary>
    <author>
      <name>Taku Onodera</name>
    </author>
    <author>
      <name>Tetsuo Shibuya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages. A preliminary version of this paper appeared in STACS'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.08285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.08547">
    <id>http://arxiv.org/abs/1804.08547v1</id>
    <updated>2018-04-23T16:38:10Z</updated>
    <published>2018-04-23T16:38:10Z</published>
    <title>Entropy bounds for grammar compression</title>
    <summary>  In grammar compression we represent a string as a context free grammar. This
model is popular both in theoretical and practical applications due to its
simplicity, good compression rate and suitability for processing of the
compressed representations. In practice, achieving compression requires
encoding such grammar as a binary string, there are a few commonly used. We
bound the size of such encodings for several compression methods, along with
well-known \RePair algorithm. For \RePair we prove that its standard encoding,
which is a combination of entropy coding and special encoding of a grammar,
achieves $1.5|S|H_k(S)$. We also show that by stopping after some iteration we
can achieve $|S|H_k(S)$. The latter is particularly important, as it explains
the phenomenon observed in practice, that introducing too many nonterminals
causes the bit-size to grow. We generalize our approach to other compressions
methods like \Greedy or wide class of irreducible grammars, and other bit
encodings (including naive, which uses fixed-length codes). Our approach not
only proves the bounds but also partially explains why \Greedy and \RePair are
much better in practice than the other grammar based methods. At last, we show
that for a wide family of dictionary compression methods (including grammar
compressors) $\Omega\left(nk \log \sigma/\log_\sigma n\right)$ bits of
redundancy are required. This shows a separation between context-based/BWT
methods and dictionary compression algorithms, as for the former there exists
methods where redundancy does not depend on $n$, but only on $k$~and~$\sigma$.
</summary>
    <author>
      <name>Michał Gańczorz</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.10186">
    <id>http://arxiv.org/abs/1804.10186v1</id>
    <updated>2018-04-26T17:36:57Z</updated>
    <published>2018-04-26T17:36:57Z</published>
    <title>Edit Distance between Unrooted Trees in Cubic Time</title>
    <summary>  Edit distance between trees is a natural generalization of the classical edit
distance between strings, in which the allowed elementary operations are
contraction, uncontraction and relabeling of an edge. Demaine et al. [ACM
Trans. on Algorithms, 6(1), 2009] showed how to compute the edit distance
between rooted trees on $n$ nodes in $\mathcal{O}(n^{3})$ time. However,
generalizing their method to unrooted trees seems quite problematic, and the
most efficient known solution remains to be the previous $\mathcal{O}(n^{3}\log
n)$ time algorithm by Klein [ESA 1998]. Given the lack of progress on improving
this complexity, it might appear that unrooted trees are simply more difficult
than rooted trees. We show that this is, in fact, not the case, and edit
distance between unrooted trees on $n$ nodes can be computed in
$\mathcal{O}(n^{3})$ time. A significantly faster solution is unlikely to
exist, as Bringmann et al. [SODA 2018] proved that the complexity of computing
the edit distance between rooted trees cannot be decreased to
$\mathcal{O}(n^{3-\epsilon})$ unless some popular conjecture fails, and the
lower bound easily extends to unrooted trees. We also show that for two
unrooted trees of size $m$ and $n$, where $m\le n$, our algorithm can be
modified to run in $\mathcal{O}(nm^2(1+\log\frac nm))$. This, again, matches
the complexity achieved by Demaine et al. for rooted trees, who also showed
that this is optimal if we restrict ourselves to the so-called decomposition
algorithms.
</summary>
    <author>
      <name>Bartłomiej Dudek</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.10062">
    <id>http://arxiv.org/abs/1804.10062v1</id>
    <updated>2018-04-26T13:50:38Z</updated>
    <published>2018-04-26T13:50:38Z</published>
    <title>QuickMergesort: Practically Efficient Constant-Factor Optimal Sorting</title>
    <summary>  We consider the fundamental problem of internally sorting a sequence of $n$
elements. In its best theoretical setting QuickMergesort, a combination
Quicksort with Mergesort with a Median-of-$\sqrt{n}$ pivot selection, requires
at most $n \log n - 1.3999n + o(n)$ element comparisons on the average. The
questions addressed in this paper is how to make this algorithm practical. As
refined pivot selection usually adds much overhead, we show that the
Median-of-3 pivot selection of QuickMergesort leads to at most $n \log n -
0{.}75n + o(n)$ element comparisons on average, while running fast on
elementary data. The experiments show that QuickMergesort outperforms
state-of-the-art library implementations, including C++'s Introsort and Java's
Dual-Pivot Quicksort. Further trade-offs between a low running time and a low
number of comparisons are studied. Moreover, we describe a practically
efficient version with $n \log n + O(n)$ comparisons in the worst case.
</summary>
    <author>
      <name>Stefan Edelkamp</name>
    </author>
    <author>
      <name>Armin Weiß</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.09907">
    <id>http://arxiv.org/abs/1804.09907v2</id>
    <updated>2018-05-05T01:44:58Z</updated>
    <published>2018-04-26T06:34:20Z</published>
    <title>On Estimating Edit Distance: Alignment, Dimension Reduction, and
  Embeddings</title>
    <summary>  Edit distance is a fundamental measure of distance between strings and has
been widely studied in computer science. While the problem of estimating edit
distance has been studied extensively, the equally important question of
actually producing an alignment (i.e., the sequence of edits) has received far
less attention. Somewhat surprisingly, we show that any algorithm to estimate
edit distance can be used in a black-box fashion to produce an approximate
alignment of strings, with modest loss in approximation factor and small loss
in run time. Plugging in the result of Andoni, Krauthgamer, and Onak, we obtain
an alignment that is a $(\log n)^{O(1/\varepsilon^2)}$ approximation in time
$\tilde{O}(n^{1 + \varepsilon})$.
  Closely related to the study of approximation algorithms is the study of
metric embeddings for edit distance. We show that min-hash techniques can be
useful in designing edit distance embeddings through three results: (1) An
embedding from Ulam distance (edit distance over permutations) to Hamming space
that matches the best known distortion of $O(\log n)$ and also implicitly
encodes a sequence of edits between the strings; (2) In the case where the edit
distance between the input strings is known to have an upper bound $K$, we show
that embeddings of edit distance into Hamming space with distortion $f(n)$ can
be modified in a black-box fashion to give distortion
$O(f(\operatorname{poly}(K)))$ for a class of periodic-free strings; (3) A
randomized dimension-reduction map with contraction $c$ and asymptotically
optimal expected distortion $O(c)$, improving on the previous $\tilde{O}(c^{1 +
2 / \log \log \log n})$ distortion result of Batu, Ergun, and Sahinalp.
</summary>
    <author>
      <name>Moses Charikar</name>
    </author>
    <author>
      <name>Ofir Geri</name>
    </author>
    <author>
      <name>Michael P. Kim</name>
    </author>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <link href="http://arxiv.org/abs/1804.09907v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.09907v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.00060">
    <id>http://arxiv.org/abs/1805.00060v1</id>
    <updated>2018-04-30T19:00:22Z</updated>
    <published>2018-04-30T19:00:22Z</published>
    <title>On improving the approximation ratio of the r-shortest common
  superstring problem</title>
    <summary>  The Shortest Common Superstring problem (SCS) consists, for a set of strings
S = {s_1,...,s_n}, in finding a minimum length string that contains all s_i,
1&lt;= i &lt;= n, as substrings. While a 2+11/30 approximation ratio algorithm has
recently been published, the general objective is now to break the conceptual
lower bound barrier of 2. This paper is a step ahead in this direction. Here we
focus on a particular instance of the SCS problem, meaning the r-SCS problem,
which requires all input strings to be of the same length, r. Golonev et al.
proved an approximation ratio which is better than the general one for r&lt;= 6.
Here we extend their approach and improve their approximation ratio, which is
now better than the general one for r&lt;= 7, and less than or equal to 2 up to r
= 6.
</summary>
    <author>
      <name>Tristan Braquelaire</name>
    </author>
    <author>
      <name>Marie Gasparoux</name>
    </author>
    <author>
      <name>Mathieu Raffinot</name>
    </author>
    <author>
      <name>Raluca Uricaru</name>
    </author>
    <link href="http://arxiv.org/abs/1805.00060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.01876">
    <id>http://arxiv.org/abs/1805.01876v3</id>
    <updated>2018-05-10T07:19:19Z</updated>
    <published>2018-05-04T17:44:48Z</published>
    <title>Detecting Mutations by eBWT</title>
    <summary>  In this paper we develop a theory describing how the extended Burrows-Wheeler
Transform (eBWT) of a collection of DNA fragments tends to cluster together the
copies of nucleotides sequenced from a genome G. Our theory accurately predicts
how many copies of any nucleotide are expected inside each such cluster, and
how an elegant and precise LCP array based procedure can locate these clusters
in the eBWT. Our findings are very general and can be applied to a wide range
of different problems. In this paper, we consider the case of alignment-free
and reference-free SNPs discovery in multiple collections of reads. We note
that, in accordance with our theoretical results, SNPs are clustered in the
eBWT of the reads collection, and we develop a tool finding SNPs with a simple
scan of the eBWT and LCP arrays. Preliminary results show that our method
requires much less coverage than state-of-the-art tools while drastically
improving precision and sensitivity.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Nadia Pisanti</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">simplified Proposition 4; extended Thm 2 to ambiguous clusters</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.01876v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01876v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.02200">
    <id>http://arxiv.org/abs/1805.02200v2</id>
    <updated>2019-02-16T03:49:13Z</updated>
    <published>2018-05-06T12:31:28Z</published>
    <title>Wormhole: A Fast Ordered Index for In-memory Data Management</title>
    <summary>  In-memory data management systems, such as key-value stores, have become an
essential infrastructure in today's big-data processing and cloud computing.
They rely on efficient index structures to access data. While unordered
indexes, such as hash tables, can perform point search with O(1) time, they
cannot be used in many scenarios where range queries must be supported. Many
ordered indexes, such as B+ tree and skip list, have a O(log N) lookup cost,
where N is number of keys in an index. For an ordered index hosting billions of
keys, it may take more than 30 key-comparisons in a lookup, which is an order
of magnitude more expensive than that on a hash table. With availability of
large memory and fast network in today's data centers, this O(log N) time is
taking a heavy toll on applications that rely on ordered indexes.
  In this paper we introduce a new ordered index structure, named Wormhole,
that takes O(log L) worst-case time for looking up a key with a length of L.
The low cost is achieved by simultaneously leveraging strengths of three
indexing structures, namely hash table, prefix tree, and B+ tree, to
orchestrate a single fast ordered index. Wormhole's range operations can be
performed by a linear scan of a list after an initial lookup. This improvement
of access efficiency does not come at a price of compromised space efficiency.
Instead, Wormhole's index space is comparable to those of B+ tree and skip
list. Experiment results show that Wormhole outperforms skip list, B+ tree,
ART, and Masstree by up to 8.4x, 4.9x, 4.3x, and 6.6x in terms of key lookup
throughput, respectively.
</summary>
    <author>
      <name>Xingbo Wu</name>
    </author>
    <author>
      <name>Fan Ni</name>
    </author>
    <author>
      <name>Song Jiang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1810479.1810540</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1810479.1810540" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages; 18 figures; 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.02200v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02200v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.03158">
    <id>http://arxiv.org/abs/1805.03158v1</id>
    <updated>2018-05-08T16:48:01Z</updated>
    <published>2018-05-08T16:48:01Z</published>
    <title>Round-Hashing for Data Storage: Distributed Servers and External-Memory
  Tables</title>
    <summary>  This paper proposes round-hashing, which is suitable for data storage on
distributed servers and for implementing external-memory tables in which each
lookup retrieves at most a single block of external memory, using a stash. For
data storage, round-hashing is like consistent hashing as it avoids a full
rehashing of the keys when new servers are added. Experiments show that the
speed to serve requests is tenfold or more than the state of the art. In
distributed data storage, this guarantees better throughput for serving
requests and, moreover, greatly reduces decision times for which data should
move to new servers as rescanning data is much faster.
</summary>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Luca Versari</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1805.03574">
    <id>http://arxiv.org/abs/1805.03574v2</id>
    <updated>2019-01-08T07:56:43Z</updated>
    <published>2018-05-09T15:04:25Z</published>
    <title>Minimum Segmentation for Pan-genomic Founder Reconstruction in Linear
  Time</title>
    <summary>  Given a threshold $L$ and a set $\mathcal{R} = \{R_1, \ldots, R_m\}$ of $m$
haplotype sequences, each having length $n$, the minimum segmentation problem
for founder reconstruction is to partition the sequences into disjoint segments
$\mathcal{R}[i_1{+}1,i_2], \mathcal{R}[i_2{+}1, i_3], \ldots,
\mathcal{R}[i_{r-1}{+}1, i_r]$, where $0 = i_1 &lt; \cdots &lt; i_r = n$ and
$\mathcal{R}[i_{j-1}{+}1, i_j]$ is the set $\{R_1[i_{j-1}{+}1, i_j], \ldots,
R_m[i_{j-1}{+}1, i_j]\}$, such that the length of each segment, $i_j -
i_{j-1}$, is at least $L$ and $K = \max_j\{ |\mathcal{R}[i_{j-1}{+}1, i_j]| \}$
is minimized. The distinct substrings in the segments $\mathcal{R}[i_{j-1}{+}1,
i_j]$ represent founder blocks that can be concatenated to form $K$ founder
sequences representing the original $\mathcal{R}$ such that crossovers happen
only at segment boundaries. We give an optimal $O(mn)$ time algorithm to solve
the problem, improving over earlier $O(mn^2)$. This improvement enables to
exploit the algorithm on a pan-genomic setting of haplotypes being complete
human chromosomes, with a goal of finding a representative set of references
that can be indexed for read alignment and variant calling.
</summary>
    <author>
      <name>Tuukka Norri</name>
    </author>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.WABI.2018.15</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.WABI.2018.15" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proc. WABI 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.03574v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03574v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.04260">
    <id>http://arxiv.org/abs/1804.04260v1</id>
    <updated>2018-04-12T00:04:05Z</updated>
    <published>2018-04-12T00:04:05Z</published>
    <title>Graph Pattern Matching Preserving Label-Repetition Constraints</title>
    <summary>  Graph pattern matching is a routine process for a wide variety of
applications such as social network analysis. It is typically defined in terms
of subgraph isomorphism which is NP-Complete. To lower its complexity, many
extensions of graph simulation have been proposed which focus on some
topological constraints of pattern graphs that can be preserved in
polynomial-time over data graphs. We discuss in this paper the satisfaction of
a new topological constraint, called Label-Repetition constraint. To the best
of our knowledge, existing polynomial approaches fail to preserve this
constraint, and moreover, one can adopt only subgraph isomorphism for this end
which is cost-prohibitive. We present first a necessary and sufficient
condition that a data subgraph must satisfy to preserve the Label-Repetition
constraints of the pattern graph. Furthermore, we define matching based on a
notion of triple simulation, an extension of graph simulation by considering
the new topological constraint. We show that with this extension, graph pattern
matching can be performed in polynomial-time, by providing such an algorithm.
Our algorithm is sub-quadratic in the size of data graphs only, and quartic in
general. We show that our results can be combined with orthogonal approaches
for more expressive graph pattern matching.
</summary>
    <author>
      <name>Houari Mahfoud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.04239">
    <id>http://arxiv.org/abs/1804.04239v1</id>
    <updated>2018-04-11T21:51:23Z</updated>
    <published>2018-04-11T21:51:23Z</published>
    <title>Graph Sketching Against Adaptive Adversaries Applied to the Minimum
  Degree Algorithm</title>
    <summary>  Motivated by the study of matrix elimination orderings in combinatorial
scientific computing, we utilize graph sketching and local sampling to give a
data structure that provides access to approximate fill degrees of a matrix
undergoing elimination in $O(\text{polylog}(n))$ time per elimination and
query. We then study the problem of using this data structure in the minimum
degree algorithm, which is a widely-used heuristic for producing elimination
orderings for sparse matrices by repeatedly eliminating the vertex with
(approximate) minimum fill degree. This leads to a nearly-linear time algorithm
for generating approximate greedy minimum degree orderings. Despite extensive
studies of algorithms for elimination orderings in combinatorial scientific
computing, our result is the first rigorous incorporation of randomized tools
in this setting, as well as the first nearly-linear time algorithm for
producing elimination orderings with provable approximation guarantees.
  While our sketching data structure readily works in the oblivious adversary
model, by repeatedly querying and greedily updating itself, it enters the
adaptive adversarial model where the underlying sketches become prone to
failure due to dependency issues with their internal randomness. We show how to
use an additional sampling procedure to circumvent this problem and to create
an independent access sequence. Our technique for decorrelating the interleaved
queries and updates to this randomized data structure may be of independent
interest.
</summary>
    <author>
      <name>Matthew Fahrbach</name>
    </author>
    <author>
      <name>Gary L. Miller</name>
    </author>
    <author>
      <name>Richard Peng</name>
    </author>
    <author>
      <name>Saurabh Sawlani</name>
    </author>
    <author>
      <name>Junxing Wang</name>
    </author>
    <author>
      <name>Shen Chen Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">58 pages, 3 figures. This is a substantially revised version of
  arXiv:1711.08446 with an emphasis on the underlying theoretical problems</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.04720">
    <id>http://arxiv.org/abs/1804.04720v1</id>
    <updated>2018-04-12T20:35:06Z</updated>
    <published>2018-04-12T20:35:06Z</published>
    <title>Fast Prefix Search in Little Space, with Applications</title>
    <summary>  It has been shown in the indexing literature that there is an essential
difference between prefix/range searches on the one hand, and predecessor/rank
searches on the other hand, in that the former provably allows faster query
resolution. Traditionally, prefix search is solved by data structures that are
also dictionaries---they actually contain the strings in $S$. For very large
collections stored in slow-access memory, we propose much more compact data
structures that support \emph{weak} prefix searches---they return the ranks of
matching strings provided that \emph{some} string in $S$ starts with the given
prefix. In fact, we show that our most space-efficient data structure is
asymptotically space-optimal. Previously, data structures such as String
B-trees (and more complicated cache-oblivious string data structures) have
implicitly supported weak prefix queries, but they all have query time that
grows logarithmically with the size of the string collection. In contrast, our
data structures are simple, naturally cache-efficient, and have query time that
depends only on the length of the prefix, all the way down to constant query
time for strings that fit in one machine word. We give several applications of
weak prefix searches, including exact prefix counting and approximate counting
of tuples matching conjunctive prefix conditions.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Paolo Boldi</name>
    </author>
    <author>
      <name>Rasmus Pagh</name>
    </author>
    <author>
      <name>Sebastiano Vigna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 18th Annual European Symposium on Algorithms (ESA),
  Liverpool (UK), September 6-8, 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.04777">
    <id>http://arxiv.org/abs/1804.04777v2</id>
    <updated>2019-01-07T01:28:14Z</updated>
    <published>2018-04-13T02:57:19Z</published>
    <title>Optimizing Bloom Filter: Challenges, Solutions, and Comparisons</title>
    <summary>  Bloom filter (BF) has been widely used to support membership query, i.e., to
judge whether a given element x is a member of a given set S or not. Recent
years have seen a flourish design explosion of BF due to its characteristic of
space-efficiency and the functionality of constant-time membership query. The
existing reviews or surveys mainly focus on the applications of BF, but fall
short in covering the current trends, thereby lacking intrinsic understanding
of their design philosophy. To this end, this survey provides an overview of BF
and its variants, with an emphasis on the optimization techniques. Basically,
we survey the existing variants from two dimensions, i.e., performance and
generalization. To improve the performance, dozens of variants devote
themselves to reducing the false positives and implementation costs. Besides,
tens of variants generalize the BF framework in more scenarios by diversifying
the input sets and enriching the output functionalities. To summarize the
existing efforts, we conduct an in-depth study of the existing literature on BF
optimization, covering more than 60 variants. We unearth the design philosophy
of these variants and elaborate how the employed optimization techniques
improve BF. Furthermore, comprehensive analysis and qualitative comparison are
conducted from the perspectives of BF components. Lastly, we highlight the
future trends of designing BFs. This is, to the best of our knowledge, the
first survey that accomplishes such goals.
</summary>
    <author>
      <name>Lailong Luo</name>
    </author>
    <author>
      <name>Deke Guo</name>
    </author>
    <author>
      <name>Richard T. B. Ma</name>
    </author>
    <author>
      <name>Ori Rottenstreich</name>
    </author>
    <author>
      <name>Xueshan Luo</name>
    </author>
    <link href="http://arxiv.org/abs/1804.04777v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04777v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.05776">
    <id>http://arxiv.org/abs/1804.05776v4</id>
    <updated>2018-07-17T04:06:55Z</updated>
    <published>2018-04-16T16:24:13Z</published>
    <title>Deterministic Document Exchange Protocols, and Almost Optimal Binary
  Codes for Edit Errors</title>
    <summary>  We study two basic problems regarding edit error, i.e. document exchange and
error correcting codes for edit errors (insdel codes). For message length $n$
and edit error upper bound $k$, it is known that in both problems the optimal
sketch size or the optimal number of redundant bits is $\Theta(k \log
\frac{n}{k})$. However, known constructions are far from achieving these
bounds.
  We significantly improve previous results on both problems. For document
exchange, we give an efficient deterministic protocol with sketch size
$O(k\log^2 \frac{n}{k})$. This significantly improves the previous best known
deterministic protocol, which has sketch size $O(k^2 + k \log^2 n)$
(Belazzougui15). For binary insdel codes, we obtain the following results:
  1. An explicit binary insdel code which encodes an $n$-bit message $x$
against $k$ errors with redundancy $O(k \log^2 \frac{n}{k})$. In particular
this implies an explicit family of binary insdel codes that can correct
$\varepsilon$ fraction of insertions and deletions with rate $1-O(\varepsilon
\log^2 (\frac{1}{\varepsilon}))=1-\widetilde{O}(\varepsilon)$.
  2. An explicit binary insdel code which encodes an $n$-bit message $x$
against $k$ errors with redundancy $O(k \log n)$. This is the first explicit
construction of binary insdel codes that has optimal redundancy for a wide
range of error parameters $k$, and this brings our understanding of binary
insdel codes much closer to that of standard binary error correcting codes.
  In obtaining our results we introduce the notion of \emph{$\varepsilon$-self
matching hash functions} and \emph{$\varepsilon$-synchronization hash
functions}. We believe our techniques can have further applications in the
literature.
</summary>
    <author>
      <name>Kuan Cheng</name>
    </author>
    <author>
      <name>Zhengzhong Jin</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Ke Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.05776v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05776v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.05420">
    <id>http://arxiv.org/abs/1804.05420v1</id>
    <updated>2018-04-15T20:15:15Z</updated>
    <published>2018-04-15T20:15:15Z</published>
    <title>A Weighted Generalization of the Graham-Diaconis Inequality for Ranked
  List Similarity</title>
    <summary>  The Graham-Diaconis inequality shows the equivalence between two well-known
methods of measuring the similarity of two given ranked lists of items:
Spearman's footrule and Kendall's tau. The original inequality assumes
unweighted items in input lists. In this paper, we first define versions of
these methods for weighted items. We then prove a generalization of the
inequality for the weighted versions.
</summary>
    <author>
      <name>Ali Dasdan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.05420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.05097">
    <id>http://arxiv.org/abs/1804.05097v1</id>
    <updated>2018-04-12T00:23:21Z</updated>
    <published>2018-04-12T00:23:21Z</published>
    <title>Design and Implementation of Dynamic Memory Management in a Reversible
  Object-Oriented Programming Language</title>
    <summary>  The reversible object-oriented programming language (ROOPL) was presented in
late 2016 and proved that object-oriented programming paradigms works in the
reversible setting. The language featured simple statically scoped objects
which made non-trivial programs tedious, if not impossible to write using the
limited tools provided. We introduce an extension to ROOPL in form the new
language ROOPL++, featuring dynamic memory management and fixed-sized arrays
for increased language expressiveness. The language is a superset of ROOPL and
has formally been defined by its language semantics, type system and
computational universality. Considerations for reversible memory manager
layouts are discussed and ultimately lead to the selection of the Buddy Memory
layout. Translations of the extensions added in ROOPL++ to the reversible
assembly language PISA are presented to provide garbage-free computations. The
dynamic memory management extension successfully increases the expressiveness
of ROOPL and as a result, shows that non-trivial reversible data structures,
such as binary trees and doubly-linked lists, are feasible and do not
contradict the reversible computing paradigm.
</summary>
    <author>
      <name>Martin Holm Cservenka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master's Thesis, 231 pages, 63 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.05097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68N15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.2; D.3.3; D.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.05956">
    <id>http://arxiv.org/abs/1804.05956v2</id>
    <updated>2018-07-22T15:13:39Z</updated>
    <published>2018-04-16T21:54:54Z</published>
    <title>k-Maximum Subarrays for Small k: Divide-and-Conquer made simpler</title>
    <summary>  Given an array A of n real numbers, the maximum subarray problem is to find a
contiguous subarray which has the largest sum. The k-maximum subarrays problem
is to find k such subarrays with the largest sums. For the 1-maximum subarray
the well known divide-and-conquer algorithm, presented in most textbooks,
although suboptimal, is easy to implement and can be made optimal with a simple
change that speeds up the combine phase. On the other hand, the only known
divide-and-conquer algorithm for k > 1, that is efficient for small values of
k, is difficult to implement, due to the intricacies of the combine phase. In
this paper we give a divide- and-conquer solution for the k-maximum subarray
problem that simplifies the combine phase considerably while preserving the
overall running time.
  In the process of designing the combine phase of the algorithm we provide a
simple, sublinear, O($k^{1/2} log^3 k$) time algorithm, for finding the k
largest sums of X + Y, where X and Y are sorted arrays of size n and $k &lt;=
n^2$. The k largest sums are implicitly represented, and can be enumerated with
an additional O(k) time. To our knowledge, this is the first sublinear time
algorithm for this well studied problem.
  Unlike previous solutions, that are fairly complicated and sometimes
difficult to implement, ours rely on simple operations such as merging sorted
arrays, binary search, and selecting the $k^{th}$ smallest number in an array.
We have implemented our algorithms and report excellent performance on test
data.
</summary>
    <author>
      <name>Hemant Malik</name>
    </author>
    <author>
      <name>Ovidiu Daescu</name>
    </author>
    <link href="http://arxiv.org/abs/1804.05956v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05956v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.06809">
    <id>http://arxiv.org/abs/1804.06809v1</id>
    <updated>2018-04-18T16:45:46Z</updated>
    <published>2018-04-18T16:45:46Z</published>
    <title>On Abelian Longest Common Factor with and without RLE</title>
    <summary>  We consider the Abelian longest common factor problem in two scenarios: when
input strings are uncompressed and are of size $n$, and when the input strings
are run-length encoded and their compressed representations have size at most
$m$. The alphabet size is denoted by $\sigma$. For the uncompressed problem, we
show an $o(n^2)$-time and $\Oh(n)$-space algorithm in the case of
$\sigma=\Oh(1)$, making a non-trivial use of tabulation. For the RLE-compressed
problem, we show two algorithms: one working in $\Oh(m^2\sigma^2 \log^3 m)$
time and $\Oh(m (\sigma^2+\log^2 m))$ space, which employs line sweep, and one
that works in $\Oh(m^3)$ time and $\Oh(m)$ space that applies in a careful way
a sliding-window-based approach. The latter improves upon the previously known
$\Oh(nm^2)$-time and $\Oh(m^4)$-time algorithms that were recently developed by
Sugimoto et al.\ (IWOCA 2017) and Grabowski (SPIRE 2017), respectively.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to a journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.07575">
    <id>http://arxiv.org/abs/1804.07575v1</id>
    <updated>2018-04-20T12:33:42Z</updated>
    <published>2018-04-20T12:33:42Z</published>
    <title>Optimal Sorting with Persistent Comparison Errors</title>
    <summary>  We consider the problem of sorting $n$ elements in the case of
\emph{persistent} comparison errors. In this model (Braverman and Mossel,
SODA'08), each comparison between two elements can be wrong with some fixed
(small) probability $p$, and \emph{comparisons cannot be repeated}. Sorting
perfectly in this model is impossible, and the objective is to minimize the
\emph{dislocation} of each element in the output sequence, that is, the
difference between its true rank and its position. Existing lower bounds for
this problem show that no algorithm can guarantee, with high probability,
\emph{maximum dislocation} and \emph{total dislocation} better than
$\Omega(\log n)$ and $\Omega(n)$, respectively, regardless of its running time.
  In this paper, we present the first \emph{$O(n\log n)$-time} sorting
algorithm that guarantees both \emph{$O(\log n)$ maximum dislocation} and
\emph{$O(n)$ total dislocation} with high probability. Besides improving over
the previous state-of-the art algorithms -- the best known algorithm had
running time $\tilde{O}(n^{3/2})$ -- our result indicates that comparison
errors do not make the problem computationally more difficult: a sequence with
the best possible dislocation can be obtained in $O(n\log n)$ time and, even
without comparison errors, $\Omega(n\log n)$ time is necessary to guarantee
such dislocation bounds.
  In order to achieve this optimal result, we solve two sub-problems, and the
respective methods have their own merits for further application. One is how to
locate a position in which to insert an element in an almost-sorted sequence
having $O(\log n)$ maximum dislocation in such a way that the dislocation of
the resulting sequence will still be $O(\log n)$. The other is how to
simultaneously insert $m$ elements into an almost sorted sequence of $m$
different elements, such that the resulting sequence of $2m$ elements remains
almost sorted.
</summary>
    <author>
      <name>Barbara Geissmann</name>
    </author>
    <author>
      <name>Stefano Leucci</name>
    </author>
    <author>
      <name>Chih-Hung Liu</name>
    </author>
    <author>
      <name>Paolo Penna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.07575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.11245">
    <id>http://arxiv.org/abs/1803.11245v4</id>
    <updated>2018-11-16T16:35:53Z</updated>
    <published>2018-03-29T20:36:11Z</published>
    <title>Prefix-Free Parsing for Building Big BWTs</title>
    <summary>  High-throughput sequencing technologies have led to explosive growth of
genomic databases; one of which will soon reach hundreds of terabytes. For many
applications we want to build and store indexes of these databases but
constructing such indexes is a challenge. Fortunately, many of these genomic
databases are highly-repetitive---a characteristic that can be exploited to
ease the computation of the Burrows-Wheeler Transform (BWT), which underlies
many popular indexes. In this paper, we introduce a preprocessing algorithm,
referred to as {\em prefix-free parsing}, that takes a text $T$ as input, and
in one-pass generates a dictionary $D$ and a parse $P$ of $T$ with the property
that the BWT of $T$ can be constructed from $D$ and $P$ using workspace
proportional to their total size and $O (|T|)$-time. Our experiments show that
$D$ and $P$ are significantly smaller than $T$ in practice, and thus, can fit
in a reasonable internal memory even when $T$ is very large. In particular, we
show that with prefix-free parsing we can build an 131-megabyte run-length
compressed FM-index (restricted to support only counting and not locating) for
1000 copies of human chromosome 19 in 2 hours using 21 gigabytes of memory,
suggesting that we can build a 6.73 gigabyte index for 1000 complete
human-genome haplotypes in approximately 102 hours using about 1 terabyte of
memory.
</summary>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Alan Kuhnle</name>
    </author>
    <author>
      <name>Ben Langmead</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Taher Mun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version appeared at WABI '18; full version submitted to a
  journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.11245v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.11245v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.11427">
    <id>http://arxiv.org/abs/1803.11427v1</id>
    <updated>2018-03-30T11:59:08Z</updated>
    <published>2018-03-30T11:59:08Z</published>
    <title>On the Diameter of Tree Associahedra</title>
    <summary>  We consider a natural notion of search trees on graphs, which we show is
ubiquitous in various areas of discrete mathematics and computer science.
Search trees on graphs can be modified by local operations called rotations,
which generalize rotations in binary search trees. The rotation graph of search
trees on a graph $G$ is the skeleton of a polytope called the graph
associahedron of $G$.
  We consider the case where the graph $G$ is a tree. We construct a family of
trees $G$ on $n$ vertices and pairs of search trees on $G$ such that the
minimum number of rotations required to transform one search tree into the
other is $\Omega (n\log n)$. This implies that the worst-case diameter of tree
associahedra is $\Theta (n\log n)$, which answers a question from Thibault
Manneville and Vincent Pilaud. The proof relies on a notion of projection of a
search tree which may be of independent interest.
</summary>
    <author>
      <name>Jean Cardinal</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <author>
      <name>Pablo Pérez-Lantero</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.11427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.11427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.01937">
    <id>http://arxiv.org/abs/1804.01937v1</id>
    <updated>2018-04-05T16:21:33Z</updated>
    <published>2018-04-05T16:21:33Z</published>
    <title>On Undetected Redundancy in the Burrows-Wheeler Transform</title>
    <summary>  The Burrows-Wheeler-Transform (BWT) is an invertible permutation of a text
known to be highly compressible but also useful for sequence analysis, what
makes the BWT highly attractive for lossless data compression. In this paper,
we present a new technique to reduce the size of a BWT using its combinatorial
properties, while keeping it invertible. The technique can be applied to any
BWT-based compressor, and, as experiments show, is able to reduce the encoding
size by 8-16 % on average and up to 33-57 % in the best cases (depending on the
BWT-compressor used), making BWT-based compressors competitive or even superior
to today's best lossless compressors.
</summary>
    <author>
      <name>Uwe Baier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, accepted for Combinatorial Pattern Matching 2018 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.01642">
    <id>http://arxiv.org/abs/1804.01642v2</id>
    <updated>2019-01-04T15:44:25Z</updated>
    <published>2018-04-05T01:13:27Z</published>
    <title>Optimal streaming and tracking distinct elements with high probability</title>
    <summary>  The distinct elements problem is one of the fundamental problems in streaming
algorithms --- given a stream of integers in the range $\{1,\ldots,n\}$, we
wish to provide a $(1+\varepsilon)$ approximation to the number of distinct
elements in the input. After a long line of research an optimal solution for
this problem with constant probability of success, using
$\mathcal{O}(\frac{1}{\varepsilon^2}+\log n)$ bits of space, was given by Kane,
Nelson and Woodruff in 2010.
  The standard approach used in order to achieve low failure probability
$\delta$ is to take the median of $\log \delta^{-1}$ parallel repetitions of
the original algorithm. We show that such a multiplicative space blow-up is
unnecessary: we provide an optimal algorithm using $\mathcal{O}(\frac{\log
\delta^{-1}}{\varepsilon^2} + \log n)$ bits of space --- matching known lower
bounds for this problem. That is, the $\log\delta^{-1}$ factor does not
multiply the $\log n$ term. This settles completely the space complexity of the
distinct elements problem with respect to all standard parameters.
  We consider also the \emph{strong tracking} (or \emph{continuous monitoring})
variant of the distinct elements problem, where we want an algorithm which
provides an approximation of the number of distinct elements seen so far, at
all times of the stream. We show that this variant can be solved using
$\mathcal{O}(\frac{\log \log n + \log \delta^{-1}}{\varepsilon^2} + \log n)$
bits of space, which we show to be optimal.
</summary>
    <author>
      <name>Jarosław Błasiok</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version of this paper appeard in SODA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01642v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01642v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.02112">
    <id>http://arxiv.org/abs/1804.02112v1</id>
    <updated>2018-04-06T02:10:25Z</updated>
    <published>2018-04-06T02:10:25Z</published>
    <title>Red-Black Trees with Constant Update Time</title>
    <summary>  We show how a few modifications to the red-black trees allow for $O(1)$
worst-case update time (once the position of the inserted or deleted element is
known). The resulting structure is based on relaxing some of the properties of
the red-black trees while guaranteeing that the height remains logarithmic with
respect to the number of nodes. Compared to the other search trees with
constant update time, our tree is the first to provide a tailored deletion
procedure without using the global rebuilding technique. In addition, our
structure is very simple to implement and allows for a simpler proof of
correctness than those alternative trees.
</summary>
    <author>
      <name>Amr Elmasry</name>
    </author>
    <author>
      <name>Mostafa Kahla</name>
    </author>
    <author>
      <name>Fady Ahdy</name>
    </author>
    <author>
      <name>Mahmoud Hashem</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.02906">
    <id>http://arxiv.org/abs/1804.02906v2</id>
    <updated>2019-01-29T11:54:20Z</updated>
    <published>2018-04-09T10:46:48Z</published>
    <title>From Regular Expression Matching to Parsing</title>
    <summary>  Given a regular expression $R$ and a string $Q$, the regular expression
parsing problem is to determine if $Q$ matches $R$ and if so, determine how it
matches, e.g., by a mapping of the characters of $Q$ to the characters in $R$.
Regular expression parsing makes finding matches of a regular expression even
more useful by allowing us to directly extract subpatterns of the match, e.g.,
for extracting IP-addresses from internet traffic analysis or extracting
subparts of genomes from genetic data bases. We present a new general
techniques for efficiently converting a large class of algorithms that
determine if a string $Q$ matches regular expression $R$ into algorithms that
can construct a corresponding mapping. As a consequence, we obtain the first
efficient linear space solutions for regular expression parsing.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02906v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02906v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.03604">
    <id>http://arxiv.org/abs/1804.03604v3</id>
    <updated>2019-09-25T23:19:16Z</updated>
    <published>2018-04-10T15:54:34Z</published>
    <title>Optimal Document Exchange and New Codes for Insertions and Deletions</title>
    <summary>  We give the first communication-optimal document exchange protocol. For any
$n$ and $k &lt; n$ our randomized scheme takes any $n$-bit file $F$ and computes a
$\Theta(k \log \frac{n}{k})$-bit summary from which one can reconstruct $F$,
with high probability, given a related file $F'$ with edit distance $ED(F,F')
\leq k$.
  The size of our summary is information-theoretically order optimal for all
values of $k$, giving a randomized solution to a longstanding open question of
[Orlitsky; FOCS'91]. It also is the first non-trivial solution for the
interesting setting where a small constant fraction of symbols have been
edited, producing an optimal summary of size $O(H(\delta)n)$ for $k=\delta n$.
This concludes a long series of better-and-better protocols which produce
larger summaries for sub-linear values of $k$ and sub-polynomial failure
probabilities. In particular, the recent break-through of [Belazzougui, Zhang;
FOCS'16] assumes that $k &lt; n^\epsilon$, produces a summary of size $O(k\log^2 k
+ k\log n)$, and succeeds with probability $1-(k \log n)^{-O(1)}$.
  We also give an efficient derandomized document exchange protocol with
summary size $O(k \log^2 \frac{n}{k})$. This improves, for any $k$, over a
deterministic document exchange protocol by Belazzougui with summary size
$O(k^2 + k \log^2 n)$. Our deterministic document exchange directly provides
new efficient systematic error correcting codes for insertions and deletions.
These (binary) codes correct any $\delta$ fraction of adversarial
insertions/deletions while having a rate of $1 - O(\delta \log^2
\frac{1}{\delta})$ and improve over the codes of Guruswami and Li and Haeupler,
Shahrasbi and Vitercik which have rate $1 - \Theta\left(\sqrt{\delta}
\log^{O(1)} \frac{1}{\epsilon}\right)$.
</summary>
    <author>
      <name>Bernhard Haeupler</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03604v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03604v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.03256">
    <id>http://arxiv.org/abs/1804.03256v1</id>
    <updated>2018-04-09T22:08:13Z</updated>
    <published>2018-04-09T22:08:13Z</published>
    <title>Restructuring expression dags for efficient parallelization</title>
    <summary>  In the field of robust geometric computation it is often necessary to make
exact decisions based on inexact floating-point arithmetic. One common approach
is to store the computation history in an arithmetic expression dag and to
re-evaluate the expression with increasing precision until an exact decision
can be made. We show that exact-decisions number types based on expression dags
can be evaluated faster in practice through parallelization on multiple cores.
We compare the impact of several restructuring methods for the expression dag
on its running time in a parallel environment.
</summary>
    <author>
      <name>Martin Wilhelm</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.04263">
    <id>http://arxiv.org/abs/1804.04263v2</id>
    <updated>2018-04-13T17:12:59Z</updated>
    <published>2018-04-12T00:15:17Z</published>
    <title>Dualities in Tree Representations</title>
    <summary>  A characterization of the tree $T^*$ such that
$\mathrm{BP}(T^*)=\overleftrightarrow{\mathrm{DFUDS}(T)}$, the reversal of
$\mathrm{DFUDS}(T)$ is given. An immediate consequence is a rigorous
characterization of the tree $\hat{T}$ such that
$\mathrm{BP}(\hat{T})=\mathrm{DFUDS}(T)$. In summary, $\mathrm{BP}$ and
$\mathrm{DFUDS}$ are unified within an encompassing framework, which might have
the potential to imply future simplifications with regard to queries in
$\mathrm{BP}$ and/or $\mathrm{DFUDS}$. Immediate benefits displayed here are to
identify so far unnoted commonalities in most recent work on the Range Minimum
Query problem, and to provide improvements for the Minimum Length Interval
Query problem.
</summary>
    <author>
      <name>Rayan Chikhi</name>
    </author>
    <author>
      <name>Alexander Schönhuth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CPM 2018, extended version</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04263v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04263v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1804.04178">
    <id>http://arxiv.org/abs/1804.04178v2</id>
    <updated>2018-04-25T20:43:21Z</updated>
    <published>2018-04-11T19:20:04Z</published>
    <title>Approximating Edit Distance in Truly Subquadratic Time: Quantum and
  MapReduce</title>
    <summary>  The edit distance between two strings is defined as the smallest number of
insertions, deletions, and substitutions that need to be made to transform one
of the strings to another one. Approximating edit distance in subquadratic time
is "one of the biggest unsolved problems in the field of combinatorial pattern
matching". Our main result is a quantum constant approximation algorithm for
computing the edit distance in truly subquadratic time. More precisely, we give
an $O(n^{1.858})$ quantum algorithm that approximates the edit distance within
a factor of $7$. We further extend this result to an $O(n^{1.781})$ quantum
algorithm that approximates the edit distance within a larger constant factor.
  Our solutions are based on a framework for approximating edit distance in
parallel settings. This framework requires as black box an algorithm that
computes the distances of several smaller strings all at once. For a quantum
algorithm, we reduce the black box to \textit{metric estimation} and provide
efficient algorithms for approximating it. We further show that this framework
enables us to approximate edit distance in distributed settings. To this end,
we provide a MapReduce algorithm to approximate edit distance within a factor
of $3$, with sublinearly many machines and sublinear memory. Also, our
algorithm runs in a logarithmic number of rounds.
</summary>
    <author>
      <name>Mahdi Boroujeni</name>
    </author>
    <author>
      <name>Soheil Ehsani</name>
    </author>
    <author>
      <name>Mohammad Ghodsi</name>
    </author>
    <author>
      <name>MohammadTaghi HajiAghayi</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary version of this paper was presented at SODA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04178v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04178v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q12" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.01723">
    <id>http://arxiv.org/abs/1803.01723v1</id>
    <updated>2018-03-05T15:22:58Z</updated>
    <published>2018-03-05T15:22:58Z</published>
    <title>Optimal Substring-Equality Queries with Applications to Sparse Text
  Indexing</title>
    <summary>  We consider the problem of encoding a string of length $n$ from an alphabet
$[0,\sigma-1]$ so that access and substring-equality queries (that is,
determining the equality of any two substrings) can be answered efficiently. A
clear lower bound on the size of any prefix-free encoding of this kind is
$n\log\sigma + \Theta(\log (n\sigma))$ bits. We describe a new encoding
matching this lower bound when $\sigma\leq n^{O(1)}$ while supporting queries
in optimal $O(1)$-time in the cell-probe model, and show how to extend the
result to the word-RAM model using $\Theta(\log^2n)$ bits of additional space.
Using our new encoding, we obtain the first optimal-space algorithms for
several string-processing problems in the word-RAM model with rewritable input.
In particular, we describe the first in-place algorithm computing the LCP array
in $O(n\log n)$ expected time and the first in-place Monte Carlo solutions to
the sparse suffix sorting, sparse LCP array construction, and suffix selection
problems. Our algorithms are also the first running in sublinear time for small
enough sets of input suffixes. Combining these solutions, we obtain the first
optimal-space and sublinear-time algorithm for building the sparse suffix tree.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1803.01723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1802.10347">
    <id>http://arxiv.org/abs/1802.10347v2</id>
    <updated>2019-11-04T16:02:38Z</updated>
    <published>2018-02-28T10:33:41Z</published>
    <title>Decompressing Lempel-Ziv Compressed Text</title>
    <summary>  We consider the problem of decompressing the Lempel--Ziv 77 representation of
a string $S$ of length $n$ using a working space as close as possible to the
size $z$ of the input. The folklore solution for the problem runs in $O(n)$
time but requires random access to the whole decompressed text. Another
folklore solution is to convert LZ77 into a grammar of size $O(z\log(n/z))$ and
then stream $S$ in linear time. In this paper, we show that $O(n)$ time and
$O(z)$ working space can be achieved for constant-size alphabets. On general
alphabets of size $\sigma$, we describe (i) a trade-off achieving
$O(n\log^\delta \sigma)$ time and $O(z\log^{1-\delta}\sigma)$ space for any
$0\leq \delta\leq 1$, and (ii) a solution achieving $O(n)$ time and
$O(z\log\log (n/z))$ space. The latter solution, in particular, dominates both
folklore algorithms for the problem. Our solutions can, more generally, extract
any specified subsequence of $S$ with little overheads on top of the linear
running time and working space. As an immediate corollary, we show that our
techniques yield improved results for pattern matching problems on
LZ77-compressed text.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Mikko Berggren Ettienne</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1802.10347v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10347v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.01695">
    <id>http://arxiv.org/abs/1803.01695v2</id>
    <updated>2018-04-17T11:53:34Z</updated>
    <published>2018-03-05T14:50:27Z</published>
    <title>String Attractors: Verification and Optimization</title>
    <summary>  String attractors [STOC 2018] are combinatorial objects recently introduced
to unify all known dictionary compression techniques in a single theory. A set
$\Gamma\subseteq [1..n]$ is a $k$-attractor for a string $S\in[1..\sigma]^n$ if
and only if every distinct substring of $S$ of length at most $k$ has an
occurrence straddling at least one of the positions in $\Gamma$. Finding the
smallest $k$-attractor is NP-hard for $k\geq3$, but polylogarithmic
approximations can be found using reductions from dictionary compressors. It is
easy to reduce the $k$-attractor problem to a set-cover instance where string's
positions are interpreted as sets of substrings. The main result of this paper
is a much more powerful reduction based on the truncated suffix tree. Our new
characterization of the problem leads to more efficient algorithms for string
attractors: we show how to check the validity and minimality of a $k$-attractor
in near-optimal time and how to quickly compute exact and approximate
solutions. For example, we prove that a minimum $3$-attractor can be found in
optimal $O(n)$ time when $\sigma\in O(\sqrt[3+\epsilon]{\log n})$ for any
constant $\epsilon>0$, and $2.45$-approximation can be computed in $O(n)$ time
on general alphabets. To conclude, we introduce and study the complexity of the
closely-related sharp-$k$-attractor problem: to find the smallest set of
positions capturing all distinct substrings of length exactly $k$. We show that
the problem is in P for $k=1,2$ and is NP-complete for constant $k\geq 3$.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Alberto Policriti</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Eva Rotenberg</name>
    </author>
    <link href="http://arxiv.org/abs/1803.01695v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01695v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1709.05314">
    <id>http://arxiv.org/abs/1709.05314v2</id>
    <updated>2017-09-19T07:44:08Z</updated>
    <published>2017-09-15T17:09:54Z</published>
    <title>String Attractors</title>
    <summary>  Let $S$ be a string of length $n$. In this paper we introduce the notion of
\emph{string attractor}: a subset of the string's positions $[1,n]$ such that
every distinct substring of $S$ has an occurrence crossing one of the
attractor's elements. We first show that the minimum attractor's size yields
upper-bounds to the string's repetitiveness as measured by its linguistic
complexity and by the length of its longest repeated substring. We then prove
that all known compressors for repetitive strings induce a string attractor
whose size is bounded by their associated repetitiveness measure, and can
therefore be considered as approximations of the smallest one. Using further
reductions, we derive the approximation ratios of these compressors with
respect to the smallest attractor and solve several open problems related to
the asymptotic relations between repetitiveness measures (in particular,
between the the sizes of the Lempel-Ziv factorization, the run-length
Burrows-Wheeler transform, the smallest grammar, and the smallest macro
scheme). These reductions directly provide approximation algorithms for the
smallest string attractor. We then apply string attractors to solve efficiently
a fundamental problem in the field of compressed computation: we present a
universal compressed data structure for text extraction that improves existing
strategies simultaneously for \emph{all} known dictionary compressors and that,
by recent lower bounds, almost matches the optimal running time within the
resulting space. To conclude, we consider generalizations of string attractors
to labeled graphs, show that the attractor problem is NP-complete on trees, and
provide a logarithmic approximation computable in polynomial time.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1709.05314v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05314v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1710.10964">
    <id>http://arxiv.org/abs/1710.10964v4</id>
    <updated>2019-05-28T15:16:16Z</updated>
    <published>2017-10-30T14:24:35Z</published>
    <title>At the Roots of Dictionary Compression: String Attractors</title>
    <summary>  A well-known fact in the field of lossless text compression is that
high-order entropy is a weak model when the input contains long repetitions.
Motivated by this, decades of research have generated myriads of so-called
dictionary compressors: algorithms able to reduce the text's size by exploiting
its repetitiveness. Lempel-Ziv 77 is one of the most successful and well-known
tools of this kind, followed by straight-line programs, run-length
Burrows-Wheeler transform, macro schemes, collage systems, and the compact
directed acyclic word graph. In this paper, we show that these techniques are
different solutions to the same, elegant, combinatorial problem: to find a
small set of positions capturing all text's substrings. We call such a set a
string attractor. We first show reductions between dictionary compressors and
string attractors. This gives the approximation ratios of dictionary
compressors with respect to the smallest string attractor and uncovers new
relations between the output sizes of different compressors. We show that the
$k$-attractor problem: deciding whether a text has a size-$t$ set of positions
capturing substrings of length at most $k$, is NP-complete for $k\geq 3$. We
provide several approximation techniques for the smallest $k$-attractor, show
that the problem is APX-complete for constant $k$, and give strong
inapproximability results. To conclude, we provide matching lower and upper
bounds for the random access problem on string attractors. The upper bound is
proved by showing a data structure supporting queries in optimal time. Our data
structure is universal: by our reductions to string attractors, it supports
random access on any dictionary-compression scheme. In particular, it matches
the lower bound also on LZ77, straight-line programs, collage systems, and
macro schemes, and therefore closes (at once) the random access problem for all
these compressors.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3188745.3188814</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3188745.3188814" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of 50th Annual ACM SIGACT Symposium on the Theory of
  Computing (STOC'18)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10964v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10964v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.08617">
    <id>http://arxiv.org/abs/1803.08617v3</id>
    <updated>2019-05-15T22:35:09Z</updated>
    <published>2018-03-23T00:22:27Z</published>
    <title>Multiversion Concurrency with Bounded Delay and Precise Garbage
  Collection</title>
    <summary>  In this paper we are interested in bounding the number of instructions taken
to process transactions. The main result is a multiversion transactional system
that supports constant delay (extra instructions beyond running in isolation)
for all read-only transactions, delay equal to the number of processes for
writing transactions that are not concurrent with other writers, and
lock-freedom for concurrent writers. The system supports precise garbage
collection in that versions are identified for collection as soon as the last
transaction releases them. As far as we know these are first results that bound
delays for multiple readers and even a single writer. The approach is
particularly useful in situations where read-transactions dominate write
transactions, or where write transactions come in as streams or batches and can
be processed by a single writer (possibly in parallel).
  The approach is based on using functional data structures to support multiple
versions, and an efficient solution to the Version Maintenance (VM) problem for
acquiring, updating and releasing versions. Our solution to the VM problem is
precise, safe and wait-free (PSWF).
  We experimentally validate our approach by applying it to balanced tree data
structures for maintaining ordered maps. We test the transactional system using
multiple algorithms for the VM problem, including our PSWF VM algorithm, and
implementations with weaker guarantees based on epochs, hazard pointers, and
read-copy-update. To evaluate the functional data structure for concurrency and
multi-versioning, we implement batched updates for functional tree structures
and compare the performance with state-of-the-art concurrent data structures
for balanced trees. The experiments indicate our approach works well in
practice over a broad set of criteria.
</summary>
    <author>
      <name>Naama Ben-David</name>
    </author>
    <author>
      <name>Guy E. Blelloch</name>
    </author>
    <author>
      <name>Yihan Sun</name>
    </author>
    <author>
      <name>Yuanhao Wei</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3323165.3323185</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3323165.3323185" rel="related"/>
    <link href="http://arxiv.org/abs/1803.08617v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08617v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.08621">
    <id>http://arxiv.org/abs/1803.08621v2</id>
    <updated>2018-08-06T20:56:08Z</updated>
    <published>2018-03-23T00:48:06Z</published>
    <title>Parallel Range, Segment and Rectangle Queries with Augmented Maps</title>
    <summary>  The range, segment and rectangle query problems are fundamental problems in
computational geometry, and have extensive applications in many domains.
Despite the significant theoretical work on these problems, efficient
implementations can be complicated. We know of very few practical
implementations of the algorithms in parallel, and most implementations do not
have tight theoretical bounds. We focus on simple and efficient parallel
algorithms and implementations for these queries, which have tight worst-case
bound in theory and good parallel performance in practice. We propose to use a
simple framework (the augmented map) to model the problem. Based on the
augmented map interface, we develop both multi-level tree structures and
sweepline algorithms supporting range, segment and rectangle queries in two
dimensions. For the sweepline algorithms, we propose a parallel paradigm and
show corresponding cost bounds. All of our data structures are work-efficient
to build in theory and achieve a low parallel depth. The query time is almost
linear to the output size.
  We have implemented all the data structures described in the paper using a
parallel augmented map library. Based on the library each data structure only
requires about 100 lines of C++ code. We test their performance on large data
sets (up to $10^8$ elements) and a machine with 72-cores (144 hyperthreads).
The parallel construction achieves 32-68x speedup. Speedup numbers on queries
are up to 126-fold. Our sequential implementation outperforms the CGAL library
by at least 2x in both construction and queries. Our sequential implementation
can be slightly slower than the R-tree in the Boost library in some cases
(0.6-2.5x), but has significantly better query performance (1.6-1400x) than
Boost.
</summary>
    <author>
      <name>Yihan Sun</name>
    </author>
    <author>
      <name>Guy E. Blelloch</name>
    </author>
    <link href="http://arxiv.org/abs/1803.08621v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08621v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.09517">
    <id>http://arxiv.org/abs/1803.09517v2</id>
    <updated>2019-10-25T19:03:40Z</updated>
    <published>2018-03-26T11:34:13Z</published>
    <title>On the Approximation Ratio of Ordered Parsings</title>
    <summary>  Shannon's entropy is a clear lower bound for statistical compression. The
situation is not so well understood for dictionary-based compression. A
plausible lower bound is $b$, the least number of phrases of a general
bidirectional parse of a text, where phrases can be copied from anywhere else
in the text. Since computing $b$ is NP-complete, a popular gold standard is
$z$, the number of phrases in the Lempel-Ziv parse of the text, which is the
optimal one when phrases can be copied only from the left. While $z$ can be
computed in linear time with a greedy algorithm, almost nothing has been known
for decades about its approximation ratio with respect to $b$. In this paper we
prove that $z=O(b\log(n/b))$, where $n$ is the text length. We also show that
the bound is tight as a function of $n$, by exhibiting a text family where $z =
\Omega(b\log n)$. Our upper bound is obtained by building a run-length
context-free grammar based on a locally consistent parsing of the text. Our
lower bound is obtained by relating $b$ with $r$, the number of equal-letter
runs in the Burrows-Wheeler transform of the text. We proceed by observing that
Lempel-Ziv is just one particular case of greedy parses, meaning that the
optimal value of $z$ is obtained by scanning the text and maximizing the phrase
length at each step, and of ordered parses, meaning that there is an increasing
order between phrases and their sources. As a new example of ordered greedy
parses, we introduce {\em lexicographical} parses, where phrases can only be
copied from lexicographically smaller text locations. We prove that the size
$v$ of the optimal lexicographical parse is also obtained greedily in $O(n)$
time, that $v=O(b\log(n/b))$, and that there exists a text family where $v =
\Omega(b\log n)$.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Carlos Ochoa</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1803.09517v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09517v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.09675">
    <id>http://arxiv.org/abs/1803.09675v2</id>
    <updated>2018-04-27T14:15:12Z</updated>
    <published>2018-03-26T15:43:50Z</published>
    <title>Extra Space during Initialization of Succinct Data Structures and
  Dynamical Initializable Arrays</title>
    <summary>  Many succinct data structures on the word RAM require precomputed tables to
start operating. Usually, the tables can be constructed in sublinear time. In
this time, most of a data structure is not initialized, i.e., there is plenty
of unused space allocated for the data structure. We present a general
framework to store temporarily extra buffers between the real data so that the
data can be processed immediately, stored first in the buffers, and then moved
into the real data structure after finishing the tables. As an application, we
apply our framework to Dodis, Patrascu, and Thorup's data structure (STOC 2010)
that emulates c-ary memory and to Farzan and Munro's succinct encoding of
arbitrary graphs (TCS 2013). We also use our framework to present an in-place
dynamical initializable array.
</summary>
    <author>
      <name>Frank Kammer</name>
    </author>
    <author>
      <name>Andrej Sajenko</name>
    </author>
    <link href="http://arxiv.org/abs/1803.09675v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09675v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.09520">
    <id>http://arxiv.org/abs/1803.09520v3</id>
    <updated>2018-09-06T08:08:17Z</updated>
    <published>2018-03-26T11:39:19Z</published>
    <title>Universal Compressed Text Indexing</title>
    <summary>  The rise of repetitive datasets has lately generated a lot of interest in
compressed self-indexes based on dictionary compression, a rich and
heterogeneous family that exploits text repetitions in different ways. For each
such compression scheme, several different indexing solutions have been
proposed in the last two decades. To date, the fastest indexes for repetitive
texts are based on the run-length compressed Burrows-Wheeler transform and on
the Compact Directed Acyclic Word Graph. The most space-efficient indexes, on
the other hand, are based on the Lempel-Ziv parsing and on grammar compression.
Indexes for more universal schemes such as collage systems and macro schemes
have not yet been proposed. Very recently, Kempa and Prezza [STOC 2018] showed
that all dictionary compressors can be interpreted as approximation algorithms
for the smallest string attractor, that is, a set of text positions capturing
all distinct substrings. Starting from this observation, in this paper we
develop the first universal compressed self-index, that is, the first indexing
data structure based on string attractors, which can therefore be built on top
of any dictionary-compressed text representation. Let $\gamma$ be the size of a
string attractor for a text of length $n$. Our index takes
$O(\gamma\log(n/\gamma))$ words of space and supports locating the $occ$
occurrences of any pattern of length $m$ in $O(m\log n + occ\log^{\epsilon}n)$
time, for any constant $\epsilon>0$. This is, in particular, the first index
for general macro schemes and collage systems. Our result shows that the
relation between indexing and compression is much deeper than what was
previously thought: the simple property standing at the core of all dictionary
compressors is sufficient to support fast indexed queries.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed with reviewer's comments</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.09520v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09520v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.04282">
    <id>http://arxiv.org/abs/1803.04282v4</id>
    <updated>2019-01-29T10:22:12Z</updated>
    <published>2018-03-12T14:29:54Z</published>
    <title>Linear-Time In-Place DFS and BFS on the Word RAM</title>
    <summary>  We present an in-place depth first search (DFS) and an in-place breadth first
search (BFS) that runs on a word RAM in linear time such that, if the adjacency
arrays of the input graph are given in a sorted order, the input is restored
after running the algorithm. To obtain our results we use properties of the
representation used to store the given graph and show several linear-time
in-place graph transformations from one representation into another.
</summary>
    <author>
      <name>Frank Kammer</name>
    </author>
    <author>
      <name>Andrej Sajenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Short version of this paper is accepted to CIAC2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.04282v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04282v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.05948">
    <id>http://arxiv.org/abs/1803.05948v2</id>
    <updated>2018-05-17T17:34:25Z</updated>
    <published>2018-03-15T18:58:15Z</published>
    <title>Average Cost of QuickXsort with Pivot Sampling</title>
    <summary>  QuickXsort is a strategy to combine Quicksort with another sorting method X,
so that the result has essentially the same comparison cost as X in isolation,
but sorts in place even when X requires a linear-size buffer. We solve the
recurrence for QuickXsort precisely up to the linear term including the
optimization to choose pivots from a sample of k elements. This allows to
immediately obtain overall average costs using only the average costs of
sorting method X (as if run in isolation). We thereby extend and greatly
simplify the analysis of QuickHeapsort and QuickMergesort with practically
efficient pivot selection, and give the first tight upper bounds including the
linear term for such methods.
</summary>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/lipics.aofa.2018.36</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/lipics.aofa.2018.36" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">updated to final version accepted for AofA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.05948v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05948v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.07199">
    <id>http://arxiv.org/abs/1803.07199v2</id>
    <updated>2018-04-13T16:11:40Z</updated>
    <published>2018-03-20T00:04:27Z</published>
    <title>Twelve Simple Algorithms to Compute Fibonacci Numbers</title>
    <summary>  The Fibonacci numbers are a sequence of integers in which every number after
the first two, 0 and 1, is the sum of the two preceding numbers. These numbers
are well known and algorithms to compute them are so easy that they are often
used in introductory algorithms courses. In this paper, we present twelve of
these well-known algorithms and some of their properties. These algorithms,
though very simple, illustrate multiple concepts from the algorithms field, so
we highlight them. We also present the results of a small-scale experimental
comparison of their runtimes on a personal laptop. Finally, we provide a list
of homework questions for the students. We hope that this paper can serve as a
useful resource for the students learning the basics of algorithms.
</summary>
    <author>
      <name>Ali Dasdan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 29 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.07199v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07199v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.01362">
    <id>http://arxiv.org/abs/1803.01362v1</id>
    <updated>2018-03-04T14:46:32Z</updated>
    <published>2018-03-04T14:46:32Z</published>
    <title>Two-Dimensional Block Trees</title>
    <summary>  The Block Tree (BT) is a novel compact data structure designed to compress
sequence collections. It obtains compression ratios close to Lempel-Ziv and
supports efficient direct access to any substring. The BT divides the text
recursively into fixed-size blocks and those appearing earlier are represented
with pointers. On repetitive collections, a few blocks can represent all the
others, and thus the BT reduces the size by orders of magnitude. In this paper
we extend the BT to two dimensions, to exploit repetitiveness in collections of
images, graphs, and maps. This two-dimensional Block Tree divides the image
regularly into subimages and replaces some of them by pointers to other
occurrences thereof. We develop a specific variant aimed at compressing the
adjacency matrices of Web graphs, obtaining space reductions of up to 50\%
compared with the $k^2$-tree, which is the best alternative supporting direct
and reverse navigation in the graph.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Adrián Gómez-Brandón</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.01362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.03033">
    <id>http://arxiv.org/abs/2004.03033v2</id>
    <updated>2020-04-15T17:26:17Z</updated>
    <published>2020-04-06T22:54:58Z</published>
    <title>SOPanG 2: online searching over a pan-genome without false positives</title>
    <summary>  Motivation: The pan-genome can be stored as elastic-degenerate (ED) string, a
recently introduced compact representation of multiple overlapping sequences.
However, a search over the ED string does not indicate which individuals (if
any) match the entire query.
  Results: We augment the ED string with sources (individuals' indexes) and
propose an extension of the SOPanG (Shift-Or for Pan-Genome) tool to report
only true positive matches, omitting those not occurring in any of the
haplotypes. The additional stage for checking the matches yields a penalty of
less than 3.5% relative speed in practice, which means that SOPanG 2 is able to
report pattern matches in a pan-genome, mapping them onto individuals, at the
single-thread throughput of above 430 MB/s on real data.
  Availability and implementation: SOPanG 2 can be downloaded here:
github.com/MrAlexSee/sopang
</summary>
    <author>
      <name>Aleksander Cisłak</name>
    </author>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <link href="http://arxiv.org/abs/2004.03033v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.03033v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.03206">
    <id>http://arxiv.org/abs/2004.03206v1</id>
    <updated>2020-04-07T08:48:41Z</updated>
    <published>2020-04-07T08:48:41Z</published>
    <title>Zipping Segment Trees</title>
    <summary>  Stabbing queries in sets of intervals are usually answered using segment
trees. A dynamic variant of segment trees has been presented by van Kreveld and
Overmars, which uses red-black trees to do rebalancing operations. This paper
presents zipping segment trees - dynamic segment trees based on zip trees,
which were recently introduced by Tarjan et al. To facilitate zipping segment
trees, we show how to uphold certain segment tree properties during the
operations of a zip tree. We present an in-depth experimental evaluation and
comparison of dynamic segment trees based on red-black trees, weight-balanced
trees and several variants of the novel zipping segment trees. Our results
indicate that zipping segment trees perform better than rotation-based
alternatives.
</summary>
    <author>
      <name>Lukas Barth</name>
    </author>
    <author>
      <name>Dorothea Wagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at SEA 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.03206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.03206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.02781">
    <id>http://arxiv.org/abs/2004.02781v1</id>
    <updated>2020-04-06T16:16:26Z</updated>
    <published>2020-04-06T16:16:26Z</published>
    <title>Indexing Highly Repetitive String Collections</title>
    <summary>  Two decades ago, a breakthrough in indexing string collections made it
possible to represent them within their compressed space while at the same time
offering indexed search functionalities. As this new technology permeated
through applications like bioinformatics, the string collections experienced a
growth that outperforms Moore's Law and challenges our ability of handling them
even in compressed form. It turns out, fortunately, that many of these rapidly
growing string collections are highly repetitive, so that their information
content is orders of magnitude lower than their plain size. The statistical
compression methods used for classical collections, however, are blind to this
repetitiveness, and therefore a new set of techniques has been developed in
order to properly exploit it. The resulting indexes form a new generation of
data structures able to handle the huge repetitive string collections that we
are facing. In this survey we cover the algorithmic developments that have led
to these data structures. We describe the distinct compression paradigms that
have been used to exploit repetitiveness, the fundamental algorithmic ideas
that form the base of all the existing indexes, and the various structures that
have been proposed, comparing them both in theoretical and practical aspects.
We conclude with the current challenges in this fascinating field.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/2004.02781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.02781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.01493">
    <id>http://arxiv.org/abs/2004.01493v3</id>
    <updated>2020-05-15T08:32:48Z</updated>
    <published>2020-04-03T12:12:01Z</published>
    <title>Enumeration of LCP values, LCP intervals and Maximal repeats in BWT-runs
  Bounded Space</title>
    <summary>  Lcp-values, lcp-intervals, and maximal repeats are powerful tools in various
string processing tasks and have a wide variety of applications. Although many
researchers have focused on developing enumeration algorithms for them, those
algorithms are inefficient in that the space usage is proportional to the
length of the input string. Recently, the run-length-encoded Burrows-Wheeler
transform (RLBWT) has attracted increased attention in string processing, and
various algorithms on the RLBWT have been developed. Developing enumeration
algorithms for lcp-intervals, lcp-values, and maximal repeats on the RLBWT,
however, remains a challenge. In this paper, we present the first such
enumeration algorithms with space usage not proportional to the string length.
The complexities of our enumeration algorithms are $O(n \log \log (n/r))$ time
and $O(r)$ words of working space for string length $n$ and RLBWT size $r$.
</summary>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/2004.01493v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.01493v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.01032">
    <id>http://arxiv.org/abs/2004.01032v1</id>
    <updated>2020-04-01T14:00:54Z</updated>
    <published>2020-04-01T14:00:54Z</published>
    <title>Grammar-Compressed Indexes with Logarithmic Search Time</title>
    <summary>  Let a text $T[1..n]$ be the only string generated by a context-free grammar
with $g$ (terminal and nonterminal) symbols, and of size $G$ (measured as the
sum of the lengths of the right-hand sides of the rules). Such a grammar,
called a grammar-compressed representation of $T$, can be encoded using
essentially $G\lg g$ bits. We introduce the first grammar-compressed index that
uses $O(G\lg n)$ bits and can find the $occ$ occurrences of patterns $P[1..m]$
in time $O((m^2+occ)\lg G)$. We implement the index and demonstrate its
practicality in comparison with the state of the art, on highly repetitive text
collections.
</summary>
    <author>
      <name>Francisco Claude</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Alejandro Pacheco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1110.4493</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.01032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.01032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.01120">
    <id>http://arxiv.org/abs/2004.01120v3</id>
    <updated>2020-04-11T08:45:01Z</updated>
    <published>2020-04-02T16:43:21Z</published>
    <title>On Locating Paths in Compressed Cardinal Trees</title>
    <summary>  A compressed index is a data structure representing a text within compressed
space and supporting fast count and locate queries: count/return all positions
where a pattern occurs. The first compressed indexes operate within a space
bounded by the text's entropy. Entropy, however, is insensitive to long
repetitions. For this reason, in recent years more powerful compressed indexes
have emerged; these are based on the Lempel-Ziv factorization, the run-length
BWT, context-free grammars and, more recently, string attractors. Labeled trees
add a whole new dimension to the problem: one needs not only to compress the
labels, but also the tree's topology. On this side, less is known. Jacobson
showed how to represent the topology of a tree with n nodes in 2n+o(n) bits of
space (succinct) while also supporting constant-time navigation queries.
Ferragina et al. presented the first entropy-compressed labeled tree
representation (the XBWT) able to count, but not locate, paths labeled with a
given pattern. Grammars and the Lempel-Ziv factorization have been extended to
trees, but those representations do not support indexing queries. In this
paper, we show for the first time how to support the powerful locate queries on
compressed trees. We start by proposing suitable generalizations of run-length
BWT, high-order entropy, and string attractors to cardinal trees (tries). We
show that the number r $\leq$ n of XBWT-runs upper-bounds the size of the
smallest tree attractor and lower-bounds the trie's high-order worst-case
entropy H. We finally present the first trie index able to locate in pre-order
nodes reached by a path labeled with a given pattern. Our index locates path
occurrences in constant time each and takes 2n + o(n) + O(r log n) $\leq$ 2n +
o(n) + O(H log n) bits of space: the reporting time is optimal and the locate
machinery fits within compressed space on top of the succinct topology.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Improved toehold lemma running time; added more detailed proofs that
  take care of all border cases in the locate strategy</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.01120v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.01120v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.01156">
    <id>http://arxiv.org/abs/2004.01156v1</id>
    <updated>2020-04-02T17:26:50Z</updated>
    <published>2020-04-02T17:26:50Z</published>
    <title>No Repetition: Fast Streaming with Highly Concentrated Hashing</title>
    <summary>  To get estimators that work within a certain error bound with high
probability, a common strategy is to design one that works with constant
probability, and then boost the probability using independent repetitions.
Important examples of this approach are small space algorithms for estimating
the number of distinct elements in a stream, or estimating the set similarity
between large sets. Using standard strongly universal hashing to process each
element, we get a sketch based estimator where the probability of a too large
error is, say, 1/4. By performing $r$ independent repetitions and taking the
median of the estimators, the error probability falls exponentially in $r$.
However, running $r$ independent experiments increases the processing time by a
factor $r$.
  Here we make the point that if we have a hash function with strong
concentration bounds, then we get the same high probability bounds without any
need for repetitions. Instead of $r$ independent sketches, we have a single
sketch that is $r$ times bigger, so the total space is the same. However, we
only apply a single hash function, so we save a factor $r$ in time, and the
overall algorithms just get simpler.
  Fast practical hash functions with strong concentration bounds were recently
proposed by Aamand em et al. (to appear in STOC 2020). Using their hashing
schemes, the algorithms thus become very fast and practical, suitable for
online processing of high volume data streams.
</summary>
    <author>
      <name>Anders Aamand</name>
    </author>
    <author>
      <name>Debarati Das</name>
    </author>
    <author>
      <name>Evangelos Kipouridis</name>
    </author>
    <author>
      <name>Jakob B. T. Knudsen</name>
    </author>
    <author>
      <name>Peter M. R. Rasmussen</name>
    </author>
    <author>
      <name>Mikkel Thorup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.01156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.01156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.13589">
    <id>http://arxiv.org/abs/2003.13589v1</id>
    <updated>2020-03-30T16:07:07Z</updated>
    <published>2020-03-30T16:07:07Z</published>
    <title>A Faster Subquadratic Algorithm for the Longest Common Increasing
  Subsequence Problem</title>
    <summary>  The Longest Common Increasing Subsequence (LCIS) is a variant of the
classical Longest Common Subsequence (LCS), in which we additionally require
the common subsequence to be strictly increasing. While the well-known "Four
Russians" technique can be used to find LCS in subquadratic time, it does not
seem applicable to LCIS. Recently, Duraj [STACS 2020] used a completely
different method based on the combinatorial properties of LCIS to design an
$\mathcal{O}(n^2(\log\log n)^2/\log^{1/6}n)$ time algorithm. We show that an
approach based on exploiting tabulation can be used to construct an
asymptotically faster $\mathcal{O}(n^2 \log\log n/\sqrt{\log n})$ time
algorithm. As our solution avoids using the specific combinatorial properties
of LCIS, it can be also adapted for the Longest Common Weakly Increasing
Subsequence (LCWIS).
</summary>
    <author>
      <name>Anadi Agrawal</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <link href="http://arxiv.org/abs/2003.13589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.13589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.11604">
    <id>http://arxiv.org/abs/2003.11604v1</id>
    <updated>2020-03-25T20:04:20Z</updated>
    <published>2020-03-25T20:04:20Z</published>
    <title>Further Results on Colored Range Searching</title>
    <summary>  We present a number of new results about range searching for colored (or
"categorical") data:
  1. For a set of $n$ colored points in three dimensions, we describe
randomized data structures with $O(n\mathop{\rm polylog}n)$ space that can
report the distinct colors in any query orthogonal range (axis-aligned box) in
$O(k\mathop{\rm polyloglog} n)$ expected time, where $k$ is the number of
distinct colors in the range, assuming that coordinates are in
$\{1,\ldots,n\}$. Previous data structures require $O(\frac{\log n}{\log\log n}
+ k)$ query time. Our result also implies improvements in higher constant
dimensions.
  2. Our data structures can be adapted to halfspace ranges in three dimensions
(or circular ranges in two dimensions), achieving $O(k\log n)$ expected query
time. Previous data structures require $O(k\log^2n)$ query time.
  3. For a set of $n$ colored points in two dimensions, we describe a data
structure with $O(n\mathop{\rm polylog}n)$ space that can answer colored
"type-2" range counting queries: report the number of occurrences of every
distinct color in a query orthogonal range. The query time is $O(\frac{\log
n}{\log\log n} + k\log\log n)$, where $k$ is the number of distinct colors in
the range. Naively performing $k$ uncolored range counting queries would
require $O(k\frac{\log n}{\log\log n})$ time.
  Our data structures are designed using a variety of techniques, including
colored variants of randomized incremental construction (which may be of
independent interest), colored variants of shallow cuttings, and bit-packing
tricks.
</summary>
    <author>
      <name>Timothy M. Chan</name>
    </author>
    <author>
      <name>Qizheng He</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">full version of a SoCG'20 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.11604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.11835">
    <id>http://arxiv.org/abs/2003.11835v1</id>
    <updated>2020-03-26T11:09:39Z</updated>
    <published>2020-03-26T11:09:39Z</published>
    <title>Succinct Dynamic Ordered Sets with Random Access</title>
    <summary>  The representation of a dynamic ordered set of $n$ integer keys drawn from a
universe of size $m$ is a fundamental data structuring problem. Many solutions
to this problem achieve optimal time but take polynomial space, therefore
preserving time optimality in the \emph{compressed} space regime is the problem
we address in this work. For a polynomial universe $m = n^{\Theta(1)}$, we give
a solution that takes $\textsf{EF}(n,m) + o(n)$ bits, where $\textsf{EF}(n,m)
\leq n\lceil \log_2(m/n)\rceil + 2n$ is the cost in bits of the
\emph{Elias-Fano} representation of the set, and supports random access to the
$i$-th smallest element in $O(\log n/ \log\log n)$ time, updates and
predecessor search in $O(\log\log n)$ time. These time bounds are optimal.
</summary>
    <author>
      <name>Giulio Ermanno Pibiri</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <link href="http://arxiv.org/abs/2003.11835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.12590">
    <id>http://arxiv.org/abs/2004.12590v1</id>
    <updated>2020-04-27T05:52:58Z</updated>
    <published>2020-04-27T05:52:58Z</published>
    <title>In-Place Bijective Burrows-Wheeler Transforms</title>
    <summary>  One of the most well-known variants of the Burrows-Wheeler transform (BWT)
[Burrows and Wheeler, 1994] is the bijective BWT (BBWT) [Gil and Scott, arXiv
2012], which applies the extended BWT (EBWT) [Mantaci et al., TCS 2007] to the
multiset of Lyndon factors of a given text. Since the EBWT is invertible, the
BBWT is a bijective transform in the sense that the inverse image of the EBWT
restores this multiset of Lyndon factors such that the original text can be
obtained by sorting these factors in non-increasing order. In this paper, we
present algorithms constructing or inverting the BBWT in-place using quadratic
time. We also present conversions from the BBWT to the BWT, or vice versa,
either (a) in-place using quadratic time, or (b) in the run-length compressed
setting using $O(n \lg r / \lg \lg r)$ time with $O(r \lg n)$ bits of words,
where $r$ is the sum of character runs in the BWT and the BBWT.
</summary>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Daiki Hashimoto</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.CPM.2020.23</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.CPM.2020.23" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In proceedings of CPM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.12590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.08350">
    <id>http://arxiv.org/abs/2004.08350v1</id>
    <updated>2020-04-17T17:13:13Z</updated>
    <published>2020-04-17T17:13:13Z</published>
    <title>Faster Approximate Pattern Matching: A Unified Approach</title>
    <summary>  Approximate pattern matching is a natural and well-studied problem on
strings: Given a text $T$, a pattern $P$, and a threshold $k$, find (the
starting positions of) all substrings of $T$ that are at distance at most $k$
from $P$. We consider the two most fundamental string metrics: the Hamming
distance and the edit distance. Under the Hamming distance, we search for
substrings of $T$ that have at most $k$ mismatches with $P$, while under the
edit distance, we search for substrings of $T$ that can be transformed to $P$
with at most $k$ edits.
  Exact occurrences of $P$ in $T$ have a very simple structure: If we assume
for simplicity that $|T| \le 3|P|/2$ and trim $T$ so that $P$ occurs both as a
prefix and as a suffix of $T$, then both $P$ and $T$ are periodic with a common
period. However, an analogous characterization for the structure of occurrences
with up to $k$ mismatches was proved only recently by Bringmann et al.
[SODA'19]: Either there are $O(k^2)$ $k$-mismatch occurrences of $P$ in $T$, or
both $P$ and $T$ are at Hamming distance $O(k)$ from strings with a common
period $O(m/k)$. We tighten this characterization by showing that there are
$O(k)$ $k$-mismatch occurrences in the case when the pattern is not
(approximately) periodic, and we lift it to the edit distance setting, where we
tightly bound the number of $k$-edit occurrences by $O(k^2)$ in the
non-periodic case. Our proofs are constructive and let us obtain a unified
framework for approximate pattern matching for both considered distances. We
showcase the generality of our framework with results for the fully-compressed
setting (where $T$ and $P$ are given as a straight-line program) and for the
dynamic setting (where we extend a data structure of Gawrychowski et al.
[SODA'18]).
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Philip Wellnitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">74 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.08350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.08350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.09051">
    <id>http://arxiv.org/abs/2004.09051v1</id>
    <updated>2020-04-20T04:53:27Z</updated>
    <published>2020-04-20T04:53:27Z</published>
    <title>Black-White Array: A New Data Structure for Dynamic Data Sets</title>
    <summary>  A new array based data structure named black-white array (BWA) is introduced
as an effective and efficient alternative to the list or tree based data
structures for dynamic data set. It consists of two sub-arrays, one white and
one black of half of the size of the white. Both of them are conceptually
partitioned into segments of different ranks with the sizes grow in geometric
sequence. The layout of BWA allows easy calculation of the meta-data about the
segments, which are used extensively in the algorithms for the basic operations
of the dynamic sets. The insertion of a sequence of unordered numbers into BWA
takes amortized time logarithmic to the length of the sequence. It is also
proven that when the searched or deleted value is present in the BWA, the
asymptotic amortized cost for the operations is O(log(n)); otherwise, the time
will fall somewhere between O(log(n)) and O(log^2(n)). It is shown that the
state variable total, which records the number of values in the BWA captures
the dynamics of state transition of BWA. This fact is exploited to produce
concise, easy- to-understand, and efficient coding for the operations. As it
uses arrays as the underlying structure for dynamic set, a BWA need neither the
space to store the pointers referencing other data nodes nor the time to chase
the pointers as with any linked data structures. A C++ implementation of the
BWA is completed. The performance data were gathered and plotted, which
confirmed the theoretic analysis. The testing results showed that the amortized
time for the insert, search, and delete operations is all just between 105.949
and 5720.49 nanoseconds for BWAs of sizes ranging from 210 to 229 under various
conditions.
</summary>
    <author>
      <name>Z. George Mou</name>
    </author>
    <link href="http://arxiv.org/abs/2004.09051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.09051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.05345">
    <id>http://arxiv.org/abs/2004.05345v1</id>
    <updated>2020-04-11T09:24:51Z</updated>
    <published>2020-04-11T09:24:51Z</published>
    <title>Locality-Sensitive Hashing Scheme based on Longest Circular Co-Substring</title>
    <summary>  Locality-Sensitive Hashing (LSH) is one of the most popular methods for
$c$-Approximate Nearest Neighbor Search ($c$-ANNS) in high-dimensional spaces.
In this paper, we propose a novel LSH scheme based on the Longest Circular
Co-Substring (LCCS) search framework (LCCS-LSH) with a theoretical guarantee.
We introduce a novel concept of LCCS and a new data structure named Circular
Shift Array (CSA) for $k$-LCCS search. The insight of LCCS search framework is
that close data objects will have a longer LCCS than the far-apart ones with
high probability. LCCS-LSH is \emph{LSH-family-independent}, and it supports
$c$-ANNS with different kinds of distance metrics. We also introduce a
multi-probe version of LCCS-LSH and conduct extensive experiments over five
real-life datasets. The experimental results demonstrate that LCCS-LSH
outperforms state-of-the-art LSH schemes.
</summary>
    <author>
      <name>Yifan Lei</name>
    </author>
    <author>
      <name>Qiang Huang</name>
    </author>
    <author>
      <name>Mohan Kankanhalli</name>
    </author>
    <author>
      <name>Anthony K. H. Tung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.05345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.05345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.05738">
    <id>http://arxiv.org/abs/2004.05738v1</id>
    <updated>2020-04-13T01:20:51Z</updated>
    <published>2020-04-13T01:20:51Z</published>
    <title>Lower Bound for Succinct Range Minimum Query</title>
    <summary>  Given an integer array $A[1..n]$, the Range Minimum Query problem (RMQ) asks
to preprocess $A$ into a data structure, supporting RMQ queries: given $a,b\in
[1,n]$, return the index $i\in[a,b]$ that minimizes $A[i]$, i.e.,
$\mathrm{argmin}_{i\in[a,b]} A[i]$. This problem has a classic solution using
$O(n)$ space and $O(1)$ query time by Gabow, Bentley, Tarjan (STOC, 1984) and
Harel, Tarjan (SICOMP, 1984). The best known data structure by Fischer, Heun
(SICOMP, 2011) and Navarro, Sadakane (TALG, 2014) uses $2n+n/(\frac{\log
n}{t})^t+\tilde{O}(n^{3/4})$ bits and answers queries in $O(t)$ time, assuming
the word-size is $w=\Theta(\log n)$. In particular, it uses
$2n+n/\mathrm{poly}\log n$ bits of space as long as the query time is a
constant.
  In this paper, we prove the first lower bound for this problem, showing that
$2n+n/\mathrm{poly}\log n$ space is necessary for constant query time. In
general, we show that if the data structure has query time $O(t)$, then it must
use at least $2n+n/(\log n)^{\tilde{O}(t^2)}$ space, in the cell-probe model
with word-size $w=\Theta(\log n)$.
</summary>
    <author>
      <name>Mingmou Liu</name>
    </author>
    <author>
      <name>Huacheng Yu</name>
    </author>
    <link href="http://arxiv.org/abs/2004.05738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.05738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68P30, 68Q17" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.4; F.1.3; F.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.05309">
    <id>http://arxiv.org/abs/2004.05309v2</id>
    <updated>2020-04-27T06:14:12Z</updated>
    <published>2020-04-11T05:27:31Z</published>
    <title>Grammar-compressed Self-index with Lyndon Words</title>
    <summary>  We introduce a new class of straight-line programs (SLPs), named the Lyndon
SLP, inspired by the Lyndon trees (Barcelo, 1990). Based on this SLP, we
propose a self-index data structure of $O(g)$ words of space that can be built
from a string $T$ in $O(n \lg n)$ expected time, retrieving the starting
positions of all occurrences of a pattern $P$ of length $m$ in $O(m + \lg m \lg
n + occ \lg g)$ time, where $n$ is the length of $T$, $g$ is the size of the
Lyndon SLP for $T$, and $occ$ is the number of occurrences of $P$ in $T$.
</summary>
    <author>
      <name>Kazuya Tsuruta</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2004.05309v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.05309v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.06474">
    <id>http://arxiv.org/abs/2004.06474v1</id>
    <updated>2020-04-09T20:00:12Z</updated>
    <published>2020-04-09T20:00:12Z</published>
    <title>Two halves of a meaningful text are statistically different</title>
    <summary>  Which statistical features distinguish a meaningful text (possibly written in
an unknown system) from a meaningless set of symbols? Here we answer this
question by comparing features of the first half of a text to its second half.
This comparison can uncover hidden effects, because the halves have the same
values of many parameters (style, genre {\it etc}). We found that the first
half has more different words and more rare words than the second half. Also,
words in the first half are distributed less homogeneously over the text in the
sense of of the difference between the frequency and the inverse spatial
period. These differences hold for the significant majority of several hundred
relatively short texts we studied. The statistical significance is confirmed
via the Wilcoxon test. Differences disappear after random permutation of words
that destroys the linear structure of the text. The differences reveal a
temporal asymmetry in meaningful texts, which is confirmed by showing that
texts are much better compressible in their natural way (i.e. along the
narrative) than in the word-inverted form. We conjecture that these results
connect the semantic organization of a text (defined by the flow of its
narrative) to its statistical features.
</summary>
    <author>
      <name>Weibing Deng</name>
    </author>
    <author>
      <name>R. Xie</name>
    </author>
    <author>
      <name>S. Deng</name>
    </author>
    <author>
      <name>Armen E. Allahverdyan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages and 14 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.06474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.06474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.04858">
    <id>http://arxiv.org/abs/2004.04858v1</id>
    <updated>2020-04-09T23:51:23Z</updated>
    <published>2020-04-09T23:51:23Z</published>
    <title>Pattern Discovery in Colored Strings</title>
    <summary>  We consider the problem of identifying patterns of interest in colored
strings. A colored string is a string in which each position is colored with
one of a finite set of colors. Our task is to find substrings that always occur
followed by the same color at the same distance. The problem is motivated by
applications in embedded systems verification, in particular, assertion mining.
The goal there is to automatically infer properties of the embedded system from
the analysis of its simulation traces. We show that the number of interesting
patterns is upper-bounded by $\mathcal{O}(n^2)$ where $n$ is the length of the
string. We introduce a baseline algorithm with $\mathcal{O}(n^2)$ running time
which identifies all interesting patterns for all colors in the string
satisfying certain minimality conditions. When one is interested in patterns
related to only one color, we provide an algorithm that identifies patterns in
$\mathcal{O}(n^2\log n)$ time, but is faster than the first algorithm in
practice, both on simulated and on real-world patterns.
</summary>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 4 figures, 3 tables, accepted at SEA 2020 (18th Symposium
  on Experimental Algorithms, Catania, Italy, June 16-18, 2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.04858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.04858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.04344">
    <id>http://arxiv.org/abs/2004.04344v1</id>
    <updated>2020-04-09T03:12:03Z</updated>
    <published>2020-04-09T03:12:03Z</published>
    <title>A Pedagogically Sound yet Efficient Deletion algorithm for Red-Black
  Trees: The Parity-Seeking Delete Algorithm</title>
    <summary>  Red-black (RB) trees are one of the most efficient variants of balanced
binary search trees. However, they have always been blamed for being too
complicated, hard to explain, and not suitable for pedagogical purposes.
Sedgewick (2008) proposed left-leaning red-black (LLRB) trees in which red
links are restricted to left children, and proposed recursive concise insert
and delete algorithms. However, the top-down deletion algorithm of LLRB is
still very complicated and highly inefficient. In this paper, we first consider
2-3 red-black trees in which both children cannot be red. We propose a
parity-seeking delete algorithm with the basic idea of making the deficient
subtree on a par with its sibling: either by fixing the deficient subtree or by
making the sibling deficient, as well, ascending deficiency to the parent node.
This is the first pedagogically sound algorithm for the delete operation in
red-black trees. Then, we amend our algorithm and propose a parity-seeking
delete algorithm for classical RB trees. Our experiments show that, despite
having more rotations, 2-3 RB trees are almost as efficient as RB trees and
twice faster than LLRB trees. Besides, RB trees with the proposed
parity-seeking delete algorithm have the same number of rotations and almost
identical running time as the classic delete algorithm. While being extremely
efficient, the proposed parity-seeking delete algorithm is easily
understandable and suitable for pedagogical purposes.
</summary>
    <author>
      <name>Kamaledin Ghiasi-Shirazi</name>
    </author>
    <author>
      <name>Taraneh Ghandi</name>
    </author>
    <author>
      <name>Ali Taghizadeh</name>
    </author>
    <author>
      <name>Ali Rahimi-Baigi</name>
    </author>
    <link href="http://arxiv.org/abs/2004.04344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.04344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.04586">
    <id>http://arxiv.org/abs/2004.04586v1</id>
    <updated>2020-04-09T15:14:15Z</updated>
    <published>2020-04-09T15:14:15Z</published>
    <title>Storing Set Families More Compactly with Top ZDDs</title>
    <summary>  Zero-suppressed Binary Decision Diagrams (ZDDs) are data structures for
representing set families in a compressed form. With ZDDs, many valuable
operations on set families can be done in time polynomial in ZDD size. In some
cases, however, the size of ZDDs for representing large set families becomes
too huge to store them in the main memory. This paper proposes top ZDD, a novel
representation of ZDDs which uses less space than existing ones. The top ZDD is
an extension of top tree, which compresses trees, to compress directed acyclic
graphs by sharing identical subgraphs. We prove that navigational operations on
ZDDs can be done in time poly-logarithmicin ZDD size, and show that there exist
set families for which the size of the top ZDD is exponentially smaller than
that of the ZDD. We also show experimentally that our top ZDDs have smaller
size than ZDDs for real data.
</summary>
    <author>
      <name>Kotaro Matsuda</name>
    </author>
    <author>
      <name>Shuhei Denzumi</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages; Accepted for SEA2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.04586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.04586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.07678">
    <id>http://arxiv.org/abs/2005.07678v1</id>
    <updated>2020-05-15T17:48:44Z</updated>
    <published>2020-05-15T17:48:44Z</published>
    <title>Edit Distance in Near-Linear Time: it's a Constant Factor</title>
    <summary>  We present an algorithm for approximating the edit distance between two
strings of length $n$ in time $n^{1+\epsilon}$, for any $\epsilon>0$, up to a
constant factor. Our result completes the research direction set forth in the
recent breakthrough paper [Chakraborty-Das-Goldenberg-Koucky-Saks, FOCS'18],
which showed the first constant-factor approximation algorithm with a
(strongly) sub-quadratic running time. Several recent results have shown
near-linear complexity under different restrictions on the inputs (eg, when the
edit distance is close to maximal, or when one of the inputs is pseudo-random).
In contrast, our algorithm obtains a constant-factor approximation in
near-linear running time for any input strings.
</summary>
    <author>
      <name>Alexandr Andoni</name>
    </author>
    <author>
      <name>Negev Shekel Nosatzki</name>
    </author>
    <link href="http://arxiv.org/abs/2005.07678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.07678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.07644">
    <id>http://arxiv.org/abs/2005.07644v1</id>
    <updated>2020-05-15T16:58:51Z</updated>
    <published>2020-05-15T16:58:51Z</published>
    <title>Breadth-First Rank/Select in Succinct Trees and Distance Oracles for
  Interval Graphs</title>
    <summary>  We present the first succinct data structure for ordinal trees that supports
the mapping between the preorder (i.e., depth-first) ranks and level-order
(breadth-first) ranks of nodes in constant time. It also provides constant-time
support for all the operations provided by different approaches in previous
work, as well as new operations that allow us to retrieve the last internal
node before or the first internal node after a given node in a level-order
traversal. This new representation gives us the functionality needed to design
the first succinct distance oracles for interval graphs, proper interval graphs
and $k$-proper/$k$-improper interval graphs.
</summary>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <author>
      <name>Kaiyu Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2005.07644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.07644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.06213">
    <id>http://arxiv.org/abs/2005.06213v1</id>
    <updated>2020-05-13T09:07:43Z</updated>
    <published>2020-05-13T09:07:43Z</published>
    <title>Efficient and Effective Query Auto-Completion</title>
    <summary>  Query Auto-Completion (QAC) is an ubiquitous feature of modern textual search
systems, suggesting possible ways of completing the query being typed by the
user. Efficiency is crucial to make the system have a real-time responsiveness
when operating in the million-scale search space. Prior work has extensively
advocated the use of a trie data structure for fast prefix-search operations in
compact space. However, searching by prefix has little discovery power in that
only completions that are prefixed by the query are returned. This may impact
negatively the effectiveness of the QAC system, with a consequent monetary loss
for real applications like Web Search Engines and eCommerce. In this work we
describe the implementation that empowers a new QAC system at eBay, and discuss
its efficiency/effectiveness in relation to other approaches at the
state-of-the-art. The solution is based on the combination of an inverted index
with succinct data structures, a much less explored direction in the
literature. This system is replacing the previous implementation based on
Apache SOLR that was not always able to meet the required
service-level-agreement.
</summary>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Giulio Ermanno Pibiri</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in SIGIR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.06213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.06213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.06329">
    <id>http://arxiv.org/abs/2005.06329v1</id>
    <updated>2020-05-13T13:58:13Z</updated>
    <published>2020-05-13T13:58:13Z</published>
    <title>k-Approximate Quasiperiodicity under Hamming and Edit Distance</title>
    <summary>  Quasiperiodicity in strings was introduced almost 30 years ago as an
extension of string periodicity. The basic notions of quasiperiodicity are
cover and seed. A cover of a text $T$ is a string whose occurrences in $T$
cover all positions of $T$. A seed of text $T$ is a cover of a superstring of
$T$. In various applications exact quasiperiodicity is still not sufficient due
to the presence of errors. We consider approximate notions of quasiperiodicity,
for which we allow approximate occurrences in $T$ with a small Hamming,
Levenshtein or weighted edit distance.
  In previous work Sip et al. (2002) and Christodoulakis et al. (2005) showed
that computing approximate covers and seeds, respectively, under weighted edit
distance is NP-hard. They, therefore, considered restricted approximate covers
and seeds which need to be factors of the original string $T$ and presented
polynomial-time algorithms for computing them. Further algorithms, considering
approximate occurrences with Hamming distance bounded by $k$, were given in
several contributions by Guth et al. They also studied relaxed approximate
quasiperiods that do not need to cover all positions of $T$.
  In case of large data the exponents in polynomial time complexity play a
crucial role. We present more efficient algorithms for computing restricted
approximate covers and seeds. In particular, we improve upon the complexities
of many of the aforementioned algorithms, also for relaxed quasiperiods. Our
solutions are especially efficient if the number (or total cost) of allowed
errors is bounded. We also show NP-hardness of computing non-restricted
approximate covers and seeds under Hamming distance.
  Approximate covers were studied in three recent contributions at CPM over the
last three years. However, these works consider a different definition of an
approximate cover of $T$.
</summary>
    <author>
      <name>Aleksander Kędzierski</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to CPM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.06329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.06329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.05681">
    <id>http://arxiv.org/abs/2005.05681v1</id>
    <updated>2020-05-12T10:49:46Z</updated>
    <published>2020-05-12T10:49:46Z</published>
    <title>Counting Distinct Patterns in Internal Dictionary Matching</title>
    <summary>  We consider the problem of preprocessing a text $T$ of length $n$ and a
dictionary $\mathcal{D}$ in order to be able to efficiently answer queries
$CountDistinct(i,j)$, that is, given $i$ and $j$ return the number of patterns
from $\mathcal{D}$ that occur in the fragment $T[i \mathinner{.\,.} j]$. The
dictionary is internal in the sense that each pattern in $\mathcal{D}$ is given
as a fragment of $T$. This way, the dictionary takes space proportional to the
number of patterns $d=|\mathcal{D}|$ rather than their total length, which
could be $\Theta(n\cdot d)$. An $\tilde{\mathcal{O}}(n+d)$-size data structure
that answers $CountDistinct(i,j)$ queries $\mathcal{O}(\log n)$-approximately
in $\tilde{\mathcal{O}}(1)$ time was recently proposed in a work that
introduced internal dictionary matching [ISAAC 2019]. Here we present an
$\tilde{\mathcal{O}}(n+d)$-size data structure that answers
$CountDistinct(i,j)$ queries $2$-approximately in $\tilde{\mathcal{O}}(1)$
time. Using range queries, for any $m$, we give an
$\tilde{\mathcal{O}}(\min(nd/m,n^2/m^2)+d)$-size data structure that answers
$CountDistinct(i,j)$ queries exactly in $\tilde{\mathcal{O}}(m)$ time. We also
consider the special case when the dictionary consists of all square factors of
the string. We design an $\mathcal{O}(n \log^2 n)$-size data structure that
allows us to count distinct squares in a text fragment $T[i \mathinner{.\,.}
j]$ in $\mathcal{O}(\log n)$ time.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Manal Mohamed</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszyński</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CPM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.05681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.00681">
    <id>http://arxiv.org/abs/2005.00681v1</id>
    <updated>2020-05-02T02:24:03Z</updated>
    <published>2020-05-02T02:24:03Z</published>
    <title>Pointer-Machine Algorithms for Fully-Online Construction of Suffix Trees
  and DAWGs on Multiple Strings</title>
    <summary>  We deal with the problem of maintaining the suffix tree indexing structure
for a fully-online collection of multiple strings, where a new character can be
prepended to any string in the collection at any time. The only previously
known algorithm for the problem, recently proposed by Takagi et al.
[Algorithmica 82(5): 1346-1377 (2020)], runs in $O(N \log \sigma)$ time and
$O(N)$ space on the word RAM model, where $N$ denotes the total length of the
strings and $\sigma$ denotes the alphabet size. Their algorithm makes heavy use
of the nearest marked ancestor (NMA) data structure on semi-dynamic trees, that
can answer queries and supports insertion of nodes in $O(1)$ amortized time on
the word RAM model. In this paper, we present a simpler fully-online
right-to-left algorithm that builds the suffix tree for a given string
collection in $O(N (\log \sigma + \log d))$ time and $O(N)$ space, where $d$ is
the maximum number of in-coming Weiner links to a node of the suffix tree. We
note that $d$ is bounded by the height of the suffix tree, which is further
bounded by the length of the longest string in the collection. The advantage of
this new algorithm is that it works on the pointer machine model, namely, it
does not use the complicated NMA data structures that involve table look-ups.
As a byproduct, we also obtain a pointer-machine algorithm for building the
directed acyclic word graph (DAWG) for a fully-online left-to-right collection
of multiple strings, which runs in $O(N (\log \sigma + \log d))$ time and
$O(N)$ space again without the aid of the NMA data structures.
</summary>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <link href="http://arxiv.org/abs/2005.00681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.00681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.01371">
    <id>http://arxiv.org/abs/2005.01371v1</id>
    <updated>2020-05-04T10:34:07Z</updated>
    <published>2020-05-04T10:34:07Z</published>
    <title>Palindromic Length of Words with Many Periodic Palindromes</title>
    <summary>  The palindromic length $\text{PL}(v)$ of a finite word $v$ is the minimal
number of palindromes whose concatenation is equal to $v$. In 2013, Frid,
Puzynina, and Zamboni conjectured that: If $w$ is an infinite word and $k$ is
an integer such that $\text{PL}(u)\leq k$ for every factor $u$ of $w$ then $w$
is ultimately periodic.
  Suppose that $w$ is an infinite word and $k$ is an integer such
$\text{PL}(u)\leq k$ for every factor $u$ of $w$. Let $\Omega(w,k)$ be the set
of all factors $u$ of $w$ that have more than $\sqrt[k]{k^{-1}\vert u\vert}$
palindromic prefixes. We show that $\Omega(w,k)$ is an infinite set and we show
that for each positive integer $j$ there are palindromes $a,b$ and a word $u\in
\Omega(w,k)$ such that $(ab)^j$ is a factor of $u$ and $b$ is nonempty. Note
that $(ab)^j$ is a periodic word and $(ab)^ia$ is a palindrome for each $i\leq
j$. These results justify the following question: What is the palindromic
length of a concatenation of a suffix of $b$ and a periodic word $(ab)^j$ with
"many" periodic palindromes?
  It is known that $\lvert\text{PL}(uv)-\text{PL}(u)\rvert\leq \text{PL}(v)$,
where $u$ and $v$ are nonempty words. The main result of our article shows that
if $a,b$ are palindromes, $b$ is nonempty, $u$ is a nonempty suffix of $b$,
$\vert ab\vert$ is the minimal period of $aba$, and $j$ is a positive integer
with $j\geq3\text{PL}(u)$ then $\text{PL}(u(ab)^j)-\text{PL}(u)\geq 0$.
</summary>
    <author>
      <name>Josef Rukavicka</name>
    </author>
    <link href="http://arxiv.org/abs/2005.01371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.01371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.02725">
    <id>http://arxiv.org/abs/2005.02725v1</id>
    <updated>2020-05-06T10:52:18Z</updated>
    <published>2020-05-06T10:52:18Z</published>
    <title>Incremental Multiple Longest Common Sub-Sequences</title>
    <summary>  We consider the problem of updating the information about multiple longest
common sub-sequences. This kind of sub-sequences is used to highlight
information that is shared across several information sequences, therefore it
is extensively used namely in bioinformatics and computational genomics. In
this paper we propose a way to maintain this information when the underlying
sequences are subject to modifications, namely when letters are added and
removed from the extremes of the sequence. Experimentally our data structure
obtains significant improvements over the state of the art.
</summary>
    <author>
      <name>Luís M. S. Russo</name>
    </author>
    <author>
      <name>Alexandre P. Francisco</name>
    </author>
    <author>
      <name>Tatiana Rocher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The work reported in this article was supported by national funds
  through Funda\c{c}\~ao para a Ci\^encia e Tecnologia (FCT) through projects
  NGPHYLO PTDC/CCI-BIO/29676/2017 and UID/CEC/50021/2019. Funded in part by
  European Union Horizon 2020 research and innovation programme under the Marie
  Sk{\l}odowska-Curie Actions grant agreement No 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.02725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.02725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.13389">
    <id>http://arxiv.org/abs/2004.13389v1</id>
    <updated>2020-04-28T09:40:13Z</updated>
    <published>2020-04-28T09:40:13Z</published>
    <title>Approximating longest common substring with $k$ mismatches: Theory and
  practice</title>
    <summary>  In the problem of the longest common substring with $k$ mismatches we are
given two strings $X, Y$ and must find the maximal length $\ell$ such that
there is a length-$\ell$ substring of $X$ and a length-$\ell$ substring of $Y$
that differ in at most $k$ positions. The length $\ell$ can be used as a robust
measure of similarity between $X, Y$. In this work, we develop new
approximation algorithms for computing $\ell$ that are significantly more
efficient that previously known solutions from the theoretical point of view.
Our approach is simple and practical, which we confirm via an experimental
evaluation, and is probably close to optimal as we demonstrate via a
conditional lower bound.
</summary>
    <author>
      <name>Garance Gourdel</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <link href="http://arxiv.org/abs/2004.13389v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.13389v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.12881">
    <id>http://arxiv.org/abs/2004.12881v1</id>
    <updated>2020-04-27T15:41:49Z</updated>
    <published>2020-04-27T15:41:49Z</published>
    <title>The Streaming k-Mismatch Problem: Tradeoffs between Space and Total Time</title>
    <summary>  We revisit the $k$-mismatch problem in the streaming model on a pattern of
length $m$ and a streaming text of length $n$, both over a size-$\sigma$
alphabet. The current state-of-the-art algorithm for the streaming $k$-mismatch
problem, by Clifford et al. [SODA 2019], uses $\tilde O(k)$ space and $\tilde
O\big(\sqrt k\big)$ worst-case time per character. The space complexity is
known to be (unconditionally) optimal, and the worst-case time per character
matches a conditional lower bound. However, there is a gap between the total
time cost of the algorithm, which is $\tilde O(n\sqrt k)$, and the fastest
known offline algorithm, which costs $\tilde O\big(n + \min\big(\frac{nk}{\sqrt
m},\sigma n\big)\big)$ time. Moreover, it is not known whether improvements
over the $\tilde O(n\sqrt k)$ total time are possible when using more than
$O(k)$ space.
  We address these gaps by designing a randomized streaming algorithm for the
$k$-mismatch problem that, given an integer parameter $k\le s \le m$, uses
$\tilde O(s)$ space and costs $\tilde O\big(n+\min\big(\frac
{nk^2}m,\frac{nk}{\sqrt s},\frac{\sigma nm}s\big)\big)$ total time. For $s=m$,
the total runtime becomes $\tilde O\big(n + \min\big(\frac{nk}{\sqrt m},\sigma
n\big)\big)$, which matches the time cost of the fastest offline algorithm.
Moreover, the worst-case time cost per character is still $\tilde O\big(\sqrt
k\big)$.
</summary>
    <author>
      <name>Shay Golan</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract to appear in CPM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.12881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.09281">
    <id>http://arxiv.org/abs/2005.09281v1</id>
    <updated>2020-05-19T08:28:03Z</updated>
    <published>2020-05-19T08:28:03Z</published>
    <title>On Weighted Prefix Normal Words</title>
    <summary>  A prefix normal word is a binary word whose prefixes contain at least as many
1s as any of its factors of the same length. Introduced by Fici and Lipt\'ak in
2011 the notion of prefix normality is so far only defined for words over the
binary alphabet. In this work we investigate possible generalisations for
finite words over arbitrary finite alphabets, namely weighted and subset prefix
normality. We prove that weighted prefix normality is more expressive than both
binary and subset prefix normality and investigate the existence of a weighted
prefix normal form. While subset prefix normality directly inherits most
properties from the binary case, weighted prefix normality comes with several
new peculiarities that did not already occur in the binary case. We
characterise these issues and solve further questions regarding the weighted
prefix normality and weighted prefix normal form.
</summary>
    <author>
      <name>Yannik Eikmeier</name>
    </author>
    <author>
      <name>Pamela Fleischmann</name>
    </author>
    <author>
      <name>Dirk Nowotka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.09281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.08950">
    <id>http://arxiv.org/abs/2005.08950v1</id>
    <updated>2020-05-16T19:22:05Z</updated>
    <published>2020-05-16T19:22:05Z</published>
    <title>Quantum string comparison method</title>
    <summary>  We propose a quantum string comparison method whose main building blocks are
a specially designed oracle construction followed by Grover's search algorithm.
The purpose of the oracle is to compare all alphabets of the string in
parallel. This requires a unique input state preparation, which when combined
with some ancillas will result in a deterministic binary success and failure
compare outcome.
</summary>
    <author>
      <name>Vikram Menon</name>
    </author>
    <author>
      <name>Ayan Chattopadhyay</name>
    </author>
    <link href="http://arxiv.org/abs/2005.08950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.09169">
    <id>http://arxiv.org/abs/2005.09169v1</id>
    <updated>2020-05-19T02:21:18Z</updated>
    <published>2020-05-19T02:21:18Z</published>
    <title>A reduction of the dynamic time warping distance to the longest
  increasing subsequence length</title>
    <summary>  The similarity between a pair of time series, i.e., sequences of indexed
values in time order, is often estimated by the dynamic time warping (DTW)
distance, instead of any in the well-studied family of measurements including
the longest common subsequence (LCS) length and the edit distance. Although it
may seem as if the DTW and LCS(-like) measurements are essentially different,
we reveal that the DTW distance can be represented by the longest increasing
subsequence (LIS) length of a sequence of integers, which is the LCS length
between the integer sequence and itself sorted. To demonstrate that techniques
developed under LCS(-like) measurements are directly applicable to analysis of
time series via our reduction of DTW to LIS, we present time-efficient
algorithms for DTW-related problems utilizing the semi-local sequence
comparison technique developed for LCS-related problems.
</summary>
    <author>
      <name>Yoshifumi Sakai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <link href="http://arxiv.org/abs/2005.09169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.09342">
    <id>http://arxiv.org/abs/2005.09342v1</id>
    <updated>2020-05-19T10:11:33Z</updated>
    <published>2020-05-19T10:11:33Z</published>
    <title>Linear Time Construction of Indexable Founder Block Graphs</title>
    <summary>  We introduce a compact pangenome representation based on an optimal
segmentation concept that aims to reconstruct founder sequences from a multiple
sequence alignment (MSA). Such founder sequences have the feature that each row
of the MSA is a recombination of the founders. Several linear time dynamic
programming algorithms have been previously devised to optimize segmentations
that induce founder blocks that then can be concatenated into a set of founder
sequences. All possible concatenation orders can be expressed as a founder
block graph. We observe a key property of such graphs: if the node labels
(founder segments) do not repeat in the paths of the graph, such graphs can be
indexed for efficient string matching. We call such graphs segment repeat-free
founder block graphs.
  We give a linear time algorithm to construct a segment repeat-free founder
block graph given an MSA. The algorithm combines techniques from the founder
segmentation algorithms (Cazaux et al. SPIRE 2019) and fully-functional
bidirectional Burrows-Wheeler index (Belazzougui and Cunial, CPM 2019). We
derive a succinct index structure to support queries of arbitrary length in the
paths of the graph.
  Experiments on an MSA of SAR-CoV-2 strains are reported. An MSA of size
$410\times 29811$ is compacted in one minute into a segment repeat-free founder
block graph of 3900 nodes and 4440 edges. The maximum length and total length
of node labels is 12 and 34968, respectively. The index on the graph takes only
$3\%$ of the size of the MSA.
</summary>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Massimo Equi</name>
    </author>
    <author>
      <name>Tuukka Norri</name>
    </author>
    <author>
      <name>Alexandru I. Tomescu</name>
    </author>
    <link href="http://arxiv.org/abs/2005.09342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2.2; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.09524">
    <id>http://arxiv.org/abs/2005.09524v1</id>
    <updated>2020-05-19T15:34:18Z</updated>
    <published>2020-05-19T15:34:18Z</published>
    <title>On repetitiveness measures of Thue-Morse words</title>
    <summary>  We show that the size $\gamma(t_n)$ of the smallest string attractor of the
$n$th Thue-Morse word $t_n$ is 4 for any $n\geq 4$, disproving the conjecture
by Mantaci et al. [ICTCS 2019] that it is $n$. We also show that $\delta(t_n) =
\frac{10}{3+2^{4-n}}$ for $n \geq 3$, where $\delta(w)$ is the maximum over all
$k = 1,\ldots,|w|$, the number of distinct substrings of length $k$ in $w$
divided by $k$, which is a measure of repetitiveness recently studied by
Kociumaka et al. [LATIN 2020]. Furthermore, we show that the number $z(t_n)$ of
factors in the self-referencing Lempel-Ziv factorization of $t_n$ is exactly
$2n$.
</summary>
    <author>
      <name>Kanaru Kutsukake</name>
    </author>
    <author>
      <name>Takuya Matsumoto</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2005.09524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.08190">
    <id>http://arxiv.org/abs/2005.08190v1</id>
    <updated>2020-05-17T08:14:43Z</updated>
    <published>2020-05-17T08:14:43Z</published>
    <title>Towards Efficient Interactive Computation of Dynamic Time Warping
  Distance</title>
    <summary>  The dynamic time warping (DTW) is a widely-used method that allows us to
efficiently compare two time series that can vary in speed. Given two strings
$A$ and $B$ of respective lengths $m$ and $n$, there is a fundamental dynamic
programming algorithm that computes the DTW distance for $A$ and $B$ together
with an optimal alignment in $\Theta(mn)$ time and space. In this paper, we
tackle the problem of interactive computation of the DTW distance for dynamic
strings, denoted $\mathrm{D^2TW}$, where character-wise edit operation
(insertion, deletion, substitution) can be performed at an arbitrary position
of the strings. Let $M$ and $N$ be the sizes of the run-length encoding (RLE)
of $A$ and $B$, respectively. We present an algorithm for $\mathrm{D^2TW}$ that
occupies $\Theta(mN+nM)$ space and uses $O(m+n+\#_{\mathrm{chg}}) \subseteq
O(mN + nM)$ time to update a compact differential representation $\mathit{DS}$
of the DP table per edit operation, where $\#_{\mathrm{chg}}$ denotes the
number of cells in $\mathit{DS}$ whose values change after the edit operation.
Our method is at least as efficient as the algorithm recently proposed by
Froese et al. running in $\Theta(mN + nM)$ time, and is faster when
$\#_{\mathrm{chg}}$ is smaller than $O(mN + nM)$ which, as our preliminary
experiments suggest, is likely to be the case in the majority of instances. In
addition, our result leads to interactive LCS/weighted edit distance
computation running in $O(m+n+\#_{\mathrm{chg}}) \subseteq O(mN + nM)$ time per
update using $\Theta(mN + nM)$ space. This improves on Hyyr\"o et al.'s
interactive algorithm that occupies $\Theta(mn)$ space and uses $O(mn)$ time
per update in the worst case.
</summary>
    <author>
      <name>Akihiro Nishi</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2005.08190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.14335">
    <id>http://arxiv.org/abs/2005.14335v1</id>
    <updated>2020-05-28T22:44:01Z</updated>
    <published>2020-05-28T22:44:01Z</published>
    <title>Classical and Quantum Algorithms for Constructing Text from Dictionary
  Problem</title>
    <summary>  We study algorithms for solving the problem of constructing a text (long
string) from a dictionary (sequence of small strings). The problem has an
application in bioinformatics and has a connection with the Sequence assembly
method for reconstructing a long DNA sequence from small fragments. The problem
is constructing a string $t$ of length $n$ from strings $s^1,\dots, s^m$ with
possible intersections. We provide a classical algorithm with running time
$O\left(n+L +m(\log n)^2\right)=\tilde{O}(n+L)$ where $L$ is the sum of lengths
of $s^1,\dots,s^m$. We provide a quantum algorithm with running time $O\left(n
+\log n\cdot(\log m+\log\log n)\cdot \sqrt{m\cdot L}\right)=\tilde{O}\left(n
+\sqrt{m\cdot L}\right)$. Additionally, we show that the lower bound for the
classical algorithm is $\Omega(n+L)$. Thus, our classical algorithm is optimal
up to a log factor, and our quantum algorithm shows speed-up comparing to any
classical algorithm in a case of non-constant length of strings in the
dictionary.
</summary>
    <author>
      <name>Kamil Khadiev</name>
    </author>
    <author>
      <name>Vladislav Remidovskii</name>
    </author>
    <link href="http://arxiv.org/abs/2005.14335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.14335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.00216">
    <id>http://arxiv.org/abs/2006.00216v1</id>
    <updated>2020-05-30T08:09:44Z</updated>
    <published>2020-05-30T08:09:44Z</published>
    <title>Longest Square Subsequence Problem Revisited</title>
    <summary>  The longest square subsequence (LSS) problem consists of computing a longest
subsequence of a given string $S$ that is a square, i.e., a longest subsequence
of form $XX$ appearing in $S$. It is known that an LSS of a string $S$ of
length $n$ can be computed using $O(n^2)$ time [Kosowski 2004], or with
(model-dependent) polylogarithmic speed-ups using $O(n^2 (\log \log n)^2 /
\log^2 n)$ time [Tiskin 2013]. We present the first algorithm for LSS whose
running time depends on other parameters, i.e., we show that an LSS of $S$ can
be computed in $O(r \min\{n, M\}\log \frac{n}{r} + M \log n)$ time with $O(M)$
space, where $r$ is the length of an LSS of $S$ and $M$ is the number of
matching points on $S$.
</summary>
    <author>
      <name>Takafumi Inoue</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <link href="http://arxiv.org/abs/2006.00216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.00216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.00605">
    <id>http://arxiv.org/abs/2006.00605v1</id>
    <updated>2020-05-31T20:49:52Z</updated>
    <published>2020-05-31T20:49:52Z</published>
    <title>A Fast Algorithm for Online k-servers Problem on Trees</title>
    <summary>  We consider online algorithms for the $k$-servers problem on trees. There is
an $k$-competitive algorithm for this problem, and it is the best competitive
ratio. M. Chrobak and L. Larmore suggested it. At the same time, the existing
implementation has $O(n)$ time complexity, where $n$ is a number of nodes in a
tree. We suggest a new time-efficient implementation of the algorithm. It has
$O(n)$ time complexity for preprocessing and $O\left(k(\log n)^2\right)$ for
processing a query.
</summary>
    <author>
      <name>Kamil Khadiev</name>
    </author>
    <author>
      <name>Maxim Yagafarov</name>
    </author>
    <link href="http://arxiv.org/abs/2006.00605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.00605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.12648">
    <id>http://arxiv.org/abs/2005.12648v1</id>
    <updated>2020-05-26T12:08:04Z</updated>
    <published>2020-05-26T12:08:04Z</published>
    <title>On the improvement of the in-place merge algorithm parallelization</title>
    <summary>  In this paper, we present several improvements in the parallelization of the
in-place merge algorithm, which merges two contiguous sorted arrays into one
with an O(T) space complexity (where T is the number of threads). The approach
divides the two arrays into as many pairs of partitions as there are threads
available; such that each thread can later merge a pair of partitions
independently of the others. We extend the existing method by proposing a new
algorithm to find the median of two partitions. Additionally, we provide a new
strategy to divide the input arrays where we minimize the data movement, but at
the cost of making this stage sequential. Finally, we provide the so-called
linear shifting algorithm that swaps two partitions in-place with contiguous
data access. We emphasize that our approach is straightforward to implement and
that it can also be used for external (out of place) merging. The results
demonstrate that it provides a significant speedup compared to sequential
executions, when the size of the arrays is greater than a thousand elements.
</summary>
    <author>
      <name>Berenger Bramas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria, ICube, CAMUS</arxiv:affiliation>
    </author>
    <author>
      <name>Quentin Bramas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ICube, UNISTRA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2005.12648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.12648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.11718">
    <id>http://arxiv.org/abs/2005.11718v2</id>
    <updated>2020-05-27T07:13:27Z</updated>
    <published>2020-05-24T11:12:02Z</published>
    <title>An inequality for the number of periods in a word</title>
    <summary>  We prove an inequality for the number of periods in a word x in terms of the
length of x and its initial critical exponent. Next, we characterize all
periods of the length-n prefix of a characteristic Sturmian word in terms of
the lazy Ostrowski representation of n, and use this result to show that our
inequality is tight for infinitely many words x. We propose two related
measures of periodicity for infinite words. Finally, we also consider special
cases where x is overlap-free or squarefree.
</summary>
    <author>
      <name>Daniel Gabric</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2005.11718v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.11718v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.10917">
    <id>http://arxiv.org/abs/2005.10917v1</id>
    <updated>2020-05-21T21:42:30Z</updated>
    <published>2020-05-21T21:42:30Z</published>
    <title>Succinct Trit-array Trie for Scalable Trajectory Similarity Search</title>
    <summary>  Massive datasets of spatial trajectories representing the mobility of a
diversity of moving objects are ubiquitous in research and industry. Similarity
search of a large collection of trajectories is indispensable for turning these
datasets into knowledge. Current methods for similarity search of trajectories
are inefficient in terms of search time and memory when applied to massive
datasets. In this paper, we address this problem by presenting a scalable
similarity search for Fr\'echet distance on trajectories, which we call
trajectory-indexing succinct trit-array trie (tSTAT). tSTAT achieves time and
memory efficiency by leveraging locality sensitive hashing (LSH) for Fr\'echet
distance and a trie data structure. We also present two novel techniques of
node reduction and a space-efficient representation for tries, which enable to
dramatically enhance a memory efficiency of tries. We experimentally test tSTAT
on its ability to retrieve similar trajectories for a query from large
collections of trajectories and show that tSTAT performs superiorly with
respect to search time and memory efficiency.
</summary>
    <author>
      <name>Shunsuke Kanda</name>
    </author>
    <author>
      <name>Koh Takeuchi</name>
    </author>
    <author>
      <name>Keisuke Fujii</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/2005.10917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.10917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.11188">
    <id>http://arxiv.org/abs/2005.11188v1</id>
    <updated>2020-05-22T13:45:23Z</updated>
    <published>2020-05-22T13:45:23Z</published>
    <title>Still Simpler Static Level Ancestors</title>
    <summary>  A level-ancestor or LA query about a rooted tree $T$ takes as arguments a
node $v$ in $T$, of depth $d_v$, say, and an integer $d$ with $0\le d\le d_v$
and returns the ancestor of $v$ in $T$ of depth $d$. The static LA problem is
to process a given rooted tree $T$ so as to support efficient subsequent
processing of LA queries about $T$. All previous efficient solutions to the
static LA problem work by reducing a given instance of the problem to a smaller
instance of the same or a related problem, solved with a less efficient data
structure, and a collection of small micro-instances for which a different
solution is provided. We indicate the first efficient solution to the static LA
problem that works directly, without resorting to reductions or
micro-instances.
</summary>
    <author>
      <name>Torben Hagerup</name>
    </author>
    <link href="http://arxiv.org/abs/2005.11188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.11188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.10668">
    <id>http://arxiv.org/abs/2005.10668v1</id>
    <updated>2020-05-20T16:14:00Z</updated>
    <published>2020-05-20T16:14:00Z</published>
    <title>Primitive Sets of Words</title>
    <summary>  Given a (finite or infinite) subset $X$ of the free monoid $A^*$ over a
finite alphabet $A$, the rank of $X$ is the minimal cardinality of a set $F$
such that $X \subseteq F^*$. We say that a submonoid $M$ generated by $k$
elements of $A^*$ is {\em $k$-maximal} if there does not exist another
submonoid generated by at most $k$ words containing $M$. We call a set $X
\subseteq A^*$ {\em primitive} if it is the basis of a $|X|$-maximal submonoid.
This definition encompasses the notion of primitive word -- in fact, $\{w\}$ is
a primitive set if and only if $w$ is a primitive word. By definition, for any
set $X$, there exists a primitive set $Y$ such that $X \subseteq Y^*$. We
therefore call $Y$ a {\em primitive root} of $X$. As a main result, we prove
that if a set has rank $2$, then it has a unique primitive root. To obtain this
result, we prove that the intersection of two $2$-maximal submonoids is either
the empty word or a submonoid generated by one single primitive word. For a
single word $w$, we say that the set $\{x,y\}$ is a {\em bi-root} of $w$ if $w$
can be written as a concatenation of copies of $x$ and $y$ and $\{x,y\}$ is a
primitive set. We prove that every primitive word $w$ has at most one bi-root
$\{x,y\}$ such that $|x|+|y|&lt;\sqrt{|w|}$. That is, the bi-root of a word is
unique provided the word is sufficiently long with respect to the size (sum of
lengths) of the root. Our results are also compared to previous approaches that
investigate pseudo-repetitions, where a morphic involutive function $\theta$ is
defined on $A^*$. In this setting, the notions of $\theta$-power,
$\theta$-primitive and $\theta$-root are defined, and it is shown that any word
has a unique $\theta$-primitive root. This result can be obtained with our
approach by showing that a word $w$ is $\theta$-primitive if and only if $\{w,
\theta(w)\}$ is a primitive set.
</summary>
    <author>
      <name>Giuseppa Castiglione</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Antonio Restivo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted. arXiv admin note: substantial text overlap with
  arXiv:1810.02182</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.10668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.10668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.10800">
    <id>http://arxiv.org/abs/2005.10800v1</id>
    <updated>2020-05-21T17:29:40Z</updated>
    <published>2020-05-21T17:29:40Z</published>
    <title>New Approximation Algorithms for Maximum Asymmetric Traveling Salesman
  and Shortest Superstring</title>
    <summary>  In the maximum asymmetric traveling salesman problem (Max ATSP) we are given
a complete directed graph with nonnegative weights on the edges and we wish to
compute a traveling salesman tour of maximum weight. In this paper we give a
fast combinatorial $\frac{7}{10}$-approximation algorithm for Max ATSP. It is
based on techniques of {\em eliminating} and {\em diluting} problematic
subgraphs with the aid of {\it half-edges} and a method of edge coloring. (A
{\it half-edge} of edge $(u,v)$ is informally speaking "either a head or a tail
of $(u,v)$".) A novel technique of {\em diluting} a problematic subgraph $S$
consists in a seeming reduction of its weight, which allows its better
handling.
  The current best approximation algorithms for Max ATSP, achieving the
approximation guarantee of $\frac 23$, are due to Kaplan, Lewenstein, Shafrir,
Sviridenko (2003) and Elbassioni, Paluch, van Zuylen (2012). Using a result by
Mucha, which states that an $\alpha$-approximation algorithm for Max ATSP
implies a $(2+\frac{11(1-\alpha)}{9-2\alpha})$-approximation algorithm for the
shortest superstring problem (SSP), we obtain also a $(2 \frac{33}{76} \approx
2,434)$-approximation algorithm for SSP, beating the previously best known
(having an approximation factor equal to $2 \frac{11}{23} \approx 2,4782$.)
</summary>
    <author>
      <name>Katarzyna Paluch</name>
    </author>
    <link href="http://arxiv.org/abs/2005.10800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.10800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.10095">
    <id>http://arxiv.org/abs/2005.10095v1</id>
    <updated>2020-05-20T14:56:59Z</updated>
    <published>2020-05-20T14:56:59Z</published>
    <title>The K-Centre Problem for Necklaces</title>
    <summary>  In graph theory, the objective of the k-centre problem is to find a set of
$k$ vertices for which the largest distance of any vertex to its closest vertex
in the $k$-set is minimised. In this paper, we introduce the $k$-centre problem
for sets of necklaces, i.e. the equivalence classes of words under the cyclic
shift. This can be seen as the k-centre problem on the complete weighted graph
where every necklace is represented by a vertex, and each edge has a weight
given by the overlap distance between any pair of necklaces. Similar to the
graph case, the goal is to choose $k$ necklaces such that the distance from any
word in the language and its nearest centre is minimised. However, in a case of
k-centre problem for languages the size of associated graph maybe exponential
in relation to the description of the language, i.e., the length of the words l
and the size of the alphabet q. We derive several approximation algorithms for
the $k$-centre problem on necklaces, with logarithmic approximation factor in
the context of l and k, and within a constant factor for a more restricted
case.
</summary>
    <author>
      <name>Duncan Adamson</name>
    </author>
    <author>
      <name>Argyrios Deligkas</name>
    </author>
    <author>
      <name>Vladimir V. Gusev</name>
    </author>
    <author>
      <name>Igor Potapov</name>
    </author>
    <link href="http://arxiv.org/abs/2005.10095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.10095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.01825">
    <id>http://arxiv.org/abs/2006.01825v1</id>
    <updated>2020-06-02T17:56:42Z</updated>
    <published>2020-06-02T17:56:42Z</published>
    <title>Efficient tree-structured categorical retrieval</title>
    <summary>  We study a document retrieval problem in the new framework where $D$ text
documents are organized in a {\em category tree} with a pre-defined number $h$
of categories. This situation occurs e.g. with taxomonic trees in biology or
subject classification systems for scientific literature. Given a string
pattern $p$ and a category (level in the category tree), we wish to efficiently
retrieve the $t$ \emph{categorical units} containing this pattern and belonging
to the category. We propose several efficient solutions for this problem. One
of them uses $n(\log\sigma(1+o(1))+\log D+O(h)) + O(\Delta)$ bits of space and
$O(|p|+t)$ query time, where $n$ is the total length of the documents, $\sigma$
the size of the alphabet used in the documents and $\Delta$ is the total number
of nodes in the category tree. Another solution uses
$n(\log\sigma(1+o(1))+O(\log D))+O(\Delta)+O(D\log n)$ bits of space and
$O(|p|+t\log D)$ query time. We finally propose other solutions which are more
space-efficient at the expense of a slight increase in query time.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Gregory Kucherov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper accepted for presentation at the 31st Annual
  Symposium on Combinatorial Pattern Matching (CPM 2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.01825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.01825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.02408">
    <id>http://arxiv.org/abs/2006.02408v1</id>
    <updated>2020-06-03T17:33:31Z</updated>
    <published>2020-06-03T17:33:31Z</published>
    <title>Dynamic Longest Common Substring in Polylogarithmic Time</title>
    <summary>  The longest common substring problem consists in finding a longest string
that appears as a (contiguous) substring of two input strings. We consider the
dynamic variant of this problem, in which we are to maintain two dynamic
strings $S$ and $T$, each of length at most $n$, that undergo substitutions of
letters, in order to be able to return a longest common substring after each
substitution. Recently, Amir et al. [ESA 2019] presented a solution for this
problem that needs only $\tilde{\mathcal{O}}(n^{2/3})$ time per update. This
brought the challenge of determining whether there exists a faster solution
with polylogarithmic update time, or (as is the case for other dynamic
problems), we should expect a polynomial (conditional) lower bound. We answer
this question by designing a significantly faster algorithm that processes each
substitution in amortized $\log^{\mathcal{O}(1)} n$ time with high probability.
Our solution relies on exploiting the local consistency of the parsing of a
collection of dynamic strings due to Gawrychowski et al. [SODA 2018], and on
maintaining two dynamic trees with labeled bicolored leaves, so that after each
update we can report a pair of nodes, one from each tree, of maximum combined
weight, which have at least one common leaf-descendant of each color. We
complement this with a lower bound of $\Omega(\log n/ \log\log n)$ for the
update time of any polynomial-size data structure that maintains the LCS of two
dynamic strings, and the same lower bound for the update time of any data
structure of size $\tilde{\mathcal{O}}(n)$ that maintains the LCS of a static
and a dynamic string. Both lower bounds hold even allowing amortization and
randomization.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Karol Pokorski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper that is to appear in the ICALP 2020
  proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.02408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.02134">
    <id>http://arxiv.org/abs/2006.02134v1</id>
    <updated>2020-06-03T10:02:51Z</updated>
    <published>2020-06-03T10:02:51Z</published>
    <title>Computing Palindromic Trees for a Sliding Window and Its Applications</title>
    <summary>  The palindromic tree (a.k.a. eertree) for a string $S$ of length $n$ is a
tree-like data structure that represents the set of all distinct palindromic
substrings of $S$, using $O(n)$ space [Rubinchik and Shur, 2018]. It is known
that, when $S$ is over an alphabet of size $\sigma$ and is given in an online
manner, then the palindromic tree of $S$ can be constructed in $O(n\log\sigma)$
time with $O(n)$ space. In this paper, we consider the sliding window version
of the problem: For a fixed window length $d$, we propose two algorithms to
maintain the palindromic tree of size $O(d)$ for every sliding window
$S[i..i+d-1]$ over $S$, one running in $O(n\log\sigma')$ time with $O(d)$ space
where $\sigma' \leq d$ is the maximum number of distinct characters in the
windows, and the other running in $O(n + d\sigma)$ time with $d\sigma + O(d)$
space. We also present applications of our algorithms for computing minimal
unique palindromic substrings (MUPS) and for computing minimal absent
palindromic words (MAPW) for a sliding window.
</summary>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Kiichi Watanabe</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2006.02134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.02219">
    <id>http://arxiv.org/abs/2006.02219v1</id>
    <updated>2020-06-03T12:30:53Z</updated>
    <published>2020-06-03T12:30:53Z</published>
    <title>LCP-Aware Parallel String Sorting</title>
    <summary>  When lexicographically sorting strings, it is not always necessary to inspect
all symbols. For example, the lexicographical rank of "europar" amongst the
strings "eureka", "eurasia", and "excells" only depends on its so called
relevant prefix "euro". The distinguishing prefix size $D$ of a set of strings
is the number of symbols that actually need to be inspected to establish the
lexicographical ordering of all strings. Efficient string sorters should be
$D$-aware, i.e. their complexity should depend on $D$ rather than on the total
number $N$ of all symbols in all strings. While there are many $D$-aware
sorters in the sequential setting, there appear to be no such results in the
PRAM model. We propose a framework yielding a $D$-aware modification of any
existing PRAM string sorter. The derived algorithms are work-optimal with
respect to their original counterpart: If the original algorithm requires
$O(w(N))$ work, the derived one requires $O(w(D))$ work. The execution time
increases only by a small factor that is logarithmic in the length of the
longest relevant prefix. Our framework universally works for deterministic and
randomized algorithms in all variations of the PRAM model, such that future
improvements in ($D$-unaware) parallel string sorting will directly result in
improvements in $D$-aware parallel string sorting.
</summary>
    <author>
      <name>Jonas Ellert</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Nodari Sitchinava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Euro-Par 2020 and to be published by Springer as part of
  the conference proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.02219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.15575">
    <id>http://arxiv.org/abs/2006.15575v2</id>
    <updated>2020-09-30T12:00:50Z</updated>
    <published>2020-06-28T11:25:40Z</published>
    <title>Random Access in Persistent Strings</title>
    <summary>  We consider compact representations of collections of similar strings that
support random access queries. The collection of strings is given by a rooted
tree where edges are labeled by an edit operation (inserting, deleting, or
replacing a character) and a node represents the string obtained by applying
the sequence of edit operations on the path from the root to the node. The goal
is to compactly represent the entire collection while supporting fast random
access to any part of a string in the collection. This problem captures natural
scenarios such as representing the past history of a edited document or
representing highly-repetitive collections. Given a tree with $n$ nodes, we
show how to represent the corresponding collection in $O(n)$ space and optimal
$O(\log n/ \log \log n)$ query time. This improves the previous time-space
trade-offs for the problem. To obtain our results, we introduce new techniques
and ideas, including a reduction to a new geometric line segment selection
together with an efficient solution.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract at ISAAC 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.15575v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.15575v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.13576">
    <id>http://arxiv.org/abs/2006.13576v2</id>
    <updated>2020-07-22T09:08:09Z</updated>
    <published>2020-06-24T09:33:08Z</published>
    <title>Lyndon Words, the Three Squares Lemma, and Primitive Squares</title>
    <summary>  We revisit the so-called "Three Squares Lemma" by Crochemore and Rytter
[Algorithmica 1995] and, using arguments based on Lyndon words, derive a more
general variant which considers three overlapping squares which do not
necessarily share a common prefix. We also give an improved upper bound of
$n\log_2 n$ on the maximum number of (occurrences of) primitively rooted
squares in a string of length $n$, also using arguments based on Lyndon words.
To the best of our knowledge, the only known upper bound was $n \log_\phi n
\approx 1.441n\log_2 n$, where $\phi$ is the golden ratio, reported by Fraenkel
and Simpson [TCS 1999] obtained via the Three Squares Lemma.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <link href="http://arxiv.org/abs/2006.13576v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.13576v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.14029">
    <id>http://arxiv.org/abs/2006.14029v1</id>
    <updated>2020-06-24T20:26:33Z</updated>
    <published>2020-06-24T20:26:33Z</published>
    <title>Small Longest Tandem Scattered Subsequences</title>
    <summary>  We consider the problem of identifying tandem scattered subsequences within a
string. Our algorithm identifies a longest subsequence which occurs twice
without overlap in a string. This algorithm is based on the Hunt-Szymanski
algorithm, therefore its performance improves if the string is not self
similar. This occurs naturally on strings over large alphabets. Our algorithm
relies on new results for data structures that support dynamic longest
increasing sub-sequences. In the process we also obtain improved algorithms for
the decremental string comparison problem.
</summary>
    <author>
      <name>Luís M. S. Russo</name>
    </author>
    <author>
      <name>Alexandre P. Francisco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The work reported in this article was supported by national funds
  through Funda\c{c}\~ao para a Ci\^encia e Tecnologia (FCT) with reference
  UIDB/50021/2020 and through project NGPHYLO PTDC/CCI-BIO/29676/2017. Funded
  in part by European Union's Horizon 2020 research and innovation programme
  under the Marie Sk{\l}odowska-Curie Actions grant agreement No 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.14029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.14029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.11687">
    <id>http://arxiv.org/abs/2006.11687v1</id>
    <updated>2020-06-21T01:29:47Z</updated>
    <published>2020-06-21T01:29:47Z</published>
    <title>PFP Data Structures</title>
    <summary>  Prefix-free parsing (PFP) was introduced by Boucher et al. (2019) as a
preprocessing step to ease the computation of Burrows-Wheeler Transforms (BWTs)
of genomic databases. Given a string $S$, it produces a dictionary $D$ and a
parse $P$ of overlapping phrases such that $\mathrm{BWT} (S)$ can be computed
from $D$ and $P$ in time and workspace bounded in terms of their combined size
$|\mathrm{PFP} (S)|$. In practice $D$ and $P$ are significantly smaller than
$S$ and computing $\mathrm{BWT} (S)$ from them is more efficient than computing
it from $S$ directly, at least when $S$ consists of genomes from individuals of
the same species. In this paper, we consider $\mathrm{PFP} (S)$ as a {\em data
structure} and show how it can be augmented to support the following queries
quickly, still in $O (|\mathrm{PFP} (S)|)$ space: longest common extension
(LCE), suffix array (SA), longest common prefix (LCP) and BWT. Lastly, we
provide experimental evidence that the PFP data structure can be efficiently
constructed for very large repetitive datasets: it takes one hour and 54 GB
peak memory for $1000$ variants of human chromosome 19, initially occupying
roughly 56 GB.
</summary>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Ondřej Cvacho</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Jan Holub</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <link href="http://arxiv.org/abs/2006.11687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.11687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.11978">
    <id>http://arxiv.org/abs/2006.11978v1</id>
    <updated>2020-06-22T02:54:20Z</updated>
    <published>2020-06-22T02:54:20Z</published>
    <title>Fast Preprocessing for Optimal Orthogonal Range Reporting and Range
  Successor with Applications to Text Indexing</title>
    <summary>  Under the word RAM model, we design three data structures that can be
constructed in $O(n\sqrt{\lg n})$ time over $n$ points in an $n \times n$ grid.
The first data structure is an $O(n\lg^{\epsilon} n)$-word structure supporting
orthogonal range reporting in $O(\lg\lg n+k)$ time, where $k$ denotes output
size and $\epsilon$ is an arbitrarily small constant. The second is an
$O(n\lg\lg n)$-word structure supporting orthogonal range successor in
$O(\lg\lg n)$ time, while the third is an $O(n\lg^{\epsilon} n)$-word structure
supporting sorted range reporting in $O(\lg\lg n+k)$ time. The query times of
these data structures are optimal when the space costs must be within $O(n\
polylog\ n)$ words. Their exact space bounds match those of the best known
results achieving the same query times, and the $O(n\sqrt{\lg n})$ construction
time beats the previous bounds on preprocessing. Previously, among 2d range
search structures, only the orthogonal range counting structure of Chan and
P\v{a}tra\c{s}cu (SODA 2010) and the linear space, $O(\lg^{\epsilon} n)$ query
time structure for orthogonal range successor by Belazzougui and Puglisi (SODA
2016) can be built in the same $O(n\sqrt{\lg n})$ time. Hence our work is the
first that achieve the same preprocessing time for optimal orthogonal range
reporting and range successor. We also apply our results to improve the
construction time of text indexes.
</summary>
    <author>
      <name>Younan Gao</name>
    </author>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/2006.11978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.11978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.10152">
    <id>http://arxiv.org/abs/2006.10152v1</id>
    <updated>2020-06-17T20:58:05Z</updated>
    <published>2020-06-17T20:58:05Z</published>
    <title>Extremal overlap-free and extremal $β$-free binary words</title>
    <summary>  An overlap-free (or $\beta$-free) word $w$ over a fixed alphabet $\Sigma$ is
extremal if every word obtained from $w$ by inserting a single letter from
$\Sigma$ at any position contains an overlap (or a factor of exponent at least
$\beta$, respectively). We find all lengths which admit an extremal
overlap-free binary word. For every extended real number $\beta$ such that
$2^+\leq\beta\leq 8/3$, we show that there are arbitrarily long extremal
$\beta$-free binary words.
</summary>
    <author>
      <name>Lucas Mol</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2006.10152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.10152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.04177">
    <id>http://arxiv.org/abs/2006.04177v1</id>
    <updated>2020-06-07T15:10:48Z</updated>
    <published>2020-06-07T15:10:48Z</published>
    <title>Sumsets of Wythoff Sequences, Fibonacci Representation, and Beyond</title>
    <summary>  Let $\alpha = (1+\sqrt{5})/2$ and define the lower and upper Wythoff
sequences by $a_i = \lfloor i \alpha \rfloor$, $b_i = \lfloor i \alpha^2
\rfloor$ for $i \geq 1$. In a recent interesting paper, Kawsumarng et al.
proved a number of results about numbers representable as sums of the form $a_i
+ a_j$, $b_i + b_j$, $a_i + b_j$, and so forth. In this paper I show how to
derive all of their results, using one simple idea and existing free software
called Walnut. The key idea is that for each of their sumsets, there is a
relatively small automaton accepting the Fibonacci representation of the
numbers represented. I also show how the automaton approach can easily prove
other results.
</summary>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2006.04177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.04177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.05104">
    <id>http://arxiv.org/abs/2006.05104v2</id>
    <updated>2020-07-16T09:44:44Z</updated>
    <published>2020-06-09T08:21:39Z</published>
    <title>Optimal-Time Queries on BWT-runs Compressed Indexes</title>
    <summary>  Although a significant number of compressed indexes for highly repetitive
strings have been proposed thus far, developing compressed indexes that support
faster queries remains a challenge. Run-length Burrows-Wheeler transform
(RLBWT) is a lossless data compression by a reversible permutation of an input
string and run-length encoding, and it has become a popular research topic in
string processing. R-index[Gagie et al., ACM'20] is an efficient compressed
index on RLBWT whose space usage depends not on string length but the number of
runs in an RLBWT, and it supports locate queries in an optimal time with
$\omega(r)$ words for the number $r$ of runs in the RLBWT of an input string.
Following this line of research, we present the first compressed index on
RLBWT, which we call \emph{r-index-f}, that supports various queries including
locate, count, extract queries, decompression and prefix search in the optimal
time with smaller working space of $O(r)$ words for small alphabets in this
paper. We present efficient data structures for computing two important
functions of LF and $\phi^{-1}$ in constant time with $O(r)$ words of space,
which is a bit step forward in computation time from the previous best result
of $O(\log \log n)$ time for string length $n$ and $O(r)$ words of space.
Finally, We present algorithms for computing queries on RLBWT by leveraging
those two data structures in optimal time with $O(r)$ words of space.
</summary>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/2006.05104v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05104v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.05871">
    <id>http://arxiv.org/abs/2006.05871v1</id>
    <updated>2020-06-10T14:55:34Z</updated>
    <published>2020-06-10T14:55:34Z</published>
    <title>Tailoring r-index for metagenomics</title>
    <summary>  A basic problem in metagenomics is to assign a sequenced read to the correct
species in the reference collection. In typical applications in genomic
epidemiology and viral metagenomics the reference collection consists of set of
species with each species represented by its highly similar strains. It has
been recently shown that accurate read assignment can be achieved with $k$-mer
hashing-based pseudoalignment: A read is assigned to species A if each of its
$k$-mer hits to reference collection is located only on strains of A. We study
the underlying primitives required in pseudoalignment and related tasks. We
propose three space-efficient solutions building upon the document listing with
frequencies problem. All the solutions use an $r$-index (Gagie et al., SODA
2018) as an underlying index structure for the text obtained as concatenation
of the set of species, as well as for each species. Given $t$ species whose
concatenation length is $n$, and whose Burrows-Wheeler transform contains $r$
runs, our first solution, based on a grammar-compressed document array with
precomputed queries at non terminal symbols, reports the frequencies for the
${\tt ndoc}$ distinct documents in which the pattern of length $m$ occurs in
${\cal O}(m + \log(n){\tt ndoc}) $ time. Our second solution is also based on a
grammar-compressed document array, but enhanced with bitvectors and reports the
frequencies in ${\cal O}(m + ((t/w)\log n + \log(n/r)){\tt ndoc})$ time, over a
machine with wordsize $w$. Our third solution, based on the interleaved LCP
array, answers the same query in ${\cal O}(m + \log(n/r){\tt ndoc})$. We
implemented our solutions and tested them on real-world and synthetic datasets.
The results show that all the solutions are fast on highly-repetitive data, and
the size overhead introduced by the indexes are comparable with the size of the
$r$-index.
</summary>
    <author>
      <name>Dustin Cobas</name>
    </author>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.05871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.03198">
    <id>http://arxiv.org/abs/2006.03198v2</id>
    <updated>2020-09-27T01:12:06Z</updated>
    <published>2020-06-05T01:55:22Z</published>
    <title>Efficient Semi-External Depth-First Search</title>
    <summary>  Computing Depth-First Search (DFS) results, i.e. depth-first order or
DFS-Tree, on the semi-external environment becomes a hot topic, because the
scales of the graphs grow rapidly which can hardly be hold in the main memory,
in the big data era. Existing semi-external DFS algorithms assume the main
memory could, at least, hold a spanning tree T of a graph G, and gradually
restructure T into a DFS-Tree, which is non-trivial. In this paper, we present
a comprehensive study of semi-external DFS problem, including the first
theoretical analysis of the main challenge of this problem, as far as we know.
Besides, we introduce a new semi-external DFS algorithm with an efficient edge
pruning principle, named EP-DFS. Unlike the traditional algorithms, we not only
focus on addressing such complex problem efficiently with less I/Os, but also
focus on that with simpler CPU calculation (Implementation-friendly) and less
random I/O access (key-to-efficiency). The former is based on our efficient
pruning principle; the latter is addressed by a lightweight index N+-index,
which is a compressed storage for a subset of the edges for G. The extensive
experimental evaluation on both synthetic and real graphs confirms that our
EP-DFS algorithm outperforms the existing techniques.
</summary>
    <author>
      <name>Xiaolong Wan</name>
    </author>
    <author>
      <name>Hongzhi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2006.03198v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.03198v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.09192">
    <id>http://arxiv.org/abs/2007.09192v1</id>
    <updated>2020-07-17T19:13:09Z</updated>
    <published>2020-07-17T19:13:09Z</published>
    <title>The Edit Distance to $k$-Subsequence Universality</title>
    <summary>  A word $u$ is a subsequence of another word $w$ if $u$ can be obtained from
$w$ by deleting some of its letters. The word $w$ with alph$(w)=\Sigma$ is
called $k$-subsequence universal if the set of subsequences of length $k$ of
$w$ contains all possible words of length $k$ over $\Sigma$. We propose a
series of efficient algorithms computing the minimal number of edit operations
(insertion, deletion, substitution) one needs to apply to a given word in order
to reach the set of $k$-subsequence universal words.
</summary>
    <author>
      <name>Pamela Fleischmann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kiel University, Computer Science Department, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Maria Kosche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Göttingen University, Computer Science Department, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Tore Koß</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Göttingen University, Computer Science Department, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Florin Manea</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Göttingen University, Computer Science Department, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Stefan Siemer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Göttingen University, Computer Science Department, Germany</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2007.09192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.08357">
    <id>http://arxiv.org/abs/2007.08357v1</id>
    <updated>2020-07-16T14:34:14Z</updated>
    <published>2020-07-16T14:34:14Z</published>
    <title>Substring Complexity in Sublinear Space</title>
    <summary>  Shannon's entropy is a definitive lower bound for statistical compression.
Unfortunately, no such clear measure exists for the compressibility of
repetitive strings. Thus, ad-hoc measures are employed to estimate the
repetitiveness of strings, e.g., the size $z$ of the Lempel-Ziv parse or the
number $r$ of equal-letter runs of the Burrows-Wheeler transform. A more recent
one is the size $\gamma$ of a smallest string attractor. Unfortunately, Kempa
and Prezza [STOC 2018] showed that computing $\gamma$ is NP-hard. Kociumaka et
al. [LATIN 2020] considered a new measure that is based on the function $S_T$
counting the cardinalities of the sets of substrings of each length of $T$,
also known as the substring complexity. This new measure is defined as $\delta=
\sup\{S_T(k)/k, k\geq 1\}$ and lower bounds all the measures previously
considered. In particular, $\delta\leq \gamma$ always holds and $\delta$ can be
computed in $\mathcal{O}(n)$ time using $\Omega(n)$ working space. Kociumaka et
al. showed that if $\delta$ is given, one can construct an $\mathcal{O}(\delta
\log \frac{n}{\delta})$-sized representation of $T$ supporting efficient direct
access and efficient pattern matching queries on $T$. Given that for highly
compressible strings, $\delta$ is significantly smaller than $n$, it is natural
to pose the following question: Can we compute $\delta$ efficiently using
sublinear working space?
  It is straightforward to show that any algorithm computing $\delta$ using
$\mathcal{O}(b)$ space requires $\Omega(n^{2-o(1)}/b)$ time through a reduction
from the element distinctness problem [Yao, SIAM J. Comput. 1994]. We present
the following results: an $\mathcal{O}(n^3/b^2)$-time and
$\mathcal{O}(b)$-space algorithm to compute $\delta$, for any $b\in[1,n]$; and
an $\tilde{\mathcal{O}}(n^2/b)$-time and $\mathcal{O}(b)$-space algorithm to
compute $\delta$, for any $b\in[n^{2/3},n]$.
</summary>
    <author>
      <name>Giulia Bernardini</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <link href="http://arxiv.org/abs/2007.08357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.08357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.08188">
    <id>http://arxiv.org/abs/2007.08188v1</id>
    <updated>2020-07-16T08:56:26Z</updated>
    <published>2020-07-16T08:56:26Z</published>
    <title>The Simplest Binary Word with Only Three Squares</title>
    <summary>  We re-examine previous constructions of infinite binary words containing few
distinct squares with the goal of finding the "simplest", in a certain sense.
We exhibit several new constructions. Rather than using tedious case-based
arguments to prove that the constructions have the desired property, we rely
instead on theorem-proving software for their correctness.
</summary>
    <author>
      <name>Daniel Gabric</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2007.08188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.08188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.07718">
    <id>http://arxiv.org/abs/2007.07718v1</id>
    <updated>2020-07-15T14:45:14Z</updated>
    <published>2020-07-15T14:45:14Z</published>
    <title>On Indexing and Compressing Finite Automata</title>
    <summary>  An index for a finite automaton is a powerful data structure that supports
locating paths labeled with a query pattern, thus solving pattern matching on
the underlying regular language. In this paper, we solve the long-standing
problem of indexing arbitrary finite automata. Our solution consists in finding
a partial co-lexicographic order of the states and proving, as in the total
order case, that states reached by a given string form one interval on the
partial order, thus enabling indexing. We provide a lower bound stating that
such an interval requires $O(p)$ words to be represented, $p$ being the order's
width (i.e. the size of its largest antichain). Indeed, we show that $p$
determines the complexity of several fundamental problems on finite automata:
(i) Letting $\sigma$ be the alphabet size, we provide an encoding for NFAs
using $\lceil\log \sigma\rceil + 2\lceil\log p\rceil + 2$ bits per transition
and a smaller encoding for DFAs using $\lceil\log \sigma\rceil + \lceil\log
p\rceil + 2$ bits per transition. This is achieved by generalizing the
Burrows-Wheeler transform to arbitrary automata. (ii) We show that indexed
pattern matching can be solved in $\tilde O(m\cdot p^2)$ query time on NFAs.
(iii) We provide a polynomial-time algorithm to index DFAs, while matching the
optimal value for $ p $. On the other hand, we prove that the problem is
NP-hard on NFAs. (iv) We show that, in the worst case, the classic powerset
construction algorithm for NFA determinization generates an equivalent DFA of
size $2^p(n-p+1)-1$, where $n$ is the number of NFA's states.
</summary>
    <author>
      <name>Nicola Cotumaccio</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/2007.07718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.07718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.06604">
    <id>http://arxiv.org/abs/2007.06604v1</id>
    <updated>2020-07-13T18:11:19Z</updated>
    <published>2020-07-13T18:11:19Z</published>
    <title>Update Query Time Trade-off for dynamic Suffix Arrays</title>
    <summary>  The Suffix Array SA(S) of a string S[1 ... n] is an array containing all the
suffixes of S sorted by lexicographic order. The suffix array is one of the
most well known indexing data structures, and it functions as a key tool in
many string algorithms. In this paper, we present a data structure for
maintaining the Suffix Array of a dynamic string. For every $0 \leq \varepsilon
\leq 1$, our data structure reports SA[i] in $\tilde{O}(n^{\varepsilon})$ time
and handles text modification in $\tilde{O}(n^{1-\varepsilon})$ time.
Additionally, our data structure enables the same query time for reporting
iSA[i], with iSA being the Inverse Suffix Array of S[1 ... n]. Our data
structure can be used to construct sub-linear dynamic variants of static
strings algorithms or data structures that are based on the Suffix Array and
the Inverse Suffix Array.
</summary>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Itai Boneh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.06604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.06604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.06167">
    <id>http://arxiv.org/abs/2007.06167v1</id>
    <updated>2020-07-13T03:20:18Z</updated>
    <published>2020-07-13T03:20:18Z</published>
    <title>Local Editing in LZ-End Compressed Data</title>
    <summary>  This paper presents an algorithm for the modification of data compressed
using LZ-End, a derivate of LZ77, without prior decompression. The performance
of the algorithm and the impact of the modifications on the compression ratio
is evaluated. Finally, we discuss the importance of this work as a first step
towards local editing in Lempel-Ziv compressed data.
</summary>
    <author>
      <name>Daniel Roodt</name>
    </author>
    <author>
      <name>Ulrich Speidel</name>
    </author>
    <author>
      <name>Vimal Kumar</name>
    </author>
    <author>
      <name>Ryan K. L. Ko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 Figure, 2 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.06167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.06167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.04128">
    <id>http://arxiv.org/abs/2007.04128v2</id>
    <updated>2020-09-29T09:18:53Z</updated>
    <published>2020-07-08T13:55:10Z</published>
    <title>String Indexing for Top-$k$ Close Consecutive Occurrences</title>
    <summary>  The classic string indexing problem is to preprocess a string $S$ into a
compact data structure that supports efficient subsequent pattern matching
queries, that is, given a pattern string $P$, report all occurrences of $P$
within $S$. In this paper, we study a basic and natural extension of string
indexing called the string indexing for top-$k$ close consecutive occurrences
problem (SITCCO). Here, a consecutive occurrence is a pair $(i,j)$, $i &lt; j$,
such that $P$ occurs at positions $i$ and $j$ in $S$ and there is no occurrence
of $P$ between $i$ and $j$, and their distance is defined as $j-i$. Given a
pattern $P$ and a parameter $k$, the goal is to report the top-$k$ consecutive
occurrences of $P$ in $S$ of minimal distance. The challenge is to compactly
represent $S$ while supporting queries in time close to length of $P$ and $k$.
We give two time-space trade-offs for the problem. Let $n$ be the length of
$S$, $m$ the length of $P$, and $\epsilon\in(0,1]$. Our first result achieves
$O(n\log n)$ space and optimal query time of $O(m+k)$, and our second result
achieves linear space and query time $O(m+k^{1+\epsilon})$. Along the way, we
develop several techniques of independent interest, including a new translation
of the problem into a line segment intersection problem and a new recursive
clustering technique for trees.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Max Rishøj Pedersen</name>
    </author>
    <author>
      <name>Eva Rotenberg</name>
    </author>
    <author>
      <name>Teresa Anna Steiner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed typos, minor changes</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.04128v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.04128v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.03040">
    <id>http://arxiv.org/abs/2007.03040v1</id>
    <updated>2020-07-06T19:58:58Z</updated>
    <published>2020-07-06T19:58:58Z</published>
    <title>Near-Linear Time Edit Distance for Indel Channels</title>
    <summary>  We consider the following model for sampling pairs of strings: $s_1$ is a
uniformly random bitstring of length $n$, and $s_2$ is the bitstring arrived at
by applying substitutions, insertions, and deletions to each bit of $s_1$ with
some probability. We show that the edit distance between $s_1$ and $s_2$ can be
computed in $O(n \ln n)$ time with high probability, as long as each bit of
$s_1$ has a mutation applied to it with probability at most a small constant.
The algorithm is simple and only uses the textbook dynamic programming
algorithm as a primitive, first computing an approximate alignment between the
two strings, and then running the dynamic programming algorithm restricted to
entries close to the approximate alignment. The analysis of our algorithm
provides theoretical justification for alignment heuristics used in practice
such as BLAST, FASTA, and MAFFT, which also start by computing approximate
alignments quickly and then find the best alignment near the approximate
alignment. Our main technical contribution is a partitioning of alignments such
that the number of the subsets in the partition is not too large and every
alignment in one subset is worse than an alignment considered by our algorithm
with high probability. Similar techniques may be of interest in the
average-case analysis of other problems commonly solved via dynamic
programming.
</summary>
    <author>
      <name>Arun Ganesh</name>
    </author>
    <author>
      <name>Aaron Sy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in WABI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.03040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.03040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.15999">
    <id>http://arxiv.org/abs/2006.15999v1</id>
    <updated>2020-06-29T12:42:39Z</updated>
    <published>2020-06-29T12:42:39Z</published>
    <title>The Number of Repetitions in 2D-Strings</title>
    <summary>  The notions of periodicity and repetitions in strings, and hence these of
runs and squares, naturally extend to two-dimensional strings. We consider two
types of repetitions in 2D-strings: 2D-runs and quartics (quartics are a
2D-version of squares in standard strings). Amir et al. introduced 2D-runs,
showed that there are $O(n^3)$ of them in an $n \times n$ 2D-string and
presented a simple construction giving a lower bound of $\Omega(n^2)$ for their
number (TCS 2020). We make a significant step towards closing the gap between
these bounds by showing that the number of 2D-runs in an $n \times n$ 2D-string
is $O(n^2 \log^2 n)$. In particular, our bound implies that the $O(n^2\log n +
\textsf{output})$ run-time of the algorithm of Amir et al. for computing
2D-runs is also $O(n^2 \log^2 n)$. We expect this result to allow for
exploiting 2D-runs algorithmically in the area of 2D pattern matching.
  A quartic is a 2D-string composed of $2 \times 2$ identical blocks
(2D-strings) that was introduced by Apostolico and Brimkov (TCS 2000), where by
quartics they meant only primitively rooted quartics, i.e. built of a primitive
block. Here our notion of quartics is more general and analogous to that of
squares in 1D-strings. Apostolico and Brimkov showed that there are $O(n^2
\log^2 n)$ occurrences of primitively rooted quartics in an $n \times n$
2D-string and that this bound is attainable. Consequently the number of
distinct primitively rooted quartics is $O(n^2 \log^2 n)$. Here, we prove that
the number of distinct general quartics is also $O(n^2 \log^2 n)$. This extends
the rich combinatorial study of the number of distinct squares in a 1D-string,
that was initiated by Fraenkel and Simpson (J. Comb. Theory A 1998), to two
dimensions.
  Finally, we show some algorithmic applications of 2D-runs. (Abstract
shortened due to arXiv requirements.)
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the ESA 2020 proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.15999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.15999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.16137">
    <id>http://arxiv.org/abs/2006.16137v1</id>
    <updated>2020-06-29T15:56:54Z</updated>
    <published>2020-06-29T15:56:54Z</published>
    <title>Pattern Masking for Dictionary Matching</title>
    <summary>  In the Pattern Masking for Dictionary Matching (PMDM) problem, we are given a
dictionary $\mathcal{D}$ of $d$ strings, each of length $\ell$, a query string
$q$ of length $\ell$, and a positive integer $z$, and we are asked to compute a
smallest set $K\subseteq\{1,\ldots,\ell\}$, so that if $q[i]$, for all $i\in
K$, is replaced by a wildcard, then $q$ matches at least $z$ strings from
$\mathcal{D}$. The PMDM problem lies at the heart of two important applications
featured in large-scale real-world systems: record linkage of databases that
contain sensitive information, and query term dropping. In both applications,
solving PMDM allows for providing data utility guarantees as opposed to
existing approaches.
  We first show, through a reduction from the well-known $k$-Clique problem,
that a decision version of the PMDM problem is NP-complete, even for strings
over a binary alphabet. We present a data structure for PMDM that answers
queries over $\mathcal{D}$ in time
$\mathcal{O}(2^{\ell/2}(2^{\ell/2}+\tau)\ell)$ and requires space
$\mathcal{O}(2^{\ell}d^2/\tau^2+2^{\ell/2}d)$, for any parameter
$\tau\in[1,d]$. We also approach the problem from a more practical perspective.
We show an $\mathcal{O}((d\ell)^{k/3}+d\ell)$-time and
$\mathcal{O}(d\ell)$-space algorithm for PMDM if $k=|K|=\mathcal{O}(1)$. We
generalize our exact algorithm to mask multiple query strings simultaneously.
We complement our results by showing a two-way polynomial-time reduction
between PMDM and the Minimum Union problem [Chlamt\'{a}\v{c} et al., SODA
2017]. This gives a polynomial-time
$\mathcal{O}(d^{1/4+\epsilon})$-approximation algorithm for PMDM, which is
tight under plausible complexity conjectures.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Huiping Chen</name>
    </author>
    <author>
      <name>Peter Christen</name>
    </author>
    <author>
      <name>Grigorios Loukides</name>
    </author>
    <author>
      <name>Nadia Pisanti</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <link href="http://arxiv.org/abs/2006.16137v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.16137v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.12097">
    <id>http://arxiv.org/abs/2007.12097v2</id>
    <updated>2020-07-24T14:07:49Z</updated>
    <published>2020-07-23T16:00:54Z</published>
    <title>A New Upper Bound for Separating Words</title>
    <summary>  We prove that for any distinct $x,y \in \{0,1\}^n$, there is a deterministic
finite automaton with $\widetilde{O}(n^{1/3})$ states that accepts $x$ but not
$y$. This improves Robson's 1989 upper bound of $\widetilde{O}(n^{2/5})$.
</summary>
    <author>
      <name>Zachary Chase</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.12097v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.12097v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.12762">
    <id>http://arxiv.org/abs/2007.12762v2</id>
    <updated>2020-11-15T03:05:29Z</updated>
    <published>2020-07-24T20:33:23Z</published>
    <title>Sublinear-Time Algorithms for Computing &amp; Embedding Gap Edit Distance</title>
    <summary>  In this paper, we design new sublinear-time algorithms for solving the gap
edit distance problem and for embedding edit distance to Hamming distance. For
the gap edit distance problem, we give an $\tilde{O}(\frac{n}{k}+k^2)$-time
greedy algorithm that distinguishes between length-$n$ input strings with edit
distance at most $k$ and those with edit distance exceeding $(3k+5)k$. This is
an improvement and a simplification upon the result of Goldenberg, Krauthgamer,
and Saha [FOCS 2019], where the $k$ vs $\Theta(k^2)$ gap edit distance problem
is solved in $\tilde{O}(\frac{n}{k}+k^3)$ time. We further generalize our
result to solve the $k$ vs $k'$ gap edit distance problem in time
$\tilde{O}(\frac{nk}{k'}+k^2+ \frac{k^2}{k'}\sqrt{nk})$, strictly improving
upon the previously known bound $\tilde{O}(\frac{nk}{k'}+k^3)$. Finally, we
show that if the input strings do not have long highly periodic substrings,
then already the $k$ vs $(1+\epsilon)k$ gap edit distance problem can be solved
in sublinear time. Specifically, if the strings contain no substring of length
$\ell$ with period at most $2k$, then the running time we achieve is
$\tilde{O}(\frac{n}{\epsilon^2 k}+k^2\ell)$.
  We further give the first sublinear-time probabilistic embedding of edit
distance to Hamming distance. For any parameter $p$, our
$\tilde{O}(\frac{n}{p})$-time procedure yields an embedding with distortion
$O(kp)$, where $k$ is the edit distance of the original strings. Specifically,
the Hamming distance of the resultant strings is between $\frac{k-p+1}{p+1}$
and $O(k^2)$ with good probability. This generalizes the linear-time embedding
of Chakraborty, Goldenberg, and Kouck\'y [STOC 2016], where the resultant
Hamming distance is between $\frac k2$ and $O(k^2)$. Our algorithm is based on
a random walk over samples, which we believe will find other applications in
sublinear-time algorithms.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Barna Saha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/FOCS46700.2020.00112</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/FOCS46700.2020.00112" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">FOCS 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.12762v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.12762v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.13356">
    <id>http://arxiv.org/abs/2007.13356v2</id>
    <updated>2020-08-15T23:56:39Z</updated>
    <published>2020-07-27T08:17:57Z</published>
    <title>Optimal construction of a layer-ordered heap</title>
    <summary>  The layer-ordered heap (LOH) is a simple, recently proposed data structure
used in optimal selection on $X+Y$, thealgorithm with the best known runtime
for selection on $X_1+X_2+\cdots+X_m$, and the fastest method in practice for
computing the most abundant isotope peaks in a chemical compound. Here, we
introduce a few algorithms for constructing LOHs, analyze their complexity, and
demonstrate that one algorithm is optimal for building a LOH of any rank
$\alpha$. These results are shown to correspond with empirical experiments of
runtimes when applying the LOH construction algorithms to a common task in
machine learning.
</summary>
    <author>
      <name>Jake Pennington</name>
    </author>
    <author>
      <name>Patrick Kreitzberg</name>
    </author>
    <author>
      <name>Kyle Lucke</name>
    </author>
    <author>
      <name>Oliver Serang</name>
    </author>
    <link href="http://arxiv.org/abs/2007.13356v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.13356v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.13241">
    <id>http://arxiv.org/abs/2007.13241v1</id>
    <updated>2020-07-26T23:18:19Z</updated>
    <published>2020-07-26T23:18:19Z</published>
    <title>Beyond the Worst-Case Analysis of Algorithms (Introduction)</title>
    <summary>  One of the primary goals of the mathematical analysis of algorithms is to
provide guidance about which algorithm is the "best" for solving a given
computational problem. Worst-case analysis summarizes the performance profile
of an algorithm by its worst performance on any input of a given size,
implicitly advocating for the algorithm with the best-possible worst-case
performance. Strong worst-case guarantees are the holy grail of algorithm
design, providing an application-agnostic certification of an algorithm's
robustly good performance. However, for many fundamental problems and
performance measures, such guarantees are impossible and a more nuanced
analysis approach is called for. This chapter surveys several alternatives to
worst-case analysis that are discussed in detail later in the book.
</summary>
    <author>
      <name>Tim Roughgarden</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Chapter 1 of the book Beyond the Worst-Case Analysis of Algorithms,
  edited by Tim Roughgarden and published by Cambridge University Press (2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.13241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.13241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.10095">
    <id>http://arxiv.org/abs/2007.10095v1</id>
    <updated>2020-07-20T13:39:57Z</updated>
    <published>2020-07-20T13:39:57Z</published>
    <title>A Big Data Approach for Sequences Indexing on the Cloud via Burrows
  Wheeler Transform</title>
    <summary>  Indexing sequence data is important in the context of Precision Medicine,
where large amounts of ``omics'' data have to be daily collected and analyzed
in order to categorize patients and identify the most effective therapies. Here
we propose an algorithm for the computation of Burrows Wheeler transform
relying on Big Data technologies, i.e., Apache Spark and Hadoop. Our approach
is the first that distributes the index computation and not only the input
dataset, allowing to fully benefit of the available cloud resources.
</summary>
    <author>
      <name>Mario Randazzo</name>
    </author>
    <author>
      <name>Simona E. Rombo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at HELPLINE@ECAI2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.10095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.10095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.09442">
    <id>http://arxiv.org/abs/2009.09442v1</id>
    <updated>2020-09-20T14:46:48Z</updated>
    <published>2020-09-20T14:46:48Z</published>
    <title>TADOC: Text Analytics Directly on Compression</title>
    <summary>  This article provides a comprehensive description of Text Analytics Directly
on Compression (TADOC), which enables direct document analytics on compressed
textual data. The article explains the concept of TADOC and the challenges to
its effective realizations. Additionally, a series of guidelines and technical
solutions that effectively address those challenges, including the adoption of
a hierarchical compression method and a set of novel algorithms and data
structure designs, are presented. Experiments on six data analytics tasks of
various complexities show that TADOC can save 90.8% storage space and 87.9%
memory usage, while halving data processing times.
</summary>
    <author>
      <name>Feng Zhang</name>
    </author>
    <author>
      <name>Jidong Zhai</name>
    </author>
    <author>
      <name>Xipeng Shen</name>
    </author>
    <author>
      <name>Dalin Wang</name>
    </author>
    <author>
      <name>Zheng Chen</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <author>
      <name>Wenguang Chen</name>
    </author>
    <author>
      <name>Xiaoyong Du</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00778-020-00636-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00778-020-00636-3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 18 figures, VLDB Journal (2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.09442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.2; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.08588">
    <id>http://arxiv.org/abs/2009.08588v1</id>
    <updated>2020-09-18T02:08:40Z</updated>
    <published>2020-09-18T02:08:40Z</published>
    <title>Longest Common Subsequence in Sublinear Space</title>
    <summary>  We present the first $\mathrm{o}(n)$-space polynomial-time algorithm for
computing the length of a longest common subsequence. Given two strings of
length $n$, the algorithm runs in $\mathrm{O}(n^{3})$ time with
$\mathrm{O}\left(\frac{n \log^{1.5} n}{2^{\sqrt{\log n}}}\right)$ bits of
space.
</summary>
    <author>
      <name>Masashi Kiyomi</name>
    </author>
    <author>
      <name>Takashi Horiyama</name>
    </author>
    <author>
      <name>Yota Otachi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.08588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.08588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.04821">
    <id>http://arxiv.org/abs/2009.04821v1</id>
    <updated>2020-09-10T12:46:07Z</updated>
    <published>2020-09-10T12:46:07Z</published>
    <title>Pushdown and Lempel-Ziv Depth</title>
    <summary>  This paper expands upon existing and introduces new formulations of Bennett's
logical depth. In previously published work by Jordon and Moser, notions of
finite-state-depth and pushdown-depth were examined and compared. These were
based on finite-state transducers and information lossless pushdown compressors
respectively. Unfortunately a full separation between the two notions was not
established. This paper introduces a new formulation of pushdown-depth based on
restricting how fast a pushdown compressor's stack can grow. This improved
formulation allows us to do a full comparison by demonstrating the existence of
sequences with high finite-state-depth and low pushdown-depth, and vice-versa.
A new notion based on the Lempel-Ziv `78 algorithm is also introduced. Its
difference from finite-state-depth is shown by demonstrating the existence of a
Lempel-Ziv deep sequence that is not finite-state deep and vice versa.
Lempel-Ziv-depth's difference from pushdown-depth is shown by building
sequences that have a pushdown-depth of roughly $1/2$ but low Lempel-Ziv depth,
and a sequence with high Lempel-Ziv depth but low pushdown-depth. Properties of
all three notions are also discussed and proved.
</summary>
    <author>
      <name>Liam Jordon</name>
    </author>
    <author>
      <name>Philippe Moser</name>
    </author>
    <link href="http://arxiv.org/abs/2009.04821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.04821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.04827">
    <id>http://arxiv.org/abs/2009.04827v1</id>
    <updated>2020-09-10T12:56:00Z</updated>
    <published>2020-09-10T12:56:00Z</published>
    <title>A Normal Sequence Compressed by PPM$^*$ but not by Lempel-Ziv 78</title>
    <summary>  In this paper we compare the difference in performance of two of the
Prediction by Partial Matching (PPM) family of compressors (PPM$^*$ and the
original Bounded PPM algorithm) and the Lempel-Ziv 78 (LZ) algorithm. We
construct an infinite binary sequence whose worst-case compression ratio for
PPM$^*$ is $0$, while Bounded PPM's and LZ's best-case compression ratios are
at least $1/2$ and $1$ respectively. This sequence is an enumeration of all
binary strings in order of length, i.e. all strings of length $1$ followed by
all strings of length $2$ and so on. It is therefore normal, and is built using
repetitions of de Bruijn strings of increasing order
</summary>
    <author>
      <name>Liam Jordon</name>
    </author>
    <author>
      <name>Philippe Moser</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-67731-2_28</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-67731-2_28" rel="related"/>
    <link href="http://arxiv.org/abs/2009.04827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.04827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.03352">
    <id>http://arxiv.org/abs/2009.03352v1</id>
    <updated>2020-09-07T18:12:58Z</updated>
    <published>2020-09-07T18:12:58Z</published>
    <title>A Fast Randomized Algorithm for Finding the Maximal Common Subsequences</title>
    <summary>  Finding the common subsequences of $L$ multiple strings has many applications
in the area of bioinformatics, computational linguistics, and information
retrieval. A well-known result states that finding a Longest Common Subsequence
(LCS) for $L$ strings is NP-hard, e.g., the computational complexity is
exponential in $L$. In this paper, we develop a randomized algorithm, referred
to as {\em Random-MCS}, for finding a random instance of Maximal Common
Subsequence ($MCS$) of multiple strings. A common subsequence is {\em maximal}
if inserting any character into the subsequence no longer yields a common
subsequence. A special case of MCS is LCS where the length is the longest. We
show the complexity of our algorithm is linear in $L$, and therefore is
suitable for large $L$. Furthermore, we study the occurrence probability for a
single instance of MCS and demonstrate via both theoretical and experimental
studies that the longest subsequence from multiple runs of {\em Random-MCS}
often yields a solution to $LCS$.
</summary>
    <author>
      <name>Jin Cao</name>
    </author>
    <author>
      <name>Dewei Zhong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.03352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.03352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.03675">
    <id>http://arxiv.org/abs/2009.03675v1</id>
    <updated>2020-09-05T19:09:41Z</updated>
    <published>2020-09-05T19:09:41Z</published>
    <title>Space efficient merging of de Bruijn graphs and Wheeler graphs</title>
    <summary>  The merging of succinct data structures is a well established technique for
the space efficient construction of large succinct indexes. In the first part
of the paper we propose a new algorithm for merging succinct representations of
de Bruijn graphs. Our algorithm has the same asymptotic cost of the state of
the art algorithm for the same problem but it uses less than half of its
working space. A novel important feature of our algorithm, not found in any of
the existing tools, is that it can compute the Variable Order succinct
representation of the union graph within the same asymptotic time/space bounds.
In the second part of the paper we consider the more general problem of merging
succinct representations of Wheeler graphs, a recently introduced graph family
which includes as special cases de Bruijn graphs and many other known succinct
indexes based on the BWT or one of its variants. We show that Wheeler graphs
merging is in general a much more difficult problem, and we provide a space
efficient algorithm for the slightly simplified problem of determining whether
the union graph has an ordering that satisfies the Wheeler conditions.
</summary>
    <author>
      <name>Lavinia Egidi</name>
    </author>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 10 figures. arXiv admin note: text overlap with
  arXiv:1902.02889</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.03675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.03675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.02934">
    <id>http://arxiv.org/abs/2009.02934v1</id>
    <updated>2020-09-07T08:09:32Z</updated>
    <published>2020-09-07T08:09:32Z</published>
    <title>On prefix palindromic length of automatic words</title>
    <summary>  The prefix palindromic length $\mathrm{PPL}_{\mathbf{u}}(n)$ of an infinite
word $\mathbf{u}$ is the minimal number of concatenated palindromes needed to
express the prefix of length $n$ of $\mathbf{u}$. Since 2013, it is still
unknown if $\mathrm{PPL}_{\mathbf{u}}(n)$ is unbounded for every aperiodic
infinite word $\mathbf{u}$, even though this has been proven for almost all
aperiodic words. At the same time, the only well-known nontrivial infinite word
for which the function $\mathrm{PPL}_{\mathbf{u}}(n)$ has been precisely
computed is the Thue-Morse word $\mathbf{t}$. This word is $2$-automatic and,
predictably, its function $\mathrm{PPL}_{\mathbf{t}}(n)$ is $2$-regular, but is
this the case for all automatic words?
  In this paper, we prove that this function is $k$-regular for every
$k$-automatic word containing only a finite number of palindromes. For two such
words, namely the paperfolding word and the Rudin-Shapiro word, we derive a
formula for this function. Our computational experiments suggest that generally
this is not true: for the period-doubling word, the prefix palindromic length
does not look $2$-regular, and for the Fibonacci word, it does not look
Fibonacci-regular. If proven, these results would give rare (if not first)
examples of a natural function of an automatic word which is not regular.
</summary>
    <author>
      <name>Anna E. Frid</name>
    </author>
    <author>
      <name>Enzo Laborde</name>
    </author>
    <author>
      <name>Jarkko Peltomäki</name>
    </author>
    <link href="http://arxiv.org/abs/2009.02934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.02934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.02233">
    <id>http://arxiv.org/abs/2009.02233v1</id>
    <updated>2020-09-04T15:05:49Z</updated>
    <published>2020-09-04T15:05:49Z</published>
    <title>Access-Adaptive Priority Search Tree</title>
    <summary>  In this paper we show that the priority search tree of McCreight, which was
originally developed to satisfy a class of spatial search queries on
2-dimensional points, can be adapted to the problem of dynamically maintaining
a set of keys so that the query complexity adapts to the distribution of
queried keys. Presently, the best-known example of such a data structure is the
splay tree, which dynamically reconfigures itself during each query so that
frequently accessed keys move to the top of the tree and thus can be retrieved
with fewer queries than keys that are lower in the tree. However, while the
splay tree is conjectured to offer optimal adaptive amortized query complexity,
it may require O(n) for individual queries. We show that an access-adaptive
priority search tree (AAPST) can provide competitive adaptive query performance
while ensuring O(log n) worst-case query performance, thus potentially making
it more suitable for certain interactive (e.g.,online and real-time)
applications for which the response time must be bounded.
</summary>
    <author>
      <name>Haley Massa</name>
    </author>
    <author>
      <name>Jeffrey Uhlmann</name>
    </author>
    <link href="http://arxiv.org/abs/2009.02233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.02233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.01353">
    <id>http://arxiv.org/abs/2009.01353v1</id>
    <updated>2020-09-02T21:29:30Z</updated>
    <published>2020-09-02T21:29:30Z</published>
    <title>Zuckerli: A New Compressed Representation for Graphs</title>
    <summary>  Zuckerli is a scalable compression system meant for large real-world graphs.
Graphs are notoriously challenging structures to store efficiently due to their
linked nature, which makes it hard to separate them into smaller, compact
components. Therefore, effective compression is crucial when dealing with large
graphs, which can have billions of nodes and edges. Furthermore, a good
compression system should give the user fast and reasonably flexible access to
parts of the compressed data without requiring full decompression, which may be
unfeasible on their system. Zuckerli improves multiple aspects of WebGraph, the
current state-of-the-art in compressing real-world graphs, by using advanced
compression techniques and novel heuristic graph algorithms. It can produce
both a compressed representation for storage and one which allows fast direct
access to the adjacency lists of the compressed graph without decompressing the
entire graph. We validate the effectiveness of Zuckerli on real-world graphs
with up to a billion nodes and 90 billion edges, conducting an extensive
experimental evaluation of both compression density and decompression
performance. We show that Zuckerli-compressed graphs are 10% to 29% smaller,
and more than 20% in most cases, with a resource usage for decompression
comparable to that of WebGraph.
</summary>
    <author>
      <name>Luca Versari</name>
    </author>
    <author>
      <name>Iulia M. Comsa</name>
    </author>
    <author>
      <name>Alessio Conte</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <link href="http://arxiv.org/abs/2009.01353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.01353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2008.13209">
    <id>http://arxiv.org/abs/2008.13209v2</id>
    <updated>2020-11-26T06:01:46Z</updated>
    <published>2020-08-30T16:23:28Z</published>
    <title>Tight Bound for the Number of Distinct Palindromes in a Tree</title>
    <summary>  For an undirected tree with $n$ edges labelled by single letters, we consider
its substrings, which are labels of the simple paths between pairs of nodes. We
prove that there are $O(n^{1.5})$ different palindromic substrings. This solves
an open problem of Brlek, Lafreni\`ere, and Proven\c{c}al (DLT 2015), who gave
a matching lower-bound construction. Hence, we settle the tight bound of
$\Theta(n^{1.5})$ for the maximum palindromic complexity of trees. For standard
strings, i.e., for paths, the palindromic complexity is $n+1$. We also propose
$O(n^{1.5} \log{n})$-time algorithm for reporting all distinct palindromes in
an undirected tree with $n$ edges.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <link href="http://arxiv.org/abs/2008.13209v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.13209v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.08840">
    <id>http://arxiv.org/abs/2010.08840v1</id>
    <updated>2020-10-17T18:24:08Z</updated>
    <published>2020-10-17T18:24:08Z</published>
    <title>Lazy Search Trees</title>
    <summary>  We introduce the lazy search tree data structure. The lazy search tree is a
comparison-based data structure on the pointer machine that supports
order-based operations such as rank, select, membership, predecessor,
successor, minimum, and maximum while providing dynamic operations insert,
delete, change-key, split, and merge. We analyze the performance of our data
structure based on a partition of current elements into a set of gaps
$\{\Delta_i\}$ based on rank. A query falls into a particular gap and splits
the gap into two new gaps at a rank $r$ associated with the query operation. If
we define $B = \sum_i |\Delta_i| \log_2(n/|\Delta_i|)$, our performance over a
sequence of $n$ insertions and $q$ distinct queries is $O(B + \min(n \log \log
n, n \log q))$. We show $B$ is a lower bound.
  Effectively, we reduce the insertion time of binary search trees from
$\Theta(\log n)$ to $O(\min(\log(n/|\Delta_i|) + \log \log |\Delta_i|, \; \log
q))$, where $\Delta_i$ is the gap in which the inserted element falls. Over a
sequence of $n$ insertions and $q$ queries, a time bound of $O(n \log q + q
\log n)$ holds; better bounds are possible when queries are non-uniformly
distributed. As an extreme case of non-uniformity, if all queries are for the
minimum element, the lazy search tree performs as a priority queue with $O(\log
\log n)$ time insert and decrease-key operations. The same data structure
supports queries for any rank, interpolating between binary search trees and
efficient priority queues.
  Lazy search trees can be implemented to operate mostly on arrays, requiring
only $O(\min(q, n))$ pointers. Via direct reduction, our data structure also
supports the efficient access theorems of the splay tree, providing a powerful
data structure for non-uniform element access, both when the number of accesses
is small and large.
</summary>
    <author>
      <name>Bryce Sandlund</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in FOCS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.08840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.09014">
    <id>http://arxiv.org/abs/2010.09014v1</id>
    <updated>2020-10-18T16:16:18Z</updated>
    <published>2020-10-18T16:16:18Z</published>
    <title>Solving Shisen-Sho boards</title>
    <summary>  We give a simple proof of that determining solvability of Shisen-Sho boards
is NP-complete. Furthermore, we show that under realistic assumptions, one can
compute in logarithmic time if two tiles form a playable pair.
  We combine an implementation of the algoritm to test playability of pairs
with my earlier algorithm to solve Mahjong Solitaire boards with peeking, to
obtain an algorithm to solve Shisen-Sho boards. We sample several Shisen-Sho
and Mahjong Solitaire layouts for solvability for Shisen-Sho and Mahjong
Solitaire.
</summary>
    <author>
      <name>Michiel de Bondt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.09014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.07960">
    <id>http://arxiv.org/abs/2010.07960v1</id>
    <updated>2020-10-15T18:05:52Z</updated>
    <published>2020-10-15T18:05:52Z</published>
    <title>Efficient constructions of the Prefer-same and Prefer-opposite de Bruijn
  sequences</title>
    <summary>  The greedy Prefer-same de Bruijn sequence construction was first presented by
Eldert et al.[AIEE Transactions 77 (1958)]. As a greedy algorithm, it has one
major downside: it requires an exponential amount of space to store the length
$2^n$ de Bruijn sequence. Though de Bruijn sequences have been heavily studied
over the last 60 years, finding an efficient construction for the Prefer-same
de Bruijn sequence has remained a tantalizing open problem. In this paper, we
unveil the underlying structure of the Prefer-same de Bruijn sequence and solve
the open problem by presenting an efficient algorithm to construct it using
$O(n)$ time per bit and only $O(n)$ space. Following a similar approach, we
also present an efficient algorithm to construct the Prefer-opposite de Bruijn
sequence.
</summary>
    <author>
      <name>Evan Sala</name>
    </author>
    <author>
      <name>Joe Sawada</name>
    </author>
    <author>
      <name>Abbas Alhakim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.07960v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07960v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.07076">
    <id>http://arxiv.org/abs/2010.07076v1</id>
    <updated>2020-10-14T13:25:51Z</updated>
    <published>2020-10-14T13:25:51Z</published>
    <title>Contextual Pattern Matching</title>
    <summary>  The research on indexing repetitive string collections has focused on the
same search problems used for regular string collections, though they can make
little sense in this scenario. For example, the basic pattern matching query
"list all the positions where pattern $P$ appears" can produce huge outputs
when $P$ appears in an area shared by many documents. All those occurrences are
essentially the same.
  In this paper we propose a new query that can be more appropriate in these
collections, which we call {\em contextual pattern matching}. The basic query
of this type gives, in addition to $P$, a context length $\ell$, and asks to
report the occurrences of all {\em distinct} strings $XPY$, with
$|X|=|Y|=\ell$.
  While this query is easily solved in optimal time and linear space, we focus
on using space related to the repetitiveness of the text collection and present
the first solution of this kind. Letting $\ovr$ be the maximum of the number of
runs in the BWT of the text $T[1..n]$ and of its reverse, our structure uses
$O(\ovr\log(n/\ovr))$ space and finds the $c$ contextual occurrences $XPY$ of
$(P,\ell)$ in time $O(|P| + c \log n)$. We give other space/time tradeoffs as
well, for compressed and uncompressed indexes.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Improvements and corrections over my SPIRE 2020 paper with the same
  title</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.07076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.05805">
    <id>http://arxiv.org/abs/2010.05805v3</id>
    <updated>2021-02-11T14:54:45Z</updated>
    <published>2020-10-12T16:01:42Z</published>
    <title>New Sublinear Algorithms and Lower Bounds for LIS Estimation</title>
    <summary>  Estimating the length of the longest increasing subsequence (LIS) in an array
is a problem of fundamental importance. Despite the significance of the LIS
estimation problem and the amount of attention it has received, there are
important aspects of the problem that are not yet fully understood. There are
no better lower bounds for LIS estimation than the obvious bounds implied by
testing monotonicity (for adaptive or nonadaptive algorithms). In this paper,
we give the first nontrivial lower bound on the complexity of LIS estimation,
and also provide novel algorithms that complement our lower bound.
  Specifically, for every constant $\epsilon \in (0,1)$, every nonadaptive
algorithm that outputs an estimate of the length of the LIS in an array of
length $n$ to within an additive error of $\epsilon \cdot n$ has to make
$\log^{\Omega(\log (1/\epsilon))} n)$ queries. Next, we design nonadaptive LIS
estimation algorithms whose complexity decreases as the the number of distinct
values, $r$, in the array decreases. We first present a simple algorithm that
makes $\tilde{O}(r/\epsilon^3)$ queries and approximates the LIS length with an
additive error bounded by $\epsilon n$. We then use it to construct a
nonadaptive algorithm with query complexity $\tilde{O}(\sqrt{r} \cdot
\text{poly}(1/\lambda))$ that, for an array with LIS length at least $\lambda
n$, outputs a multiplicative $\Omega(\lambda)$-approximation to the LIS length.
  Finally, we describe a nonadaptive erasure-resilient tester for sortedness,
with query complexity $O(\log n)$. Our result implies that nonadaptive tolerant
testing is strictly harder than nonadaptive erasure-resilient testing for the
natural property of monotonicity.
</summary>
    <author>
      <name>Ilan Newman</name>
    </author>
    <author>
      <name>Nithin Varma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.05805v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.05805v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.05005">
    <id>http://arxiv.org/abs/2010.05005v2</id>
    <updated>2020-11-13T07:10:25Z</updated>
    <published>2020-10-10T14:01:21Z</published>
    <title>Decode efficient prefix codes</title>
    <summary>  Data compression is used in a wide variety of tasks, including compression of
databases, large learning models, videos, images, etc. The cost of
decompressing (decoding) data can be prohibitive for certain real-time
applications. In many scenarios, it is acceptable to sacrifice (to some extent)
on compression in the interest of fast decoding. In this work, we introduce and
study a novel problem of finding a prefix tree having the best decode time
under the constraint that the code length does not exceed a certain threshold
for a natural class of memory access cost functions that use blocking (also
referred to as lookup tables), i.e., these decoding schemes access multiple
prefix tree entries in a single access, using associative memory table
look-ups. We present (i) an exact algorithm for this problem that is polynomial
in the number of characters and the codelength; (ii) a strongly polynomial
pseudo approximation algorithm that achieves the best decode time by relaxing
the codelength constraint by a small factor; and (iii) a more efficient version
of the pseudo approximation algorithm that achieves near optimal decode time by
relaxing the codelength constraint by a small factor. All our algorithms are
based on dynamic programming and capitalize on an interesting structure of the
optimal solution. To the best of our knowledge, there is no prior work that
gives any provable theoretical guarantees for minimizing decode time along with
the code length. We also demonstrate the performance benefits of our algorithm
on different types of real-world data sets, namely (i) a deep learning model
(Mobilenet-V2); (ii) image and (iii) text data. We also implement and evaluate
the performance of our algorithms on the GPU.
</summary>
    <author>
      <name>Shashwat Banchhor</name>
    </author>
    <author>
      <name>Rishikesh Gajjala</name>
    </author>
    <author>
      <name>Yogish Sabharwal</name>
    </author>
    <author>
      <name>Sandeep Sen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.05005v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.05005v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.04752">
    <id>http://arxiv.org/abs/2010.04752v1</id>
    <updated>2020-10-09T18:20:14Z</updated>
    <published>2020-10-09T18:20:14Z</published>
    <title>A Tale of Two Trees: New Analysis for AVL Tree and Binary Heap</title>
    <summary>  In this paper, we provide new insights and analysis for the two elementary
tree-based data structures - the AVL tree and binary heap. We presented two
simple properties that gives a more direct way of relating the size of an AVL
tree and the Fibonacci recurrence to establish the AVL tree's logarithmic
height. We then give a potential function-based analysis of the bottom-up heap
construction to get a simpler and tight bound for its worst-case running-time.
</summary>
    <author>
      <name>Russel L. Villacarlos</name>
    </author>
    <author>
      <name>Jaime M. Samaniego</name>
    </author>
    <author>
      <name>Arian J. Jacildo</name>
    </author>
    <author>
      <name>Maria Art Antonette D. Clariño</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.04752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.04752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.11789">
    <id>http://arxiv.org/abs/2009.11789v1</id>
    <updated>2020-09-24T16:33:22Z</updated>
    <published>2020-09-24T16:33:22Z</published>
    <title>A Case for Partitioned Bloom Filters</title>
    <summary>  In a partitioned Bloom Filter the $m$ bit vector is split into $k$ disjoint
$m/k$ sized parts, one per hash function. Contrary to hardware designs, where
they prevail, software implementations mostly adopt standard Bloom filters,
considering partitioned filters slightly worse, due to the slightly larger
false positive rate (FPR). In this paper, by performing an in-depth analysis,
first we show that the FPR advantage of standard Bloom filters is smaller than
thought; more importantly, by studying the per-element FPR, we show that
standard Bloom filters have weak spots in the domain: elements which will be
tested as false positives much more frequently than expected. This is relevant
in scenarios where an element is tested against many filters, e.g., in packet
forwarding. Moreover, standard Bloom filters are prone to exhibit extremely
weak spots if naive double hashing is used, something occurring in several,
even mainstream, libraries. Partitioned Bloom filters exhibit a uniform
distribution of the FPR over the domain and are robust to the naive use of
double hashing, having no weak spots. Finally, by surveying several usages
other than testing set membership, we point out the many advantages of having
disjoint parts: they can be individually sampled, extracted, added or retired,
leading to superior designs for, e.g., SIMD usage, size reduction, test of set
disjointness, or duplicate detection in streams. Partitioned Bloom filters are
better, and should replace the standard form, both in general purpose libraries
and as the base for novel designs.
</summary>
    <author>
      <name>Paulo Sérgio Almeida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.11789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.11559">
    <id>http://arxiv.org/abs/2009.11559v1</id>
    <updated>2020-09-24T09:13:17Z</updated>
    <published>2020-09-24T09:13:17Z</published>
    <title>Dynamic Similarity Search on Integer Sketches</title>
    <summary>  Similarity-preserving hashing is a core technique for fast similarity
searches, and it randomly maps data points in a metric space to strings of
discrete symbols (i.e., sketches) in the Hamming space. While traditional
hashing techniques produce binary sketches, recent ones produce integer
sketches for preserving various similarity measures. However, most similarity
search methods are designed for binary sketches and inefficient for integer
sketches. Moreover, most methods are either inapplicable or inefficient for
dynamic datasets, although modern real-world datasets are updated over time. We
propose dynamic filter trie (DyFT), a dynamic similarity search method for both
binary and integer sketches. An extensive experimental analysis using large
real-world datasets shows that DyFT performs superiorly with respect to
scalability, time performance, and memory efficiency. For example, on a huge
dataset of 216 million data points, DyFT performs a similarity search 6,000
times faster than a state-of-the-art method while reducing to one-thirteenth in
memory.
</summary>
    <author>
      <name>Shunsuke Kanda</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE ICDM 2020 as a full paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.11559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.10045">
    <id>http://arxiv.org/abs/2009.10045v1</id>
    <updated>2020-09-21T17:36:38Z</updated>
    <published>2020-09-21T17:36:38Z</published>
    <title>Space/time-efficient RDF stores based on circular suffix sorting</title>
    <summary>  In recent years, RDF has gained popularity as a format for the standardized
publication and exchange of information in the Web of Data. In this paper we
introduce RDFCSA, a data structure that is able to self-index an RDF dataset in
small space and supports efficient querying. RDFCSA regards the triples of the
RDF store as short circular strings and applies suffix sorting on those
strings, so that triple-pattern queries reduce to prefix searching on the
string set. The RDF store is then represented compactly using a Compressed
Suffix Array (CSA), a proved technology in text indexing that efficiently
supports prefix searches. Our experimental evaluation shows that RDFCSA is able
to answer triple-pattern queries in a few microseconds per result while using
less than 60% of the space required by the raw original data. We also support
join queries, which provide the basis for full SPARQL query support. Even
though smaller-space solutions exist, as well as faster ones, RDFCSA is shown
to provide an excellent space/time tradeoff, with fast and consistent query
times within much less space than alternatives that compete in time.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Ana Cerdeira-Pena</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Antonio Fariña</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE TKDE for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.10045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.10045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.10870">
    <id>http://arxiv.org/abs/2011.10870v1</id>
    <updated>2020-11-21T21:32:05Z</updated>
    <published>2020-11-21T21:32:05Z</published>
    <title>Erdös-Szekeres Partitioning Problem</title>
    <summary>  In this note, we present a substantial improvement on the computational
complexity of the Erd\"{o}s-Szekeres partitioning problem and review recent
works on dynamic \textsf{LIS}.
</summary>
    <author>
      <name>Michael Mitzenmacher</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <link href="http://arxiv.org/abs/2011.10870v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10870v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.11772">
    <id>http://arxiv.org/abs/2011.11772v1</id>
    <updated>2020-11-23T22:30:32Z</updated>
    <published>2020-11-23T22:30:32Z</published>
    <title>Selectable Heaps and Optimal Lazy Search Trees</title>
    <summary>  We show the $O(\log n)$ time extract minimum function of efficient priority
queues can be generalized to the extraction of the $k$ smallest elements in
$O(k \log(n/k))$ time, where we define $\log(x)$ as $\max(\log_2(x), 1)$. We
first show heap-ordered tree selection (Kaplan et al., SOSA '19) can be applied
on the heap-ordered trees of the classic Fibonacci heap to support the
extraction in $O(k \log(n/k))$ amortized time. We then show selection is
possible in a priority queue with optimal worst-case guarantees by applying
heap-ordered tree selection on Brodal queues (SODA '96), supporting the
operation in $O(k \log(n/k))$ worst-case time. Via a reduction from the
multiple selection problem, $\Omega(k \log(n/k))$ time is necessary if
insertion is supported in $o(\log n)$ time.
  We then apply the result to lazy search trees (Sandlund &amp; Wild, FOCS '20),
creating a new interval data structure based on selectable heaps. This gives
optimal $O(B+n)$ time lazy search tree performance, lowering insertion
complexity into a gap $\Delta_i$ to $O(\log(n/|\Delta_i|))$ time. An $O(1)$
time merge operation is also made possible when used as a priority queue, among
other situations. If Brodal queues are used, runtimes of the lazy search tree
can be made worst-case in the general case of two-sided gaps. The presented
data structure makes fundamental use of soft heaps (Chazelle, J. ACM '00),
biased search trees, and efficient priority queues, approaching the
theoretically-best data structure for ordered data.
</summary>
    <author>
      <name>Bryce Sandlund</name>
    </author>
    <author>
      <name>Lingyi Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2011.11772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.11772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.10874">
    <id>http://arxiv.org/abs/2011.10874v2</id>
    <updated>2021-03-09T20:37:39Z</updated>
    <published>2020-11-21T21:39:37Z</published>
    <title>Improved Dynamic Algorithms for Longest Increasing Subsequence</title>
    <summary>  We study dynamic algorithms for the longest increasing subsequence
(\textsf{LIS}) problem. A dynamic \textsf{LIS} algorithm maintains a sequence
subject to operations of the following form arriving one by one: (i) insert an
element, (ii) delete an element, or (iii) substitute an element for another.
After performing each operation, the algorithm must report the length of the
longest increasing subsequence of the current sequence.
  Our main contribution is the first exact dynamic \textsf{LIS} algorithm with
sublinear update time. More precisely, we present a randomized algorithm that
performs each operation in time $\tilde O(n^{2/3})$ and after each update,
reports the answer to the \textsf{LIS} problem correctly with high probability.
We use several novel techniques and observations for this algorithm that may
find their applications in future work.
  In the second part of the paper, we study approximate dynamic \textsf{LIS}
algorithms, which are allowed to underestimate the solution size within a
bounded multiplicative factor. In this setting, we give a deterministic
algorithm with update time $O(n^{o(1)})$ and approximation factor $1-o(1)$.
This result substantially improves upon the previous work of Mitzenmacher and
Seddighin (STOC'20) that presents an $\Omega(\epsilon
^{O(1/\epsilon)})$-approximation algorithm with update time $\tilde
O(n^\epsilon)$ for any constant $\epsilon > 0$.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <link href="http://arxiv.org/abs/2011.10874v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10874v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.09761">
    <id>http://arxiv.org/abs/2011.09761v2</id>
    <updated>2020-12-03T12:13:19Z</updated>
    <published>2020-11-19T10:39:58Z</published>
    <title>Fully Dynamic Approximation of LIS in Polylogarithmic Time</title>
    <summary>  We revisit the problem of maintaining the longest increasing subsequence
(LIS) of an array under (i) inserting an element, and (ii) deleting an element
of an array. In a recent breakthrough, Mitzenmacher and Seddighin [STOC 2020]
designed an algorithm that maintains an
$\mathcal{O}((1/\epsilon)^{\mathcal{O}(1/\epsilon)})$-approximation of LIS
under both operations with worst-case update time $\mathcal{\tilde
O}(n^{\epsilon})$, for any constant $\epsilon>0$. We exponentially improve on
their result by designing an algorithm that maintains an
$(1+\epsilon)$-approximation of LIS under both operations with worst-case
update time $\mathcal{\tilde O}(\epsilon^{-5})$. Instead of working with the
grid packing technique introduced by Mitzenmacher and Seddighin, we take a
different approach building on a new tool that might be of independent
interest: LIS sparsification.
  A particularly interesting consequence of our result is an improved solution
for the so-called Erd\H{o}s-Szekeres partitioning, in which we seek a partition
of a given permutation of $\{1,2,\ldots,n\}$ into $\mathcal{O}(\sqrt{n})$
monotone subsequences. This problem has been repeatedly stated as one of the
natural examples in which we see a large gap between the decision-tree
complexity and algorithmic complexity. The result of Mitzenmacher and Seddighin
implies an $\mathcal{O}(n^{1+\epsilon})$ time solution for this problem, for
any $\epsilon>0$. Our algorithm (in fact, its simpler decremental version)
further improves this to $\mathcal{\tilde O}(n)$.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Wojciech Janczewski</name>
    </author>
    <link href="http://arxiv.org/abs/2011.09761v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.09761v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.10008">
    <id>http://arxiv.org/abs/2011.10008v2</id>
    <updated>2020-12-13T10:01:03Z</updated>
    <published>2020-11-19T18:07:53Z</published>
    <title>Subpath Queries on Compressed Graphs: a Survey</title>
    <summary>  Text indexing is a classical algorithmic problem that has been studied for
over four decades: given a text $T$, pre-process it off-line so that, later, we
can quickly count and locate the occurrences of any string (the query pattern)
in $T$ in time proportional to the query's length. The earliest optimal-time
solution to the problem, the suffix tree, dates back to 1973 and requires up to
two orders of magnitude more space than the plain text just to be stored. In
the year 2000, two breakthrough works showed that efficient queries can be
achieved without this space overhead: a fast index be stored in a space
proportional to the text's entropy. These contributions had an enormous impact
in bioinformatics: nowadays, virtually any DNA aligner employs compressed
indexes. Recent trends considered more powerful compression schemes (dictionary
compressors) and generalizations of the problem to labeled graphs: after all,
texts can be viewed as labeled directed paths. In turn, since finite state
automata can be considered as a particular case of labeled graphs, these
findings created a bridge between the fields of compressed indexing and regular
language theory, ultimately allowing to index regular languages and promising
to shed new light on problems such as regular expression matching. This survey
is a gentle introduction to the main landmarks of the fascinating journey that
took us from suffix trees to today's compressed indexes for labeled graphs and
regular languages.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed some typos and references to Boyer-Moore-Galil's and
  Apostolico-Giancarlo's algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.10008v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10008v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.08119">
    <id>http://arxiv.org/abs/2011.08119v1</id>
    <updated>2020-11-16T17:31:51Z</updated>
    <published>2020-11-16T17:31:51Z</published>
    <title>The Longest Run Subsequence Problem: Further Complexity Results</title>
    <summary>  Longest Run Subsequence is a problem introduced recently in the context of
the scaffolding phase of genome assembly (Schrinner et al.,WABI 2020). The
problem asks for a maximum length subsequence of a given string that contains
at most one run for each symbol (a run is a maximum substring of consecutive
identical symbols). The problem has been shown to be NP-hard and to be
fixed-parameter tractable when the parameter is the size of the alphabet on
which the input string is defined. In this paper we further investigate the
complexity of the problem and we show that it is fixed-parameter tractable when
it is parameterized by the number of runs in a solution, a smaller parameter.
Moreover, we investigate the kernelization complexity of Longest Run
Subsequence and we prove that it does not admit a polynomial kernel when
parameterized by the size of the alphabet or by the number of runs. Finally, we
consider the restriction of Longest Run Subsequence when each symbol has at
most two occurrences in the input string and we show that it is APX-hard.
</summary>
    <author>
      <name>Riccardo Dondi</name>
    </author>
    <author>
      <name>Florian Sikora</name>
    </author>
    <link href="http://arxiv.org/abs/2011.08119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.08119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.07999">
    <id>http://arxiv.org/abs/2011.07999v1</id>
    <updated>2020-11-13T03:16:02Z</updated>
    <published>2020-11-13T03:16:02Z</published>
    <title>A grammar compressor for collections of reads with applications to the
  construction of the BWT</title>
    <summary>  We describe a grammar for DNA sequencing reads from which we can compute the
BWT directly. Our motivation is to perform in succinct space genomic analyses
that require complex string queries not yet supported by repetition-based
self-indexes. Our approach is to store the set of reads as a grammar, but when
required, compute its BWT to carry out the analysis by using self-indexes. Our
experiments in real data showed that the space reduction we achieve with our
compressor is competitive with LZ-based methods and better than entropy-based
approaches. Compared to other popular grammars, in this kind of data, we
achieve, on average, 12\% of extra compression and require less working space
and time.
</summary>
    <author>
      <name>Diego Díaz-Domínguez</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/2011.07999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.07143">
    <id>http://arxiv.org/abs/2011.07143v1</id>
    <updated>2020-11-13T21:57:18Z</updated>
    <published>2020-11-13T21:57:18Z</published>
    <title>Substring Query Complexity of String Reconstruction</title>
    <summary>  Suppose an oracle knows a string $S$ that is unknown to us and we want to
determine. The oracle can answer queries of the form "Is $s$ a substring of
$S$?". The \emph{Substring Query Complexity} of a string $S$, denoted
$\chi(S)$, is the minimum number of adaptive substring queries that are needed
to exactly reconstruct (or learn) $S$. It has been introduced in 1995 by Skiena
and Sundaram, who showed that $\chi(S) \geq \sigma n/4 -O(n)$ in the worst
case, where $\sigma$ is the size of the alphabet of $S$ and $n$ its length, and
gave an algorithm that spends $(\sigma-1)n+O(\sigma \sqrt{n})$ queries to
reconstruct $S$. We show that for any binary string $S$, $\chi(S)$ is
asymptotically equal to the Kolmogorov complexity of $S$ and therefore lower
bounds any other measure of compressibility. However, since this result does
not yield an efficient algorithm for the reconstruction, we present new
algorithms to compute a set of substring queries whose size grows as a function
of other known measures of complexity, e.g., the number {\sf rle} of runs in
$S$, the size $g$ of the smallest grammar producing (only) $S$ or the size
$z_{no}$ of the non-overlapping LZ77 factorization of $S$. We first show that
any string of length $n$ over an integer alphabet of size $\sigma$ with {\sf
rle} runs can be reconstructed with $q=O({\sf rle} (\sigma + \log \frac{n}{{\sf
rle}}))$ substring queries in linear time and space. We then present an
algorithm that spends $q \in O(\sigma g\log n) \subseteq O(\sigma z_{no}\log
(n/z_{no})\log n)$ substring queries and runs in $O(n(\log n + \log \sigma)+
q)$ time using linear space. This algorithm actually reconstructs the suffix
tree of the string using a dynamic approach based on the centroid
decomposition.
</summary>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <link href="http://arxiv.org/abs/2011.07143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.05610">
    <id>http://arxiv.org/abs/2011.05610v2</id>
    <updated>2021-02-11T06:56:52Z</updated>
    <published>2020-11-11T07:50:10Z</published>
    <title>PHONI: Streamed Matching Statistics with Multi-Genome References</title>
    <summary>  Computing the matching statistics of patterns with respect to a text is a
fundamental task in bioinformatics, but a formidable one when the text is a
highly compressed genomic database. Bannai et al. gave an efficient solution
for this case, which Rossi et al. recently implemented, but it uses two passes
over the patterns and buffers a pointer for each character during the first
pass. In this paper, we simplify their solution and make it streaming, at the
cost of slowing it down slightly. This means that, first, we can compute the
matching statistics of several long patterns (such as whole human chromosomes)
in parallel while still using a reasonable amount of RAM; second, we can
compute matching statistics online with low latency and thus quickly recognize
when a pattern becomes incompressible relative to the database.
</summary>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Ben Langmead</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Alejandro Pacheco</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Our code is available at https://github.com/koeppl/phoni</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.05610v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.05610v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.04010">
    <id>http://arxiv.org/abs/2011.04010v1</id>
    <updated>2020-11-08T16:09:20Z</updated>
    <published>2020-11-08T16:09:20Z</published>
    <title>Scout Algorithm For Fast Substring Matching</title>
    <summary>  Exact substring matching is a common task in many software applications.
Despite the existence of several algorithms for finding whether or not a
pattern string is present in a target string, the most common implementation is
a na\"ive, brute force approach. Alternative approaches either do not provide
enough of a benefit for the added complexity, or are impractical for modern
character sets, e.g., Unicode. We present a new algorithm, Scout, that is
straightforward, quick and appropriate for all applications. We also compare
the performance characteristics of the Scout algorithm with several others.
</summary>
    <author>
      <name>Anand Natrajan</name>
    </author>
    <author>
      <name>Mallige Anand</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.04010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.04010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.08589">
    <id>http://arxiv.org/abs/2012.08589v1</id>
    <updated>2020-12-15T20:00:01Z</updated>
    <published>2020-12-15T20:00:01Z</published>
    <title>Sorting Lists with Equal Keys Using Mergesort in Linear Time</title>
    <summary>  This article introduces a new optimization method to improve mergesort's
runtime complexity, when sorting sequences that have equal keys to $O(n log_2
k)$, where $k$ is the number of distinct keys in the sequence. When $k$ is
constant, it is evident that mergesort is capable of achieving linear time by
utilizing linked lists as its underlying data structure. Mergesort linked list
implementations can be optimized by introducing a new mechanism to group
elements with equal keys together, thus allowing merge algorithm to achieve
linear time.
</summary>
    <author>
      <name>Albert Tedja</name>
    </author>
    <link href="http://arxiv.org/abs/2012.08589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.08878">
    <id>http://arxiv.org/abs/2012.08878v1</id>
    <updated>2020-12-16T11:41:45Z</updated>
    <published>2020-12-16T11:41:45Z</published>
    <title>Greedy-reduction from Shortest Linear Superstring to Shortest Circular
  Superstring</title>
    <summary>  A superstring of a set of strings correspond to a string which contains all
the other strings as substrings. The problem of finding the Shortest Linear
Superstring is a well-know and well-studied problem in stringology. We present
here a variant of this problem, the Shortest Circular Superstring problem where
the sought superstring is a circular string. We show a strong link between
these two problems and prove that the Shortest Circular Superstring problem is
NP-complete. Moreover, we propose a new conjecture on the approximation ratio
of the Shortest Circular Superstring problem.
</summary>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Eric Rivals</name>
    </author>
    <link href="http://arxiv.org/abs/2012.08878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.10092">
    <id>http://arxiv.org/abs/2012.10092v2</id>
    <updated>2021-02-04T03:26:22Z</updated>
    <published>2020-12-18T07:53:48Z</published>
    <title>The Parameterized Suffix Tray</title>
    <summary>  Let $\Sigma$ and $\Pi$ be disjoint alphabets, respectively called the static
alphabet and the parameterized alphabet. Two strings $x$ and $y$ over $\Sigma
\cup \Pi$ of equal length are said to parameterized match (p-match) if there
exists a renaming bijection $f$ on $\Sigma$ and $\Pi$ which is identity on
$\Sigma$ and maps the characters of $x$ to those of $y$ so that the two strings
become identical. The indexing version of the problem of finding p-matching
occurrences of a given pattern in the text is a well-studied topic in string
matching. In this paper, we present a state-of-the-art indexing structure for
p-matching called the parameterized suffix tray of an input text $T$, denoted
by $\mathsf{PSTray}(T)$. We show that $\mathsf{PSTray}(T)$ occupies $O(n)$
space and supports pattern matching queries in $O(m + \log (\sigma+\pi) +
\mathit{occ})$ time, where $n$ is the length of $T$, $m$ is the length of a
query pattern $P$, $\pi$ is the number of distinct symbols of $|\Pi|$ in $T$,
$\sigma$ is the number of distinct symbols of $|\Sigma|$ in $T$ and
$\mathit{occ}$ is the number of p-matching occurrences of $P$ in $T$. We also
present how to build $\mathsf{PSTray}(T)$ in $O(n)$ time from the parameterized
suffix tree of $T$.
</summary>
    <author>
      <name>Noriki Fujisato</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for CIAC 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.10092v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.10092v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.09376">
    <id>http://arxiv.org/abs/2012.09376v2</id>
    <updated>2020-12-25T12:45:42Z</updated>
    <published>2020-12-17T03:13:45Z</published>
    <title>Quantum Algorithm for Lexicographically Minimal String Rotation</title>
    <summary>  Lexicographically minimal string rotation (LMSR) is a problem to find the
minimal one among all rotations of a string in the lexicographical order, which
is widely used in equality checking of graphs, polygons, automata and chemical
structures.
  In this paper, we propose an $O(n^{3/4})$ quantum query algorithm for LMSR.
In particular, the algorithm has average-case query complexity $O(\sqrt n \log
n)$, which is shown to be asymptotically optimal up to a polylogarithmic
factor, compared with its $\Omega\left(\sqrt{n/\log n}\right)$ lower bound.
Furthermore, we claim that our quantum algorithm outperforms any (classical)
randomized algorithms in both worst-case and average-case query complexities by
showing that every (classical) randomized algorithm for LMSR has worst-case
query complexity $\Omega(n)$ and average-case query complexity $\Omega(n/\log
n)$.
  Our quantum algorithm for LMSR is developed in a framework of nested quantum
algorithms, based on two new results: (i) an $O(\sqrt{n})$ (optimal) quantum
minimum finding on bounded-error quantum oracles; and (ii) its $O\left(\sqrt{n
\log(1/\varepsilon)}\right)$ (optimal) error reduction. As a byproduct, we
obtain some better upper bounds of independent interest: (i) $O(\sqrt{N})$
(optimal) for constant-depth MIN-MAX trees on $N$ variables; and (ii)
$O(\sqrt{n \log m})$ for pattern matching which removes
$\operatorname{polylog}(n)$ factors.
</summary>
    <author>
      <name>Qisheng Wang</name>
    </author>
    <author>
      <name>Mingsheng Ying</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 6 algorithms, minor corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.09376v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.09376v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.07892">
    <id>http://arxiv.org/abs/2012.07892v1</id>
    <updated>2020-12-14T19:15:19Z</updated>
    <published>2020-12-14T19:15:19Z</published>
    <title>A New Approach to Regular &amp; Indeterminate Strings</title>
    <summary>  In this paper we propose a new, more appropriate definition of regular and
indeterminate strings. A regular string is one that is "isomorphic" to a string
whose entries all consist of a single letter, but which nevertheless may itself
include entries containing multiple letters. A string that is not regular is
said to be indeterminate. We begin by proposing a new model for the
representation of strings, regular or indeterminate, then go on to describe a
linear time algorithm to determine whether or not a string $x = x[1..n]$ is
regular and, if so, to replace it by a lexicographically least (lex-least)
string $y$ whose entries are all single letters. Furthermore, we connect the
regularity of a string to the transitive closure problem on a graph, which in
our special case can be efficiently solved. We then introduce the idea of a
feasible palindrome array MP of a string, and prove that every feasible MP
corresponds to some (regular or indeterminate) string. We describe an algorithm
that constructs a string $x$ corresponding to given feasible MP, while ensuring
that whenever possible $x$ is regular and if so, then lex-least. A final
section outlines new research directions suggested by this changed perspective
on regular and indeterminate strings.
</summary>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Neerja Mhaskar</name>
    </author>
    <author>
      <name>W. F. Smyth</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2020.12.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2020.12.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to TCS</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.07892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.03926">
    <id>http://arxiv.org/abs/2012.03926v1</id>
    <updated>2020-12-07T18:55:11Z</updated>
    <published>2020-12-07T18:55:11Z</published>
    <title>Counting ternary square-free words quickly</title>
    <summary>  An efficient, when compared to exhaustive enumeration, algorithm for
computing the number of square-free words of length $n$ over the alphabet $\{a,
b, c\}$ is presented.
</summary>
    <author>
      <name>Vladislav Makarov</name>
    </author>
    <link href="http://arxiv.org/abs/2012.03926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.03926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.03996">
    <id>http://arxiv.org/abs/2012.03996v1</id>
    <updated>2020-12-07T19:08:31Z</updated>
    <published>2020-12-07T19:08:31Z</published>
    <title>Galloping in natural merge sorts</title>
    <summary>  We study the algorithm TimSort and the sub-routine it uses to merge monotonic
(non-decreasing) sub-arrays, hereafter called runs. More precisely, we look at
the impact on the number of element comparisons performed of using this
sub-routine instead of a naive routine.
  In this article, we introduce a new object for measuring the complexity of
arrays. This notion dual to the notion of runs on which TimSort built its
success so far, hence we call it dual runs. It induces complexity measures that
are dual to those induced by runs. We prove, for this new complexity measure,
results that are similar to those already known when considering standard
run-induced measures. Although our new results do not lead to any improvement
on the number of element moves performed, they may lead to dramatic
improvements on the number of element comparisons performed by the algorithm.
  In order to do so, we introduce new notions of fast- and middle-growth for
natural merge sorts, which allow deriving the same upper bounds. After using
these notions successfully on TimSort, we prove that they can be applied to a
wealth of variants of TimSort and other natural merge sorts.
</summary>
    <author>
      <name>Vincent Jugé</name>
    </author>
    <author>
      <name>Ghazal Khalighinejad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.03996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.03996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.00866">
    <id>http://arxiv.org/abs/2012.00866v1</id>
    <updated>2020-12-01T22:07:59Z</updated>
    <published>2020-12-01T22:07:59Z</published>
    <title>Huskysort</title>
    <summary>  Much of the copious literature on the subject of sorting has concentrated on
minimizing the number of comparisons and/or exchanges/copies. However, a more
appropriate yardstick for the performance of sorting algorithms is based on the
total number of array accesses that are required (the "work"). For a sort that
is based on divide-and-conquer (including iterative variations on that theme),
we can divide the work into linear, i.e. $\textbf{O}(N)$, work and
linearithmic, i.e. $\textbf{O}(N log N)$, work. An algorithm that moves work
from the linearithmic phase to the linear phase may be able to reduce the total
number of array accesses and, indirectly, processing time. This paper describes
an approach to sorting which reduces the number of expensive comparisons in the
linearithmic phase as much as possible by substituting inexpensive comparisons.
In Java, the two system sorts are dual-pivot quicksort (for primitives) and
Timsort for objects. We demonstrate that a combination of these two algorithms
can run significantly faster than either algorithm alone for the types of
objects which are expensive to compare. We call this improved sorting algorithm
Huskysort.
</summary>
    <author>
      <name>R. C. Hillyard</name>
    </author>
    <author>
      <name>Yunlu Liaozheng</name>
    </author>
    <author>
      <name>Sai Vineeth K. R</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, Github repo for the algorithm included</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.00866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.00866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.12742">
    <id>http://arxiv.org/abs/2011.12742v1</id>
    <updated>2020-11-25T13:52:42Z</updated>
    <published>2020-11-25T13:52:42Z</published>
    <title>Left Lyndon tree construction</title>
    <summary>  We extend the left-to-right Lyndon factorisation of a word to the left Lyndon
tree construction of a Lyndon word. It yields an algorithm to sort the prefixes
of a Lyndon word according to the infinite ordering defined by Dolce et al.
(2019). A straightforward variant computes the left Lyndon forest of a word.
All algorithms run in linear time on a general alphabet, that is, in the
letter-comparison model.
</summary>
    <author>
      <name>Golnaz Badkobeh</name>
    </author>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.12742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32, 68W27" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.12898">
    <id>http://arxiv.org/abs/2011.12898v1</id>
    <updated>2020-11-25T17:26:46Z</updated>
    <published>2020-11-25T17:26:46Z</published>
    <title>Grammar Compression By Induced Suffix Sorting</title>
    <summary>  A grammar compression algorithm, called GCIS, is introduced in this work.
GCIS is based on the induced suffix sorting algorithm SAIS, presented by Nong
et al. in 2009. The proposed solution builds on the factorization performed by
SAIS during suffix sorting. A context-free grammar is used to replace factors
by non-terminals. The algorithm is then recursively applied on the shorter
sequence of non-terminals. The resulting grammar is encoded by exploiting some
redundancies, such as common prefixes between right-hands of rules, sorted
according to SAIS. GCIS excels for its low space and time required for
compression while obtaining competitive compression ratios. Our experiments on
regular and repetitive, moderate and very large texts, show that GCIS stands as
a very convenient choice compared to well-known compressors such as Gzip,
7-Zip, and RePair, the gold standard in grammar compression. In exchange, GCIS
is slow at decompressing. Yet, grammar compressors are more convenient than
Lempel-Ziv compressors in that one can access text substrings directly in
compressed form, without ever decompressing the text. We demonstrate that GCIS
is an excellent candidate for this scenario because it shows to be competitive
among its RePair based alternatives. We also show, how GCIS relation with SAIS
makes it a good intermediate structure to build the suffix array and the LCP
array during decompression of the text.
</summary>
    <author>
      <name>Daniel S. N. Nunes</name>
    </author>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Mauricio Ayala-Rincón</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/2011.12898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.05329">
    <id>http://arxiv.org/abs/2101.05329v1</id>
    <updated>2021-01-13T20:11:47Z</updated>
    <published>2021-01-13T20:11:47Z</published>
    <title>Improving Run Length Encoding by Preprocessing</title>
    <summary>  The Run Length Encoding (RLE) compression method is a long standing simple
lossless compression scheme which is easy to implement and achieves a good
compression on input data which contains repeating consecutive symbols. In its
pure form RLE is not applicable on natural text or other input data with short
sequences of identical symbols. We present a combination of preprocessing steps
that turn arbitrary input data in a byte-wise encoding into a bit-string which
is highly suitable for RLE compression. The main idea is to first read all most
significant bits of the input byte-string, followed by the second most
significant bit, and so on. We combine this approach by a dynamic byte
remapping as well as a Burrows-Wheeler-Scott transform on a byte level.
Finally, we apply a Huffman Encoding on the output of the bit-wise RLE encoding
to allow for more dynamic lengths of code words encoding runs of the RLE. With
our technique we can achieve a lossless average compression which is better
than the standard RLE compression by a factor of 8 on average.
</summary>
    <author>
      <name>Sven Fiergolla</name>
    </author>
    <author>
      <name>Petra Wolf</name>
    </author>
    <link href="http://arxiv.org/abs/2101.05329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.05329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P30, 94A08" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.03978">
    <id>http://arxiv.org/abs/2101.03978v1</id>
    <updated>2021-01-11T15:36:35Z</updated>
    <published>2021-01-11T15:36:35Z</published>
    <title>Strictly In-Place Algorithms for Permuting and Inverting Permutations</title>
    <summary>  We revisit the problem of permuting an array of length $n$ according to a
given permutation in place, that is, using only a small number of bits of extra
storage. Fich, Munro and Poblete [FOCS 1990, SICOMP 1995] obtained an elegant
$\mathcal{O}(n\log n)$-time algorithm using only $\mathcal{O}(\log^{2}n)$ bits
of extra space for this basic problem by designing a procedure that scans the
permutation and outputs exactly one element from each of its cycles. However,
in the strict sense in place should be understood as using only an
asymptotically optimal $\mathcal{O}(\log n)$ bits of extra space, or storing a
constant number of indices. The problem of permuting in this version is, in
fact, a well-known interview question, with the expected solution being a
quadratic-time algorithm. Surprisingly, no faster algorithm seems to be known
in the literature.
  Our first contribution is a strictly in-place generalisation of the method of
Fich et al. that works in $\mathcal{O}_{\varepsilon}(n^{1+\varepsilon})$ time,
for any $\varepsilon > 0$. Then, we build on this generalisation to obtain a
strictly in-place algorithm for inverting a given permutation on $n$ elements
working in the same complexity. This is a significant improvement on a recent
result of Gu\'spiel [arXiv 2019], who designed an $\mathcal{O}(n^{1.5})$-time
algorithm.
</summary>
    <author>
      <name>Bartłomiej Dudek</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Karol Pokorski</name>
    </author>
    <link href="http://arxiv.org/abs/2101.03978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.03978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.03165">
    <id>http://arxiv.org/abs/2101.03165v1</id>
    <updated>2021-01-08T18:50:28Z</updated>
    <published>2021-01-08T18:50:28Z</published>
    <title>Cantor Mapping Technique</title>
    <summary>  A new technique specific to String ordering utilizing a method called "Cantor
Mapping" is explained in this paper and used to perform string comparative sort
in loglinear time while utilizing linear extra space.
</summary>
    <author>
      <name>Kaustubh Joshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, 2 algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.03165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.03165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.12369">
    <id>http://arxiv.org/abs/2012.12369v1</id>
    <updated>2020-12-22T21:44:42Z</updated>
    <published>2020-12-22T21:44:42Z</published>
    <title>Integer Division by Constants: Optimal Bounds</title>
    <summary>  The integer division of a numerator n by a divisor d gives a quotient q and a
remainder r. Optimizing compilers accelerate software by replacing the division
of n by d with the division of c * n (or c * n + c) by m for convenient
integers c and m chosen so that they approximate the reciprocal: c/m ~= 1/d.
Such techniques are especially advantageous when m is chosen to be a power of
two and when d is a constant so that c and m can be precomputed. The literature
contains many bounds on the distance between c/m and the divisor d. Some of
these bounds are optimally tight, while others are not. Using accessible
mathematics, we present optimally tight bounds for quotient and remainder
computations.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Colin Bartlett</name>
    </author>
    <author>
      <name>Owen Kaser</name>
    </author>
    <link href="http://arxiv.org/abs/2012.12369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.12369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.00314">
    <id>http://arxiv.org/abs/2101.00314v1</id>
    <updated>2021-01-01T20:14:33Z</updated>
    <published>2021-01-01T20:14:33Z</published>
    <title>SetSketch: Filling the Gap between MinHash and HyperLogLog</title>
    <summary>  MinHash and HyperLogLog are sketching algorithms that have become
indispensable for set summaries in big data applications. While HyperLogLog
allows counting different elements with very little space, MinHash is suitable
for the fast comparison of sets as it allows estimating the Jaccard similarity
and other joint quantities. This work presents a new data structure called
SetSketch that is able to continuously fill the gap between both use cases. Its
commutative and idempotent insert operation and its mergeable state make it
suitable for distributed environments. Robust and easy-to-implement estimators
for cardinality and joint quantities, as well as the ability to use SetSketch
for similarity search, enable versatile applications. The developed methods can
also be used for HyperLogLog sketches and allow estimation of joint quantities
such as the intersection size with a smaller error compared to the common
estimation approach based on the inclusion-exclusion principle.
</summary>
    <author>
      <name>Otmar Ertl</name>
    </author>
    <link href="http://arxiv.org/abs/2101.00314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.00172">
    <id>http://arxiv.org/abs/2101.00172v1</id>
    <updated>2021-01-01T05:45:56Z</updated>
    <published>2021-01-01T05:45:56Z</published>
    <title>Chunk List: Concurrent Data Structures</title>
    <summary>  Chunking data is obviously no new concept; however, I had never found any
data structures that used chunking as the basis of their implementation. I
figured that by using chunking alongside concurrency, I could create an
extremely fast run-time in regards to particular methods as searching and/or
sorting. By using chunking and concurrency to my advantage, I came up with the
chunk list - a dynamic list-based data structure that would separate large
amounts of data into specifically sized chunks, each of which should be able to
be searched at the exact same time by searching each chunk on a separate
thread. As a result of implementing this concept into its own class, I was able
to create something that almost consistently gives around 20x-300x faster
results than a regular ArrayList. However, should speed be a particular issue
even after implementation, users can modify the size of the chunks and
benchmark the speed of using smaller or larger chunks, depending on the amount
of data being stored.
</summary>
    <author>
      <name>Daniel Szelogowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 3 figures A full implementation can be found at
  https://github.com/danielathome19/Chunk-List</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.00172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.00718">
    <id>http://arxiv.org/abs/2101.00718v1</id>
    <updated>2021-01-03T22:21:51Z</updated>
    <published>2021-01-03T22:21:51Z</published>
    <title>Text Searching Allowing for Non-Overlapping Adjacent Unbalanced
  Translocations</title>
    <summary>  In this paper we investigate the \emph{approximate string matching problem}
when the allowed edit operations are \emph{non-overlapping unbalanced
translocations of adjacent factors}. Such kind of edit operations take place
when two adjacent sub-strings of the text swap, resulting in a modified string.
The two involved substrings are allowed to be of different lengths.
  Such large-scale modifications on strings have various applications. They are
among the most frequent chromosomal alterations, accounted for 30\% of all
losses of heterozygosity, a major genetic event causing inactivation of cancer
suppressor genes. In addition, among other applications, they are frequent
modifications accounted in musical or in natural language information
retrieval. However, despite of their central role in so many fields of text
processing, little attention has been devoted to the problem of matching
strings allowing for this kind of edit operation.
  In this paper we present three algorithms for solving the problem, all of
them with a $\bigO(nm^3)$ worst-case and a $\bigO(m^2)$-space complexity, where
$m$ and $n$ are the length of the pattern and of the text, respectively. % In
particular, our first algorithm is based on the dynamic-programming approach.
Our second solution improves the previous one by making use of the Directed
Acyclic Word Graph of the pattern. Finally our third algorithm is based on an
alignment procedure. We also show that under the assumptions of equiprobability
and independence of characters, our second algorithm has a
$\bigO(n\log^2_{\sigma} m)$ average time complexity, for an alphabet of size
$\sigma \geq 4$.
</summary>
    <author>
      <name>Domenico Cantone</name>
    </author>
    <author>
      <name>Simone Faro</name>
    </author>
    <author>
      <name>Arianna Pavone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1812.00421</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.00718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.01108">
    <id>http://arxiv.org/abs/2101.01108v1</id>
    <updated>2021-01-04T17:32:47Z</updated>
    <published>2021-01-04T17:32:47Z</published>
    <title>Binary Dynamic Time Warping in Linear Time</title>
    <summary>  Dynamic time warping distance (DTW) is a widely used distance measure between
time series $x, y \in \Sigma^n$. It was shown by Abboud, Backurs, and Williams
that in the \emph{binary case}, where $|\Sigma| = 2$, DTW can be computed in
time $O(n^{1.87})$. We improve this running time $O(n)$.
  Moreover, if $x$ and $y$ are run-length encoded, then there is an algorithm
running in time $\tilde{O}(k + \ell)$, where $k$ and $\ell$ are the number of
runs in $x$ and $y$, respectively. This improves on the previous best bound of
$O(k\ell)$ due to Dupont and Marteau.
</summary>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <link href="http://arxiv.org/abs/2101.01108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.01108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.13698">
    <id>http://arxiv.org/abs/2012.13698v1</id>
    <updated>2020-12-26T08:08:46Z</updated>
    <published>2020-12-26T08:08:46Z</published>
    <title>Arithmetic Binary Search Trees: Static Optimality in the Matching Model</title>
    <summary>  Motivated by recent developments in optical switching and reconfigurable
network design, we study dynamic binary search trees (BSTs) in the matching
model. In the classical dynamic BST model, the cost of both link traversal and
basic reconfiguration (rotation) is $O(1)$. However, in the matching model, the
BST is defined by two optical switches (that represent two matchings in an
abstract way), and each switch (or matching) reconfiguration cost is $\alpha$
while a link traversal cost is still $O(1)$. In this work, we propose
Arithmetic BST (A-BST), a simple dynamic BST algorithm that is based on dynamic
Shannon-Fano-Elias coding, and show that A-BST is statically optimal for
sequences of length $\Omega(n \alpha \log \alpha)$ where $n$ is the number of
nodes (keys) in the tree.
</summary>
    <author>
      <name>Chen Avin</name>
    </author>
    <link href="http://arxiv.org/abs/2012.13698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.13698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.06840">
    <id>http://arxiv.org/abs/2012.06840v5</id>
    <updated>2021-01-07T21:42:26Z</updated>
    <published>2020-12-12T15:34:12Z</published>
    <title>String Attractors for Automatic Sequences</title>
    <summary>  We show that it is decidable, given an automatic sequence $\bf s$ and a
constant $c$, whether all prefixes of $\bf s$ have a string attractor of size
$\leq c$. Using a decision procedure based on this result, we show that all
prefixes of the period-doubling sequence of length $\geq 2$ have a string
attractor of size $2$. We also prove analogous results for other sequences,
including the Thue-Morse sequence and the Tribonacci sequence.
  We also provide general upper and lower bounds on string attractor size for
different kinds of sequences. For example, if $\bf s$ has a finite appearance
constant, then there is a string attractor for ${\bf s}[0..n-1]$ of size
$O(\log n)$. If further $\bf s$ is linearly recurrent, then there is a string
attractor for ${\bf s}[0..n-1]$ of size $O(1)$. For automatic sequences, the
size of the smallest string attractor for ${\bf s}[0..n-1]$ is either
$\Theta(1)$ or $\Theta(\log n)$, and it is decidable which case occurs.
Finally, we close with some remarks about greedy string attractors.
</summary>
    <author>
      <name>Luke Schaeffer</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">revision adding significant new results due to Luke Schaeffer</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.06840v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.06840v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.03961">
    <id>http://arxiv.org/abs/2102.03961v1</id>
    <updated>2021-02-08T02:10:34Z</updated>
    <published>2021-02-08T02:10:34Z</published>
    <title>Efficient construction of the extended BWT from grammar-compressed DNA
  sequencing reads</title>
    <summary>  We present an algorithm for building the extended BWT (eBWT) of a string
collection from its grammar-compressed representation. Our technique exploits
the string repetitions captured by the grammar to boost the computation of the
eBWT. Thus, the more repetitive the collection is, the lower are the resources
we use per input symbol. We rely on a new grammar recently proposed at DCC'21
whose nonterminals serve as building blocks for inducing the eBWT. A relevant
application for this idea is the construction of self-indexes for analyzing
sequencing reads -- massive and repetitive string collections of raw genomic
data. Self-indexes have become increasingly popular in Bioinformatics as they
can encode more information in less space. Our efficient eBWT construction
opens the door to perform accurate bioinformatic analyses on more massive
sequence datasets, which are not tractable with current eBWT construction
techniques.
</summary>
    <author>
      <name>Diego Diaz-Dominguez annd Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/2102.03961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.03961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.02505">
    <id>http://arxiv.org/abs/2102.02505v1</id>
    <updated>2021-02-04T09:32:52Z</updated>
    <published>2021-02-04T09:32:52Z</published>
    <title>Gapped Indexing for Consecutive Occurrences</title>
    <summary>  The classic string indexing problem is to preprocess a string S into a
compact data structure that supports efficient pattern matching queries.
Typical queries include existential queries (decide if the pattern occurs in
S), reporting queries (return all positions where the pattern occurs), and
counting queries (return the number of occurrences of the pattern). In this
paper we consider a variant of string indexing, where the goal is to compactly
represent the string such that given two patterns P1 and P2 and a gap range
[\alpha,\beta] we can quickly find the consecutive occurrences of P1 and P2
with distance in [\alpha,\beta], i.e., pairs of occurrences immediately
following each other and with distance within the range. We present data
structures that use \~O(n) space and query time \~O(|P1|+|P2|+n^(2/3)) for
existence and counting and \~O(|P1|+|P2|+n^(2/3)*occ^(1/3)) for reporting. We
complement this with a conditional lower bound based on the set intersection
problem showing that any solution using \~O(n) space must use
\tilde{\Omega}}(|P1|+|P2|+\sqrt{n}) query time. To obtain our results we
develop new techniques and ideas of independent interest including a new suffix
tree decomposition and hardness of a variant of the set intersection problem.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Max Rishøj Pedersen</name>
    </author>
    <author>
      <name>Teresa Anna Steiner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.02505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.02505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.12341">
    <id>http://arxiv.org/abs/2101.12341v1</id>
    <updated>2021-01-29T01:12:29Z</updated>
    <published>2021-01-29T01:12:29Z</published>
    <title>$r$-indexing Wheeler graphs</title>
    <summary>  Let $G$ be a Wheeler graph and $r$ be the number of runs in a Burrows-Wheeler
Transform of $G$, and suppose $G$ can be decomposed into $\upsilon$
edge-disjoint directed paths whose internal vertices each have in- and
out-degree exactly 1. We show how to store $G$ in $O (r + \upsilon)$ space such
that later, given a pattern $P$, in $O (|P| \log \log |G|)$ time we can count
the vertices of $G$ reachable by directed paths labelled $P$, and then report
those vertices in $O (\log \log |G|)$ time per vertex.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/2101.12341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.12341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.11421">
    <id>http://arxiv.org/abs/2101.11421v1</id>
    <updated>2021-01-27T14:15:46Z</updated>
    <published>2021-01-27T14:15:46Z</published>
    <title>Deriving monadic quicksort (Declarative Pearl)</title>
    <summary>  To demonstrate derivation of monadic programs, we present a specification of
sorting using the non-determinism monad, and derive pure quicksort on lists and
state-monadic quicksort on arrays. In the derivation one may switch between
point-free and pointwise styles, and deploy techniques familiar to functional
programmers such as pattern matching and induction on structures or on sizes.
Derivation of stateful programs resembles reasoning backwards from the
postcondition.
</summary>
    <author>
      <name>Shin-Cheng Mu</name>
    </author>
    <author>
      <name>Tsung-Ju Chiang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-59025-3_8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-59025-3_8" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Nakano K., Sagonas K. (eds) Functional and Logic Programming
  (FLOPS 2020). LNCS 12073. pp 124-138. 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.11421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.11421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.11350">
    <id>http://arxiv.org/abs/2101.11350v1</id>
    <updated>2021-01-27T12:29:03Z</updated>
    <published>2021-01-27T12:29:03Z</published>
    <title>Entropy of Mersenne-Twisters</title>
    <summary>  The Mersenne-Twister is one of the most popular generators of uniform
pseudo-random numbers. It is used in many numerical libraries and software. In
this paper, we look at the Komolgorov entropy of the original Mersenne-Twister,
as well as of more modern variations such as the 64-bit Mersenne-Twisters, the
Well generators, and the Melg generators.
</summary>
    <author>
      <name>Fabien Le Floc'h</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.11350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.11350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.11408">
    <id>http://arxiv.org/abs/2101.11408v4</id>
    <updated>2021-03-23T00:52:54Z</updated>
    <published>2021-01-11T20:31:27Z</published>
    <title>Number Parsing at a Gigabyte per Second</title>
    <summary>  With disks and networks providing gigabytes per second, parsing decimal
numbers from strings becomes a bottleneck. We consider the problem of parsing
decimal numbers to the nearest binary floating-point value. The general problem
requires variable-precision arithmetic. However, we need at most 17 digits to
represent 64-bit standard floating-point numbers (IEEE 754). Thus we can
represent the decimal significand with a single 64-bit word. By combining the
significand and precomputed tables, we can compute the nearest floating-point
number using as few as one or two 64-bit multiplications. Our implementation
can be several times faster than conventional functions present in standard C
libraries on modern 64-bit systems (Intel, AMD, ARM and POWER9). Our work is
available as open source software used by major systems such as Apache Arrow
and Yandex ClickHouse. The Go standard library has adopted a version of our
approach.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Software at https://github.com/fastfloat/fast_float and
  https://github.com/lemire/simple_fastfloat_benchmark/</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.11408v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.11408v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.10890">
    <id>http://arxiv.org/abs/2101.10890v1</id>
    <updated>2021-01-25T18:37:35Z</updated>
    <published>2021-01-25T18:37:35Z</published>
    <title>Spanner Evaluation over SLP-Compressed Documents</title>
    <summary>  We consider the problem of evaluating regular spanners over compressed
documents, i.e., we wish to solve evaluation tasks directly on the compressed
data, without decompression. As compressed forms of the documents we use
straight-line programs (SLPs) -- a lossless compression scheme for textual data
widely used in different areas of theoretical computer science and particularly
well-suited for algorithmics on compressed data. In terms of data complexity,
our results are as follows. For a regular spanner M and an SLP S that
represents a document D, we can solve the tasks of model checking and of
checking non-emptiness in time O(size(S)). Computing the set M(D) of all
span-tuples extracted from D can be done in time O(size(S) size(M(D))), and
enumeration of M(D) can be done with linear preprocessing O(size(S)) and a
delay of O(depth(S)), where depth(S) is the depth of S's derivation tree. Note
that size(S) can be exponentially smaller than the document's size |D|; and,
due to known balancing results for SLPs, we can always assume that depth(S) =
O(log(|D|)) independent of D's compressibility. Hence, our enumeration
algorithm has a delay logarithmic in the size of the non-compressed data and a
preprocessing time that is at best (i.e., in the case of highly compressible
documents) also logarithmic, but at worst still linear. Therefore, in a
big-data perspective, our enumeration algorithm for SLP-compressed documents
may nevertheless beat the known linear preprocessing and constant delay
algorithms for non-compressed documents.
</summary>
    <author>
      <name>Markus L. Schmid</name>
    </author>
    <author>
      <name>Nicole Schweikardt</name>
    </author>
    <link href="http://arxiv.org/abs/2101.10890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.10890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2009.11514">
    <id>http://arxiv.org/abs/2009.11514v1</id>
    <updated>2020-09-24T06:50:27Z</updated>
    <published>2020-09-24T06:50:27Z</published>
    <title>On One-way Functions and Kolmogorov Complexity</title>
    <summary>  We prove that the equivalence of two fundamental problems in the theory of
computing. For every polynomial $t(n)\geq (1+\varepsilon)n, \varepsilon>0$, the
following are equivalent:
  - One-way functions exists (which in turn is equivalent to the existence of
secure private-key encryption schemes, digital signatures, pseudorandom
generators, pseudorandom functions, commitment schemes, and more);
  - $t$-time bounded Kolmogorov Complexity, $K^t$, is mildly hard-on-average
(i.e., there exists a polynomial $p(n)>0$ such that no PPT algorithm can
compute $K^t$, for more than a $1-\frac{1}{p(n)}$ fraction of $n$-bit strings).
  In doing so, we present the first natural, and well-studied, computational
problem characterizing the feasibility of the central private-key primitives
and protocols in Cryptography.
</summary>
    <author>
      <name>Yanyi Liu</name>
    </author>
    <author>
      <name>Rafael Pass</name>
    </author>
    <link href="http://arxiv.org/abs/2009.11514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.07360">
    <id>http://arxiv.org/abs/2101.07360v1</id>
    <updated>2021-01-18T22:55:32Z</updated>
    <published>2021-01-18T22:55:32Z</published>
    <title>Dynamic Longest Increasing Subsequence and the Erdös-Szekeres
  Partitioning Problem</title>
    <summary>  In this paper, we provide new approximation algorithms for dynamic variations
of the longest increasing subsequence (\textsf{LIS}) problem, and the
complementary distance to monotonicity (\textsf{DTM}) problem. In this setting,
operations of the following form arrive sequentially: (i) add an element, (ii)
remove an element, or (iii) substitute an element for another. At every point
in time, the algorithm has an approximation to the longest increasing
subsequence (or distance to monotonicity). We present a
$(1+\epsilon)$-approximation algorithm for \textsf{DTM} with polylogarithmic
worst-case update time and a constant factor approximation algorithm for
\textsf{LIS} with worst-case update time $\tilde O(n^\epsilon)$ for any
constant $\epsilon > 0$.% $n$ in the runtime denotes the size of the array at
the time the operation arrives.
  Our dynamic algorithm for \textsf{LIS} leads to an almost optimal algorithm
for the Erd\"{o}s-Szekeres partitioning problem. Erd\"{o}s-Szekeres
partitioning problem was introduced by Erd\"{o}s and Szekeres in 1935 and was
known to be solvable in time $O(n^{1.5}\log n)$. Subsequent work improve the
runtime to $O(n^{1.5})$ only in 1998. Our dynamic \textsf{LIS} algorithm leads
to a solution for Erd\"{o}s-Szekeres partitioning problem with runtime $\tilde
O_{\epsilon}(n^{1+\epsilon})$ for any constant $\epsilon > 0$.
</summary>
    <author>
      <name>Michael Mitzenmacher</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <link href="http://arxiv.org/abs/2101.07360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.07360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.00462">
    <id>http://arxiv.org/abs/2103.00462v1</id>
    <updated>2021-02-28T11:28:13Z</updated>
    <published>2021-02-28T11:28:13Z</published>
    <title>Weighted Ancestors in Suffix Trees Revisited</title>
    <summary>  The weighted ancestor problem is a well-known generalization of the
predecessor problem to trees. It is known that it requires $\Omega(\log\log n)$
time for queries provided $O(n\mathop{\mathrm{polylog}} n)$ space is available
and weights are from $[0..n]$, where $n$ is the number of tree nodes. However,
when applied to suffix trees, the problem, surprisingly, admits an $O(n)$-space
solution with constant query time as was shown by Gawrychowski, Lewenstein, and
Nicholson. This variant of the problem can be reformulated as follows: given
the suffix tree of a string $s$, we need a data structure that can locate in
the tree any substring $s[p..q]$ of $s$ in $O(1)$ time (as if one descended
from the root reading $s[p..q]$ along the way). Unfortunately, the data
structure of Gawrychowski et al. has no efficient construction algorithm, which
apparently prevents its wide usage. In this paper we resolve this issue
describing a data structure for weighted ancestors in suffix trees with
constant query time and a linear construction algorithm. Our solution is based
on a novel approach using so-called irreducible LCP values.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.00462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.00462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.00713">
    <id>http://arxiv.org/abs/2103.00713v1</id>
    <updated>2021-03-01T02:49:34Z</updated>
    <published>2021-03-01T02:49:34Z</published>
    <title>Lower Bounds and Improved Algorithms for Asymmetric Streaming Edit
  Distance and Longest Common Subsequence</title>
    <summary>  In this paper, we study edit distance (ED) and longest common subsequence
(LCS) in the asymmetric streaming model, introduced by Saks and Seshadhri
[SS13]. As an intermediate model between the random access model and the
streaming model, this model allows one to have streaming access to one string
and random access to the other string.
  Our first main contribution is a systematic study of space lower bounds for
ED and LCS in the asymmetric streaming model. Previously, there are no
explicitly stated results in this context, although some lower bounds about LCS
can be inferred from the lower bounds for longest increasing subsequence (LIS)
in [SW07][GG10][EJ08]. Yet these bounds only work for large alphabet size. In
this paper, we develop several new techniques to handle ED in general and LCS
for small alphabet size, thus establishing strong lower bounds for both
problems. In particular, our lower bound for ED provides an exponential
separation between edit distance and Hamming distance in the asymmetric
streaming model. Our lower bounds also extend to LIS and longest non-decreasing
sequence (LNS) in the standard streaming model. Together with previous results,
our bounds provide an almost complete picture for these two problems.
  As our second main contribution, we give improved algorithms for ED and LCS
in the asymmetric streaming model. For ED, we improve the space complexity of
the constant factor approximation algorithms in [FHRS20][CJLZ20] from
$\tilde{O}(\frac{n^\delta}{\delta})$ to
$O(\frac{d^\delta}{\delta}\;\mathsf{polylog}(n))$, where $n$ is the length of
each string and $d$ is the edit distance between the two strings. For LCS, we
give the first $1/2+\epsilon$ approximation algorithm with space $n^{\delta}$
for any constant $\delta>0$, over a binary alphabet.
</summary>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Yu Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/2103.00713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.00713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.01052">
    <id>http://arxiv.org/abs/2103.01052v2</id>
    <updated>2021-03-09T14:45:51Z</updated>
    <published>2021-03-01T14:50:29Z</published>
    <title>On the Cost of Unsuccessful Searches in Search Trees with Two-way
  Comparisons</title>
    <summary>  Search trees are commonly used to implement access operations to a set of
stored keys. If this set is static and the probabilities of membership queries
are known in advance, then one can precompute an optimal search tree, namely
one that minimizes the expected access cost. For a non-key query, a search tree
can determine its approximate location by returning the inter-key interval
containing the query. This is in contrast to other dictionary data structures,
like hash tables, that only report a failed search. We address the question
"what is the additional cost of determining approximate locations for non-key
queries"? We prove that for two-way comparison trees this additional cost is at
most 1. Our proof is based on a novel probabilistic argument that involves
converting a search tree that does not identify non-key queries into a random
tree that does.
</summary>
    <author>
      <name>Marek Chrobak</name>
    </author>
    <author>
      <name>Mordecai Golin</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Neal E. Young</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ic.2021.104707</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ic.2021.104707" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v2 has updated bibliography</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information and Computation (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.01052v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.01052v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P10, 68P30, 68W25, 94A45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; G.1.6; G.2.2; H.3.1; I.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.01084">
    <id>http://arxiv.org/abs/2103.01084v2</id>
    <updated>2021-03-09T16:14:30Z</updated>
    <published>2021-03-01T15:53:28Z</published>
    <title>A Simple Algorithm for Optimal Search Trees with Two-Way Comparisons</title>
    <summary>  We present a simple $O(n^4)$-time algorithm for computing optimal search
trees with two-way comparisons. The only previous solution to this problem, by
Anderson et al., has the same running time, but is significantly more
complicated and is restricted to the variant where only successful queries are
allowed. Our algorithm extends directly to solve the standard full variant of
the problem, which also allows unsuccessful queries and for which no
polynomial-time algorithm was previously known. The correctness proof of our
algorithm relies on a new structural theorem for two-way-comparison search
trees.
</summary>
    <author>
      <name>Marek Chrobak</name>
    </author>
    <author>
      <name>Mordecai Golin</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Neal E. Young</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v2 has updated references</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.01084v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.01084v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P10, 68P30, 68W25, 94A45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; G.1.6; G.2.2; H.3.1; I.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.12824">
    <id>http://arxiv.org/abs/2102.12824v2</id>
    <updated>2021-02-27T07:28:28Z</updated>
    <published>2021-02-25T12:51:33Z</published>
    <title>A Linear Time Algorithm for Constructing Hierarchical Overlap Graphs</title>
    <summary>  The hierarchical overlap graph (HOG) is a graph that encodes overlaps from a
given set P of n strings, as the overlap graph does. A best known algorithm
constructs HOG in O(||P|| log n) time and O(||P||) space, where ||P|| is the
sum of lengths of the strings in P. In this paper we present a new algorithm to
construct HOG in O(||P||) time and space. Hence, the construction time and
space of HOG are better than those of the overlap graph, which are O(||P|| +
n^2).
</summary>
    <author>
      <name>Sangsoo Park</name>
    </author>
    <author>
      <name>Sung Gwan Park</name>
    </author>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Kunsoo Park</name>
    </author>
    <author>
      <name>Eric Rivals</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, submitted to CPM 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.12824v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12824v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.12822">
    <id>http://arxiv.org/abs/2102.12822v3</id>
    <updated>2021-03-17T18:39:58Z</updated>
    <published>2021-02-25T12:47:13Z</published>
    <title>Algorithms and Complexity on Indexing Founder Graphs</title>
    <summary>  We introduce a compact pangenome representation based on an optimal
segmentation concept that aims to reconstruct founder sequences from a multiple
sequence alignment (MSA). Such founder sequences have the feature that each row
of the MSA is a recombination of the founders. Several linear time dynamic
programming algorithms have been previously devised to optimize segmentations
that induce founder blocks that then can be concatenated into a set of founder
sequences. All possible concatenation orders can be expressed as a founder
graph. We observe a key property of such graphs: if the node labels (founder
segments) do not repeat in the paths of the graph, such graphs can be indexed
for efficient string matching. We call such graphs repeat-free founder graphs
when constructed from a gapless MSA and repeat-free elastic founder graphs when
constructed from a general MSA with gaps. We give a linear time algorithm and a
parameterized near linear time algorithm to construct a repeat-free founder
graph and a repeat-free elastic founder graph, respectively. We derive a
tailored succinct index structure to support queries of arbitrary length in the
paths of a repeat-free (elastic) founder graph. In addition, we show how to
turn a repeat-free (elastic) founder graph into a Wheeler graph in polynomial
time. Furthermore, we show that a property such as repeat-freeness is essential
for indexability. In particular, we show that unless the Strong Exponential
Time Hypothesis (SETH) fails, one cannot build an index on an elastic founder
graph in polynomial time to support fast queries.
</summary>
    <author>
      <name>Massimo Equi</name>
    </author>
    <author>
      <name>Tuukka Norri</name>
    </author>
    <author>
      <name>Jarno Alanko</name>
    </author>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Alexandru I. Tomescu</name>
    </author>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extended version of WABI 2020 paper
  (https://doi.org/10.4230/LIPIcs.WABI.2020.7), whose preprint is in
  arXiv:2005.09342</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.12822v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12822v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.4; F.1.3; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.11797">
    <id>http://arxiv.org/abs/2102.11797v1</id>
    <updated>2021-02-23T17:04:25Z</updated>
    <published>2021-02-23T17:04:25Z</published>
    <title>Conditional Lower Bounds for Variants of Dynamic LIS</title>
    <summary>  In this note, we consider the complexity of maintaining the longest
increasing subsequence (LIS) of an array under (i) inserting an element, and
(ii) deleting an element of an array. We show that no algorithm can support
queries and updates in time $\mathcal{O}(n^{1/2-\epsilon})$ and
$\mathcal{O}(n^{1/3-\epsilon})$ for the dynamic LIS problem, for any constant
$\epsilon>0$, when the elements are weighted or the algorithm supports
1D-queries (on subarrays), respectively, assuming the All-Pairs Shortest Paths
(APSP) conjecture or the Online Boolean Matrix-Vector Multiplication (OMv)
conjecture. The main idea in our construction comes from the work of Abboud and
Dahlgaard [FOCS 2016], who proved conditional lower bounds for dynamic planar
graph algorithm. However, this needs to be appropriately adjusted and
translated to obtain an instance of the dynamic LIS problem.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Wojciech Janczewski</name>
    </author>
    <link href="http://arxiv.org/abs/2102.11797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.11797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.09463">
    <id>http://arxiv.org/abs/2102.09463v1</id>
    <updated>2021-02-18T16:25:13Z</updated>
    <published>2021-02-18T16:25:13Z</published>
    <title>Range Minimum Queries in Minimal Space</title>
    <summary>  We consider the problem of computing a sequence of range minimum queries. We
assume a sequence of commands that contains values and queries. Our goal is to
quickly determine the minimum value that exists between the current position
and a previous position $i$. Range minimum queries are used as a sub-routine of
several algorithms, namely related to string processing. We propose a data
structure that can process these commands sequences. We obtain efficient
results for several variations of the problem, in particular we obtain $O(1)$
time per command for the offline version and $O(\alpha(n))$ amortized time for
the online version, where $\alpha(n)$ is the inverse Ackermann function and $n$
the number of values in the sequence. This data structure also has very small
space requirements, namely $O(\ell)$ where $\ell$ is the maximum number active
$i$ positions. We implemented our data structure and show that it is
competitive against existing alternatives. We obtain comparable command
processing time, in the nano second range, and much smaller space requirements.
</summary>
    <author>
      <name>Luís M. S. Russo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 3 figures, 3 tables, 6 algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.09463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.09463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.08670">
    <id>http://arxiv.org/abs/2102.08670v1</id>
    <updated>2021-02-17T10:25:06Z</updated>
    <published>2021-02-17T10:25:06Z</published>
    <title>Linear Time Runs over General Ordered Alphabets</title>
    <summary>  A run in a string is a maximal periodic substring. For example, the string
$\texttt{bananatree}$ contains the runs $\texttt{anana} = (\texttt{an})^{3/2}$
and $\texttt{ee} = \texttt{e}^2$. There are less than $n$ runs in any
length-$n$ string, and computing all runs for a string over a linearly-sortable
alphabet takes $\mathcal{O}(n)$ time (Bannai et al., SODA 2015). Kosolobov
conjectured that there also exists a linear time runs algorithm for general
ordered alphabets (Inf. Process. Lett. 2016). The conjecture was almost proven
by Crochemore et al., who presented an $\mathcal{O}(n\alpha(n))$ time algorithm
(where $\alpha(n)$ is the extremely slowly growing inverse Ackermann function).
We show how to achieve $\mathcal{O}(n)$ time by exploiting combinatorial
properties of the Lyndon array, thus proving Kosolobov's conjecture.
</summary>
    <author>
      <name>Jonas Ellert</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to ICALP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.08670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.08670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.06798">
    <id>http://arxiv.org/abs/2102.06798v1</id>
    <updated>2021-02-12T22:25:30Z</updated>
    <published>2021-02-12T22:25:30Z</published>
    <title>Which Regular Languages can be Efficiently Indexed?</title>
    <summary>  In the present work, we study the hierarchy of $p$-sortable languages:
regular languages accepted by automata of width $p$. In this hierarchy, regular
languages are sorted according to the new fundamental measure of NFA complexity
$p$. Our main contributions are the following: (i) we show that the hierarchy
is strict and does not collapse, (ii) we provide (exponential) upper and lower
bounds relating the minimum widths of equivalent NFAs and DFAs, and (iii) we
characterize DFAs of minimum $p$ for a given $\mathcal L$ via a
co-lexicographic variant of the Myhill-Nerode theorem. Our findings imply that
in polynomial time we can build an index breaking the worst-case conditional
lower bound of $\Omega(\pi m)$, whenever the input NFA's width is at most
$\epsilon \log_2 m$, for any constant $0 \leq \epsilon &lt; 1/2$.
</summary>
    <author>
      <name>Nicola Cotumaccio</name>
    </author>
    <author>
      <name>Giovanna D'Agostino</name>
    </author>
    <author>
      <name>Alberto Policriti</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/2102.06798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.06798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.05460">
    <id>http://arxiv.org/abs/2103.05460v1</id>
    <updated>2021-03-09T14:50:49Z</updated>
    <published>2021-03-09T14:50:49Z</published>
    <title>Dynamic Range Mode Enumeration</title>
    <summary>  The range mode problem is a fundamental problem and there is a lot of work
about it. There is also some work for the dynamic version of it and the
enumerating version of it, but there is no previous research about the dynamic
and enumerating version of it. We found an efficient algorithm for it.
</summary>
    <author>
      <name>Tetto Obata</name>
    </author>
    <link href="http://arxiv.org/abs/2103.05460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.05460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.03468">
    <id>http://arxiv.org/abs/2103.03468v1</id>
    <updated>2021-03-05T04:35:35Z</updated>
    <published>2021-03-05T04:35:35Z</published>
    <title>Compressed Communication Complexity of Hamming Distance</title>
    <summary>  We consider the communication complexity of the Hamming distance of two
strings. Bille et al. [SPIRE 2018] considered the communication complexity of
the longest common prefix (LCP) problem in the setting where the two parties
have their strings in a compressed form, i.e., represented by the Lempel-Ziv 77
factorization (LZ77) with/without self-references. We present a randomized
public-coin protocol for a joint computation of the Hamming distance of two
strings represented by LZ77 without self-references. While our scheme is
heavily based on Bille et al.'s LCP protocol, our complexity analysis is
original which uses Crochemore's C-factorization and Rytter's AVL-grammar. As a
byproduct, we also show that LZ77 with/without self-references are not
monotonic in the sense that their sizes can increase by a factor of 4/3 when a
prefix of the string is removed.
</summary>
    <author>
      <name>Shiori Mitsuya</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2103.03468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.03468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.03294">
    <id>http://arxiv.org/abs/2103.03294v1</id>
    <updated>2021-03-04T20:07:00Z</updated>
    <published>2021-03-04T20:07:00Z</published>
    <title>An Almost Optimal Edit Distance Oracle</title>
    <summary>  We consider the problem of preprocessing two strings $S$ and $T$, of lengths
$m$ and $n$, respectively, in order to be able to efficiently answer the
following queries: Given positions $i,j$ in $S$ and positions $a,b$ in $T$,
return the optimal alignment of $S[i \mathinner{.\,.} j]$ and $T[a
\mathinner{.\,.} b]$. Let $N=mn$. We present an oracle with preprocessing time
$N^{1+o(1)}$ and space $N^{1+o(1)}$ that answers queries in $\log^{2+o(1)}N$
time. In other words, we show that we can query the alignment of every two
substrings in almost the same time it takes to compute just the alignment of
$S$ and $T$. Our oracle uses ideas from our distance oracle for planar graphs
[STOC 2019] and exploits the special structure of the alignment graph.
Conditioned on popular hardness conjectures, this result is optimal up to
subpolynomial factors. Our results apply to both edit distance and longest
common subsequence (LCS).
  The best previously known oracle with construction time and size
$\mathcal{O}(N)$ has slow $\Omega(\sqrt{N})$ query time [Sakai, TCS 2019], and
the one with size $N^{1+o(1)}$ and query time $\log^{2+o(1)}N$ (using a planar
graph distance oracle) has slow $\Omega(N^{3/2})$ construction time [Long &amp;
Pettie, SODA 2021]. We improve both approaches by roughly a $\sqrt N$ factor.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Shay Mozes</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <link href="http://arxiv.org/abs/2103.03294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.03294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.13457">
    <id>http://arxiv.org/abs/2104.13457v1</id>
    <updated>2021-04-27T20:11:47Z</updated>
    <published>2021-04-27T20:11:47Z</published>
    <title>Hypersuccinct Trees -- New universal tree source codes for optimal
  compressed tree data structures</title>
    <summary>  We present a new universal source code for unlabeled binary and ordinal trees
that achieves asymptotically optimal compression for all tree sources covered
by existing universal codes. At the same time, it supports answering many
navigational queries on the compressed representation in constant time on the
word-RAM; this is not known to be possible for any existing tree compression
method. The resulting data structures, "hypersuccinct trees", hence combine the
compression achieved by the best known universal codes with the operation
support of the best succinct tree data structures. Compared to prior work on
succinct data structures, we do not have to tailor our data structure to
specific applications; hypersuccinct trees automatically adapt to the trees at
hand. We show that it simultaneously achieves the asymptotically optimal space
usage for a wide range of distributions over tree shapes, including: random
binary search trees (BSTs) / Cartesian trees of random arrays, random
fringe-balanced BSTs, binary trees with a given number of binary/unary/leaf
nodes, random binary tries generated from memoryless sources, full binary
trees, unary paths, as well as uniformly chosen weight-balanced BSTs, AVL
trees, and left-leaning red-black trees. Using hypersuccinct trees, we further
obtain the first data structure that answers range-minimum queries on a random
permutation of $n$ elements in constant time and using the optimal $1.736n +
o(n)$ bits on average, solving an open problem of Davoodi et al. (2014) and
Golin et al. (2016).
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Patrick K. Nicholson</name>
    </author>
    <author>
      <name>Louisa Seelbach Benkner</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <link href="http://arxiv.org/abs/2104.13457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.13457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.10939">
    <id>http://arxiv.org/abs/2104.10939v1</id>
    <updated>2021-04-22T09:10:31Z</updated>
    <published>2021-04-22T09:10:31Z</published>
    <title>HINT: A Hierarchical Index for Intervals in Main Memory</title>
    <summary>  Indexing intervals is a fundamental problem, finding a wide range of
applications. Recent work on managing large collections of intervals in main
memory focused on overlap joins and temporal aggregation problems. In this
paper, we propose novel and efficient in-memory indexing techniques for
intervals, with a focus on interval range queries, which are a basic component
of many search and analysis tasks. First, we propose an optimized version of a
single-level (flat) domain-partitioning approach, which may have large space
requirements due to excessive replication. Then, we propose a hierarchical
partitioning approach, which assigns each interval to at most two partitions
per level and has controlled space requirements. Novel elements of our
techniques include the division of the intervals at each partition into groups
based on whether they begin inside or before the partition boundaries, reducing
the information stored at each partition to the absolutely necessary, and the
effective handling of data sparsity and skew. Experimental results on real and
synthetic interval sets of different characteristics show that our approaches
are typically one order of magnitude faster than the state-of-the-art.
</summary>
    <author>
      <name>George Christodoulou</name>
    </author>
    <author>
      <name>Panagiotis Bouros</name>
    </author>
    <author>
      <name>Nikos Mamoulis</name>
    </author>
    <link href="http://arxiv.org/abs/2104.10939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.10939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.10402">
    <id>http://arxiv.org/abs/2104.10402v2</id>
    <updated>2021-05-28T08:58:38Z</updated>
    <published>2021-04-21T08:22:07Z</published>
    <title>PTHash: Revisiting FCH Minimal Perfect Hashing</title>
    <summary>  Given a set $S$ of $n$ distinct keys, a function $f$ that bijectively maps
the keys of $S$ into the range $\{0,\ldots,n-1\}$ is called a minimal perfect
hash function for $S$. Algorithms that find such functions when $n$ is large
and retain constant evaluation time are of practical interest; for instance,
search engines and databases typically use minimal perfect hash functions to
quickly assign identifiers to static sets of variable-length keys such as
strings. The challenge is to design an algorithm which is efficient in three
different aspects: time to find $f$ (construction time), time to evaluate $f$
on a key of $S$ (lookup time), and space of representation for $f$. Several
algorithms have been proposed to trade-off between these aspects. In 1992, Fox,
Chen, and Heath (FCH) presented an algorithm at SIGIR providing very fast
lookup evaluation. However, the approach received little attention because of
its large construction time and higher space consumption compared to other
subsequent techniques. Almost thirty years later we revisit their framework and
present an improved algorithm that scales well to large sets and reduces space
consumption altogether, without compromising the lookup time. We conduct an
extensive experimental assessment and show that the algorithm finds functions
that are competitive in space with state-of-the art techniques and provide
$2-4\times$ better lookup time.
</summary>
    <author>
      <name>Giulio Ermanno Pibiri</name>
    </author>
    <author>
      <name>Roberto Trani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3404835.3462849</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3404835.3462849" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SIGIR 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.10402v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.10402v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.06740">
    <id>http://arxiv.org/abs/2104.06740v1</id>
    <updated>2021-04-14T09:47:06Z</updated>
    <published>2021-04-14T09:47:06Z</published>
    <title>Engineering Predecessor Data Structures for Dynamic Integer Sets</title>
    <summary>  We present highly optimized data structures for the dynamic predecessor
problem, where the task is to maintain a set $S$ of $w$-bit numbers under
insertions, deletions, and predecessor queries (return the largest element in
$S$ no larger than a given key). The problem of finding predecessors can be
viewed as a generalized form of the membership problem, or as a simple version
of the nearest neighbour problem. It lies at the core of various real-world
problems such as internet routing.
  In this work, we engineer (1) a simple implementation of the idea of universe
reduction, similar to van-Emde-Boas trees (2) variants of y-fast tries
[Willard, IPL'83], and (3) B-trees with different strategies for organizing the
keys contained in the nodes, including an implementation of dynamic fusion
nodes [P\v{a}tra\c{s}cu and Thorup, FOCS'14]. We implement our data structures
for $w=32,40,64$, which covers most typical scenarios.
  Our data structures finish workloads faster than previous approaches while
being significantly more space-efficient, e.g., they clearly outperform
standard implementations of the STL by finishing up to four times as fast using
less than a third of the memory. Our tests also provide more general insights
on data structure design, such as how small sets should be stored and handled
and if and when new CPU instructions such as advanced vector extensions pay
off.
</summary>
    <author>
      <name>Patrick Dinklage</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Alexander Herlez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages plus 5 page appendix, to be published in the proceedings of
  the 19th Symposium on Experimental Algorithms (SEA) 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.06740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.06740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.09985">
    <id>http://arxiv.org/abs/2104.09985v1</id>
    <updated>2021-04-19T08:13:39Z</updated>
    <published>2021-04-19T08:13:39Z</published>
    <title>A Separation of $γ$ and $b$ via Thue--Morse Words</title>
    <summary>  We prove that for $n\geq 2$, the size $b(t_n)$ of the smallest bidirectional
scheme for the $n$th Thue--Morse word $t_n$ is $n+2$. Since Kutsukake et al.
[SPIRE 2020] show that the size $\gamma(t_n)$ of the smallest string attractor
for $t_n$ is $4$ for $n \geq 4$, this shows for the first time that there is a
separation between the size of the smallest string attractor $\gamma$ and the
size of the smallest bidirectional scheme $b$, i.e., there exist string
families such that $\gamma = o(b)$.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Dominik Koeppl</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <link href="http://arxiv.org/abs/2104.09985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.09985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.08751">
    <id>http://arxiv.org/abs/2104.08751v1</id>
    <updated>2021-04-18T07:22:29Z</updated>
    <published>2021-04-18T07:22:29Z</published>
    <title>Load-Balancing Succinct B Trees</title>
    <summary>  We propose a B tree representation storing $n$ keys, each of $k$ bits, in
either (a) $nk + O(nk / \lg n)$ bits or (b) $nk + O(nk \lg \lg n/ \lg n)$ bits
of space supporting all B tree operations in either (a) $O(\lg n )$ time or (b)
$O(\lg n / \lg \lg n)$ time, respectively. We can augment each node with an
aggregate value such as the minimum value within its subtree, and maintain
these aggregate values within the same space and time complexities. Finally, we
give the sparse suffix tree as an application, and present a linear-time
algorithm computing the sparse longest common prefix array from the suffix AVL
tree of Irving et al. [JDA'2003].
</summary>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <link href="http://arxiv.org/abs/2104.08751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.02461">
    <id>http://arxiv.org/abs/2104.02461v1</id>
    <updated>2021-04-06T12:39:28Z</updated>
    <published>2021-04-06T12:39:28Z</published>
    <title>Sorted Range Reporting</title>
    <summary>  In sorted range selection problem, the aim is to preprocess a given array
A[1: n] so as to answers queries of type: given two indices i,j ($1 \le i\le j
\le n$) and an integer k, report k smallest elements in sorted order present in
the sub-array A[i: j] Brodal et.al.[2] have shown that the problem can be
solved in O(k) time after O(n log n) preprocessing in linear space. In this
paper we discuss another tradeoff. We reduce preprocessing time to O(n), but
query time is O(k log k), again using linear space. Our method is very simple.
</summary>
    <author>
      <name>Waseem Akram</name>
    </author>
    <author>
      <name>Sanjeev Saxena</name>
    </author>
    <link href="http://arxiv.org/abs/2104.02461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2103.15329">
    <id>http://arxiv.org/abs/2103.15329v1</id>
    <updated>2021-03-29T04:54:14Z</updated>
    <published>2021-03-29T04:54:14Z</published>
    <title>A Fast and Small Subsampled R-index</title>
    <summary>  The $r$-index (Gagie et al., JACM 2020) represented a breakthrough in
compressed indexing of repetitive text collections, outperforming its
alternatives by orders of magnitude. Its space usage, $\mathcal{O}(r)$ where
$r$ is the number of runs in the Burrows-Wheeler Transform of the text, is
however larger than Lempel-Ziv and grammar-based indexes, and makes it
uninteresting in various real-life scenarios of milder repetitiveness. In this
paper we introduce the $sr$-index, a variant that limits the space to
$\mathcal{O}(\min(r,n/s))$ for a text of length $n$ and a given parameter $s$,
at the expense of multiplying by $s$ the time per occurrence reported. The
$sr$-index is obtained by carefully subsampling the text positions indexed by
the $r$-index, in a way that we prove is still able to support pattern matching
with guaranteed performance. Our experiments demonstrate that the $sr$-index
sharply outperforms virtually every other compressed index on repetitive texts,
both in time and space, even matching the performance of the $r$-index while
using 1.5--3.0 times less space. Only some Lempel-Ziv-based indexes achieve
better compression than the $sr$-index, using about half the space, but they
are an order of magnitude slower.
</summary>
    <author>
      <name>Dustin Cobas</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/2103.15329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.07073">
    <id>http://arxiv.org/abs/2105.07073v1</id>
    <updated>2021-05-14T21:24:34Z</updated>
    <published>2021-05-14T21:24:34Z</published>
    <title>N-ary Huffman Encoding Using High-Degree Trees -- A Performance
  Comparison</title>
    <summary>  In this paper we implement an n-ary Huffman Encoding and Decoding application
using different degrees of tree structures. Our goal is to compare the
performance of the algorithm in terms of compression ratio, decompression speed
and weighted path length when using higher degree trees, compared to the 2-ary
Huffman Code. The Huffman tree degrees that we compare are 2-ary, 3-ary, 4-ary,
5-ary, 6-ary, 7-ary, 8-ary and 16-mal. We also present the impact that branch
prediction has on the performance of the n-ary Huffman Decoding.
</summary>
    <author>
      <name>Ioannis S. Xezonakis</name>
    </author>
    <author>
      <name>Svoronos Leivadaros</name>
    </author>
    <link href="http://arxiv.org/abs/2105.07073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.07073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.10027">
    <id>http://arxiv.org/abs/2102.10027v5</id>
    <updated>2021-05-07T13:33:26Z</updated>
    <published>2021-02-19T16:53:54Z</published>
    <title>Sorting Short Integers</title>
    <summary>  We build boolean circuits of size $O(nm^2)$ and depth $O(\log(n) + m
\log(m))$ for sorting $n$ integers each of $m$-bits. We build also circuits
that sort $n$ integers each of $m$-bits according to their first $k$ bits that
are of size $O(nmk(1 + \log^*(n) - \log^*(m)))$ and depth $O(\log^{3}(n))$.
This improves on the result of Asharov et al. arXiv:2010.09884 and resolves
some of their open questions.
</summary>
    <author>
      <name>Michal Koucký</name>
    </author>
    <author>
      <name>Karel Král</name>
    </author>
    <link href="http://arxiv.org/abs/2102.10027v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.10027v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.10327">
    <id>http://arxiv.org/abs/2105.10327v1</id>
    <updated>2021-05-21T12:59:19Z</updated>
    <published>2021-05-21T12:59:19Z</published>
    <title>Weighted Burrows-Wheeler Compression</title>
    <summary>  A weight based dynamic compression method has recently been proposed, which
is especially suitable for the encoding of files with locally skewed
distributions. Its main idea is to assign larger weights to closer to be
encoded symbols by means of an increasing weight function, rather than
considering each position in the text evenly. A well known transformation that
tends to convert input files into files with a more skewed distribution is the
Burrows-Wheeler Transform. This paper employs the weighted approach on
Burrows-Wheeler transformed files and provides empirical evidence of the
efficiency of this combination.
</summary>
    <author>
      <name>Aharon Fruchtman</name>
    </author>
    <author>
      <name>Yoav Gross</name>
    </author>
    <author>
      <name>Shmuel T. Klein</name>
    </author>
    <author>
      <name>Dana Shapira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.10327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.10327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.08496">
    <id>http://arxiv.org/abs/2105.08496v1</id>
    <updated>2021-05-18T13:15:44Z</updated>
    <published>2021-05-18T13:15:44Z</published>
    <title>Combinatorics of minimal absent words for a sliding window</title>
    <summary>  A string $w$ is called a minimal absent word (MAW) for another string $T$ if
$w$ does not occur in $T$ but the proper substrings of $w$ occur in $T$. For
example, let $\Sigma = \{\mathtt{a, b, c}\}$ be the alphabet. Then, the set of
MAWs for string $w = \mathtt{abaab}$ is $\{\mathtt{aaa, aaba, bab, bb, c}\}$.
In this paper, we study combinatorial properties of MAWs in the sliding window
model, namely, how the set of MAWs changes when a sliding window of fixed
length $d$ is shifted over the input string $T$ of length $n$, where $1 \leq d
&lt; n$. We present \emph{tight} upper and lower bounds on the maximum number of
changes in the set of MAWs for a sliding window over $T$, both in the cases of
general alphabets and binary alphabets. Our bounds improve on the previously
known best bounds [Crochemore et al., 2020].
</summary>
    <author>
      <name>Tooru Akagi</name>
    </author>
    <author>
      <name>Yuki Kuhara</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A part of the results of this article appeared in Proc. SOFSEM 2020.
  arXiv admin note: text overlap with arXiv:1909.02804</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.08496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.08496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.06166">
    <id>http://arxiv.org/abs/2105.06166v1</id>
    <updated>2021-05-13T09:51:51Z</updated>
    <published>2021-05-13T09:51:51Z</published>
    <title>The Dynamic k-Mismatch Problem</title>
    <summary>  The text-to-pattern Hamming distances problem asks to compute the Hamming
distances between a given pattern of length $m$ and all length-$m$ substrings
of a given text of length $n\ge m$. We focus on the $k$-mismatch version of the
problem, where a distance needs to be returned only if it does not exceed a
threshold $k$. We assume $n\le 2m$ (in general, one can partition the text into
overlapping blocks). In this work, we show data structures for the dynamic
version of this problem supporting two operations: An update performs a
single-letter substitution in the pattern or the text, and a query, given an
index $i$, returns the Hamming distance between the pattern and the text
substring starting at position $i$, or reports that it exceeds $k$.
  First, we show a data structure with $\tilde{O}(1)$ update and $\tilde{O}(k)$
query time. Then we show that $\tilde{O}(k)$ update and $\tilde{O}(1)$ query
time is also possible. These two provide an optimal trade-off for the dynamic
$k$-mismatch problem with $k \le \sqrt{n}$: we prove that, conditioned on the
strong 3SUM conjecture, one cannot simultaneously achieve $k^{1-\Omega(1)}$
time for all operations.
  For $k\ge \sqrt{n}$, we give another lower bound, conditioned on the Online
Matrix-Vector conjecture, that excludes algorithms taking $n^{1/2-\Omega(1)}$
time per operation. This is tight for constant-sized alphabets: Clifford et al.
(STACS 2018) achieved $\tilde{O}(\sqrt{n})$ time per operation in that case,
but with $\tilde{O}(n^{3/4})$ time per operation for large alphabets. We
improve and extend this result with an algorithm that, given $1\le x\le k$,
achieves update time $\tilde{O}(\frac{n}{k} +\sqrt{\frac{nk}{x}})$ and query
time $\tilde{O}(x)$. In particular, for $k\ge \sqrt{n}$, an appropriate choice
of $x$ yields $\tilde{O}(\sqrt[3]{nk})$ time per operation, which is
$\tilde{O}(n^{2/3})$ when no threshold $k$ is provided.
</summary>
    <author>
      <name>Raphaël Clifford</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Daniel P. Martin</name>
    </author>
    <author>
      <name>Przemysław Uznański</name>
    </author>
    <link href="http://arxiv.org/abs/2105.06166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.06166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.04802">
    <id>http://arxiv.org/abs/2105.04802v1</id>
    <updated>2021-05-11T06:29:06Z</updated>
    <published>2021-05-11T06:29:06Z</published>
    <title>Tree Edit Distance with Variables. Measuring the Similarity between
  Mathematical Formulas</title>
    <summary>  In this article, we propose tree edit distance with variables, which is an
extension of the tree edit distance to handle trees with variables and has a
potential application to measuring the similarity between mathematical
formulas, especially, those appearing in mathematical models of biological
systems. We analyze the computational complexities of several variants of this
new model. In particular, we show that the problem is NP-complete for ordered
trees. We also show for unordered trees that the problem of deciding whether or
not the distance is 0 is graph isomorphism complete but can be solved in
polynomial time if the maximum outdegree of input trees is bounded by a
constant. This distance model is then extended for measuring the
difference/similarity between two systems of differential equations, for which
results of preliminary computational experiments using biological models are
provided.
</summary>
    <author>
      <name>Tatsuya Akutsu</name>
    </author>
    <author>
      <name>Tomoya Mori</name>
    </author>
    <author>
      <name>Naotoshi Nakamura</name>
    </author>
    <author>
      <name>Satoshi Kozawa</name>
    </author>
    <author>
      <name>Yuhei Ueno</name>
    </author>
    <author>
      <name>Thomas N. Sato</name>
    </author>
    <link href="http://arxiv.org/abs/2105.04802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.04802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.04965">
    <id>http://arxiv.org/abs/2105.04965v1</id>
    <updated>2021-05-11T12:01:51Z</updated>
    <published>2021-05-11T12:01:51Z</published>
    <title>Compact Euler Tours of Trees with Small Maximum Degree</title>
    <summary>  We show how an Euler tour for a tree on $n$ vertices with maximum degree $d$
can be stored in $2 n + o (n)$ bits such that queries take $O (\log n)$ time
and updates take $O (d \log^{1 + \epsilon} n)$ time.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/2105.04965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.04965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.03782">
    <id>http://arxiv.org/abs/2105.03782v1</id>
    <updated>2021-05-08T21:24:55Z</updated>
    <published>2021-05-08T21:24:55Z</published>
    <title>Construction of Sparse Suffix Trees and LCE Indexes in Optimal Time and
  Space</title>
    <summary>  The notions of synchronizing and partitioning sets are recently introduced
variants of locally consistent parsings with great potential in
problem-solving. In this paper we propose a deterministic algorithm that
constructs for a given readonly string of length $n$ over the alphabet
$\{0,1,\ldots,n^{\mathcal{O}(1)}\}$ a version of $\tau$-partitioning set with
size $\mathcal{O}(b)$ and $\tau = \frac{n}{b}$ using $\mathcal{O}(b)$ space and
$\mathcal{O}(\frac{1}{\epsilon}n)$ time provided $b \ge n^\epsilon$, for
$\epsilon > 0$. As a corollary, for $b \ge n^\epsilon$ and constant $\epsilon >
0$, we obtain linear construction algorithms with $\mathcal{O}(b)$ space on top
of the string for two major small-space indexes: a sparse suffix tree, which is
a compacted trie built on $b$ chosen suffixes of the string, and a longest
common extension (LCE) index, which occupies $\mathcal{O}(b)$ space and allows
us to compute the longest common prefix for any pair of substrings in
$\mathcal{O}(n/b)$ time. For both, the $\mathcal{O}(b)$ construction storage is
asymptotically optimal since the tree itself takes $\mathcal{O}(b)$ space and
any LCE index with $\mathcal{O}(n/b)$ query time must occupy at least
$\mathcal{O}(b)$ space by a known trade-off (at least for $b \ge \Omega(n /
\log n)$). In case of arbitrary $b \ge \Omega(\log^2 n)$, we present
construction algorithms for the partitioning set, sparse suffix tree, and LCE
index with $\mathcal{O}(n\log_b n)$ running time and $\mathcal{O}(b)$ space,
thus also improving the state of the art.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Nikita Sivukhin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.03782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.03782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.03028">
    <id>http://arxiv.org/abs/2105.03028v1</id>
    <updated>2021-05-07T01:44:13Z</updated>
    <published>2021-05-07T01:44:13Z</published>
    <title>Improved Approximation for Longest Common Subsequence over Small
  Alphabets</title>
    <summary>  This paper investigates the approximability of the Longest Common Subsequence
(LCS) problem. The fastest algorithm for solving the LCS problem exactly runs
in essentially quadratic time in the length of the input, and it is known that
under the Strong Exponential Time Hypothesis the quadratic running time cannot
be beaten. There are no such limitations for the approximate computation of the
LCS however, except in some limited scenarios. There is also a scarcity of
approximation algorithms. When the two given strings are over an alphabet of
size $k$, returning the subsequence formed by the most frequent symbol
occurring in both strings achieves a $1/k$ approximation for the LCS. It is an
open problem whether a better than $1/k$ approximation can be achieved in truly
subquadratic time ($O(n^{2-\delta})$ time for constant $\delta>0$).
  A recent result [Rubinstein and Song SODA'2020] showed that a $1/2+\epsilon$
approximation for the LCS over a binary alphabet is possible in truly
subquadratic time, provided the input strings have the same length. In this
paper we show that if a $1/2+\epsilon$ approximation (for $\epsilon>0$) is
achievable for binary LCS in truly subquadratic time when the input strings can
be unequal, then for every constant $k$, there is a truly subquadratic time
algorithm that achieves a $1/k+\delta$ approximation for $k$-ary alphabet LCS
for some $\delta>0$. Thus the binary case is the hardest. We also show that for
every constant $k$, if one is given two strings of \emph{equal} length over a
$k$-ary alphabet, one can obtain a $1/k+\epsilon$ approximation for some
constant $\epsilon>0$ in truly subquadratic time, thus extending the Rubinstein
and Song result to all alphabets of constant size.
</summary>
    <author>
      <name>Shyan Akmal</name>
    </author>
    <author>
      <name>Virginia Vassilevska Williams</name>
    </author>
    <link href="http://arxiv.org/abs/2105.03028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.03028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.03106">
    <id>http://arxiv.org/abs/2105.03106v1</id>
    <updated>2021-05-07T08:19:25Z</updated>
    <published>2021-05-07T08:19:25Z</published>
    <title>Faster Algorithms for Longest Common Substring</title>
    <summary>  In the classic longest common substring (LCS) problem, we are given two
strings $S$ and $T$, each of length at most $n$, over an alphabet of size
$\sigma$, and we are asked to find a longest string occurring as a fragment of
both $S$ and $T$. Weiner, in his seminal paper that introduced the suffix tree,
presented an $\mathcal{O}(n \log \sigma)$-time algorithm for this problem [SWAT
1973]. For polynomially-bounded integer alphabets, the linear-time construction
of suffix trees by Farach yielded an $\mathcal{O}(n)$-time algorithm for the
LCS problem [FOCS 1997]. However, for small alphabets, this is not necessarily
optimal for the LCS problem in the word RAM model of computation, in which the
strings can be stored in $\mathcal{O}(n \log \sigma/\log n )$ space and read in
$\mathcal{O}(n \log \sigma/\log n )$ time. We show that, in this model, we can
compute an LCS in time $\mathcal{O}(n \log \sigma / \sqrt{\log n})$, which is
sublinear in $n$ if $\sigma=2^{o(\sqrt{\log n})}$ (in particular, if
$\sigma=\mathcal{O}(1)$), using optimal space $\mathcal{O}(n \log \sigma/\log
n)$.
  We then lift our ideas to the problem of computing a $k$-mismatch LCS, which
has received considerable attention in recent years. In this problem, the aim
is to compute a longest substring of $S$ that occurs in $T$ with at most $k$
mismatches. Thankachan et al.~showed how to compute a $k$-mismatch LCS in
$\mathcal{O}(n \log^k n)$ time for $k=\mathcal{O}(1)$ [J. Comput. Biol. 2016].
We show an $\mathcal{O}(n \log^{k-1/2} n)$-time algorithm, for any constant
$k>0$ and irrespective of the alphabet size, using $\mathcal{O}(n)$ space as
the previous approaches. We thus notably break through the well-known $n \log^k
n$ barrier, which stems from a recursive heavy-path decomposition technique
that was first introduced in the seminal paper of Cole et al. [STOC 2004] for
string indexing with $k$ errors.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <link href="http://arxiv.org/abs/2105.03106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.03106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.02428">
    <id>http://arxiv.org/abs/2105.02428v1</id>
    <updated>2021-05-06T03:54:27Z</updated>
    <published>2021-05-06T03:54:27Z</published>
    <title>Faster Algorithms for Bounded Tree Edit Distance</title>
    <summary>  Tree edit distance is a well-studied measure of dissimilarity between rooted
trees with node labels. It can be computed in $O(n^3)$ time [Demaine, Mozes,
Rossman, and Weimann, ICALP 2007], and fine-grained hardness results suggest
that the weighted version of this problem cannot be solved in truly subcubic
time unless the APSP conjecture is false [Bringmann, Gawrychowski, Mozes, and
Weimann, SODA 2018].
  We consider the unweighted version of tree edit distance, where every
insertion, deletion, or relabeling operation has unit cost. Given a parameter
$k$ as an upper bound on the distance, the previous fastest algorithm for this
problem runs in $O(nk^3)$ time [Touzet, CPM 2005], which improves upon the
cubic-time algorithm for $k\ll n^{2/3}$. In this paper, we give a faster
algorithm taking $O(nk^2 \log n)$ time, improving both of the previous results
for almost the full range of $\log n \ll k\ll n/\sqrt{\log n}$.
</summary>
    <author>
      <name>Shyan Akmal</name>
    </author>
    <author>
      <name>Ce Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICALP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.02428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.02428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.14903">
    <id>http://arxiv.org/abs/2105.14903v1</id>
    <updated>2021-05-31T12:06:09Z</updated>
    <published>2021-05-31T12:06:09Z</published>
    <title>Lower Bounds for the Number of Repetitions in 2D Strings</title>
    <summary>  A two-dimensional string is simply a two-dimensional array. We continue the
study of the combinatorial properties of repetitions in such strings over the
binary alphabet, namely the number of distinct tandems, distinct quartics, and
runs. First, we construct an infinite family of $n\times n$ 2D strings with
$\Omega(n^{3})$ distinct tandems. Second, we construct an infinite family of
$n\times n$ 2D strings with $\Omega(n^{2}\log n)$ distinct quartics. Third, we
construct an infinite family of $n\times n$ 2D strings with $\Omega(n^{2}\log
n)$ runs. This resolves an open question of Charalampopoulos, Radoszewski,
Rytter, Wale\'n, and Zuba [ESA 2020], who asked if the number of distinct
quartics and runs in an $n\times n$ 2D string is $\mathcal{O}(n^{2})$.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Samah Ghazawi</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <link href="http://arxiv.org/abs/2105.14903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.14903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.14990">
    <id>http://arxiv.org/abs/2105.14990v1</id>
    <updated>2021-05-31T14:22:20Z</updated>
    <published>2021-05-31T14:22:20Z</published>
    <title>A new distance based on minimal absent words and applications to
  biological sequences</title>
    <summary>  A minimal absent word of a sequence x, is a sequence yt hat is not a factorof
x, but all of its proper factors are factors of x as well. The set of minimal
absent words uniquely defines the sequence itself. In recent times minimal
absent words have been used in order to compare sequences. In fact, to do this,
one can compare the sets of their minimal absent words. Chairungasee and
Crochemorein [2] define a distance between pairs of sequences x and y, where
the symmetric difference of the sets of minimal absent words of x and y is
involved. Here, weconsider a different distance, introduced in [1], based on a
specific subset of such symmetric difference that, in our opinion, better
capture the different features ofthe considered sequences. We show the result
of some experiments where the distance is tested on a dataset of genetic
sequences by 11 living species, in order to compare the new distance with the
ones existing in literature.
</summary>
    <author>
      <name>Giuseppa Castiglione</name>
    </author>
    <author>
      <name>Jia Gao</name>
    </author>
    <author>
      <name>Sabrina Mantaci</name>
    </author>
    <author>
      <name>Antonio Restivo</name>
    </author>
    <link href="http://arxiv.org/abs/2105.14990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.14990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.13595">
    <id>http://arxiv.org/abs/2105.13595v1</id>
    <updated>2021-05-28T05:24:53Z</updated>
    <published>2021-05-28T05:24:53Z</published>
    <title>On Stricter Reachable Repetitiveness Measures*</title>
    <summary>  The size $b$ of the smallest bidirectional macro scheme, which is arguably
the most general copy-paste scheme to generate a given sequence, is considered
to be the strictest reachable measure of repetitiveness. It is strictly
lower-bounded by measures like $\gamma$ and $\delta$, which are known or
believed to be unreachable and to capture the entropy of repetitiveness. In
this paper we study another sequence generation mechanism, namely compositions
of a morphism. We show that these form another plausible mechanism to
characterize repetitive sequences and define NU-systems, which combine such a
mechanism with macro schemes. We show that the size $\nu \leq b$ of the
smallest NU-system is reachable and can be $o(\delta)$ for some string
families, thereby implying that the limit of compressibility of repetitive
sequences can be even smaller than previously thought. We also derive several
other results characterizing $\nu$.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Chile</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CeBiB</arxiv:affiliation>
    </author>
    <author>
      <name>Cristian Urbina</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Chile</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CeBiB</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Funded in part by Basal Funds FB0001, Fondecyt Grant 1-200038, and a
  Conicyt Doctoral Scholarship, ANID, Chile</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.13595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.13595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.13744">
    <id>http://arxiv.org/abs/2105.13744v1</id>
    <updated>2021-05-28T11:15:05Z</updated>
    <published>2021-05-28T11:15:05Z</published>
    <title>Grammar Index By Induced Suffix Sorting</title>
    <summary>  Pattern matching is the most central task for text indices. Most recent
indices leverage compression techniques to make pattern matching feasible for
massive but highly-compressible datasets. Within this kind of indices, we
propose a new compressed text index built upon a grammar compression based on
induced suffix sorting [Nunes et al., DCC'18]. We show that this grammar
exhibits a locality sensitive parsing property, which allows us to specify,
given a pattern $P$, certain substrings of $P$, called cores, that are
similarly parsed in the text grammar whenever these occurrences are extensible
to occurrences of $P$. Supported by the cores, given a pattern of length $m$,
we can locate all its $occ$ occurrences in a text $T$ of length $n$ within $O(m
\lg |\mathcal{S}| + occ_C \lg|\mathcal{S}| \lg n + occ)$ time, where
$\mathcal{S}$ is the set of all characters and non-terminals, $occ$ is the
number of occurrences, and $occ_C$ is the number of occurrences of a chosen
core $C$ of $P$ in the right hand side of all production rules of the grammar
of $T$. Our grammar index requires $O(g)$ words of space and can be built in
$O(n)$ time using $O(g)$ working space, where $g$ is the sum of the right hand
sides of all production rules. We underline the strength of our grammar index
with an exhaustive practical evaluation that gives evidence that our proposed
solution excels at locating long patterns in highly-repetitive texts.
</summary>
    <author>
      <name>Tooru Akagi</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Our implementation is available at
  https://github.com/TooruAkagi/GCIS_Index</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.13744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.13744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.11693">
    <id>http://arxiv.org/abs/2105.11693v1</id>
    <updated>2021-05-25T06:22:04Z</updated>
    <published>2021-05-25T06:22:04Z</published>
    <title>Minimal unique palindromic substrings after single-character
  substitution</title>
    <summary>  A palindrome is a string that reads the same forward and backward. A
palindromic substring $w$ of a string $T$ is called a minimal unique
palindromic substring (MUPS) of $T$ if $w$ occurs only once in $T$ and any
proper palindromic substring of $w$ occurs at least twice in $T$. MUPSs are
utilized for answering the shortest unique palindromic substring problem, which
is motivated by molecular biology [Inoue et al., 2018]. Given a string $T$ of
length $n$, all MUPSs of $T$ can be computed in $O(n)$ time. In this paper, we
study the problem of updating the set of MUPSs when a character in the input
string $T$ is substituted by another character. We first analyze the number $d$
of changes of MUPSs when a character is substituted, and show that $d$ is in
$O(\log n)$. Further, we present an algorithm that uses $O(n)$ time and space
for preprocessing, and updates the set of MUPSs in $O(\log\sigma + (\log\log
n)^2 + d)$ time where $\sigma$ is the alphabet size. We also propose a variant
of the algorithm, which runs in optimal $O(d)$ time when the alphabet size is
constant.
</summary>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <link href="http://arxiv.org/abs/2105.11693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.10622">
    <id>http://arxiv.org/abs/2105.10622v1</id>
    <updated>2021-05-22T02:33:34Z</updated>
    <published>2021-05-22T02:33:34Z</published>
    <title>Support Optimality and Adaptive Cuckoo Filters</title>
    <summary>  Filters (such as Bloom Filters) are data structures that speed up network
routing and measurement operations by storing a compressed representation of a
set. Filters are space efficient, but can make bounded one-sided errors: with
tunable probability epsilon, they may report that a query element is stored in
the filter when it is not. This is called a false positive. Recent research has
focused on designing methods for dynamically adapting filters to false
positives, reducing the number of false positives when some elements are
queried repeatedly.
  Ideally, an adaptive filter would incur a false positive with bounded
probability epsilon for each new query element, and would incur o(epsilon)
total false positives over all repeated queries to that element. We call such a
filter support optimal.
  In this paper we design a new Adaptive Cuckoo Filter and show that it is
support optimal (up to additive logarithmic terms) over any n queries when
storing a set of size n. Our filter is simple: fixing previous false positives
requires a simple cuckoo operation, and the filter does not need to store any
additional metadata. This data structure is the first practical data structure
that is support optimal, and the first filter that does not require additional
space to fix false positives.
  We complement these bounds with experiments showing that our data structure
is effective at fixing false positives on network traces, outperforming
previous Adaptive Cuckoo Filters.
  Finally, we investigate adversarial adaptivity, a stronger notion of
adaptivity in which an adaptive adversary repeatedly queries the filter, using
the result of previous queries to drive the false positive rate as high as
possible. We prove a lower bound showing that a broad family of filters,
including all known Adaptive Cuckoo Filters, can be forced by such an adversary
to incur a large number of false positives.
</summary>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Samuel McCauley</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <link href="http://arxiv.org/abs/2105.10622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.10622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2105.11052">
    <id>http://arxiv.org/abs/2105.11052v1</id>
    <updated>2021-05-24T00:27:53Z</updated>
    <published>2021-05-24T00:27:53Z</published>
    <title>Fast and Space-Efficient Construction of AVL Grammars from the LZ77
  Parsing</title>
    <summary>  Grammar compression is, next to Lempel-Ziv (LZ77) and run-length
Burrows-Wheeler transform (RLBWT), one of the most flexible approaches to
representing and processing highly compressible strings. The main idea is to
represent a text as a context-free grammar whose language is precisely the
input string. This is called a straight-line grammar (SLG). An AVL grammar,
proposed by Rytter [Theor. Comput. Sci., 2003] is a type of SLG that
additionally satisfies the AVL-property: the heights of parse-trees for
children of every nonterminal differ by at most one. In contrast to other SLG
constructions, AVL grammars can be constructed from the LZ77 parsing in
compressed time: $\mathcal{O}(z \log n)$ where $z$ is the size of the LZ77
parsing and $n$ is the length of the input text. Despite these advantages, AVL
grammars are thought to be too large to be practical.
  We present a new technique for rapidly constructing a small AVL grammar from
an LZ77 or LZ77-like parse. Our algorithm produces grammars that are always at
least five times smaller than those produced by the original algorithm, and
never more than double the size of grammars produced by the practical Re-Pair
compressor [Larsson and Moffat, Proc. IEEE, 2000]. Our algorithm also achieves
low peak RAM usage. By combining this algorithm with recent advances in
approximating the LZ77 parsing, we show that our method has the potential to
construct a run-length BWT from an LZ77 parse in about one third of the time
and peak RAM required by other approaches. Overall, we show that AVL grammars
are surprisingly practical, opening the door to much faster construction of key
compressed data structures.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Ben Langmead</name>
    </author>
    <link href="http://arxiv.org/abs/2105.11052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.05123">
    <id>http://arxiv.org/abs/2106.05123v1</id>
    <updated>2021-06-09T15:00:29Z</updated>
    <published>2021-06-09T15:00:29Z</published>
    <title>Pattern-defeating Quicksort</title>
    <summary>  A new solution for the Dutch national flag problem is proposed, requiring no
three-way comparisons, which gives quicksort a proper worst-case runtime of
$O(nk)$ for inputs with $k$ distinct elements. This is used together with other
known and novel techniques to construct a hybrid sort that is never
significantly slower than regular quicksort while speeding up drastically for
many input distributions.
</summary>
    <author>
      <name>Orson R. L. Peters</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.05123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.05123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.03202">
    <id>http://arxiv.org/abs/2106.03202v1</id>
    <updated>2021-06-06T18:20:58Z</updated>
    <published>2021-06-06T18:20:58Z</published>
    <title>Closed Ziv-Lempel factorization of the $m$-bonacci words</title>
    <summary>  A word $w$ is said to be closed if it has a proper factor $x$ which occurs
exactly twice in $w$, as a prefix and as a suffix of $w$. Based on the concept
of Ziv-Lempel factorization, we define the closed $z$-factorization of finite
and infinite words. Then we find the closed $z$-factorization of the infinite
$m$-bonacci words for all $m \geq 2$. We also classify closed prefixes of the
infinite $m$-bonacci words.
</summary>
    <author>
      <name>Marieh Jahannia</name>
    </author>
    <author>
      <name>Morteza Mohammad-noori</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <author>
      <name>Manon Stipulanti</name>
    </author>
    <link href="http://arxiv.org/abs/2106.03202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.03202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.01190">
    <id>http://arxiv.org/abs/2106.01190v2</id>
    <updated>2021-07-13T11:31:26Z</updated>
    <published>2021-06-02T14:32:55Z</published>
    <title>Counting Lyndon Subsequences</title>
    <summary>  Counting substrings/subsequences that preserve some property (e.g.,
palindromes, squares) is an important mathematical interest in stringology.
Recently, Glen et al. studied the number of Lyndon factors in a string. A
string $w = uv$ is called a Lyndon word if it is the lexicographically smallest
among all of its conjugates $vu$. In this paper, we consider a more general
problem "counting Lyndon subsequences". We show (1) the maximum total number of
Lyndon subsequences in a string, (2) the expected total number of Lyndon
subsequences in a string, (3) the expected number of distinct Lyndon
subsequences in a string.
</summary>
    <author>
      <name>Ryo Hirakawa</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2106.01190v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.01190v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.02309">
    <id>http://arxiv.org/abs/2106.02309v1</id>
    <updated>2021-06-04T07:41:58Z</updated>
    <published>2021-06-04T07:41:58Z</published>
    <title>On (co-lex) Ordering Automata</title>
    <summary>  The states of a deterministic finite automaton A can be identified with
collections of words in Pf(L(A)) -- the set of prefixes of words belonging to
the regular language accepted by A. But words can be ordered and among the many
possible orders a very natural one is the co-lexicographic one. Such
naturalness stems from the fact that it suggests a transfer of the order from
words to the automaton's states. In a number of papers automata admitting a
total ordering of states coherent with the ordering of the set of words
reaching them have been proposed. Such class of ordered automata -- the Wheeler
automata -- turned out to be efficiently stored/searched using an index.
Unfortunately not all automata can be totally ordered as previously outlined.
However, automata can always be partially ordered and an intrinsic measure of
their complexity can be defined and effectively determined, as the minimum
width of one of their admissible partial orders. As shown in previous works,
this new concept of width of an automaton has useful consequences in the fields
of graph compression, indexing data structures, and automata theory. In this
paper we prove that a canonical, minimum-width, partially-ordered automaton
accepting a language L -- dubbed the Hasse automaton H of L -- can be
exhibited. H provides, in a precise sense, the best possible way to (partially)
order the states of any automaton accepting L, as long as we want to maintain
an operational link with the (co-lexicographic) order of Pf(L(A)). Using H we
prove that the width of the language can be effectively computed from the
minimum automaton recognizing the language. Finally, we explore the
relationship between two (often conflicting) objectives: minimizing the width
and minimizing the number of states of an automaton.
</summary>
    <author>
      <name>Giovanna D'Agostino</name>
    </author>
    <author>
      <name>Nicola Cotumaccio</name>
    </author>
    <author>
      <name>Alberto Policriti</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/2106.02309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.02309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.02350">
    <id>http://arxiv.org/abs/2106.02350v1</id>
    <updated>2021-06-04T09:02:36Z</updated>
    <published>2021-06-04T09:02:36Z</published>
    <title>Parallel and External-Memory Construction of Minimal Perfect Hash
  Functions with PTHash</title>
    <summary>  A minimal perfect hash function $f$ for a set $S$ of $n$ keys is a bijective
function of the form $f : S \rightarrow \{0,\ldots,n-1\}$. These functions are
important for many practical applications in computing, such as search engines,
computer networks, and databases. Several algorithms have been proposed to
build minimal perfect hash functions that: scale well to large sets, retain
fast evaluation time, and take very little space, e.g., 2 - 3 bits/key. PTHash
is one such algorithm, achieving very fast evaluation in compressed space,
typically several times faster than other techniques. In this work, we propose
a new construction algorithm for PTHash enabling: (1) multi-threading, to
either build functions more quickly or more space-efficiently, and (2)
external-memory processing to scale to inputs much larger than the available
internal memory. Only few other algorithms in the literature share these
features, despite of their big practical impact. We conduct an extensive
experimental assessment on large real-world string collections and show that,
with respect to other techniques, PTHash is competitive in construction time
and space consumption, but retains 2 - 6$\times$ better lookup time.
</summary>
    <author>
      <name>Giulio Ermanno Pibiri</name>
    </author>
    <author>
      <name>Roberto Trani</name>
    </author>
    <link href="http://arxiv.org/abs/2106.02350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.02350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.01595">
    <id>http://arxiv.org/abs/2106.01595v2</id>
    <updated>2021-08-14T09:10:08Z</updated>
    <published>2021-06-03T04:53:23Z</published>
    <title>Position Heaps for Cartesian-tree Matching on Strings and Tries</title>
    <summary>  The Cartesian-tree pattern matching is a recently introduced scheme of
pattern matching that detects fragments in a sequential data stream which have
a similar structure as a query pattern. Formally, Cartesian-tree pattern
matching seeks all substrings $S'$ of the text string $S$ such that the
Cartesian tree of $S'$ and that of a query pattern $P$ coincide. In this paper,
we present a new indexing structure for this problem called the Cartesian-tree
Position Heap (CPH). Let $n$ be the length of the input text string $S$, $m$
the length of a query pattern $P$, and $\sigma$ the alphabet size. We show that
the CPH of $S$, denoted $\mathsf{CPH}(S)$, supports pattern matching queries in
$O(m (\sigma + \log (\min\{h, m\})) + occ)$ time with $O(n)$ space, where $h$
is the height of the CPH and $occ$ is the number of pattern occurrences. We
show how to build $\mathsf{CPH}(S)$ in $O(n \log \sigma)$ time with $O(n)$
working space. Further, we extend the problem to the case where the text is a
labeled tree (i.e. a trie). Given a trie $T$ with $N$ nodes, we show that the
CPH of $T$, denoted $\mathsf{CPH}(T)$, supports pattern matching queries on the
trie in $O(m (\sigma^2 + \log (\min\{h, m\})) + occ)$ time with $O(N \sigma)$
space. We also show a construction algorithm for $\mathsf{CPH}(T)$ running in
$O(N \sigma)$ time and $O(N \sigma)$ working space.
</summary>
    <author>
      <name>Akio Nishimoto</name>
    </author>
    <author>
      <name>Noriki Fujisato</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <link href="http://arxiv.org/abs/2106.01595v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.01595v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.01763">
    <id>http://arxiv.org/abs/2106.01763v1</id>
    <updated>2021-06-03T11:32:50Z</updated>
    <published>2021-06-03T11:32:50Z</published>
    <title>Internal Shortest Absent Word Queries in Constant Time and Linear Space</title>
    <summary>  Given a string $T$ of length $n$ over an alphabet $\Sigma\subset
\{1,2,\ldots,n^{O(1)}\}$ of size $\sigma$, we are to preprocess $T$ so that
given a range $[i,j]$, we can return a representation of a shortest string over
$\Sigma$ that is absent in the fragment $T[i]\cdots T[j]$ of $T$. We present an
$O(n)$-space data structure that answers such queries in constant time and can
be constructed in $O(n\log_\sigma n)$ time.
</summary>
    <author>
      <name>Golnaz Badkobeh</name>
    </author>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.01763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.01763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.02026">
    <id>http://arxiv.org/abs/2106.02026v3</id>
    <updated>2021-10-12T20:37:27Z</updated>
    <published>2021-06-03T17:50:33Z</published>
    <title>Breaking the Cubic Barrier for (Unweighted) Tree Edit Distance</title>
    <summary>  The (unweighted) tree edit distance problem for $n$ node trees asks to
compute a measure of dissimilarity between two rooted trees with node labels.
The current best algorithm from more than a decade ago runs in $O(n ^ 3)$ time
[Demaine, Mozes, Rossman, and Weimann, ICALP 2007]. The same paper also showed
that $O(n ^ 3)$ is the best possible running time for any algorithm using the
so-called decomposition strategy, which underlies almost all the known
algorithms for this problem. These algorithms would also work for the weighted
tree edit distance problem, which cannot be solved in truly sub-cubic time
under the APSP conjecture [Bringmann, Gawrychowski, Mozes, and Weimann, SODA
2018]. In this paper, we break the cubic barrier by showing an $O(n ^
{2.9546})$ time algorithm for the unweighted tree edit distance problem. We
consider an equivalent maximization problem and use a dynamic programming
scheme involving matrices with many special properties. By using a
decomposition scheme as well as several combinatorial techniques, we reduce
tree edit distance to the max-plus product of bounded-difference matrices,
which can be solved in truly sub-cubic time [Bringmann, Grandoni, Saha, and
Vassilevska Williams, FOCS 2016].
</summary>
    <author>
      <name>Xiao Mao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to FOCS'21</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.02026v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.02026v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.01173">
    <id>http://arxiv.org/abs/2106.01173v2</id>
    <updated>2021-08-16T02:40:40Z</updated>
    <published>2021-06-02T14:11:12Z</published>
    <title>On the approximation ratio of LZ-End to LZ77</title>
    <summary>  A family of Lempel-Ziv factorizations is a well-studied string structure. The
LZ-End factorization is a member of the family that achieved faster extraction
of any substrings (Kreft &amp; Navarro, TCS 2013). One of the interests for LZ-End
factorizations is the possible difference between the size of LZ-End and LZ77
factorizations. They also showed families of strings where the approximation
ratio of the number of LZ-End phrases to the number of LZ77 phrases
asymptotically approaches 2. However, the alphabet size of these strings is
unbounded. In this paper, we analyze the LZ-End factorization of the
period-doubling sequence. We also show that the approximation ratio for the
period-doubling sequence asymptotically approaches 2 for the binary alphabet.
</summary>
    <author>
      <name>Takumi Ideue</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2106.01173v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.01173v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2110.09696">
    <id>http://arxiv.org/abs/2110.09696v2</id>
    <updated>2021-10-20T23:47:57Z</updated>
    <published>2021-10-19T02:32:18Z</published>
    <title>Near-Optimal Quantum Algorithms for String Problems</title>
    <summary>  We study quantum algorithms for several fundamental string problems,
including Longest Common Substring, Lexicographically Minimal String Rotation,
and Longest Square Substring. These problems have been widely studied in the
stringology literature since the 1970s, and are known to be solvable by
near-linear time classical algorithms. In this work, we give quantum algorithms
for these problems with near-optimal query complexities and time complexities.
Specifically, we show that:
  - Longest Common Substring can be solved by a quantum algorithm in $\tilde
O(n^{2/3})$ time, improving upon the recent $\tilde O(n^{5/6})$-time algorithm
by Le Gall and Seddighin (2020). Our algorithm uses the MNRS quantum walk
framework, together with a careful combination of string synchronizing sets
(Kempa and Kociumaka, 2019) and generalized difference covers.
  - Lexicographically Minimal String Rotation can be solved by a quantum
algorithm in $n^{1/2 + o(1)}$ time, improving upon the recent $\tilde
O(n^{3/4})$-time algorithm by Wang and Ying (2020). We design our algorithm by
first giving a new classical divide-and-conquer algorithm in near-linear time
based on exclusion rules, and then speeding it up quadratically using nested
Grover search and quantum minimum finding.
  - Longest Square Substring can be solved by a quantum algorithm in $\tilde
O(\sqrt{n})$ time. Our algorithm is an adaptation of the algorithm by Le Gall
and Seddighin (2020) for the Longest Palindromic Substring problem, but uses
additional techniques to overcome the difficulty that binary search no longer
applies.
  Our techniques naturally extend to other related string problems, such as
Longest Repeated Substring, Longest Lyndon Substring, and Minimal Suffix.
</summary>
    <author>
      <name>Shyan Akmal</name>
    </author>
    <author>
      <name>Ce Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in SODA 2022. Fixed cleveref issues</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.09696v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.09696v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.06249">
    <id>http://arxiv.org/abs/2106.06249v1</id>
    <updated>2021-06-11T09:00:37Z</updated>
    <published>2021-06-11T09:00:37Z</published>
    <title>Matching Patterns with Variables under Hamming Distance</title>
    <summary>  A pattern $\alpha$ is a string of variables and terminal letters. We say that
$\alpha$ matches a word $w$, consisting only of terminal letters, if $w$ can be
obtained by replacing the variables of $\alpha$ by terminal words. The matching
problem, i.e., deciding whether a given pattern matches a given word, was
heavily investigated: it is NP-complete in general, but can be solved
efficiently for classes of patterns with restricted structure. In this paper,
we approach this problem in a generalized setting, by considering approximate
pattern matching under Hamming distance. More precisely, we are interested in
what is the minimum Hamming distance between $w$ and any word $u$ obtained by
replacing the variables of $\alpha$ by terminal words. Firstly, we address the
class of regular patterns (in which no variable occurs twice) and propose
efficient algorithms for this problem, as well as matching conditional lower
bounds. We show that the problem can still be solved efficiently if we allow
repeated variables, but restrict the way the different variables can be
interleaved according to a locality parameter. However, as soon as we allow a
variable to occur more than once and its occurrences can be interleaved
arbitrarily with those of other variables, even if none of them occurs more
than once, the problem becomes intractable.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Stefan Siemer</name>
    </author>
    <link href="http://arxiv.org/abs/2106.06249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.06249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2107.00446">
    <id>http://arxiv.org/abs/2107.00446v1</id>
    <updated>2021-07-01T13:45:48Z</updated>
    <published>2021-07-01T13:45:48Z</published>
    <title>Compression by Contracting Straight-Line Programs</title>
    <summary>  In grammar-based compression a string is represented by a context-free
grammar, also called a straight-line program (SLP), that generates only that
string. We refine a recent balancing result stating that one can transform an
SLP of size $g$ in linear time into an equivalent SLP of size $O(g)$ so that
the height of the unique derivation tree is $O(\log N)$ where $N$ is the length
of the represented string (FOCS 2019). We introduce a new class of balanced
SLPs, called contracting SLPs, where for every rule $A \to \beta_1 \dots
\beta_k$ the string length of every variable $\beta_i$ on the right-hand side
is smaller by a constant factor than the string length of $A$. In particular,
the derivation tree of a contracting SLP has the property that every subtree
has logarithmic height in its leaf size. We show that a given SLP of size $g$
can be transformed in linear time into an equivalent contracting SLP of size
$O(g)$ with rules of constant length.
  We present an application to the navigation problem in compressed unranked
trees, represented by forest straight-line programs (FSLPs). We extend a linear
space data structure by Reh and Sieber (2020) by the operation of moving to the
$i$-th child in time $O(\log d)$ where $d$ is the degree of the current node.
Contracting SLPs are also applied to the finger search problem over
SLP-compressed strings where one wants to access positions near to a
pre-specified finger position, ideally in $O(\log d)$ time where $d$ is the
distance between the accessed position and the finger. We give a linear space
solution where one can access symbols or move the finger in time $O(\log d +
\log^{(t)} N)$ for any constant $t$ where $\log^{(t)} N$ is the $t$-fold
logarithm of $N$. This improves a previous solution by Bille, Christiansen,
Cording, and G{\o}rtz (2018) with access/move time $O(\log d + \log \log N)$.
</summary>
    <author>
      <name>Moses Ganardi</name>
    </author>
    <link href="http://arxiv.org/abs/2107.00446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.00446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.16173">
    <id>http://arxiv.org/abs/2106.16173v1</id>
    <updated>2021-06-30T16:09:29Z</updated>
    <published>2021-06-30T16:09:29Z</published>
    <title>String Comparison on a Quantum Computer Using Hamming Distance</title>
    <summary>  The Hamming distance is ubiquitous in computing. Its computation gets
expensive when one needs to compare a string against many strings. Quantum
computers (QCs) may speed up the comparison.
  In this paper, we extend an existing algorithm for computing the Hamming
distance. The extension can compare strings with symbols drawn from an
arbitrary-long alphabet (which the original algorithm could not). We implement
our extended algorithm using the QisKit framework to be executed by a
programmer without the knowledge of a QC (the code is publicly available). We
then provide four pedagogical examples: two from the field of bioinformatics
and two from the field of software engineering. We finish by discussing
resource requirements and the time horizon of the QCs becoming practical for
string comparison.
</summary>
    <author>
      <name>Mushahid Khan</name>
    </author>
    <author>
      <name>Andriy Miranskyy</name>
    </author>
    <link href="http://arxiv.org/abs/2106.16173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.16173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.12189">
    <id>http://arxiv.org/abs/2106.12189v1</id>
    <updated>2021-06-23T06:30:00Z</updated>
    <published>2021-06-23T06:30:00Z</published>
    <title>A Bloom Filter Survey: Variants for Different Domain Applications</title>
    <summary>  There is a plethora of data structures, algorithms, and frameworks dealing
with major data-stream problems like estimating the frequency of items,
answering set membership, association and multiplicity queries, and several
other statistics that can be extracted from voluminous data streams. In this
survey, we are focusing on exploring randomized data structures called Bloom
Filters. This data structure answers whether an item exists or not in a data
stream with a false positive probability fpp. In this survey, many variants of
the Bloom filter will be covered by showing the strengths of each structure and
its drawbacks i.e. some Bloom filters deal with insertion and deletions and
others don't, some variants use the memory efficiently but increase the fpp
where others pay the trade-off in the reversed way. Furthermore, in each Bloom
filter structure, the false positive probability will be highlighted alongside
the most important technical details showing the improvement it is presenting,
while the main aim of this work is to provide an overall comparison between the
variants of the Bloom filter structure according to the application domain that
it fits in.
</summary>
    <author>
      <name>Anes Abdennebi</name>
    </author>
    <author>
      <name>Kamer Kaya</name>
    </author>
    <link href="http://arxiv.org/abs/2106.12189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.12189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="ACM-class: E.1, E.2, A.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.12725">
    <id>http://arxiv.org/abs/2106.12725v1</id>
    <updated>2021-06-24T02:14:13Z</updated>
    <published>2021-06-24T02:14:13Z</published>
    <title>Breaking the $O(n)$-Barrier in the Construction of Compressed Suffix
  Arrays</title>
    <summary>  The suffix array, describing the lexicographic order of suffixes of a given
text, is the central data structure in string algorithms. The suffix array of a
length-$n$ text uses $\Theta(n \log n)$ bits, which is prohibitive in many
applications. To address this, Grossi and Vitter [STOC 2000] and,
independently, Ferragina and Manzini [FOCS 2000] introduced space-efficient
versions of the suffix array, known as the compressed suffix array (CSA) and
the FM-index. For a length-$n$ text over an alphabet of size $\sigma$, these
data structures use only $O(n \log \sigma)$ bits. Immediately after their
discovery, they almost completely replaced plain suffix arrays in practical
applications, and a race started to develop efficient construction procedures.
Yet, after more than 20 years, even for $\sigma=2$, the fastest algorithm
remains stuck at $O(n)$ time [Hon et al., FOCS 2003], which is slower by a
$\Theta(\log n)$ factor than the lower bound of $\Omega(n / \log n)$ (following
simply from the necessity to read the entire input). We break this
long-standing barrier with a new data structure that takes $O(n \log \sigma)$
bits, answers suffix array queries in $O(\log^{\epsilon} n)$ time, and can be
constructed in $O(n\log \sigma / \sqrt{\log n})$ time using $O(n\log \sigma)$
bits of space. Our result is based on several new insights into the recently
developed notion of string synchronizing sets [STOC 2019]. In particular,
compared to their previous applications, we eliminate orthogonal range queries,
replacing them with new queries that we dub prefix rank and prefix selection
queries. As a further demonstration of our techniques, we present a new
pattern-matching index that simultaneously minimizes the construction time and
the query time among all known compact indexes (i.e., those using $O(n \log
\sigma)$ bits).
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.12725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.12725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.10541">
    <id>http://arxiv.org/abs/2106.10541v2</id>
    <updated>2021-07-23T09:36:31Z</updated>
    <published>2021-06-19T17:41:30Z</published>
    <title>Checking whether a word is Hamming-isometric in linear time</title>
    <summary>  A finite word $f$ is Hamming-isometric if for any two word $u$ and $v$ of
same length avoiding $f$, $u$ can be transformed into $v$ by changing one by
one all the letters on which $u$ differs from $v$, in such a way that all of
the new words obtained in this process also avoid~$f$. Words which are not
Hamming-isometric have been characterized as words having a border with two
mismatches. We derive from this characterization a linear-time algorithm to
check whether a word is Hamming-isometric. It is based on pattern matching
algorithms with $k$ mismatches. Lee-isometric words over a four-letter alphabet
have been characterized as words having a border with two Lee-errors. We derive
from this characterization a linear-time algorithm to check whether a word over
an alphabet of size four is Lee-isometric.
</summary>
    <author>
      <name>Marie-Pierre Béal</name>
    </author>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A second algorithm for checking whether a word is Hamming-isometric
  is added using the result given in reference [5]</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.10541v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.10541v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.11191">
    <id>http://arxiv.org/abs/2106.11191v1</id>
    <updated>2021-06-21T15:29:13Z</updated>
    <published>2021-06-21T15:29:13Z</published>
    <title>Computing the original eBWT faster, simpler, and with less memory</title>
    <summary>  Mantaci et al. [TCS 2007] defined the eBWT to extend the definition of the
BWT to a collection of strings, however, since this introduction, it has been
used more generally to describe any BWT of a collection of strings and the
fundamental property of the original definition (i.e., the independence from
the input order) is frequently disregarded. In this paper, we propose a simple
linear-time algorithm for the construction of the original eBWT, which does not
require the preprocessing of Bannai et al. [CPM 2021]. As a byproduct, we
obtain the first linear-time algorithm for computing the BWT of a single string
that uses neither an end-of-string symbol nor Lyndon rotations. We combine our
new eBWT construction with a variation of prefix-free parsing to allow for
scalable construction of the eBWT. We evaluate our algorithm (pfpebwt) on sets
of human chromosomes 19, Salmonella, and SARS-CoV2 genomes, and demonstrate
that it is the fastest method for all collections, with a maximum speedup of
7.6x on the second best method. The peak memory is at most 2x larger than the
second best method. Comparing with methods that are also, as our algorithm,
able to report suffix array samples, we obtain a 57.1x improvement in peak
memory. The source code is publicly available at
https://github.com/davidecenzato/PFP-eBWT.
</summary>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Davide Cenzato</name>
    </author>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 5 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.11191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.06037">
    <id>http://arxiv.org/abs/2106.06037v1</id>
    <updated>2021-06-10T20:32:20Z</updated>
    <published>2021-06-10T20:32:20Z</published>
    <title>Small space and streaming pattern matching with k edits</title>
    <summary>  In this work, we revisit the fundamental and well-studied problem of
approximate pattern matching under edit distance. Given an integer $k$, a
pattern $P$ of length $m$, and a text $T$ of length $n \ge m$, the task is to
find substrings of $T$ that are within edit distance $k$ from $P$. Our main
result is a streaming algorithm that solves the problem in $\tilde{O}(k^5)$
space and $\tilde{O}(k^8)$ amortised time per character of the text, providing
answers correct with high probability. (Hereafter, $\tilde{O}(\cdot)$ hides a
$\mathrm{poly}(\log n)$ factor.) This answers a decade-old question: since the
discovery of a $\mathrm{poly}(k\log n)$-space streaming algorithm for pattern
matching under Hamming distance by Porat and Porat [FOCS 2009], the existence
of an analogous result for edit distance remained open. Up to this work, no
$\mathrm{poly}(k\log n)$-space algorithm was known even in the simpler
semi-streaming model, where $T$ comes as a stream but $P$ is available for
read-only access. In this model, we give a deterministic algorithm that
achieves slightly better complexity.
  In order to develop the fully streaming algorithm, we introduce a new edit
distance sketch parametrised by integers $n\ge k$. For any string of length at
most $n$, the sketch is of size $\tilde{O}(k^2)$ and it can be computed with an
$\tilde{O}(k^2)$-space streaming algorithm. Given the sketches of two strings,
in $\tilde{O}(k^3)$ time we can compute their edit distance or certify that it
is larger than $k$. This result improves upon $\tilde{O}(k^8)$-size sketches of
Belazzougui and Zhu [FOCS 2016] and very recent $\tilde{O}(k^3)$-size sketches
of Jin, Nelson, and Wu [STACS 2021].
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <link href="http://arxiv.org/abs/2106.06037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.06037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2107.04660">
    <id>http://arxiv.org/abs/2107.04660v1</id>
    <updated>2021-07-09T20:25:40Z</updated>
    <published>2021-07-09T20:25:40Z</published>
    <title>Optimal Space and Time for Streaming Pattern Matching</title>
    <summary>  In this work, we study longest common substring, pattern matching, and
wildcard pattern matching in the asymmetric streaming model. In this streaming
model, we have random access to one string and streaming access to the other
one. We present streaming algorithms with provable guarantees for these three
fundamental problems. In particular, our algorithms for pattern matching
improve the upper bound and beat the unconditional lower bounds on the memory
of randomized and deterministic streaming algorithms. In addition to this, we
present algorithms for wildcard pattern matching in the asymmetric streaming
model that have optimal space and time.
</summary>
    <author>
      <name>Tung Mai</name>
    </author>
    <author>
      <name>Anup Rao</name>
    </author>
    <author>
      <name>Ryan A. Rossi</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <link href="http://arxiv.org/abs/2107.04660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.04660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2107.04919">
    <id>http://arxiv.org/abs/2107.04919v1</id>
    <updated>2021-07-10T22:08:43Z</updated>
    <published>2021-07-10T22:08:43Z</published>
    <title>Analysis of Smooth Heaps and Slim Heaps</title>
    <summary>  The smooth heap is a recently introduced self-adjusting heap [Kozma,
Saranurak, 2018] similar to the pairing heap [Fredman, Sedgewick, Sleator,
Tarjan, 1986]. The smooth heap was obtained as a heap-counterpart of Greedy
BST, a binary search tree updating strategy conjectured to be
\emph{instance-optimal} [Lucas, 1988], [Munro, 2000]. Several adaptive
properties of smooth heaps follow from this connection; moreover, the smooth
heap itself has been conjectured to be instance-optimal within a certain class
of heaps. Nevertheless, no general analysis of smooth heaps has existed until
now, the only previous analysis showing that, when used in \emph{sorting mode}
($n$ insertions followed by $n$ delete-min operations), smooth heaps sort $n$
numbers in $O(n\lg n)$ time.
  In this paper we describe a simpler variant of the smooth heap we call the
\emph{slim heap}. We give a new, self-contained analysis of smooth heaps and
slim heaps in unrestricted operation, obtaining amortized bounds that match the
best bounds known for self-adjusting heaps. Previous experimental work has
found the pairing heap to dominate other data structures in this class in
various settings. Our tests show that smooth heaps and slim heaps are
competitive with pairing heaps, outperforming them in some cases, while being
comparably easy to implement.
</summary>
    <author>
      <name>Maria Hartmann</name>
    </author>
    <author>
      <name>László Kozma</name>
    </author>
    <author>
      <name>Corwin Sinnamon</name>
    </author>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.04919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.04919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2107.03290">
    <id>http://arxiv.org/abs/2107.03290v1</id>
    <updated>2021-07-05T21:38:48Z</updated>
    <published>2021-07-05T21:38:48Z</published>
    <title>Defeating duplicates: A re-design of the LearnedSort algorithm</title>
    <summary>  LearnedSort is a novel sorting algorithm that, unlike traditional methods,
uses fast ML models to boost the sorting speed. The models learn to estimate
the input's distribution and arrange the keys in sorted order by predicting
their empirical cumulative distribution function (eCDF) values. LearnedSort has
shown outstanding performance compared to state-of-the-art sorting algorithms
on several datasets, both synthetic and real. However, given the nature of the
eCDF model, its performance is affected in the cases when the input data
contains a large number of repeated keys (i.e., duplicates). This work analyzes
this scenario in depth and introduces LearnedSort 2.0: a re-design of the
algorithm that addresses this issue and enables the algorithm to maintain the
leading edge even for high-duplicate inputs. Our extensive benchmarks on a
large set of diverse datasets demonstrate that the new design performs at much
higher sorting rates than the original version: an average of 4.78x improvement
for high-duplicate datasets, and 1.60x for low-duplicate datasets while taking
the lead among sorting algorithms.
</summary>
    <author>
      <name>Ani Kristo</name>
    </author>
    <author>
      <name>Kapil Vaidya</name>
    </author>
    <author>
      <name>Tim Kraska</name>
    </author>
    <link href="http://arxiv.org/abs/2107.03290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.03290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2107.03341">
    <id>http://arxiv.org/abs/2107.03341v1</id>
    <updated>2021-07-07T16:34:01Z</updated>
    <published>2021-07-07T16:34:01Z</published>
    <title>Burrows Wheeler Transform on a Large Scale: Algorithms Implemented in
  Apache Spark</title>
    <summary>  With the rapid growth of Next Generation Sequencing (NGS) technologies, large
amounts of "omics" data are daily collected and need to be processed. Indexing
and compressing large sequences datasets are some of the most important tasks
in this context. Here we propose algorithms for the computation of Burrows
Wheeler transform relying on Big Data technologies, i.e., Apache Spark and
Hadoop. Our algorithms are the first ones that distribute the index computation
and not only the input dataset, allowing to fully benefit of the available
cloud resources.
</summary>
    <author>
      <name>Ylenia Galluzzo</name>
    </author>
    <author>
      <name>Raffaele Giancarlo</name>
    </author>
    <author>
      <name>Mario Randazzo</name>
    </author>
    <author>
      <name>Simona E. Rombo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures, 2 tables. arXiv admin note: substantial text
  overlap with arXiv:2007.10095</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.03341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.03341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2107.02503">
    <id>http://arxiv.org/abs/2107.02503v1</id>
    <updated>2021-07-06T09:48:41Z</updated>
    <published>2021-07-06T09:48:41Z</published>
    <title>On Arithmetically Progressed Suffix Arrays and related Burrows-Wheeler
  Transforms</title>
    <summary>  We characterize those strings whose suffix arrays are based on arithmetic
progressions, in particular, arithmetically progressed permutations where all
pairs of successive entries of the permutation have the same difference modulo
the respective string length. We show that an arithmetically progressed
permutation $P$ coincides with the suffix array of a unary, binary, or ternary
string. We further analyze the conditions of a given $P$ under which we can
find a uniquely defined string over either a binary or ternary alphabet having
$P$ as its suffix array. For the binary case, we show its connection to lower
Christoffel words, balanced words, and Fibonacci words. In addition to solving
the arithmetically progressed suffix array problem, we give the shape of the
Burrows-Wheeler transform of those strings solving this problem. These results
give rise to numerous future research directions.
</summary>
    <author>
      <name>Jacqueline W. Daykin</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>David Kübel</name>
    </author>
    <author>
      <name>Florian Stober</name>
    </author>
    <link href="http://arxiv.org/abs/2107.02503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.02503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2107.02787">
    <id>http://arxiv.org/abs/2107.02787v1</id>
    <updated>2021-07-06T17:50:19Z</updated>
    <published>2021-07-06T17:50:19Z</published>
    <title>Space Efficient Two-Dimensional Orthogonal Colored Range Counting</title>
    <summary>  In the two-dimensional orthogonal colored range counting problem, we
preprocess a set, $P$, of $n$ colored points on the plane, such that given an
orthogonal query rectangle, the number of distinct colors of the points
contained in this rectangle can be computed efficiently.
  For this problem, we design three new solutions, and the bounds of each can
be expressed in some form of time-space tradeoff.
  By setting appropriate parameter values for these solutions, we can achieve
new specific results with (the space are in words and $\epsilon$ is an
arbitrary constant in $(0,1)$):
  ** $O(n\lg^3 n)$ space and $O(\sqrt{n}\lg^{5/2} n \lg \lg n)$ query time;
  ** $O(n\lg^2 n)$ space and $O(\sqrt{n}\lg^{4+\epsilon} n)$ query time;
  ** $O(n\frac{\lg^2 n}{\lg \lg n})$ space and $O(\sqrt{n}\lg^{5+\epsilon} n)$
query time;
  ** $O(n\lg n)$ space and $O(n^{1/2+\epsilon})$ query time.
  A known conditional lower bound to this problem based on Boolean matrix
multiplication gives some evidence on the difficulty of achieving near-linear
space solutions with query time better than $\sqrt{n}$ by more than a
polylogarithmic factor using purely combinatorial approaches. Thus the time and
space bounds in all these results are efficient.
  Previously, among solutions with similar query times, the most
space-efficient solution uses $O(n\lg^4 n)$ space to answer queries in
$O(\sqrt{n}\lg^8 n)$ time (SIAM. J. Comp.~2008).
  Thus the new results listed above all achieve improvements in space
efficiency, while all but the last result achieve speed-up in query time as
well.
</summary>
    <author>
      <name>Younan Gao</name>
    </author>
    <author>
      <name>Meng He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">full version of an ESA 2021 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.02787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.02787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2108.07759">
    <id>http://arxiv.org/abs/2108.07759v2</id>
    <updated>2021-08-30T17:36:52Z</updated>
    <published>2021-08-17T17:06:52Z</published>
    <title>Arbitrary-length analogs to de Bruijn sequences</title>
    <summary>  Let $\widetilde{\alpha}$ be a length-$L$ cyclic sequence of characters from a
size-$K$ alphabet $\mathcal{A}$ such that the number of occurrences of any
length-$m$ string on $\mathcal{A}$ as a substring of $\widetilde{\alpha}$ is
$\lfloor L / K^m \rfloor$ or $\lceil L / K^m \rceil$. When $L = K^N$ for any
positive integer $N$, $\widetilde{\alpha}$ is a de Bruijn sequence of order
$N$, and when $L \neq K^N$, $\widetilde{\alpha}$ shares many properties with de
Bruijn sequences. We describe an algorithm that outputs some
$\widetilde{\alpha}$ for any combination of $K \geq 2$ and $L \geq 1$ in $O(L)$
time using $O(L \log K)$ space. This algorithm extends Lempel's recursive
construction of a binary de Bruijn sequence. An implementation written in
Python is available at https://github.com/nelloreward/pkl.
</summary>
    <author>
      <name>Abhinav Nellore</name>
    </author>
    <author>
      <name>Rachel Ward</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 3 algorithms, 1 table; v2 refines language and fixes
  references</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.07759v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.07759v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2108.05495">
    <id>http://arxiv.org/abs/2108.05495v1</id>
    <updated>2021-08-12T02:14:54Z</updated>
    <published>2021-08-12T02:14:54Z</published>
    <title>Space-Efficient Huffman Codes Revisited</title>
    <summary>  Canonical Huffman code is an optimal prefix-free compression code whose
codewords enumerated in the lexicographical order form a list of binary words
in non-decreasing lengths. Gagie et al. (2015) gave a representation of this
coding capable to encode or decode a symbol in constant worst case time. It
uses $\sigma \lg \ell_{\text{max}} + o(\sigma) + O(\ell_{\text{max}}^2)$ bits
of space, where $\sigma$ and $\ell_{\text{max}}$ are the alphabet size and
maximum codeword length, respectively. We refine their representation to reduce
the space complexity to $\sigma \lg \ell_{\text{max}} (1 + o(1))$ bits while
preserving the constant encode and decode times. Our algorithmic idea can be
applied to any canonical code.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <link href="http://arxiv.org/abs/2108.05495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.05495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2108.04988">
    <id>http://arxiv.org/abs/2108.04988v1</id>
    <updated>2021-08-11T01:53:09Z</updated>
    <published>2021-08-11T01:53:09Z</published>
    <title>Practical evaluation of Lyndon factors via alphabet reordering</title>
    <summary>  We evaluate the influence of different alphabet orderings on the Lyndon
factorization of a string. Experiments with Pizza &amp; Chili datasets show that
for most alphabet reorderings, the number of Lyndon factors is usually small,
and the length of the longest Lyndon factor can be as large as the input
string, which is unfavorable for algorithms and indexes that depend on the
number of Lyndon factors. We present results with randomized alphabet
permutations that can be used as a baseline to assess the effectiveness of
heuristics and methods designed to modify the Lyndon factorization of a string
via alphabet reordering.
</summary>
    <author>
      <name>Marcelo K. Albertini</name>
    </author>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <link href="http://arxiv.org/abs/2108.04988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2108.04458">
    <id>http://arxiv.org/abs/2108.04458v2</id>
    <updated>2021-11-05T15:55:59Z</updated>
    <published>2021-08-10T05:56:29Z</published>
    <title>A Tight Analysis of Slim Heaps and Smooth Heaps</title>
    <summary>  The smooth heap and the closely related slim heap are recently invented
self-adjusting implementations of the heap (priority queue) data structure. We
analyze the efficiency of these data structures. We obtain the following
amortized bounds on the time per operation: $O(1)$ for make-heap, insert,
find-min, and meld; $O(\log\log n)$ for decrease-key; and $O(\log n)$ for
delete-min and delete, where $n$ is the current number of items in the heap.
These bounds are tight not only for smooth and slim heaps but for any heap
implementation in Iacono and \"{O}zkan's pure heap model, intended to capture
all possible "self-adjusting" heap implementations. Slim and smooth heaps are
the first known data structures to match Iacono and \"{O}zkan's lower bounds
and to satisfy the constraints of their model. Our analysis builds on Pettie's
insights into the efficiency of pairing heaps, a classical self-adjusting heap
implementation.
</summary>
    <author>
      <name>Corwin Sinnamon</name>
    </author>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was withdrawn by the authors due to a nontrivial error in
  Lemma 13. The proof does not adequately account for phantom links</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.04458v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04458v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2107.13801">
    <id>http://arxiv.org/abs/2107.13801v1</id>
    <updated>2021-07-29T07:59:12Z</updated>
    <published>2021-07-29T07:59:12Z</published>
    <title>A New Lossless Data Compression Algorithm Exploiting Positional
  Redundancy</title>
    <summary>  A new run length encoding algorithm for lossless data compression that
exploits positional redundancy by representing data in a two-dimensional model
of concentric circles is presented. This visual transform enables detection of
runs (each of a different character) in which runs need not be contiguous and
hence, is a generalization of run length encoding. Its advantages and drawbacks
are characterized by comparing its performance with TurboRLE.
</summary>
    <author>
      <name>Pranav Venkatram</name>
    </author>
    <link href="http://arxiv.org/abs/2107.13801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.13801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2107.09421">
    <id>http://arxiv.org/abs/2107.09421v1</id>
    <updated>2021-07-20T11:33:44Z</updated>
    <published>2021-07-20T11:33:44Z</published>
    <title>Critical factorisation in square-free words</title>
    <summary>  A position $p$ in a word $w$ is critical if the minimal local period at $p$
is equal to the global period of $w$. According to the Critical Factorisation
Theorem all words of length at least two have a critical point. We study the
number $\eta(w)$ of critical points of square-free ternary words $w$, i.e.,
words over a three letter alphabet. We show that the sufficiently long
square-free words $w$ satisfy $\eta(w) \le |w|-5$ where $|w|$ denotes the
length of $w$. Moreover, the bound $|w|-5$ is reached by infinitely many words.
On the other hand, every square-free word $w$ has at least $|w|/4$ critical
points, and there is a sequence of these words closing to this bound.
</summary>
    <author>
      <name>Tero Harju</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.09421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.09421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2107.09480">
    <id>http://arxiv.org/abs/2107.09480v3</id>
    <updated>2021-07-23T13:18:02Z</updated>
    <published>2021-07-19T16:06:55Z</published>
    <title>Learned Sorted Table Search and Static Indexes in Small Space:
  Methodological and Practical Insights via an Experimental Study</title>
    <summary>  Sorted Table Search Procedures are the quintessential query-answering tool,
still very useful, e.g, Search Engines (Google Chrome). Speeding them up, in
small additional space with respect to the table being searched into, is still
a quite significant achievement. Static Learned Indexes have been very
successful in achieving such a speed-up, but leave open a major question: To
what extent one can enjoy the speed-up of Learned Indexes while using constant
or nearly constant additional space. By generalizing the experimental
methodology of a recent benchmarking study on Learned Indexes, we shed light on
this question, by considering two scenarios. The first, quite elementary, i.e.,
textbook code, and the second using advanced Learned Indexing algorithms and
the supporting sophisticated software platforms. Although in both cases one
would expect a positive answer, its achievement is not as simple as it seems.
Indeed, our extensive set of experiments reveal a complex relationship between
query time and model space. The findings regarding this relationship and the
corresponding quantitative estimates, across memory levels, can be of interest
to algorithm designers and of use to practitioners as well. As an essential
part of our research, we introduce two new models that are of interest in their
own right. The first is a constant space model that can be seen as a
generalization of $k$-ary search, while the second is a synoptic {\bf RMI}, in
which we can control model space usage.
</summary>
    <author>
      <name>Domenico Amato</name>
    </author>
    <author>
      <name>Raffaele Giancarlo</name>
    </author>
    <author>
      <name>Giosuè Lo Bosco</name>
    </author>
    <link href="http://arxiv.org/abs/2107.09480v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.09480v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2107.09206">
    <id>http://arxiv.org/abs/2107.09206v1</id>
    <updated>2021-07-20T00:24:32Z</updated>
    <published>2021-07-20T00:24:32Z</published>
    <title>Hardness of Detecting Abelian and Additive Square Factors in Strings</title>
    <summary>  We prove 3SUM-hardness (no strongly subquadratic-time algorithm, assuming the
3SUM conjecture) of several problems related to finding Abelian square and
additive square factors in a string. In particular, we conclude conditional
optimality of the state-of-the-art algorithms for finding such factors.
  Overall, we show 3SUM-hardness of (a) detecting an Abelian square factor of
an odd half-length, (b) computing centers of all Abelian square factors, (c)
detecting an additive square factor in a length-$n$ string of integers of
magnitude $n^{\mathcal{O}(1)}$, and (d) a problem of computing a double 3-term
arithmetic progression (i.e., finding indices $i \ne j$ such that
$(x_i+x_j)/2=x_{(i+j)/2}$) in a sequence of integers $x_1,\dots,x_n$ of
magnitude $n^{\mathcal{O}(1)}$.
  Problem (d) is essentially a convolution version of the AVERAGE problem that
was proposed in a manuscript of Erickson. We obtain a conditional lower bound
for it with the aid of techniques recently developed by Dudek et al. [STOC
2020]. Problem (d) immediately reduces to problem (c) and is a step in
reductions to problems (a) and (b). In conditional lower bounds for problems
(a) and (b) we apply an encoding of Amir et al. [ICALP 2014] and extend it
using several string gadgets that include arbitrarily long Abelian-square-free
strings.
  Our reductions also imply conditional lower bounds for detecting Abelian
squares in strings over a constant-sized alphabet. We also show a subquadratic
upper bound in this case, applying a result of Chan and Lewenstein [STOC 2015].
</summary>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszyński</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ESA 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.09206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.09206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2107.08615">
    <id>http://arxiv.org/abs/2107.08615v1</id>
    <updated>2021-07-19T05:23:30Z</updated>
    <published>2021-07-19T05:23:30Z</published>
    <title>Sensitivity of string compressors and repetitiveness measures</title>
    <summary>  The sensitivity of a string compression algorithm $C$ asks how much the
output size $C(T)$ for an input string $T$ can increase when a single character
edit operation is performed on $T$. This notion enables one to measure the
robustness of compression algorithms in terms of errors and/or dynamic changes
occurring in the input string. In this paper, we analyze the worst-case
multiplicative sensitivity of string compression algorithms, defined by
$\max_{T \in \Sigma^n}\{C(T')/C(T) : ed(T, T') = 1\}$, where $ed(T, T')$
denotes the edit distance between $T$ and $T'$. For the most common versions of
the Lempel-Ziv 77 compressors, we prove that the worst-case multiplicative
sensitivity is only a small constant (2 or 3, depending on the version of the
Lempel-Ziv 77 and the edit operation type). We strengthen our upper bound
results by presenting matching lower bounds on the worst-case sensitivity for
all these major versions of the Lempel-Ziv 77 factorizations. This contrasts
with the previously known related results such that the size $z_{\rm 78}$ of
the Lempel-Ziv 78 factorization can increase by a factor of $\Omega(n^{3/4})$
[Lagarde and Perifel, 2018], and the number $r$ of runs in the Burrows-Wheeler
transform can increase by a factor of $\Omega(\log n)$ [Giuliani et al., 2021]
when a character is prepended to an input string of length $n$. We also study
the worst-case sensitivity of several grammar compression algorithms including
Bisection, AVL-grammar, GCIS, and CDAWG. Further, we extend the notion of the
worst-case sensitivity to string repetitiveness measures such as the smallest
string attractor size $\gamma$ and the substring complexity $\delta$, and
present matching upper and lower bounds of the worst-case multiplicative
sensitivity for $\gamma$ and $\delta$.
</summary>
    <author>
      <name>Tooru Akagi</name>
    </author>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <link href="http://arxiv.org/abs/2107.08615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.08615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.05579">
    <id>http://arxiv.org/abs/2102.05579v1</id>
    <updated>2021-02-10T17:21:39Z</updated>
    <published>2021-02-10T17:21:39Z</published>
    <title>All instantiations of the greedy algorithm for the shortest superstring
  problem are equivalent</title>
    <summary>  In the Shortest Common Superstring problem (SCS), one needs to find the
shortest superstring for a set of strings. While SCS is NP-hard and
MAX-SNP-hard, the Greedy Algorithm "choose two strings with the largest
overlap; merge them; repeat" achieves a constant factor approximation that is
known to be at most 3.5 and conjectured to be equal to 2. The Greedy Algorithm
is not deterministic, so its instantiations with different tie-breaking rules
may have different approximation factors. In this paper, we show that it is not
the case: all factors are equal. To prove this, we show how to transform a set
of strings so that all overlaps are different whereas their ratios stay roughly
the same.
  We also reveal connections between the original version of SCS and the
following one: find a~superstring minimizing the number of occurrences of a
given symbol. It turns out that the latter problem is equivalent to the
original one.
</summary>
    <author>
      <name>Maksim Nikolaev</name>
    </author>
    <link href="http://arxiv.org/abs/2102.05579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.05579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2109.09306">
    <id>http://arxiv.org/abs/2109.09306v1</id>
    <updated>2021-09-20T05:51:37Z</updated>
    <published>2021-09-20T05:51:37Z</published>
    <title>Abelian Repetition Threshold Revisited</title>
    <summary>  Abelian repetition threshold ART(k) is the number separating fractional
Abelian powers which are avoidable and unavoidable over the k-letter alphabet.
The exact values of ART(k) are unknown; the lower bounds were proved in [A.V.
Samsonov, A.M. Shur. On Abelian repetition threshold. RAIRO ITA, 2012] and
conjectured to be tight. We present a method of study of Abelian power-free
languages using random walks in prefix trees and some experimental results
obtained by this method. On the base of these results, we conjecture that the
lower bounds for ART(k) by Samsonov and Shur are not tight for all k except for
k=5 and prove this conjecture for k=6,7,8,9,10. Namely, we show that ART(k) >
(k-2)/(k-3) in all these cases.
</summary>
    <author>
      <name>Elena A. Petrova</name>
    </author>
    <author>
      <name>Arseny M. Shur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.09306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.09306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2109.09588">
    <id>http://arxiv.org/abs/2109.09588v1</id>
    <updated>2021-09-20T14:44:39Z</updated>
    <published>2021-09-20T14:44:39Z</published>
    <title>Resilient Level Ancestor, Bottleneck, and Lowest Common Ancestor Queries
  in Dynamic Trees</title>
    <summary>  We study the problem of designing a \emph{resilient} data structure
maintaining a tree under the Faulty-RAM model [Finocchi and Italiano, STOC'04]
in which up to $\delta$ memory words can be corrupted by an adversary. Our data
structure stores a rooted dynamic tree that can be updated via the addition of
new leaves, requires linear size, and supports \emph{resilient} (weighted)
level ancestor queries, lowest common ancestor queries, and bottleneck vertex
queries in $O(\delta)$ worst-case time per operation.
</summary>
    <author>
      <name>Luciano Gualà</name>
    </author>
    <author>
      <name>Stefano Leucci</name>
    </author>
    <author>
      <name>Isabella Ziccardi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 4 figures, full version of the paper accepted at ISAAC 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.09588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.09588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2109.02997">
    <id>http://arxiv.org/abs/2109.02997v2</id>
    <updated>2021-11-09T08:11:02Z</updated>
    <published>2021-09-07T11:17:59Z</published>
    <title>Simple Worst-Case Optimal Adaptive Prefix-Free Coding</title>
    <summary>  Gagie and Nekrich (2009) gave an algorithm for adaptive prefix-free coding
that, given a string $S [1..n]$ over the alphabet $\{1, \ldots, \sigma\}$ with
$\sigma = o (n / \log^{5 / 2} n)$, encodes $S$ in at most $n (H + 1) + o (n)$
bits, where $H$ is the empirical entropy of $S$, such that encoding and
decoding $S$ take $O (n)$ time. They also proved their bound on the encoding
length is optimal, even when the empirical entropy is high. Their algorithm is
impractical, however, because it uses complicated data structures. In this
paper we give an algorithm with the same bounds, except that we require $\sigma
= o (n^{1 / 2} / \log n)$, that uses no data structures more complicated than a
lookup table. Moreover, when Gagie and Nekrich's algorithm is used for optimal
adaptive alphabetic coding it takes $O (n \log \log n)$ time for decoding, but
ours still takes $O (n)$ time.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/2109.02997v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.02997v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2109.00287">
    <id>http://arxiv.org/abs/2109.00287v1</id>
    <updated>2021-09-01T09:52:31Z</updated>
    <published>2021-09-01T09:52:31Z</published>
    <title>Complex Event Forecasting with Prediction Suffix Trees: Extended
  Technical Report</title>
    <summary>  Complex Event Recognition (CER) systems have become popular in the past two
decades due to their ability to "instantly" detect patterns on real-time
streams of events. However, there is a lack of methods for forecasting when a
pattern might occur before such an occurrence is actually detected by a CER
engine. We present a formal framework that attempts to address the issue of
Complex Event Forecasting (CEF). Our framework combines two formalisms: a)
symbolic automata which are used to encode complex event patterns; and b)
prediction suffix trees which can provide a succinct probabilistic description
of an automaton's behavior. We compare our proposed approach against
state-of-the-art methods and show its advantage in terms of accuracy and
efficiency. In particular, prediction suffix trees, being variable-order Markov
models, have the ability to capture long-term dependencies in a stream by
remembering only those past sequences that are informative enough. Our
experimental results demonstrate the benefits, in terms of accuracy, of being
able to capture such long-term dependencies. This is achieved by increasing the
order of our model beyond what is possible with full-order Markov models that
need to perform an exhaustive enumeration of all possible past sequences of a
given order. We also discuss extensively how CEF solutions should be best
evaluated on the quality of their forecasts.
</summary>
    <author>
      <name>Elias Alevizos</name>
    </author>
    <author>
      <name>Alexander Artikis</name>
    </author>
    <author>
      <name>Georgios Paliouras</name>
    </author>
    <link href="http://arxiv.org/abs/2109.00287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.00287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.3; G.3; I.2.6; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2108.13968">
    <id>http://arxiv.org/abs/2108.13968v1</id>
    <updated>2021-08-31T16:49:32Z</updated>
    <published>2021-08-31T16:49:32Z</published>
    <title>Absent Subsequences in Words</title>
    <summary>  An absent factor of a string $w$ is a string $u$ which does not occur as a
contiguous substring (a.k.a. factor) inside $w$. We extend this well-studied
notion and define absent subsequences: a string $u$ is an absent subsequence of
a string $w$ if $u$ does not occur as subsequence (a.k.a. scattered factor)
inside $w$. Of particular interest to us are minimal absent subsequences, i.e.,
absent subsequences whose every subsequence is not absent, and shortest absent
subsequences, i.e., absent subsequences of minimal length. We show a series of
combinatorial and algorithmic results regarding these two notions. For
instance: we give combinatorial characterisations of the sets of minimal and,
respectively, shortest absent subsequences in a word, as well as compact
representations of these sets; we show how we can test efficiently if a string
is a shortest or minimal absent subsequence in a word, and we give efficient
algorithms computing the lexicographically smallest absent subsequence of each
kind; also, we show how a data structure for answering shortest absent
subsequence-queries for the factors of a given string can be efficiently
computed.
</summary>
    <author>
      <name>Maria Kosche</name>
    </author>
    <author>
      <name>Tore Koß</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Stefan Siemer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extended abstract will appear in the proceedings of the 15th
  International Conference on Reachability Problems RP2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.13968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.13968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2108.11475">
    <id>http://arxiv.org/abs/2108.11475v1</id>
    <updated>2021-08-25T21:01:13Z</updated>
    <published>2021-08-25T21:01:13Z</published>
    <title>Faster Exponential Algorithm for Permutation Pattern Matching</title>
    <summary>  The Permutation Pattern Matching problem asks, given two permutations
$\sigma$ on $n$ elements and $\pi$, whether $\sigma$ admits a subsequence with
the same relative order as $\pi$ (or, in the counting version, how many such
subsequences are there). This natural problem was shown by Bose, Buss and Lubiw
[IPL 1998] to be NP-complete, and hence we should seek exact exponential time
solutions. The asymptotically fastest such solution up to date, by Berendsohn,
Kozma and Marx [IPEC 2019], works in $\mathcal{O}(1.6181^n)$ time. We design a
simple and faster $\mathcal{O}(1.415^{n})$ time algorithm for both the
detection and the counting version. We also prove that this is optimal among a
certain natural class of algorithms.
</summary>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <author>
      <name>Mateusz Rzepecki</name>
    </author>
    <link href="http://arxiv.org/abs/2108.11475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.11475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2108.10776">
    <id>http://arxiv.org/abs/2108.10776v2</id>
    <updated>2021-08-26T05:40:05Z</updated>
    <published>2021-08-24T14:44:52Z</published>
    <title>Succinct Data Structures for Series-Parallel, Block-Cactus and 3-Leaf
  Power Graphs</title>
    <summary>  We design succinct encodings of {\it series-parallel, block-cactus} and {\it
3-leaf power} graphs while supporting the basic navigational queries such as
degree, adjacency and neighborhood {\it optimally} in the RAM model with
logarithmic word size. One salient feature of our representation is that it can
achieve optimal space even though the exact space lower bound for these graph
classes is not known. For these graph classes, we provide succinct data
structures with optimal query support for the first time in the literature. For
series-parallel multigraphs, our work also extends the works of Uno et al.
(Disc. Math. Alg. and Appl., 2013) and Blelloch and Farzan (CPM, 2010) to
produce optimal bounds.
</summary>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Seungbum Jo</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <link href="http://arxiv.org/abs/2108.10776v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.10776v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2108.09115">
    <id>http://arxiv.org/abs/2108.09115v1</id>
    <updated>2021-08-20T11:22:49Z</updated>
    <published>2021-08-20T11:22:49Z</published>
    <title>Does Preprocessing help in Fast Sequence Comparisons?</title>
    <summary>  We study edit distance computation with preprocessing: the preprocessing
algorithm acts on each string separately, and then the query algorithm takes as
input the two preprocessed strings. This model is inspired by scenarios where
we would like to compute edit distance between many pairs in the same pool of
strings.
  Our results include:
  Permutation-LCS: If the LCS between two permutations has length $n-k$, we can
compute it \textit{ exactly} with $O(n \log(n))$ preprocessing and $O(k
\log(n))$ query time.
  Small edit distance: For general strings, if their edit distance is at most
$k$, we can compute it \textit{ exactly} with $O(n\log(n))$ preprocessing and
$O(k^2 \log(n))$ query time.
  Approximate edit distance: For the most general input, we can approximate the
edit distance to within factor $(7+o(1))$ with preprocessing time
$\tilde{O}(n^2)$ and query time $\tilde{O}(n^{1.5+o(1)})$.
  All of these results significantly improve over the state of the art in edit
distance computation without preprocessing. Interestingly, by combining ideas
from our algorithms with preprocessing, we provide new improved results for
approximating edit distance without preprocessing in subquadratic time.
</summary>
    <author>
      <name>Elazar Goldenberg</name>
    </author>
    <author>
      <name>Aviad Rubinstein</name>
    </author>
    <author>
      <name>Barna Saha</name>
    </author>
    <link href="http://arxiv.org/abs/2108.09115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.09115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2108.08613">
    <id>http://arxiv.org/abs/2108.08613v1</id>
    <updated>2021-08-19T10:49:58Z</updated>
    <published>2021-08-19T10:49:58Z</published>
    <title>A Conditional Lower Bound for Episode Matching</title>
    <summary>  Given two strings $S$ and $P$, the Episode Matching problem is to compute the
length of the shortest substring of $S$ that contains $P$ as a subsequence. The
best known upper bound for this problem is $\tilde O(nm)$ by Das et al. (1997),
where $n,m$ are the lengths of $S$ and $P$, respectively. Although the problem
is well studied and has many applications in data mining, this bound has never
been improved. In this paper we show why this is the case by proving that an
$O((nm)^{1-\epsilon})$ algorithm (even for binary strings) would refute the
popular Strong Exponential Time Hypothesis (SETH). The proof is based on a
simple reduction from Orthogonal Vectors.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Shay Mozes</name>
    </author>
    <author>
      <name>Teresa Anna Steiner</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.08613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.08613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.00376">
    <id>http://arxiv.org/abs/2111.00376v1</id>
    <updated>2021-10-31T01:42:27Z</updated>
    <published>2021-10-31T01:42:27Z</published>
    <title>Computing Matching Statistics on Repetitive Texts</title>
    <summary>  Computing the {\em matching statistics} of a string $P[1..m]$ with respect to
a text $T[1..n]$ is a fundamental problem which has application to genome
sequence comparison. In this paper, we study the problem of computing the
matching statistics upon highly repetitive texts. We design three different
data structures that are similar to LZ-compressed indexes. The space costs of
all of them can be measured by $\gamma$, the size of the smallest string
attractor [STOC'2018] and $\delta$, a better measure of repetitiveness
[LATIN'2020].
</summary>
    <author>
      <name>Younan Gao</name>
    </author>
    <link href="http://arxiv.org/abs/2111.00376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.00376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.00259">
    <id>http://arxiv.org/abs/2111.00259v1</id>
    <updated>2021-10-30T14:21:03Z</updated>
    <published>2021-10-30T14:21:03Z</published>
    <title>Counting and Verifying Abelian Border Arrays of Binary Words</title>
    <summary>  In this note, we consider the problem of counting and verifying abelian
border arrays of binary words. We show that the number of valid abelian border
arrays of length \(n\) is \(2^{n-1}\). We also show that verifying whether a
given array is the abelian border array of some binary word reduces to
computing the abelian border array of a specific binary word. Thus, assuming
the word-RAM model, we present an \(O\left(\frac{n^2}{\log^2n}\right)\) time
algorithm for the abelian border array verification problem.
</summary>
    <author>
      <name>Mursalin Habib</name>
    </author>
    <author>
      <name>Md. Salman Shamil</name>
    </author>
    <author>
      <name>M. Sohel Rahman</name>
    </author>
    <link href="http://arxiv.org/abs/2111.00259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.00259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2110.15535">
    <id>http://arxiv.org/abs/2110.15535v1</id>
    <updated>2021-10-29T04:47:15Z</updated>
    <published>2021-10-29T04:47:15Z</published>
    <title>An $O(k \log{n})$ algorithm for prefix based ranked autocomplete</title>
    <summary>  Many search engines such as Google, Bing &amp; Yahoo! show search suggestions
when users enter search phrases on their interfaces. These suggestions are
meant to assist the user in finding what she wants quickly and also suggesting
common searches that may result in finding information that is more relevant.
It also serves the purpose of helping the user if she is not sure of what to
search for, but has a vague idea of what it is that she wants. We present an
algorithm that takes time proportional to $O(k \log{n})$, and $O(n)$ extra
space for providing the user with the top $k$ ranked suggestions out of a
corpus of $n$ possible suggestions based on the prefix of the query that she
has entered so far.
</summary>
    <author>
      <name>Dhruv Matani</name>
    </author>
    <link href="http://arxiv.org/abs/2110.15535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.15535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2110.12402">
    <id>http://arxiv.org/abs/2110.12402v1</id>
    <updated>2021-10-24T10:20:31Z</updated>
    <published>2021-10-24T10:20:31Z</published>
    <title>Approximating LCS and Alignment Distance over Multiple Sequences</title>
    <summary>  We study the problem of aligning multiple sequences with the goal of finding
an alignment that either maximizes the number of aligned symbols (the longest
common subsequence (LCS)), or minimizes the number of unaligned symbols (the
alignment distance (AD)). Multiple sequence alignment is a well-studied problem
in bioinformatics and is used to identify regions of similarity among DNA, RNA,
or protein sequences to detect functional, structural, or evolutionary
relationships among them. It is known that exact computation of LCS or AD of
$m$ sequences each of length $n$ requires $\Theta(n^m)$ time unless the Strong
Exponential Time Hypothesis is false. In this paper, we provide several results
to approximate LCS and AD of multiple sequences.
  If the LCS of $m$ sequences each of length $n$ is $\lambda n$ for some
$\lambda \in [0,1]$, then in $\tilde{O}_m(n^{\lfloor\frac{m}{2}\rfloor+1})$
time, we can return a common subsequence of length at least $\frac{\lambda^2
n}{2+\epsilon}$ for any arbitrary constant $\epsilon >0$.
  It is possible to approximate the AD within a factor of two in time
$\tilde{O}_m(n^{\lceil\frac{m}{2}\rceil})$. However, going below-2
approximation requires breaking the triangle inequality barrier which is a
major challenge in this area. No such algorithm with a running time of
$O(n^{\alpha m})$ for any $\alpha &lt; 1$ is known. If the AD is $\theta n$, then
we design an algorithm that approximates the AD within an approximation factor
of $\left(2-\frac{3\theta}{16}+\epsilon\right)$ in
$\tilde{O}_m(n^{\lfloor\frac{m}{2}\rfloor+2})$ time. Thus, if $\theta$ is a
constant, we get a below-two approximation in
$\tilde{O}_m(n^{\lfloor\frac{m}{2}\rfloor+2})$ time. Moreover, we show if just
one out of $m$ sequences is $(p,B)$-pseudorandom then, we get a below-2
approximation in $\tilde{O}_m(nB^{m-1}+n^{\lfloor \frac{m}{2}\rfloor+3})$ time
irrespective of $\theta$.
</summary>
    <author>
      <name>Debarati Das</name>
    </author>
    <author>
      <name>Barna Saha</name>
    </author>
    <link href="http://arxiv.org/abs/2110.12402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.12402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2110.13802">
    <id>http://arxiv.org/abs/2110.13802v1</id>
    <updated>2021-10-26T15:58:55Z</updated>
    <published>2021-10-26T15:58:55Z</published>
    <title>Linear Approximate Pattern Matching Algorithm</title>
    <summary>  Pattern matching is a fundamental process in almost every scientific domain.
The problem involves finding the positions of a given pattern (usually of short
length) in a reference stream of data (usually of large length). The matching
can be as an exact or as an approximate (inexact) matching. Exact matching is
to search for the pattern without allowing for mismatches (or insertions and
deletions) of one or more characters in the pattern), while approximate
matching is the opposite. For exact matching, several data structures that can
be built in linear time and space are used and in practice nowadays. For
approximate matching, the solutions proposed to solve this matching are
non-linear and currently impractical. In this paper, we designed and
implemented a structure that can be built in linear time and space and solve
the approximate matching problem in ($O(m + \frac {log_\Sigma ^kn}{k!} + occ$)
search costs, where $m$ is the length of the pattern, $n$ is the length of the
reference, and $k$ is the number of tolerated mismatches (and insertion and
deletions).
</summary>
    <author>
      <name>Anas Al-okaily</name>
    </author>
    <author>
      <name>Abdelghani Tbakhi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages double spaced</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.13802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.13802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2110.05168">
    <id>http://arxiv.org/abs/2110.05168v1</id>
    <updated>2021-10-08T10:39:07Z</updated>
    <published>2021-10-08T10:39:07Z</published>
    <title>On Solving the Minimum Common String Partition Problem by Decision
  Diagrams</title>
    <summary>  In the Minimum Common String Partition Problem (MCSP), we are given two
strings on input, and we want to partition both into the same collection of
substrings, minimizing the number of the substrings in the partition. This
combinatorial optimization problem has applications in computational biology
and is NP-hard. Many different heuristic and exact methods exist for this
problem, such as a Greedy approach, Ant Colony Optimization, or Integer Linear
Programming. In this paper, we formulate the MCSP as a Dynamic Program and
develop an exact solution algorithm based on Decision Diagrams for it. We also
introduce a restricted Decision Diagram that allows to compute heuristic
solutions to the MCSP and compare the quality of solution and runtime on
instances from literature with existing approaches. Our approach scales well
and is suitable for heuristic solution of large-scale instances.
</summary>
    <author>
      <name>Miloš Chromý</name>
    </author>
    <author>
      <name>Markus Sinnl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 tables, 5 graphs, submitted to ICORES'22</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.05168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.05168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2110.01111">
    <id>http://arxiv.org/abs/2110.01111v1</id>
    <updated>2021-10-03T22:27:37Z</updated>
    <published>2021-10-03T22:27:37Z</published>
    <title>Is this the simplest (and most surprising) sorting algorithm ever?</title>
    <summary>  We present an extremely simple sorting algorithm. It may look like it is
obviously wrong, but we prove that it is in fact correct. We compare it with
other simple sorting algorithms, and analyse some of its curious properties.
</summary>
    <author>
      <name>Stanley P. Y. Fung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.01111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.01111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2110.01181">
    <id>http://arxiv.org/abs/2110.01181v1</id>
    <updated>2021-10-04T04:34:24Z</updated>
    <published>2021-10-04T04:34:24Z</published>
    <title>FM-Indexing Grammars Induced by Suffix Sorting for Long Patterns</title>
    <summary>  The run-length compressed Burrows-Wheeler transform (RLBWT) used in
conjunction with the backward search introduced in the FM index is the
centerpiece of most compressed indexes working on highly-repetitive data sets
like biological sequences. Compared to grammar indexes, the size of the RLBWT
is often much bigger, but queries like counting the occurrences of long
patterns can be done much faster than on any existing grammar index so far. In
this paper, we combine the virtues of a grammar with the RLBWT by building the
RLBWT on top of a special grammar based on induced suffix sorting. Our
experiments reveal that our hybrid approach outperforms the classic RLBWT with
respect to the index sizes, and with respect to query times on biological data
sets for sufficiently long patterns.
</summary>
    <author>
      <name>Jin Jie Deng</name>
    </author>
    <author>
      <name>Wing-Kai Hon</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <link href="http://arxiv.org/abs/2110.01181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.01181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.05016">
    <id>http://arxiv.org/abs/2111.05016v1</id>
    <updated>2021-11-09T09:24:52Z</updated>
    <published>2021-11-09T09:24:52Z</published>
    <title>Pattern Matching on Grammar-Compressed Strings in Linear Time</title>
    <summary>  The most fundamental problem considered in algorithms for text processing is
pattern matching: given a pattern $p$ of length $m$ and a text $t$ of length
$n$, does $p$ occur in $t$? Multiple versions of this basic question have been
considered, and by now we know algorithms that are fast both in practice and in
theory. However, the rapid increase in the amount of generated and stored data
brings the need of designing algorithms that operate directly on compressed
representations of data. In the compressed pattern matching problem we are
given a compressed representation of the text, with $n$ being the length of the
compressed representation and $N$ being the length of the text, and an
uncompressed pattern of length $m$. The most challenging (and yet relevant when
working with highly repetitive data, say biological information) scenario is
when the chosen compression method is capable of describing a string of
exponential length (in the size of its representation). An elegant formalism
for such a compression method is that of straight-line programs, which are
simply context-free grammars describing exactly one string. While it has been
known that compressed pattern matching problem can be solved in $O(m+n\log N)$
time for this compression method, designing a linear-time algorithm remained
open. We resolve this open question by presenting an $O(n+m)$ time algorithm
that, given a context-free grammar of size $n$ that produces a single string
$t$ and a pattern $p$ of length $m$, decides whether $p$ occurs in $t$ as a
substring. To this end, we devise improved solutions for the weighted ancestor
problem and the substring concatenation problem.
</summary>
    <author>
      <name>Moses Ganardi</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <link href="http://arxiv.org/abs/2111.05016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.05016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.03968">
    <id>http://arxiv.org/abs/2111.03968v1</id>
    <updated>2021-11-06T22:14:50Z</updated>
    <published>2021-11-06T22:14:50Z</published>
    <title>Improved Approximation Guarantees for Shortest Superstrings using Cycle
  Classification by Overlap to Length Ratios</title>
    <summary>  In the Shortest Superstring problem, we are given a set of strings and we are
asking for a common superstring, which has the minimum number of characters.
The Shortest Superstring problem is NP-hard and several constant-factor
approximation algorithms are known for it. Of particular interest is the GREEDY
algorithm, which repeatedly merges two strings of maximum overlap until a
single string remains. The GREEDY algorithm, being simpler than other
well-performing approximation algorithms for this problem, has attracted
attention since the 1980s and is commonly used in practical applications.
  Tarhio and Ukkonen (TCS 1988) conjectured that GREEDY gives a
2-approximation. In a seminal work, Blum, Jiang, Li, Tromp, and Yannakakis
(STOC 1991) proved that the superstring computed by GREEDY is a
4-approximation, and this upper bound was improved to 3.5 by Kaplan and Shafrir
(IPL 2005).
  We show that the approximation guarantee of GREEDY is at most
$(13+\sqrt{57})/6 \approx 3.425$, making the first progress on this question
since 2005. Furthermore, we prove that the Shortest Superstring can be
approximated within a factor of $(37+\sqrt{57})/18\approx 2.475$, improving
slightly upon the currently best $2\frac{11}{23}$-approximation algorithm by
Mucha (SODA 2013).
</summary>
    <author>
      <name>Matthias Englert</name>
    </author>
    <author>
      <name>Nicolaos Matsakis</name>
    </author>
    <author>
      <name>Pavel Veselý</name>
    </author>
    <link href="http://arxiv.org/abs/2111.03968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.03968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.04332">
    <id>http://arxiv.org/abs/2111.04332v1</id>
    <updated>2021-11-08T08:45:26Z</updated>
    <published>2021-11-08T08:45:26Z</published>
    <title>Succinct Data Structure for Path Graphs</title>
    <summary>  We consider the problem of designing a succinct data structure for path
graphs (which are a proper subclass of chordal graphs and a proper superclass
of interval graphs) with $n$ vertices while supporting degree, adjacency, and
neighborhood queries efficiently. We provide two solutions for this problem.
Our first data structure is succinct and occupies $n \log n+o(n \log n)$ bits
while answering adjacency query in $O(\log n)$ time, and neighborhood and
degree queries in $O(d \log^2 n)$ time where $d$ is the degree of the queried
vertex. Our second data structure answers adjacency queries faster at the
expense of slightly more space. More specifically, we provide an $O(n \log^2
n)$ bit data structure that supports adjacency query in $O(1)$ time, and the
neighborhood query in $O(d \log n)$ time where $d$ is the degree of the queried
vertex. Central to our data structures is the usage of the classical heavy path
decomposition, followed by a careful bookkeeping using an orthogonal range
search data structure among others, which maybe of independent interest for
designing succinct data structures for other graphs. It is the use of the
results of Acan et al. in the second data structure that permits a simple and
efficient implementation at the expense of more space.
</summary>
    <author>
      <name>Girish Balakrishnan</name>
    </author>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>N S Narayanaswamy</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure, 4 sections</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.04332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.04332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.04595">
    <id>http://arxiv.org/abs/2111.04595v1</id>
    <updated>2021-11-08T16:00:25Z</updated>
    <published>2021-11-08T16:00:25Z</published>
    <title>Graphs can be succinctly indexed for pattern matching in $ O(|E|^2 +
  |V|^{5 / 2}) $ time</title>
    <summary>  For the first time we provide a succinct pattern matching index for arbitrary
graphs that can be built in polynomial time, which requires less space and
answers queries more efficiently than the one in [SODA 2021]. We show that,
given an edge-labeled graph $ G = (V, E) $, there exists a data structures of
$|E /_{\le_G}|(\lceil \log|\Sigma|\rceil + \lceil\log q\rceil + 2)\cdot
(1+o(1)) + |V /_{\le_G}|\cdot (1+o(1))$ bits which can be built in $ O(|E|^2 +
|V /_{\le_G}|^{5 / 2}) $ time and supports pattern matching on $ G $ in $O(|P|
\cdot q^2 \cdot \log(q\cdot |\Sigma|))$ time, where $ G /_{\le_G} = (V
/_{\le_G}, E /_{\le_G}) $ is a quotient graph obtained by collapsing some nodes
in $ G $ (so $ |V /_{\le_G}| \le |V| $ and $ |E /_{\le_G}| \le |E| $) and $ q $
is the width of the maximum co-lex relation on $ G $.
  Our results have relevant applications in automata theory. First, we can
build a succinct data structure to decide whether a string is accepted by a
given automaton. Second, starting from an automaton $ \mathcal{A} $, one can
define a relation $ \preceq_\mathcal{A} $ and a quotient automaton that capture
the nondeterminism of $ \mathcal{A} $, improving the results in [SODA 2021].
</summary>
    <author>
      <name>Nicola Cotumaccio</name>
    </author>
    <link href="http://arxiv.org/abs/2111.04595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.04595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.02336">
    <id>http://arxiv.org/abs/2111.02336v1</id>
    <updated>2021-11-03T16:38:37Z</updated>
    <published>2021-11-03T16:38:37Z</published>
    <title>An Improved Algorithm for The $k$-Dyck Edit Distance Problem</title>
    <summary>  A Dyck sequence is a sequence of opening and closing parentheses (of various
types) that is balanced. The Dyck edit distance of a given sequence of
parentheses $S$ is the smallest number of edit operations (insertions,
deletions, and substitutions) needed to transform $S$ into a Dyck sequence. We
consider the threshold Dyck edit distance problem, where the input is a
sequence of parentheses $S$ and a positive integer $k$, and the goal is to
compute the Dyck edit distance of $S$ only if the distance is at most $k$, and
otherwise report that the distance is larger than $k$. Backurs and Onak
[PODS'16] showed that the threshold Dyck edit distance problem can be solved in
$O(n+k^{16})$ time.
  In this work, we design new algorithms for the threshold Dyck edit distance
problem which costs $O(n+k^{4.782036})$ time with high probability or
$O(n+k^{4.853059})$ deterministically. Our algorithms combine several new
structural properties of the Dyck edit distance problem, a refined algorithm
for fast $(\min,+)$ matrix product, and a careful modification of ideas used in
Valiant's parsing algorithm.
</summary>
    <author>
      <name>Dvir Fried</name>
    </author>
    <author>
      <name>Shay Golan</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in SODA 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.02336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.02336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.02318">
    <id>http://arxiv.org/abs/2111.02318v1</id>
    <updated>2021-11-03T16:11:22Z</updated>
    <published>2021-11-03T16:11:22Z</published>
    <title>Nearly Tight Lower Bounds for Succinct Range Minimum Query</title>
    <summary>  Given an array of distinct integers $A[1\ldots n]$, the Range Minimum Query
(RMQ) problem requires us to construct a data structure from $A$, supporting
the RMQ query: given an interval $[a,b]\subseteq[1,n]$, return the index of the
minimum element in subarray $A[a\ldots b]$, i.e. return
$\text{argmin}_{i\in[a,b]}A[i]$. The fundamental problem has a long history.
The textbook solution which uses $O(n)$ words of space and $O(1)$ time by
Gabow, Bentley, Tarjan (STOC 1984) and Harel, Tarjan (SICOMP 1984) dates back
to 1980s. The state-of-the-art solution is presented by Fischer, Heun (SICOMP
2011) and Navarro, Sadakane (TALG 2014). The solution uses
$2n+n/\left(\frac{\log n}{t}\right)^t+\tilde{O}(n^{3/4})$ bits of space and
$O(t)$ query time, assuming the word-size is $\Theta(\log n)$ bits. On the
other hand, the only known lower bound is proven by Liu and Yu (STOC 2020).
They show that any data structure which solves RMQ in $t$ query time must use
$2n+n/(\log n)^{O(t^2\log^2t)}$ bits of space, assuming the word-size is
$\Theta(\log n)$ bits.
  In this paper, we prove nearly tight lower bound for this problem. We show
that, for any data structure which solves RMQ in $t$ query time, $2n+n/(\log
n)^{O(t\log^2t)}$ bits of space is necessary in the cell-probe model with
word-size $\Theta(\log n)$. We emphasize that, for any $r$, we present a lower
bound of $t=\Omega(t_{opt}/\log^3 t_{opt})$, where $t_{opt}$ is the optimal
time cost when the data structure is allowed to consume $2n+r$ bits of space.
Hence our lower bound is nearly tight.
</summary>
    <author>
      <name>Mingmou Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2111.02318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.02318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68P30, 68Q17" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.4; F.1.3; F.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.02478">
    <id>http://arxiv.org/abs/2111.02478v1</id>
    <updated>2021-11-03T19:06:10Z</updated>
    <published>2021-11-03T19:06:10Z</published>
    <title>HOLZ: High-Order Entropy Encoding of Lempel-Ziv Factor Distances</title>
    <summary>  We propose a new representation of the offsets of the Lempel-Ziv (LZ)
factorization based on the co-lexicographic order of the processed prefixes.
The selected offsets tend to approach the k-th order empirical entropy. Our
evaluations show that this choice of offsets is superior to the rightmost LZ
parsing and the bit-optimal LZ parsing on datasets with small high-order
entropy.
</summary>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/2111.02478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.02478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.00602">
    <id>http://arxiv.org/abs/2111.00602v2</id>
    <updated>2021-11-04T00:16:29Z</updated>
    <published>2021-10-31T21:58:06Z</published>
    <title>On the Optimal Time/Space Tradeoff for Hash Tables</title>
    <summary>  For nearly six decades, the central open question in the study of hash tables
has been to determine the optimal achievable tradeoff curve between time and
space. State-of-the-art hash tables offer the following guarantee: If
keys/values are Theta(log n) bits each, then it is possible to achieve
constant-time insertions/deletions/queries while wasting only O(loglog n) bits
of space per key when compared to the information-theoretic optimum. Even prior
to this bound being achieved, the target of O(loglog n) wasted bits per key was
known to be a natural end goal, and was proven to be optimal for a number of
closely related problems (e.g., stable hashing, dynamic retrieval, and
dynamically-resized filters).
  This paper shows that O(loglog n) wasted bits per key is not the end of the
line for hashing. In fact, for any k \in [log* n], it is possible to achieve
O(k)-time insertions/deletions, O(1)-time queries, and O(\log^{(k)} n) wasted
bits per key (all with high probability in n). This means that, each time we
increase insertion/deletion time by an \emph{additive constant}, we reduce the
wasted bits per key \emph{exponentially}. We further show that this tradeoff
curve is the best achievable by any of a large class of hash tables, including
any hash table designed using the current framework for making constant-time
hash tables succinct.
</summary>
    <author>
      <name>Michael A. Bender</name>
    </author>
    <author>
      <name>Martín Farach-Colton</name>
    </author>
    <author>
      <name>John Kuszmaul</name>
    </author>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <author>
      <name>Mingmou Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">48 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.00602v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.00602v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W40" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.12706">
    <id>http://arxiv.org/abs/2111.12706v1</id>
    <updated>2021-11-24T18:58:49Z</updated>
    <published>2021-11-24T18:58:49Z</published>
    <title>Gap Edit Distance via Non-Adaptive Queries: Simple and Optimal</title>
    <summary>  We study the problem of approximating edit distance in sublinear time. This
is formalized as a promise problem $(k,k^c)$-Gap Edit Distance, where the input
is a pair of strings $X,Y$ and parameters $k,c>1$, and the goal is to return
YES if $ED(X,Y)\leq k$ and NO if $ED(X,Y)> k^c$. Recent years have witnessed
significant interest in designing sublinear-time algorithms for Gap Edit
Distance.
  We resolve the non-adaptive query complexity of Gap Edit Distance, improving
over several previous results. Specifically, we design a non-adaptive algorithm
with query complexity $\tilde{O}(\frac{n}{k^{c-0.5}})$, and further prove that
this bound is optimal up to polylogarithmic factors.
  Our algorithm also achieves optimal time complexity
$\tilde{O}(\frac{n}{k^{c-0.5}})$ whenever $c\geq 1.5$. For $1&lt;c&lt;1.5$, the
running time of our algorithm is $\tilde{O}(\frac{n}{k^{2c-1}})$. For the
restricted case of $k^c=\Omega(n)$, this matches a known result [Batu, Erg\"un,
Kilian, Magen, Raskhodnikova, Rubinfeld, and Sami, STOC 2003], and in all other
(nontrivial) cases, our running time is strictly better than all previous
algorithms, including the adaptive ones.
</summary>
    <author>
      <name>Elazar Goldenberg</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Robert Krauthgamer</name>
    </author>
    <author>
      <name>Barna Saha</name>
    </author>
    <link href="http://arxiv.org/abs/2111.12706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.12706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.10538">
    <id>http://arxiv.org/abs/2111.10538v1</id>
    <updated>2021-11-20T08:12:16Z</updated>
    <published>2021-11-20T08:12:16Z</published>
    <title>Approximation Algorithms for LCS and LIS with Truly Improved Running
  Times</title>
    <summary>  Longest common subsequence ($\mathsf{LCS}$) is a classic and central problem
in combinatorial optimization. While $\mathsf{LCS}$ admits a quadratic time
solution, recent evidence suggests that solving the problem may be impossible
in truly subquadratic time. A special case of $\mathsf{LCS}$ wherein each
character appears at most once in every string is equivalent to the longest
increasing subsequence problem ($\mathsf{LIS}$) which can be solved in
quasilinear time. In this work, we present novel algorithms for approximating
$\mathsf{LCS}$ in truly subquadratic time and $\mathsf{LIS}$ in truly sublinear
time. Our approximation factors depend on the ratio of the optimal solution
size over the input size. We denote this ratio by $\lambda$ and obtain the
following results for $\mathsf{LCS}$ and $\mathsf{LIS}$ without any prior
knowledge of $\lambda$.
  $\bullet$ A truly subquadratic time algorithm for $\mathsf{LCS}$ with
approximation factor $\Omega(\lambda^3)$.
  $\bullet$A truly sublinear time algorithm for $\mathsf{LIS}$ with
approximation factor $\Omega(\lambda^3)$.
  Triangle inequality was recently used by [Boroujeni, Ehsani, Ghodsi,
HajiAghayi and Seddighin SODA 2018] and [Charkraborty, Das, Goldenberg, Koucky
and Saks FOCS 2018] to present new approximation algorithms for edit distance.
Our techniques for $\mathsf{LCS}$ extend the notion of triangle inequality to
non-metric settings.
</summary>
    <author>
      <name>Aviad Rubinstein</name>
    </author>
    <author>
      <name>Saeed Seddighin</name>
    </author>
    <author>
      <name>Zhao Song</name>
    </author>
    <author>
      <name>Xiaorui Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">FOCS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.10538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.10538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.09253">
    <id>http://arxiv.org/abs/2111.09253v2</id>
    <updated>2021-11-21T15:30:30Z</updated>
    <published>2021-11-17T17:34:11Z</published>
    <title>Prefixes of the Fibonacci word that end with a cube</title>
    <summary>  We study the prefixes of the Fibonacci word that end with a cube. Using
Walnut we obtain an exact description of the positions of the Fibonacci word at
which a cube ends.
</summary>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, minor edits to "motivation" of the problem</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.09253v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.09253v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.10376">
    <id>http://arxiv.org/abs/2112.10376v1</id>
    <updated>2021-12-20T07:56:31Z</updated>
    <published>2021-12-20T07:56:31Z</published>
    <title>String Sampling with Bidirectional String Anchors</title>
    <summary>  The minimizers sampling mechanism is a popular mechanism for string sampling
introduced independently by Schleimer et al. [SIGMOD 2003] and by Roberts et
al. [Bioinf. 2004]. Given two positive integers $w$ and $k$, it selects the
lexicographically smallest length-$k$ substring in every fragment of $w$
consecutive length-$k$ substrings (in every sliding window of length $w + k -
1$). Minimizers samples are approximately uniform, locally consistent, and
computable in linear time. Two main disadvantages of minimizers sampling
mechanisms are: first, they do not have good guarantees on the expected size of
their samples for every combination of $w$ and $k$; and, second, indexes that
are constructed over their samples do not have good worst-case guarantees for
on-line pattern searches.
  We introduce bidirectional string anchors (bd-anchors), a new string sampling
mechanism. Given a positive integer $\ell$, our mechanism selects the
lexicographically smallest rotation in every length-$\ell$ fragment (in every
sliding window of length $\ell$). We show that bd-anchors samples are also
approximately uniform, locally consistent, and computable in linear time. In
addition, our experiments using several datasets demonstrate that the
bd-anchors sample sizes decrease proportionally to $\ell$; and that these sizes
are competitive to or smaller than the minimizers sample sizes using the
analogous sampling parameters. We provide theoretical justification for these
results by analyzing the expected size of bd-anchors samples. As a negative
result, we show that computing a total order $\leq$ on the input alphabet,
which minimizes the bd-anchors sample size, is NP-hard. We also show that by
using any bd-anchors sample, we can construct, in near-linear time, an index
which requires linear (extra) space in the size of the sample and answers
on-line pattern searches in near-optimal time.
</summary>
    <author>
      <name>Grigorios Loukides</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Michelle Sweering</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of an ESA 2021 paper; it includes proofs omitted
  from the conference version and new results obtained by Michelle Sweering</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.10376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.10376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.09971">
    <id>http://arxiv.org/abs/2112.09971v1</id>
    <updated>2021-12-18T17:10:10Z</updated>
    <published>2021-12-18T17:10:10Z</published>
    <title>Beyond Single-Deletion Correcting Codes: Substitutions and
  Transpositions</title>
    <summary>  We consider the problem of designing low-redundancy codes in settings where
one must correct deletions in conjunction with substitutions or adjacent
transpositions; a combination of errors that is usually observed in DNA-based
data storage. One of the most basic versions of this problem was settled more
than 50 years ago by Levenshtein, or one substitution, with nearly optimal
redundancy. However, this approach fails to extend to many simple and natural
variations of the binary single-edit error setting. In this work, we make
progress on the code design problem above in three such variations:
  We construct linear-time encodable and decodable length-$n$ non-binary codes
correcting a single edit error with nearly optimal redundancy $\log
n+O(\log\log n)$, providing an alternative simpler proof of a result by Cai,
Chee, Gabrys, Kiah, and Nguyen (IEEE Trans. Inf. Theory 2021). This is achieved
by employing what we call weighted VT sketches, a notion that may be of
independent interest.
  We construct linear-time encodable and list-decodable binary codes with
list-size $2$ for one deletion and one substitution with redundancy $4\log
n+O(\log\log n)$. This matches the existential bound up to an $O(\log\log n)$
additive term.
  We show the existence of a binary code correcting one deletion or one
adjacent transposition with nearly optimal redundancy $\log n+O(\log\log n)$.
</summary>
    <author>
      <name>Ryan Gabrys</name>
    </author>
    <author>
      <name>Venkatesan Guruswami</name>
    </author>
    <author>
      <name>João Ribeiro</name>
    </author>
    <author>
      <name>Ke Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.09971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.09971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.05836">
    <id>http://arxiv.org/abs/2112.05836v1</id>
    <updated>2021-12-10T21:27:22Z</updated>
    <published>2021-12-10T21:27:22Z</published>
    <title>How Compression and Approximation Affect Efficiency in String Distance
  Measures</title>
    <summary>  Real-world data often comes in compressed form. Analyzing compressed data
directly (without decompressing it) can save space and time by orders of
magnitude. In this work, we focus on fundamental sequence comparison problems
and try to quantify the gain in time complexity when the underlying data is
highly compressible. We consider grammar compression, which unifies many
practically relevant compression schemes. For two strings of total length $N$
and total compressed size $n$, it is known that the edit distance and a longest
common subsequence (LCS) can be computed exactly in time $\tilde{O}(nN)$, as
opposed to $O(N^2)$ for the uncompressed setting. Many applications need to
align multiple sequences simultaneously, and the fastest known exact algorithms
for median edit distance and LCS of $k$ strings run in $O(N^k)$ time. This
naturally raises the question of whether compression can help to reduce the
running time significantly for $k \geq 3$, perhaps to $O(N^{k/2}n^{k/2})$ or
$O(Nn^{k-1})$. Unfortunately, we show lower bounds that rule out any
improvement beyond $\Omega(N^{k-1}n)$ time for any of these problems assuming
the Strong Exponential Time Hypothesis.
  At the same time, we show that approximation and compression together can be
surprisingly effective. We develop an $\tilde{O}(N^{k/2}n^{k/2})$-time FPTAS
for the median edit distance of $k$ sequences. In comparison, no
$O(N^{k-\Omega(1)})$-time PTAS is known for the median edit distance problem in
the uncompressed setting. For two strings, we get an
$\tilde{O}(N^{2/3}n^{4/3})$-time FPTAS for both edit distance and LCS. In
contrast, for uncompressed strings, there is not even a subquadratic algorithm
for LCS that has less than a polynomial gap in the approximation factor.
Building on the insight from our approximation algorithms, we also obtain
results for many distance measures including the edit, Hamming, and shift
distances.
</summary>
    <author>
      <name>Arun Ganesh</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Andrea Lincoln</name>
    </author>
    <author>
      <name>Barna Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to SODA 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.05836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.05836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.05866">
    <id>http://arxiv.org/abs/2112.05866v1</id>
    <updated>2021-12-10T23:24:37Z</updated>
    <published>2021-12-10T23:24:37Z</published>
    <title>Improved Approximation Algorithms for Dyck Edit Distance and RNA Folding</title>
    <summary>  The Dyck language, which consists of well-balanced sequences of parentheses,
is one of the most fundamental context-free languages. The Dyck edit distance
quantifies the number of edits (character insertions, deletions, and
substitutions) required to make a given parenthesis sequence well-balanced. RNA
Folding involves a similar problem, where a closing parenthesis can match an
opening parenthesis of the same type irrespective of their ordering. For
example, in RNA Folding, both $\tt{()}$ and $\tt{)(}$ are valid matches,
whereas the Dyck language only allows $\tt{()}$ as a match. Using fast matrix
multiplication, it is possible to compute their exact solutions of both
problems in time $O(n^{2.824})$. Whereas combinatorial algorithms would be more
desirable, the two problems are known to be at least as hard as Boolean matrix
multiplication. In terms of fast approximation algorithms that are
combinatorial in nature, both problems admit an $\epsilon n$-additive
approximation in $\tilde{O}(\frac{n^2}{\epsilon})$ time. Further, there is a
$O(\log n)$-factor approximation algorithm for Dyck edit distance in
near-linear time.
  In this paper, we design a constant-factor approximation algorithm for Dyck
edit distance that runs in $O(n^{1.971})$ time. Moreover, we develop a
$(1+\epsilon)$-factor approximation algorithm running in
$\tilde{O}(\frac{n^2}{\epsilon})$ time, which improves upon the earlier
additive approximation. Finally, we design a $(3+\epsilon)$-approximation that
takes $\tilde{O}(\frac{nd}{\epsilon})$ time, where $d\ge 1$ is an upper bound
on the sought distance. As for RNA folding, for any $s\ge1$, we design a
factor-$s$ approximation algorithm that runs in $O(n+(\frac{n}{s})^3)$ time. To
the best of our knowledge, this is the first nontrivial approximation algorithm
for RNA Folding that can go below the $n^2$ barrier. All our algorithms are
combinatorial.
</summary>
    <author>
      <name>Debarati Das</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Barna Saha</name>
    </author>
    <link href="http://arxiv.org/abs/2112.05866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.05866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.05725">
    <id>http://arxiv.org/abs/2112.05725v2</id>
    <updated>2022-01-04T17:43:50Z</updated>
    <published>2021-12-10T18:28:33Z</published>
    <title>Beyond the Longest Letter-duplicated Subsequence Problem</title>
    <summary>  Given a sequence $S$ of length $n$, a letter-duplicated subsequence is a
subsequence of $S$ in the form of $x_1^{d_1}x_2^{d_2}\cdots x_k^{d_k}$ with
$x_i\in\Sigma$, $x_j\neq x_{j+1}$ and $d_i\geq 2$ for all $i$ in $[k]$ and $j$
in $[k-1]$. A linear time algorithm for computing the longest letter-duplicated
subsequence (LLDS) of $S$ can be easily obtained. In this paper, we focus on
two variants of this problem. We first consider the constrained version when
$\Sigma$ is unbounded, each letter appears in $S$ at least 6 times and all the
letters in $\Sigma$ must appear in the solution. We show that the problem is
NP-hard (a further twist indicates that the problem does not admit any
polynomial time approximation). The reduction is from possibly the simplest
version of SAT that is NP-complete, $(\leq 2,1,\leq 3)$-SAT, where each
variable appears at most twice positively and exact once negatively, and each
clause contains at most three literals and some clauses must contain exactly
two literals. (We hope that this technique will serve as a general tool to help
us proving the NP-hardness for some more tricky sequence problems involving
only one sequence -- much harder than with at least two input sequences, which
we apply successfully at the end of the paper on some extra variations of the
LLDS problem.) We then show that when each letter appears in $S$ at most 3
times, then the problem admits a factor $1.5-O(\frac{1}{n})$ approximation.
Finally, we consider the weighted version, where the weight of a block
$x_i^{d_i} (d_i\geq 2)$ could be any positive function which might not grow
with $d_i$. We give a non-trivial $O(n^2)$ time dynamic programming algorithm
for this version, i.e., computing an LD-subsequence of $S$ whose weight is
maximized.
</summary>
    <author>
      <name>Wenfeng Lai</name>
    </author>
    <author>
      <name>Adiesha Liyanage</name>
    </author>
    <author>
      <name>Binhai Zhu</name>
    </author>
    <author>
      <name>Peng Zou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.05725v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.05725v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W01, 68W32" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.08454">
    <id>http://arxiv.org/abs/2112.08454v1</id>
    <updated>2021-12-15T20:03:06Z</updated>
    <published>2021-12-15T20:03:06Z</published>
    <title>Approximating the Longest Common Subsequence problem within a
  sub-polynomial factor in linear time</title>
    <summary>  The Longest Common Subsequence (LCS) of two strings is a fundamental string
similarity measure with a classical dynamic programming solution taking
quadratic time. Despite significant efforts, little progress was made in
improving the runtime. Even in the realm of approximation, not much was known
for linear time algorithms beyond the trivial $\sqrt{n}$-approximation. Recent
breakthrough result provided a $n^{0.497}$-factor approximation algorithm
[HSSS19], which was more recently improved to a $n^{0.4}$-factor one [BCD21].
The latter paper also showed a $n^{2-2.5\alpha}$ time algorithm which outputs a
$n^{\alpha}$ approximation to the LCS, but so far no sub-polynomial
approximation is known in truly subquadratic time.
  In this work, we show an algorithm which runs in $O(n)$ time, and outputs a
$n^{o(1)}$-factor approximation to LCS$(x,y)$, with high probability, for any
pair of length $n$ input strings.
  Our entire algorithm is merely an efficient black-box reduction to the
Block-LIS problem, introduced very recently in [ANSS21], and solving the
Block-LIS problem directly.
</summary>
    <author>
      <name>Negev Shekel Nosatzki</name>
    </author>
    <link href="http://arxiv.org/abs/2112.08454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.08454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.05106">
    <id>http://arxiv.org/abs/2112.05106v1</id>
    <updated>2021-12-09T18:45:13Z</updated>
    <published>2021-12-09T18:45:13Z</published>
    <title>Estimating the Longest Increasing Subsequence in Nearly Optimal Time</title>
    <summary>  Longest Increasing Subsequence (LIS) is a fundamental statistic of a
sequence, and has been studied for decades. While the LIS of a sequence of
length $n$ can be computed exactly in time $O(n\log n)$, the complexity of
estimating the (length of the) LIS in sublinear time, especially when LIS $\ll
n$, is still open.
  We show that for any integer $n$ and any $\lambda = o(1)$, there exists a
(randomized) non-adaptive algorithm that, given a sequence of length $n$ with
LIS $\ge \lambda n$, approximates the LIS up to a factor of $1/\lambda^{o(1)}$
in $n^{o(1)} / \lambda$ time.
  Our algorithm improves upon prior work substantially in terms of both
approximation and run-time: (i) we provide the first sub-polynomial
approximation for LIS in sub-linear time; and (ii) our run-time complexity
essentially matches the trivial sample complexity lower bound of
$\Omega(1/\lambda)$, which is required to obtain any non-trivial approximation
of the LIS.
  As part of our solution, we develop two novel ideas which may be of
independent interest: First, we define a new Genuine-LIS problem, where each
sequence element may either be genuine or corrupted. In this model, the user
receives unrestricted access to actual sequence, but does not know apriori
which elements are genuine. The goal is to estimate the LIS using genuine
elements only, with the minimal number of "genuiness tests". The second idea,
Precision Forest, enables accurate estimations for composition of general
functions from "coarse" (sub-)estimates. Precision Forest essentially
generalizes classical precision sampling, which works only for summations. As a
central tool, the Precision Forest is initially pre-processed on a set of
samples, which thereafter is repeatedly reused by multiple sub-parts of the
algorithm, improving their amortized complexity.
</summary>
    <author>
      <name>Alexandr Andoni</name>
    </author>
    <author>
      <name>Negev Shekel Nosatzki</name>
    </author>
    <author>
      <name>Sandip Sinha</name>
    </author>
    <author>
      <name>Clifford Stein</name>
    </author>
    <link href="http://arxiv.org/abs/2112.05106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.05106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.04271">
    <id>http://arxiv.org/abs/2112.04271v1</id>
    <updated>2021-12-08T13:05:32Z</updated>
    <published>2021-12-08T13:05:32Z</published>
    <title>RLBWT Tricks</title>
    <summary>  Experts would probably have guessed that compressed sparse bitvectors were an
essential component of pan-genomic indexes based on the run-length compressed
Burrows-Wheeler Transform -- until Nishimoto and Tabei (2021) recently showed
how to replace them. In this paper we experimentally demonstrate the
practicality of part of their result and adapt fractional cascading to obtain a
similar result for the positional Burrows-Wheeler Transform.
</summary>
    <author>
      <name>Nathaniel K. Brown</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <link href="http://arxiv.org/abs/2112.04271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.04271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.12800">
    <id>http://arxiv.org/abs/2111.12800v1</id>
    <updated>2021-11-24T21:14:33Z</updated>
    <published>2021-11-24T21:14:33Z</published>
    <title>Tiny Pointers</title>
    <summary>  This paper introduces a new data-structural object that we call the tiny
pointer. In many applications, traditional $\log n $-bit pointers can be
replaced with $o (\log n )$-bit tiny pointers at the cost of only a
constant-factor time overhead. We develop a comprehensive theory of tiny
pointers, and give optimal constructions for both fixed-size tiny pointers
(i.e., settings in which all of the tiny pointers must be the same size) and
variable-size tiny pointers (i.e., settings in which the average tiny-pointer
size must be small, but some tiny pointers can be larger). If a tiny pointer
references an element in an array filled to load factor $1 - 1 / k$, then the
optimal tiny-pointer size is $\Theta(\log \log \log n + \log k) $ bits in the
fixed-size case, and $ \Theta (\log k) $ expected bits in the variable-size
case. Our tiny-pointer constructions also require us to revisit several classic
problems having to do with balls and bins; these results may be of independent
interest.
  Using tiny pointers, we revisit five classic data-structure problems: the
data-retrieval problem, succinct dynamic binary search trees, space-efficient
stable dictionaries, space-efficient dictionaries with variable-size keys, and
the internal-memory stash problem. These are all well-studied problems, and in
each case tiny pointers allow for us to take a natural space-inefficient
solution that uses pointers and make it space-efficient for free.
</summary>
    <author>
      <name>Michael A. Bender</name>
    </author>
    <author>
      <name>Alex Conway</name>
    </author>
    <author>
      <name>Martín Farach-Colton</name>
    </author>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <author>
      <name>Guido Tagliavini</name>
    </author>
    <link href="http://arxiv.org/abs/2111.12800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.12800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2201.01727">
    <id>http://arxiv.org/abs/2201.01727v1</id>
    <updated>2022-01-01T13:26:04Z</updated>
    <published>2022-01-01T13:26:04Z</published>
    <title>X3: Lossless Data Compressor</title>
    <summary>  X3 is a lossless optimizing dictionary-based data compressor. The algorithm
uses a combination of a dictionary, context modeling, and arithmetic coding.
Optimization adds the ability to find the most appropriate parameters for each
file. Even without optimization, x3 can compress data with a compression ratio
comparable to the best dictionary compression methods like LZMA, zstd, or
Brotli.
</summary>
    <author>
      <name>David Barina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted as a poster on the DCC 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.01727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.01727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2201.01554">
    <id>http://arxiv.org/abs/2201.01554v1</id>
    <updated>2022-01-05T11:46:16Z</updated>
    <published>2022-01-05T11:46:16Z</published>
    <title>Standard Vs Uniform Binary Search and Their Variants in Learned Static
  Indexing: The Case of the Searching on Sorted Data Benchmarking Software
  Platform</title>
    <summary>  The Searching on Sorted Data ({\bf SOSD}, in short) is a highly engineered
software platform for benchmarking Learned Indexes, those latter being a novel
and quite effective proposal of how to search in a sorted table by combining
Machine Learning techniques with classic Algorithms. In such a platform and in
the related benchmarking experiments, following a natural and intuitive choice,
the final search stage is performed via the Standard (textbook) Binary Search
procedure. However, recent studies, that do not use Machine Learning
predictions, indicate that Uniform Binary Search, streamlined to avoid
\vir{branching} in the main loop, is superior in performance to its Standard
counterpart when the table to be searched into is relatively small, e.g.,
fitting in L1 or L2 cache. Analogous results hold for k-ary Search, even on
large tables. One would expect an analogous behaviour within Learned Indexes.
Via a set of extensive experiments, coherent with the State of the Art, we show
that for Learned Indexes, and as far as the {\bf SOSD} software is concerned,
the use of the Standard routine (either Binary or k-ary Search) is superior to
the Uniform one, across all the internal memory levels. This fact provides a
quantitative justification of the natural choice made so far. Our experiments
also indicate that Uniform Binary and k-ary Search can be advantageous to use
in order to save space in Learned Indexes, while granting a good performance in
time. Our findings are of methodological relevance for this novel and
fast-growing area and informative to practitioners interested in using Learned
Indexes in application domains, e.g., Data Bases and Search Engines.
</summary>
    <author>
      <name>Domenico Amato</name>
    </author>
    <author>
      <name>Giosuè Lo Bosco</name>
    </author>
    <author>
      <name>Raffaele Giancarlo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2107.09480</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.01554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.01554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; I.2; H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2201.01742">
    <id>http://arxiv.org/abs/2201.01742v1</id>
    <updated>2022-01-05T18:05:41Z</updated>
    <published>2022-01-05T18:05:41Z</published>
    <title>What Does Dynamic Optimality Mean in External Memory?</title>
    <summary>  In this paper, we revisit the question of how the dynamic optimality of
search trees should be defined in external memory. A defining characteristic of
external-memory data structures is that there is a stark asymmetry between
queries and inserts/updates/deletes: by making the former slightly
asymptotically slower, one can make the latter significantly asymptotically
faster (even allowing for operations with sub-constant amortized I/Os). This
asymmetry makes it so that rotation-based search trees are not optimal (or even
close to optimal) in insert/update/delete-heavy external-memory workloads. To
study dynamic optimality for such workloads, one must consider a different
class of data structures.
  The natural class of data structures to consider are what we call
buffered-propagation trees. Such trees can adapt dynamically to the locality
properties of an input sequence in order to optimize the interactions between
different inserts/updates/deletes and queries. We also present a new form of
beyond-worst-case analysis that allows for us to formally study a continuum
between static and dynamic optimality. Finally, we give a novel data structure,
called the \jellotree, that is statically optimal and that achieves dynamic
optimality for a large natural class of inputs defined by our beyond-worst-case
analysis.
</summary>
    <author>
      <name>Micael A. Bender</name>
    </author>
    <author>
      <name>Martín Farach-Colton</name>
    </author>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.01742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.01742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2201.01285">
    <id>http://arxiv.org/abs/2201.01285v1</id>
    <updated>2022-01-04T18:28:45Z</updated>
    <published>2022-01-04T18:28:45Z</published>
    <title>Dynamic Suffix Array with Polylogarithmic Queries and Updates</title>
    <summary>  The suffix array $SA[1..n]$ of a text $T$ of length $n$ is a permutation of
$\{1,\ldots,n\}$ describing the lexicographical ordering of suffixes of $T$,
and it is considered to be among of the most important data structures in
string algorithms, with dozens of applications in data compression,
bioinformatics, and information retrieval. One of the biggest drawbacks of the
suffix array is that it is very difficult to maintain under text updates: even
a single character substitution can completely change the contents of the
suffix array. Thus, the suffix array of a dynamic text is modelled using suffix
array queries, which return the value $SA[i]$ given any $i\in[1..n]$.
  Prior to this work, the fastest dynamic suffix array implementations were by
Amir and Boneh. At ISAAC 2020, they showed how to answer suffix array queries
in $\tilde{O}(k)$ time, where $k\in[1..n]$ is a trade-off parameter, with
$\tilde{O}(\frac{n}{k})$-time text updates. In a very recent preprint [2021],
they also provided a solution with $O(\log^5 n)$-time queries and
$\tilde{O}(n^{2/3})$-time updates.
  We propose the first data structure that supports both suffix array queries
and text updates in $O({\rm polylog}\,n)$ time (achieving $O(\log^4 n)$ and
$O(\log^{3+o(1)} n)$ time, respectively). Our data structure is deterministic
and the running times for all operations are worst-case. In addition to the
standard single-character edits (character insertions, deletions, and
substitutions), we support (also in $O(\log^{3+o(1)} n)$ time) the "cut-paste"
operation that moves any (arbitrarily long) substring of $T$ to any place in
$T$. We complement our structure by a hardness result: unless the Online
Matrix-Vector Multiplication (OMv) Conjecture fails, no data structure with
$O({\rm polylog}\,n)$-time suffix array queries can support the "copy-paste"
operation in $O(n^{1-\epsilon})$ time for any $\epsilon>0$.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">83 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.01285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.01285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.12678">
    <id>http://arxiv.org/abs/2112.12678v1</id>
    <updated>2021-12-23T16:14:35Z</updated>
    <published>2021-12-23T16:14:35Z</published>
    <title>Dynamic Suffix Array with Sub-linear update time and Poly-logarithmic
  Lookup Time</title>
    <summary>  The Suffix Array $SA_S[1\ldots n]$ of an $n$-length string $S$ is a
lexicographically sorted array of the suffixes of $S$. The suffix array is one
of the most well known and widely used data structures in string algorithms. We
present a data structure for maintaining a representation of the suffix array
of a dynamic string which undergoes symbol substitutions, deletions, and
insertions.
  For every string manipulation, our data structure can be updated in
$O(n^{\frac{2}{3}})$ time (ignoring multiplicative polylogarithmic factors)
with $n$ being the current length of the string. For an input query $i\in
[1\ldots n]$, our data structure reports $SA_S[i]$ in $O(\log^5(n))$ time.
  We also present a faster data structure, with $O(\sqrt{n})$ update time
(ignoring multiplicative polylogarithmic factors), for maintaining the Inverted
Suffix Array of a dynamic string undergoing symbol substitutions updates. For
an input query $i\in [1\ldots n]$, our data structure reports the $i$'th entry
in the inverted suffix array in $O(\log^4(n))$ time.
  Our data structures can be used to obtain sub-linear dynamic algorithms for
several classical string problems for which efficient dynamic solutions were
not previously known.
</summary>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Itai Boneh</name>
    </author>
    <link href="http://arxiv.org/abs/2112.12678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.12678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.12013">
    <id>http://arxiv.org/abs/2012.12013v1</id>
    <updated>2020-12-21T16:41:59Z</updated>
    <published>2020-12-21T16:41:59Z</published>
    <title>SARS-CoV-2 Coronavirus Data Compression Benchmark</title>
    <summary>  This paper introduces a lossless data compression competition that benchmarks
solutions (computer programs) by the compressed size of the 44,981 concatenated
SARS-CoV-2 sequences, with a total uncompressed size of 1,339,868,341 bytes.
The data, downloaded on 13 December 2020, from the severe acute respiratory
syndrome coronavirus 2 data hub of ncbi.nlm.nih.gov is presented in FASTA and
2Bit format. The aim of this competition is to encourage multidisciplinary
research to find the shortest lossless description for the sequences and to
demonstrate that data compression can serve as an objective and repeatable
measure to align scientific breakthroughs across disciplines. The shortest
description of the data is the best model; therefore, further reducing the size
of this description requires a fundamental understanding of the underlying
context and data. This paper presents preliminary results with multiple
well-known compression algorithms for baseline measurements, and insights
regarding promising research avenues. The competition's progress will be
reported at \url{https://coronavirus.innar.com}, and the benchmark is open for
all to participate and contribute.
</summary>
    <author>
      <name>Innar Liiv</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.12013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.12013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2201.11550">
    <id>http://arxiv.org/abs/2201.11550v1</id>
    <updated>2022-01-27T14:44:21Z</updated>
    <published>2022-01-27T14:44:21Z</published>
    <title>Predecessor on the Ultra-Wide Word RAM</title>
    <summary>  We consider the predecessor problem on the ultra-wide word RAM model of
computation, which extends the word RAM model with 'ultrawords' consisting of
$w^2$ bits [TAMC, 2015]. The model supports arithmetic and boolean operations
on ultrawords, in addition to 'scattered' memory operations that access or
modify $w$ (potentially non-contiguous) memory addresses simultaneously. The
ultra-wide word RAM model captures (and idealizes) modern vector processor
architectures.
  Our main result is a simple, linear space data structure that supports
predecessor in constant time and updates in amortized, expected constant time.
This improves the space of the previous constant time solution that uses space
in the order of the size of the universe.
  Our result is based on a new implementation of the classic $x$-fast trie data
structure of Willard [Inform.~Process.~Lett. 17(2), 1983] combined with a new
dictionary data structure that supports fast parallel lookups.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Tord Stordalen</name>
    </author>
    <link href="http://arxiv.org/abs/2201.11550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.11550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2202.08447">
    <id>http://arxiv.org/abs/2202.08447v2</id>
    <updated>2022-04-14T23:52:57Z</updated>
    <published>2022-02-17T04:37:27Z</published>
    <title>RePair Grammars are the Smallest Grammars for Fibonacci Words</title>
    <summary>  Grammar-based compression is a loss-less data compression scheme that
represents a given string $w$ by a context-free grammar that generates only
$w$. While computing the smallest grammar which generates a given string $w$ is
NP-hard in general, a number of polynomial-time grammar-based compressors which
work well in practice have been proposed. RePair, proposed by Larsson and
Moffat in 1999, is a grammar-based compressor which recursively replaces all
possible occurrences of a most frequently occurring bigrams in the string.
Since there can be multiple choices of the most frequent bigrams to replace,
different implementations of RePair can result in different grammars. In this
paper, we show that the smallest grammars generating the Fibonacci words $F_k$
can be completely characterized by RePair, where $F_k$ denotes the $k$-th
Fibonacci word. Namely, all grammars for $F_k$ generated by any implementation
of RePair are the smallest grammars for $F_k$, and no other grammars can be the
smallest for $F_k$. To the best of our knowledge, Fibonacci words are the first
non-trivial infinite family of strings for which RePair is optimal.
</summary>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Takashi Horiyama</name>
    </author>
    <link href="http://arxiv.org/abs/2202.08447v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.08447v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2202.07189">
    <id>http://arxiv.org/abs/2202.07189v1</id>
    <updated>2022-02-15T04:52:01Z</updated>
    <published>2022-02-15T04:52:01Z</published>
    <title>Longest (Sub-)Periodic Subsequence</title>
    <summary>  We present an algorithm computing the longest periodic subsequence of a
string of length $n$ in $O(n^7)$ time with $O(n^4)$ words of space. We obtain
improvements when restricting the exponents or extending the search allowing
the reported subsequence to be subperiodic down to $O(n^3)$ time and $O(n^2)$
words of space.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <link href="http://arxiv.org/abs/2202.07189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.07189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2202.06834">
    <id>http://arxiv.org/abs/2202.06834v1</id>
    <updated>2022-02-06T16:25:11Z</updated>
    <published>2022-02-06T16:25:11Z</published>
    <title>Memory Efficient Tries for Sequential Pattern Mining</title>
    <summary>  The rapid and continuous growth of data has increased the need for scalable
mining algorithms in unsupervised learning and knowledge discovery. In this
paper, we focus on Sequential Pattern Mining (SPM), a fundamental topic in
knowledge discovery that faces a well-known memory bottleneck. We examine
generic dataset modeling techniques and show how they can be used to improve
SPM algorithms in time and memory usage. In particular, we develop trie-based
dataset models and associated mining algorithms that can represent as well as
effectively mine orders of magnitude larger datasets compared to the state of
the art. Numerical results on real-life large-size test instances show that our
algorithms are also faster and more memory efficient in practice.
</summary>
    <author>
      <name>Amin Hosseininasab</name>
    </author>
    <author>
      <name>Willem-Jan van Hoeve</name>
    </author>
    <author>
      <name>Andre A. Cire</name>
    </author>
    <link href="http://arxiv.org/abs/2202.06834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.06834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2202.05085">
    <id>http://arxiv.org/abs/2202.05085v1</id>
    <updated>2022-02-10T15:13:37Z</updated>
    <published>2022-02-10T15:13:37Z</published>
    <title>MONI can find k-MEMs</title>
    <summary>  Maximal exact matches (MEMs) have been widely used in bioinformatics at least
since Li (2013) presented BWA-MEM. Building on work by Bannai, Gagie and I
(2018), Rossi et al.\ (2022) recently built an index called MONI, based on the
run-length compressed Burrows-Wheeler Transform, that can find MEMs efficiently
with respect to pangenomes. In this paper we define $k$-MEMs to be maximal
substrings of a pattern that each occur exactly at $k$ times in a text (so a
MEM is a 1-MEM) and show that, when $k$ is given at construction time, MONI can
find $k$-MEMs efficiently as well.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/2202.05085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.05085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2202.04349">
    <id>http://arxiv.org/abs/2202.04349v2</id>
    <updated>2022-04-15T00:19:19Z</updated>
    <published>2022-02-09T09:00:13Z</published>
    <title>Cartesian Tree Subsequence Matching</title>
    <summary>  Park et al. [TCS 2020] observed that the similarity between two (numerical)
strings can be captured by the Cartesian trees: The Cartesian tree of a string
is a binary tree recursively constructed by picking up the smallest value of
the string as the root of the tree. Two strings of equal length are said to
Cartesian-tree match if their Cartesian trees are isomorphic. Park et al. [TCS
2020] introduced the following Cartesian tree substring matching (CTMStr)
problem: Given a text string $T$ of length $n$ and a pattern string of length
$m$, find every consecutive substring $S = T[i..j]$ of a text string $T$ such
that $S$ and $P$ Cartesian-tree match. They showed how to solve this problem in
$\tilde{O}(n+m)$ time. In this paper, we introduce the Cartesian tree
subsequence matching (CTMSeq) problem, that asks to find every minimal
substring $S = T[i..j]$ of $T$ such that $S$ contains a subsequence $S'$ which
Cartesian-tree matches $P$. We prove that the CTMSeq problem can be solved
efficiently, in $O(m n p(n))$ time, where $p(n)$ denotes the update/query time
for dynamic predecessor queries. By using a suitable dynamic predecessor data
structure, we obtain $O(mn \log \log n)$-time and $O(n \log m)$-space solution
for CTMSeq. This contrasts CTMSeq with closely related order-preserving
subsequence matching (OPMSeq) which was shown to be NP-hard by Bose et al. [IPL
1998].
</summary>
    <author>
      <name>Tsubasa Oizumi</name>
    </author>
    <author>
      <name>Takeshi Kai</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hiroki Arimura</name>
    </author>
    <link href="http://arxiv.org/abs/2202.04349v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.04349v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2202.02609">
    <id>http://arxiv.org/abs/2202.02609v1</id>
    <updated>2022-02-05T18:16:47Z</updated>
    <published>2022-02-05T18:16:47Z</published>
    <title>Logarithmic equal-letter runs for BWT of purely morphic words</title>
    <summary>  In this paper we study the number $r_{bwt}$ of equal-letter runs produced by
the Burrows-Wheeler transform ($BWT$) when it is applied to purely morphic
finite words, which are words generated by iterating prolongable morphisms.
Such a parameter $r_{bwt}$ is very significant since it provides a measure of
the performances of the $BWT$, in terms of both compressibility and indexing.
In particular, we prove that, when $BWT$ is applied to any purely morphic
finite word on a binary alphabet, $r_{bwt}$ is $\mathcal{O}(\log n)$, where $n$
is the length of the word. Moreover, we prove that $r_{bwt}$ is $\Theta(\log
n)$ for the binary words generated by a large class of prolongable binary
morphisms. These bounds are proved by providing some new structural properties
of the \emph{bispecial circular factors} of such words.
</summary>
    <author>
      <name>Andrea Frosini</name>
    </author>
    <author>
      <name>Ilaria Mancini</name>
    </author>
    <author>
      <name>Simone Rinaldi</name>
    </author>
    <author>
      <name>Giuseppe Romana</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <link href="http://arxiv.org/abs/2202.02609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.02609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2201.10372">
    <id>http://arxiv.org/abs/2201.10372v1</id>
    <updated>2022-01-25T15:02:42Z</updated>
    <published>2022-01-25T15:02:42Z</published>
    <title>Safety and Completeness in Flow Decompositions for RNA Assembly</title>
    <summary>  Decomposing a network flow into weighted paths has numerous applications.
Some applications require any decomposition that is optimal w.r.t. some
property such as number of paths, robustness, or length. Many bioinformatic
applications require a specific decomposition where the paths correspond to
some underlying data that generated the flow. For real inputs, no optimization
criteria guarantees to uniquely identify the correct decomposition. Therefore,
we propose to report safe paths, i.e., subpaths of at least one path in every
flow decomposition.
  Ma, Zheng, and Kingsford [WABI 2020] addressed the existence of multiple
optimal solutions in a probabilistic framework, i.e., non-identifiability.
Later [RECOMB 2021], they gave a quadratic-time algorithm based on a global
criterion for solving a problem called AND-Quant, which generalizes the problem
of reporting whether a given path is safe.
  We give the first local characterization of safe paths for flow
decompositions in directed acyclic graphs (DAGs), leading to a practical
algorithm for finding the complete set of safe paths. We evaluated our
algorithms against the trivial safe algorithms (unitigs, extended unitigs) and
the popularly used heuristic (greedy-width) for flow decomposition on RNA
transcripts datasets. Despite maintaining perfect precision our algorithm
reports significantly higher coverage ($\approx 50\%$ more) than trivial safe
algorithms. The greedy-width algorithm though reporting a better coverage, has
significantly lower precision on complex graphs. Overall, our algorithm
outperforms (by $\approx 20\%$) greedy-width on a unified metric (F-Score) when
the dataset has significant number of complex graphs. Moreover, it has superior
time ($3-5\times$) and space efficiency ($1.2-2.2\times$), resulting in a
better and more practical approach for bioinformatics applications of flow
decomposition.
</summary>
    <author>
      <name>Shahbaz Khan</name>
    </author>
    <author>
      <name>Milla Kortelainen</name>
    </author>
    <author>
      <name>Manuel Cáceres</name>
    </author>
    <author>
      <name>Lucia Williams</name>
    </author>
    <author>
      <name>Alexandru I. Tomescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RECOMB 2022. arXiv admin note: text overlap with arXiv:2102.06480</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.10372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.10372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2201.06773">
    <id>http://arxiv.org/abs/2201.06773v1</id>
    <updated>2022-01-18T07:14:49Z</updated>
    <published>2022-01-18T07:14:49Z</published>
    <title>Computing Longest (Common) Lyndon Subsequences</title>
    <summary>  Given a string $T$ with length $n$ whose characters are drawn from an ordered
alphabet of size $\sigma$, its longest Lyndon subsequence is a longest
subsequence of $T$ that is a Lyndon word. We propose algorithms for finding
such a subsequence in $O(n^3)$ time with $O(n)$ space, or online in $O(n^3
\sigma)$ space and time. Our first result can be extended to find the longest
common Lyndon subsequence of two strings of length $n$ in $O(n^4 \sigma)$ time
using $O(n^3)$ space.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <link href="http://arxiv.org/abs/2201.06773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.06773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2201.05198">
    <id>http://arxiv.org/abs/2201.05198v1</id>
    <updated>2022-01-13T20:19:35Z</updated>
    <published>2022-01-13T20:19:35Z</published>
    <title>Multiple Genome Analytics Framework: The Case of All SARS-CoV-2 Complete
  Variants</title>
    <summary>  Pattern detection and string matching are fundamental problems in computer
science and the accelerated expansion of bioinformatics and computational
biology have made them a core topic for both disciplines. The SARS-CoV-2
pandemic has made such problems more demanding with hundreds or thousands of
new genome variants discovered every week, because of constant mutations, and
there is a desperate need for fast and accurate analyses. The requirement for
computational tools for genomic analyses, such as sequence alignment, is very
important, although, in most cases the resources and computational power
required are enormous. The presented Multiple Genome Analytics Framework
combines data structures and algorithms, specifically built for text mining and
pattern detection, that can help to efficiently address several computational
biology and bioinformatics problems concurrently with minimal resources. A
single execution of advanced algorithms, with space and time complexity
O(nlogn), is enough to acquire knowledge on all repeated patterns that exist in
multiple genome sequences and this information can be used from other
meta-algorithms for further meta-analyses. The potential of the proposed
framework is demonstrated with the analysis of more than 300,000 SARS-CoV-2
genome sequences and the detection of all repeated patterns with length up to
60 nucleotides in these sequences. These results have been used to provide
answers to questions such as common patterns among all variants, sequence
alignment, palindromes and tandem repeats detection, different organism genome
comparisons, polymerase chain reaction primers detection, etc.
</summary>
    <author>
      <name>Konstantinos Xylogiannopoulos</name>
    </author>
    <link href="http://arxiv.org/abs/2201.05198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.05198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2202.07885">
    <id>http://arxiv.org/abs/2202.07885v1</id>
    <updated>2022-02-16T06:37:59Z</updated>
    <published>2022-02-16T06:37:59Z</published>
    <title>An Optimal-Time RLBWT Construction in BWT-runs Bounded Space</title>
    <summary>  The compression of highly repetitive strings (i.e., strings with many
repetitions) has been a central research topic in string processing, and quite
a few compression methods for these strings have been proposed thus far. Among
them, an efficient compression format gathering increasing attention is the
run-length Burrows--Wheeler transform (RLBWT), which is a run-length encoded
BWT as a reversible permutation of an input string on the lexicographical order
of suffixes. State-of-the-art construction algorithms of RLBWT have a serious
issue with respect to (i) non-optimal computation time or (ii) a working space
that is linearly proportional to the length of an input string. In this paper,
we present \emph{r-comp}, the first optimal-time construction algorithm of
RLBWT in BWT-runs bounded space. That is, the computational complexity of
r-comp is $O(n + r \log{r})$ time and $O(r\log{n})$ bits of working space for
the length $n$ of an input string and the number $r$ of equal-letter runs in
BWT. The computation time is optimal (i.e., $O(n)$) for strings with the
property $r=O(n/\log{n})$, which holds for most highly repetitive strings.
Experiments using a real-world dataset of highly repetitive strings show the
effectiveness of r-comp with respect to computation time and space.
</summary>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>Shunsuke Kanda</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/2202.07885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.07885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.04422">
    <id>http://arxiv.org/abs/2204.04422v1</id>
    <updated>2022-04-09T08:15:31Z</updated>
    <published>2022-04-09T08:15:31Z</published>
    <title>Reduction ratio of the IS-algorithm: worst and random cases</title>
    <summary>  We study the IS-algorithm, a well-known linear-time algorithm for computing
the suffix array of a word. This algorithm relies on transforming the input
word $w$ into another word, called the reduced word of $w$, that will be at
least twice shorter; then, the algorithm recursively computes the suffix array
of the reduced word. In this article, we study the reduction ratio of the
IS-algorithm, i.e., the ratio between the lengths of the input word and the
word obtained after reducing $k$ times the input word. We investigate both
worst cases, in which we find precise results, and random cases, where we prove
some strong convergence phenomena. Finally, we prove that, if the input word is
a randomly chosen word of length $n$, we should not expect much more than
$\log(\log(n))$ recursive function calls.
</summary>
    <author>
      <name>Vincent Jugé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages. Article to be published in the proceedings of the 33rd
  Annual Symposium on Combinatorial Pattern Matching (CPM 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.04422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.04422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.04500">
    <id>http://arxiv.org/abs/2204.04500v1</id>
    <updated>2022-04-09T15:52:50Z</updated>
    <published>2022-04-09T15:52:50Z</published>
    <title>Faster Min-Plus Product for Monotone Instances</title>
    <summary>  In this paper, we show that the time complexity of monotone min-plus product
of two $n\times n$ matrices is
$\tilde{O}(n^{(3+\omega)/2})=\tilde{O}(n^{2.687})$, where $\omega &lt; 2.373$ is
the fast matrix multiplication exponent [Alman and Vassilevska Williams 2021].
That is, when $A$ is an arbitrary integer matrix and $B$ is either row-monotone
or column-monotone with integer elements bounded by $O(n)$, computing the
min-plus product $C$ where $C_{i,j}=\min_k\{A_{i,k}+B_{k,j}\}$ takes
$\tilde{O}(n^{(3+\omega)/2})$ time, which greatly improves the previous time
bound of $\tilde{O}(n^{(12+\omega)/5})=\tilde{O}(n^{2.875})$ [Gu, Polak,
Vassilevska Williams and Xu 2021]. Then by simple reductions, this means the
following problems also have $\tilde{O}(n^{(3+\omega)/2})$ time algorithms:
  (1) $A$ and $B$ are both bounded-difference, that is, the difference between
any two adjacent entries is a constant. The previous results give time
complexities of $\tilde{O}(n^{2.824})$ [Bringmann, Grandoni, Saha and
Vassilevska Williams 2016] and $\tilde{O}(n^{2.779})$ [Chi, Duan and Xie 2022].
  (2) $A$ is arbitrary and the columns or rows of $B$ are bounded-difference.
Previous result gives time complexity of $\tilde{O}(n^{2.922})$ [Bringmann,
Grandoni, Saha and Vassilevska Williams 2016].
  (3) The problems reducible to these problems, such as language edit distance,
RNA-folding, scored parsing problem on BD grammars. [Bringmann, Grandoni, Saha
and Vassilevska Williams 2016].
  Finally, we also consider the problem of min-plus convolution between two
integral sequences which are monotone and bounded by $O(n)$, and achieve a
running time upper bound of $\tilde{O}(n^{1.5})$. Previously, this task
requires running time $\tilde{O}(n^{(9+\sqrt{177})/12}) = O(n^{1.859})$ [Chan
and Lewenstein 2015].
</summary>
    <author>
      <name>Shucheng Chi</name>
    </author>
    <author>
      <name>Ran Duan</name>
    </author>
    <author>
      <name>Tianle Xie</name>
    </author>
    <author>
      <name>Tianyi Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.04500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.04500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.03087">
    <id>http://arxiv.org/abs/2204.03087v1</id>
    <updated>2022-04-06T21:03:38Z</updated>
    <published>2022-04-06T21:03:38Z</published>
    <title>Faster Pattern Matching under Edit Distance</title>
    <summary>  We consider the approximate pattern matching problem under the edit distance.
Given a text $T$ of length $n$, a pattern $P$ of length $m$, and a threshold
$k$, the task is to find the starting positions of all substrings of $T$ that
can be transformed to $P$ with at most $k$ edits. More than 20 years ago, Cole
and Hariharan [SODA'98, J. Comput.'02] gave an $\mathcal{O}(n+k^4 \cdot n/
m)$-time algorithm for this classic problem, and this runtime has not been
improved since.
  Here, we present an algorithm that runs in time $\mathcal{O}(n+k^{3.5}
\sqrt{\log m \log k} \cdot n/m)$, thus breaking through this long-standing
barrier. In the case where $n^{1/4+\varepsilon} \leq k \leq
n^{2/5-\varepsilon}$ for some arbitrarily small positive constant
$\varepsilon$, our algorithm improves over the state-of-the-art by polynomial
factors: it is polynomially faster than both the algorithm of Cole and
Hariharan and the classic $\mathcal{O}(kn)$-time algorithm of Landau and
Vishkin [STOC'86, J. Algorithms'89].
  We observe that the bottleneck case of the alternative $\mathcal{O}(n+k^4
\cdot n/m)$-time algorithm of Charalampopoulos, Kociumaka, and Wellnitz
[FOCS'20] is when the text and the pattern are (almost) periodic. Our new
algorithm reduces this case to a new dynamic problem (Dynamic Puzzle Matching),
which we solve by building on tools developed by Tiskin [SODA'10,
Algorithmica'15] for the so-called seaweed monoid of permutation matrices. Our
algorithm relies only on a small set of primitive operations on strings and
thus also applies to the fully-compressed setting (where text and pattern are
given as straight-line programs) and to the dynamic setting (where we maintain
a collection of strings under creation, splitting, and concatenation),
improving over the state of the art.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Philip Wellnitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">94 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.03087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.03087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2203.16908">
    <id>http://arxiv.org/abs/2203.16908v2</id>
    <updated>2022-04-18T11:11:35Z</updated>
    <published>2022-03-31T09:20:43Z</published>
    <title>Suffix tree-based linear algorithms for multiple prefixes, single suffix
  counting and listing problems</title>
    <summary>  Given two strings $T$ and $S$ and a set of strings $P$, for each string $p
\in P$, consider the unique substrings of $T$ that have $p$ as their prefix and
$S$ as their suffix. Two problems then come to mind; the first problem being
the counting of such substrings, and the second problem being the problem of
listing all such substrings. In this paper, we describe linear-time,
linear-space suffix tree-based algorithms for both problems. More specifically,
we describe an $O(|T| + |P|)$ time algorithm for the counting problem, and an
$O(|T| + |P| + \#(ans))$ time algorithm for the listing problem, where
$\#(ans)$ refers to the number of strings being listed in total, and $|P|$
refers to the total length of the strings in $P$. We also consider the reversed
version of the problems, where one prefix condition string and multiple suffix
condition strings are given instead, and similarly describe linear-time,
linear-space algorithms to solve them.
</summary>
    <author>
      <name>Laurentius Leonard</name>
    </author>
    <author>
      <name>Ken Tanaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.16908v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.16908v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2203.14540">
    <id>http://arxiv.org/abs/2203.14540v2</id>
    <updated>2022-03-30T15:58:37Z</updated>
    <published>2022-03-28T07:31:07Z</published>
    <title>Improving Matrix-vector Multiplication via Lossless Grammar-Compressed
  Matrices</title>
    <summary>  As nowadays Machine Learning (ML) techniques are generating huge data
collections, the problem of how to efficiently engineer their storage and
operations is becoming of paramount importance. In this article we propose a
new lossless compression scheme for real-valued matrices which achieves
efficient performance in terms of compression ratio and time for linear-algebra
operations. Experiments show that, as a compressor, our tool is clearly
superior to gzip and it is usually within 20% of xz in terms of compression
ratio. In addition, our compressed format supports matrix-vector
multiplications in time and space proportional to the size of the compressed
representation, unlike gzip and xz that require the full decompression of the
compressed matrix. To our knowledge our lossless compressor is the first one
achieving time and space complexities which match the theoretical limit
expressed by the $k$-th order statistical entropy of the input.
  To achieve further time/space reductions, we propose column-reordering
algorithms hinging on a novel column-similarity score. Our experiments on
various data sets of ML matrices show that, with a modest preprocessing time,
our column reordering can yield a further reduction of up to 16% in the peak
memory usage during matrix-vector multiplication.
  Finally, we compare our proposal against the state-of-the-art Compressed
Linear Algebra (CLA) approach showing that ours runs always at least twice
faster (in a multi-thread setting) and achieves better compressed space
occupancy for most of the tested data sets. This experimentally confirms the
provably effective theoretical bounds we show for our compressed-matrix
approach.
</summary>
    <author>
      <name>Paolo Ferragina</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Manuel Striani</name>
    </author>
    <author>
      <name>Francesco Tosoni</name>
    </author>
    <link href="http://arxiv.org/abs/2203.14540v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.14540v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2203.11341">
    <id>http://arxiv.org/abs/2203.11341v1</id>
    <updated>2022-03-21T21:13:18Z</updated>
    <published>2022-03-21T21:13:18Z</published>
    <title>Binary codes that do not preserve primitivity</title>
    <summary>  A code $X$ is not primitivity preserving if there is a primitive list
${\mathbf w} \in {\tt lists} X$ whose concatenation is imprimitive. We
formalize a full characterization of such codes in the binary case in the proof
assistant Isabelle/HOL. Part of the formalization, interesting on its own, is a
description of $\{x,y\}$-interpretations of the square $xx$ if $|y| \leq |x|$.
We also provide a formalized parametric solution of the related equation
$x^jy^k = z^\ell$.
</summary>
    <author>
      <name>Štěpán Holub</name>
    </author>
    <author>
      <name>Martin Raška</name>
    </author>
    <author>
      <name>Štěpán Starosta</name>
    </author>
    <link href="http://arxiv.org/abs/2203.11341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.11341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2203.10504">
    <id>http://arxiv.org/abs/2203.10504v1</id>
    <updated>2022-03-20T09:33:26Z</updated>
    <published>2022-03-20T09:33:26Z</published>
    <title>Note on a Fibonacci Parity Sequence</title>
    <summary>  Let ftm = 0111010010001... be the analogue of the Thue-Morse sequence in
Fibonacci representation. In this note we show how, using the Walnut
theorem-prover, to obtain a measure of its complexity, previously studied by
Jamet, Popoli, and Stoll. We strengthen one of their theorems and disprove one
of their conjectures.
</summary>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2203.10504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.10504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2202.13884">
    <id>http://arxiv.org/abs/2202.13884v1</id>
    <updated>2022-02-28T15:33:37Z</updated>
    <published>2022-02-28T15:33:37Z</published>
    <title>Numeric Lyndon-based feature embedding of sequencing reads for machine
  learning approaches</title>
    <summary>  Feature embedding methods have been proposed in literature to represent
sequences as numeric vectors to be used in some bioinformatics investigations,
such as family classification and protein structure prediction. Recent
theoretical results showed that the well-known Lyndon factorization preserves
common factors in overlapping strings. Surprisingly, the fingerprint of a
sequencing read, which is the sequence of lengths of consecutive factors in
variants of the Lyndon factorization of the read, is effective in preserving
sequence similarities, suggesting it as basis for the definition of novels
representations of sequencing reads. We propose a novel feature embedding
method for Next-Generation Sequencing (NGS) data using the notion of
fingerprint. We provide a theoretical and experimental framework to estimate
the behaviour of fingerprints and of the k-mers extracted from it, called
k-fingers, as possible feature embeddings for sequencing reads. As a case study
to assess the effectiveness of such embeddings, we use fingerprints to
represent RNA-Seq reads and to assign them to the most likely gene from which
they were originated as fragments of transcripts of the gene. We provide an
implementation of the proposed method in the tool lyn2vec, which produces
Lyndon-based feature embeddings of sequencing reads.
</summary>
    <author>
      <name>Paola Bonizzoni</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Milano-Bicocca</arxiv:affiliation>
    </author>
    <author>
      <name>Matteo Costantini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Milano-Bicocca</arxiv:affiliation>
    </author>
    <author>
      <name>Clelia De Felice</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Salerno</arxiv:affiliation>
    </author>
    <author>
      <name>Alessia Petescia</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Milano-Bicocca</arxiv:affiliation>
    </author>
    <author>
      <name>Yuri Pirola</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Milano-Bicocca</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Previtali</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Milano-Bicocca</arxiv:affiliation>
    </author>
    <author>
      <name>Raffaella Rizzi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Milano-Bicocca</arxiv:affiliation>
    </author>
    <author>
      <name>Jens Stoye</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Bielefeld</arxiv:affiliation>
    </author>
    <author>
      <name>Rocco Zaccagnino</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Salerno</arxiv:affiliation>
    </author>
    <author>
      <name>Rosalba Zizza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Salerno</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2202.13884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.13884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; F.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2202.13235">
    <id>http://arxiv.org/abs/2202.13235v1</id>
    <updated>2022-02-26T21:42:04Z</updated>
    <published>2022-02-26T21:42:04Z</published>
    <title>A theoretical and experimental analysis of BWT variants for string
  collections</title>
    <summary>  The extended Burrows-Wheeler-Transform (eBWT), introduced by Mantaci et al.
[Theor. Comput. Sci., 2007], is a generalization of the
Burrows-Wheeler-Transform (BWT) to multisets of strings. While the original BWT
is based on the lexicographic order, the eBWT uses the omega-order, which
differs from the lexicographic order in important ways. A number of tools are
available that compute the BWT of string collections; however, the data
structures they generate in most cases differ from the one originally defined,
as well as from each other. In this paper, we review the differences between
these BWT variants, both from a theoretical and from a practical point of view,
comparing them on several real-life datasets with different characteristics. We
find that the differences can be extensive, depending on the dataset
characteristics, and are largest on collections of many highly similar short
sequences. The widely-used parameter $r$, the number of runs of the BWT, also
shows notable variation between the different BWT variants; on our datasets, it
varied by a multiplicative factor of up to $4.2$.
</summary>
    <author>
      <name>Davide Cenzato</name>
    </author>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.13235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.13235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2202.13591">
    <id>http://arxiv.org/abs/2202.13591v2</id>
    <updated>2022-04-14T23:34:10Z</updated>
    <published>2022-02-28T07:49:16Z</published>
    <title>Minimal Absent Words on Run-Length Encoded Strings</title>
    <summary>  A string $w$ is called a minimal absent word (MAW) for another string $T$ if
$w$ does not occur (as a substring) in $T$ and any proper substring of $w$
occurs in $T$. State-of-the-art data structures for reporting the set
$\mathsf{MAW}(T)$ of MAWs from a given string $T$ of length $n$ require $O(n)$
space, can be built in $O(n)$ time, and can report all MAWs in
$O(|\mathsf{MAW}(T)|)$ time upon a query. This paper initiates the problem of
computing MAWs from a compressed representation of a string. In particular, we
focus on the most basic compressed representation of a string, run-length
encoding (RLE), which represents each maximal run of the same characters $a$ by
$a^p$ where $p$ is the length of the run. Let $m$ be the RLE-size of string
$T$. After categorizing the MAWs into five disjoint sets $\mathcal{M}_1$,
$\mathcal{M}_2$, $\mathcal{M}_3$, $\mathcal{M}_4$, $\mathcal{M}_5$ using RLE,
we present matching upper and lower bounds for the number of MAWs in
$\mathcal{M}_i$ for $i = 1,2,4,5$ in terms of RLE-size $m$, except for
$\mathcal{M}_3$ whose size is unbounded by $m$. We then present a compact
$O(m)$-space data structure that can report all MAWs in optimal
$O(|\mathsf{MAW}(T)|)$ time.
</summary>
    <author>
      <name>Tooru Akagi</name>
    </author>
    <author>
      <name>Kouta Okabe</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for CPM 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.13591v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.13591v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.12801">
    <id>http://arxiv.org/abs/2204.12801v1</id>
    <updated>2022-04-27T09:43:00Z</updated>
    <published>2022-04-27T09:43:00Z</published>
    <title>Speeding Hirschberg Algorithm for Sequence Alignment</title>
    <summary>  The use of Hirschberg algorithm reduces the spatial cost of recovering the
Longest Common Subsequence to linear space. The same technique can be applied
to similar problems like Sequence Alignment. However, the price to pay is a
duplication of temporal cost. We present here a technique to reduce this time
overhead to a negligible amount.
</summary>
    <author>
      <name>David Llorens</name>
    </author>
    <author>
      <name>Juan Miguel Vilar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages. Submitted to Fundamenta Informaticae</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.12801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.12801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.10932">
    <id>http://arxiv.org/abs/2204.10932v1</id>
    <updated>2022-04-22T21:25:12Z</updated>
    <published>2022-04-22T21:25:12Z</published>
    <title>Listing, Verifying and Counting Lowest Common Ancestors in DAGs:
  Algorithms and Fine-Grained Lower Bounds</title>
    <summary>  The AP-LCA problem asks, given an $n$-node directed acyclic graph (DAG), to
compute for every pair of vertices $u$ and $v$ in the DAG a lowest common
ancestor (LCA) of $u$ and $v$ if one exists. In this paper we study several
interesting variants of AP-LCA, providing both algorithms and fine-grained
lower bounds for them. The lower bounds we obtain are the first conditional
lower bounds for LCA problems higher than $n^{\omega-o(1)}$, where $\omega$ is
the matrix multiplication exponent. Some of our results include:
  - In any DAG, we can detect all vertex pairs that have at most two LCAs and
list all of their LCAs in $O(n^\omega)$ time. This algorithm extends a result
of [Kowaluk and Lingas ESA'07] which showed an $\tilde{O}(n^\omega)$ time
algorithm that detects all pairs with a unique LCA in a DAG and outputs their
corresponding LCAs.
  - Listing $7$ LCAs per vertex pair in DAGs requires $n^{3-o(1)}$ time under
the popular assumption that 3-uniform 5-hyperclique detection requires
$n^{5-o(1)}$ time. This is surprising since essentially cubic time is
sufficient to list all LCAs (if $\omega=2$).
  - Counting the number of LCAs for every vertex pair in a DAG requires
$n^{3-o(1)}$ time under the Strong Exponential Time Hypothesis, and
$n^{\omega(1,2,1)-o(1)}$ time under the $4$-Clique hypothesis. This shows that
the algorithm of [Echkardt, M\"{u}hling and Nowak ESA'07] for listing all LCAs
for every pair of vertices is likely optimal.
  - Given a DAG and a vertex $w_{u,v}$ for every vertex pair $u,v$, verifying
whether all $w_{u,v}$ are valid LCAs requires $n^{2.5-o(1)}$ time assuming
3-uniform 4-hyperclique requires $n^{4 - o(1)}$ time. This defies the common
intuition that verification is easier than computation since returning some LCA
per vertex pair can be solved in $O(n^{2.447})$ time [Grandoni et al. SODA'21].
</summary>
    <author>
      <name>Surya Mathialagan</name>
    </author>
    <author>
      <name>Virginia Vassilevska Williams</name>
    </author>
    <author>
      <name>Yinzhan Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICALP 2022. Abstract shortened to fit arXiv requirement</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.10932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.10932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.11213">
    <id>http://arxiv.org/abs/2204.11213v1</id>
    <updated>2022-04-24T08:12:49Z</updated>
    <published>2022-04-24T08:12:49Z</published>
    <title>String Rearrangement Inequalities and a Total Order Between Primitive
  Words</title>
    <summary>  We study the following rearrangement problem: Given $n$ words, rearrange and
concatenate them so that the obtained string is lexicographically smallest (or
largest, respectively). We show that this problem reduces to sorting the given
words so that their repeating strings are non-decreasing (or non-increasing,
respectively), where the repeating string of a word $A$ refers to the infinite
string $AAA\ldots$. Moreover, for fixed size alphabet $\Sigma$, we design an
$O(L)$ time sorting algorithm of the words (in the mentioned orders), where $L$
denotes the total length of the input words. Hence we obtain an $O(L)$ time
algorithm for the rearrangement problem. Finally, we point out that comparing
primitive words via comparing their repeating strings leads to a total order,
which can further be extended to a total order on the finite words (or all
words).
</summary>
    <author>
      <name>Ruixi Luo</name>
    </author>
    <author>
      <name>Taikun Zhu</name>
    </author>
    <author>
      <name>Kai Jin</name>
    </author>
    <link href="http://arxiv.org/abs/2204.11213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.11213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.09535">
    <id>http://arxiv.org/abs/2204.09535v1</id>
    <updated>2022-04-20T15:19:37Z</updated>
    <published>2022-04-20T15:19:37Z</published>
    <title>Theoretical analysis of edit distance algorithms: an applied perspective</title>
    <summary>  Given its status as a classic problem and its importance to both
theoreticians and practitioners, edit distance provides an excellent lens
through which to understand how the theoretical analysis of algorithms impacts
practical implementations. From an applied perspective, the goals of
theoretical analysis are to predict the empirical performance of an algorithm
and to serve as a yardstick to design novel algorithms that perform well in
practice. In this paper, we systematically survey the types of theoretical
analysis techniques that have been applied to edit distance and evaluate the
extent to which each one has achieved these two goals. These techniques include
traditional worst-case analysis, worst-case analysis parametrized by edit
distance or entropy or compressibility, average-case analysis, semi-random
models, and advice-based models. We find that the track record is mixed. On one
hand, two algorithms widely used in practice have been born out of theoretical
analysis and their empirical performance is captured well by theoretical
predictions. On the other hand, all the algorithms developed using theoretical
analysis as a yardstick since then have not had any practical relevance. We
conclude by discussing the remaining open problems and how they can be tackled.
</summary>
    <author>
      <name>Paul Medvedev</name>
    </author>
    <link href="http://arxiv.org/abs/2204.09535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.10204">
    <id>http://arxiv.org/abs/2204.10204v2</id>
    <updated>2022-04-25T20:33:51Z</updated>
    <published>2022-04-21T15:40:24Z</published>
    <title>On the number of squares in a finite word</title>
    <summary>  A {\em square} is a word of the form $uu$. In this paper we prove that for a
given finite word $w$, the number of distinct square factors of $w$ is bounded
by $|w|-|\Alphabet(w)|+1$, where $|w|$ denotes the length of $w$ and
$|\Alphabet(w)|$ denotes the number of distinct letters in $w$. This result
answers a conjecture of Fraenkel and Simpson stated in 1998.
</summary>
    <author>
      <name>Srečko Brlek</name>
    </author>
    <author>
      <name>Shuo Li</name>
    </author>
    <link href="http://arxiv.org/abs/2204.10204v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.10204v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15, 68R10" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.09562">
    <id>http://arxiv.org/abs/2204.09562v1</id>
    <updated>2022-04-20T15:59:47Z</updated>
    <published>2022-04-20T15:59:47Z</published>
    <title>Fast Circular Pattern Matching</title>
    <summary>  The Exact Circular Pattern Matching (ECPM) problem consists of reporting
every occurrence of a rotation of a pattern $P$ in a text $T$. In many
real-world applications, specifically in computational biology, circular
rotations are of interest because of their prominence in virus DNA. Thus, given
no restrictions on pre-processing time, how quickly all such circular rotation
occurrences is of interest to many areas of study. We highlight, to the best of
our knowledge, a novel approach to the ECPM problem and present four data
structures that accompany this approach, each with their own time-space
trade-offs, in addition to experimental results to determine the most
computationally feasible data structure.
</summary>
    <author>
      <name>Will Solow</name>
    </author>
    <author>
      <name>Matthew Barich</name>
    </author>
    <author>
      <name>Brendan Mumey</name>
    </author>
    <link href="http://arxiv.org/abs/2204.09562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.08331">
    <id>http://arxiv.org/abs/2204.08331v2</id>
    <updated>2022-05-02T19:27:20Z</updated>
    <published>2022-04-18T14:15:43Z</published>
    <title>Practical KMP/BM Style Pattern-Matching on Indeterminate Strings</title>
    <summary>  In this paper we describe two simple, fast, space-efficient algorithms for
finding all matches of an indeterminate pattern $p = p[1..m]$ in an
indeterminate string $x = x[1..n]$, where both $p$ and $x$ are defined on a
"small" ordered alphabet $\Sigma$ $-$ say, $\sigma = |\Sigma| \le 9$. Both
algorithms depend on a preprocessing phase that replaces $\Sigma$ by an integer
alphabet $\Sigma_I$ of size $\sigma_I = \sigma$ which (reversibly, in time
linear in string length) maps both $x$ and $p$ into equivalent regular strings
$y$ and $q$, respectively, on $\Sigma_I$, whose maximum (indeterminate) letter
can be expressed in a 32-bit word (for $\sigma \le 4$, thus for DNA sequences,
an 8-bit representation suffices). We first describe an efficient version KMP
Indet of the venerable Knuth-Morris-Pratt algorithm to find all occurrences of
$q$ in $y$ (that is, of $p$ in $x$), but, whenever necessary, using the prefix
array, rather than the border array, to control shifts of the transformed
pattern $q$ along the transformed string $y$. We go on to describe a similar
efficient version BM Indet of the Boyer- Moore algorithm that turns out to
execute significantly faster than KMP Indet over a wide range of test cases. A
noteworthy feature is that both algorithms require very little additional
space: $\Theta(m)$ words. We conjecture that a similar approach may yield
practical and efficient indeterminate equivalents to other well-known
pattern-matching algorithms, in particular the several variants of Boyer-Moore.
</summary>
    <author>
      <name>Hossein Dehghani</name>
    </author>
    <author>
      <name>Neerja Mhaskar</name>
    </author>
    <author>
      <name>W. F. Smyth</name>
    </author>
    <link href="http://arxiv.org/abs/2204.08331v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.08331v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.07916">
    <id>http://arxiv.org/abs/2204.07916v1</id>
    <updated>2022-04-17T03:33:43Z</updated>
    <published>2022-04-17T03:33:43Z</published>
    <title>An $n H_k$-compressed searchable partial-sums data structure for static
  sequences of sublogarithmic positive integers</title>
    <summary>  We consider the space needed to store a searchable partial-sums data
structure with constant query time for a static sequence $S$ of $n$ positive
integers in $o \left( \frac{\log n}{(\log \log n)^2} \right)$. Arroyuelo and
Raman (2022) recently showed that such a structure can fit in $n H_0 (S) + o
(n)$ bits. Starting with Ferragina and Venturini's (2007) $n H_k$-compressed
representation of strings that supports fast random access, and augmenting it
with sublinear data structures reminiscent of those Raman, Raman and Rao (2002)
used in their succinct bitvectors, we slightly improve Arroyuelo and Raman's
bound to $n H_k (S) + o (n)$ bits for $k \in o \left( \frac{\log n}{(\log \log
n)^2} \right)$.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/2204.07916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.07916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.07327">
    <id>http://arxiv.org/abs/2204.07327v1</id>
    <updated>2022-04-15T04:51:32Z</updated>
    <published>2022-04-15T04:51:32Z</published>
    <title>Shortest Unique Palindromic Substring Queries in Semi-dynamic Settings</title>
    <summary>  A palindromic substring $T[i.. j]$ of a string $T$ is said to be a shortest
unique palindromic substring (SUPS) in $T$ for an interval $[p, q]$ if $T[i..
j]$ is a shortest one such that $T[i.. j]$ occurs only once in $T$, and $[i,
j]$ contains $[p, q]$. The SUPS problem is, given a string $T$ of length $n$,
to construct a data structure that can compute all the SUPSs for any given
query interval. It is known that any SUPS query can be answered in $O(\alpha)$
time after $O(n)$-time preprocessing, where $\alpha$ is the number of SUPSs to
output [Inoue et al., 2018]. In this paper, we first show that $\alpha$ is at
most $4$, and the upper bound is tight. Also, we present an algorithm to solve
the SUPS problem for a sliding window that can answer any query in $O(\log\log
W)$ time and update data structures in amortized $O(\log\sigma)$ time, where
$W$ is the size of the window, and $\sigma$ is the alphabet size. Furthermore,
we consider the SUPS problem in the after-edit model and present an efficient
algorithm. Namely, we present an algorithm that uses $O(n)$ time for
preprocessing and answers any $k$ SUPS queries in $O(\log n\log\log n +
k\log\log n)$ time after single character substitution. As a by-product, we
propose a fully-dynamic data structure for range minimum queries (RmQs) with a
constraint where the width of each query range is limited to polylogarithmic.
The constrained RmQ data structure can answer such a query in constant time and
support a single-element edit operation in amortized constant time.
</summary>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <link href="http://arxiv.org/abs/2204.07327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.07327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.05969">
    <id>http://arxiv.org/abs/2204.05969v1</id>
    <updated>2022-04-12T17:34:17Z</updated>
    <published>2022-04-12T17:34:17Z</published>
    <title>Efficient Construction of the BWT for Repetitive Text Using String
  Compression</title>
    <summary>  We present a new semi-external algorithm that builds the Burrows-Wheeler
transform variant of Bauer et al. (a.k.a., BCR BWT) in linear expected time.
Our method uses compression techniques to reduce the computational costs when
the input is massive and repetitive. Concretely, we build on induced suffix
sorting (ISS) and resort to run-length and grammar compression to maintain our
intermediate results in compact form. Our compression format not only saves
space, but it also speeds up the required computations. Our experiments show
important savings in both space and computation time when the text is
repetitive. On average, we are 3.7x faster than the baseline compressed
approach, while maintaining a similar memory consumption. These results make
our method stand out as the only one (to our knowledge) that can build the BCR
BWT of a collection of 25 human genomes (75 GB) in about 7.3 hours, and using
only 27 GB of working memory.
</summary>
    <author>
      <name>Diego Díaz-Domínguez</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CPM'22</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.05969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.05969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1510.06257">
    <id>http://arxiv.org/abs/1510.06257v1</id>
    <updated>2015-10-21T14:05:24Z</updated>
    <published>2015-10-21T14:05:24Z</published>
    <title>Computing LZ77 in Run-Compressed Space</title>
    <summary>  In this paper, we show that the LZ77 factorization of a text T {\in\Sigma^n}
can be computed in O(R log n) bits of working space and O(n log R) time, R
being the number of runs in the Burrows-Wheeler transform of T reversed. For
extremely repetitive inputs, the working space can be as low as O(log n) bits:
exponentially smaller than the text itself. As a direct consequence of our
result, we show that a class of repetition-aware self-indexes based on a
combination of run-length encoded BWT and LZ77 can be built in asymptotically
optimal O(R + z) words of working space, z being the size of the LZ77 parsing.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Alberto Policriti</name>
    </author>
    <link href="http://arxiv.org/abs/1510.06257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.06257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1401.0936">
    <id>http://arxiv.org/abs/1401.0936v3</id>
    <updated>2016-05-23T13:54:58Z</updated>
    <published>2014-01-05T20:26:13Z</published>
    <title>Linear time construction of compressed text indices in compact space</title>
    <summary>  We show that the compressed suffix array and the compressed suffix tree for a
string of length $n$ over an integer alphabet of size $\sigma\leq n$ can both
be built in $O(n)$ (randomized) time using only $O(n\log\sigma)$ bits of
working space. The previously fastest construction algorithms that used
$O(n\log\sigma)$ bits of space took times $O(n\log\log\sigma)$ and
$O(n\log^{\epsilon}n)$ respectively (where $\epsilon$ is any positive constant
smaller than $1$). In the passing, we show that the Burrows-Wheeler transform
of a string of length $n$ over an alphabet of size $\sigma$ can be built in
deterministic $O(n)$ time and space $O(n\log\sigma)$. We also show that within
the same time and space, we can carry many sequence analysis tasks and
construct some variants of the compressed suffix array and compressed suffix
tree.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Expanded version of a paper appeared in proceedings of STOC 2014
  conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.0936v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0936v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2205.01576">
    <id>http://arxiv.org/abs/2205.01576v1</id>
    <updated>2022-05-03T15:47:56Z</updated>
    <published>2022-05-03T15:47:56Z</published>
    <title>Computing Maximal Unique Matches with the r-index</title>
    <summary>  In recent years, pangenomes received increasing attention from the scientific
community for their ability to incorporate population variation information and
alleviate reference genome bias. Maximal Exact Matches (MEMs) and Maximal
Unique Matches (MUMs) have proven themselves to be useful in multiple
bioinformatic contexts, for example short-read alignment and multiple-genome
alignment. However, standard techniques using suffix trees and FM-indexes do
not scale to a pangenomic level. Recently, Gagie et al. [JACM 20] introduced
the $r$-index that is a Burrows-Wheeler Transform (BWT)-based index able to
handle hundreds of human genomes. Later, Rossi et al. [JCB 22] enabled the
computation of MEMs using the $r$-index, and Boucher et al. [DCC 21] showed how
to compute them in a streaming fashion. In this paper, we show how to augment
Boucher et al.'s approach to enable the computation of MUMs on the $r$-index,
while preserving the space and time bounds. We add additional $O(r)$ samples of
the longest common prefix (LCP) array, where $r$ is the number of equal-letter
runs of the BWT, that permits the computation of the second longest match of
the pattern suffix with respect to the input text, which in turn allows the
computation of candidate MUMs. We implemented a proof-of-concept of our
approach, that we call mum-phinder, and tested on real-world datasets. We
compared our approach with competing methods that are able to compute MUMs. We
observe that our method is up to 8 times smaller, while up to 19 times slower
when the dataset is not highly repetitive, while on highly repetitive data, our
method is up to 6.5 times slower and uses up to 25 times less memory.
</summary>
    <author>
      <name>Sara Giuliani</name>
    </author>
    <author>
      <name>Giuseppe Romana</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Our code is available at: https://github.com/saragiuliani/mum-phinder</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.01576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.01576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2205.00441">
    <id>http://arxiv.org/abs/2205.00441v1</id>
    <updated>2022-05-01T10:41:18Z</updated>
    <published>2022-05-01T10:41:18Z</published>
    <title>Dynamic data structures for parameterized string problems</title>
    <summary>  We revisit classic string problems considered in the area of parameterized
complexity, and study them through the lens of dynamic data structures. That
is, instead of asking for a static algorithm that solves the given instance
efficiently, our goal is to design a data structure that efficiently maintains
a solution, or reports a lack thereof, upon updates in the instance.
  We first consider the Closest String problem, for which we design randomized
dynamic data structures with amortized update times $d^{\mathcal{O}(d)}$ and
$|\Sigma|^{\mathcal{O}(d)}$, respectively, where $\Sigma$ is the alphabet and
$d$ is the assumed bound on the maximum distance. These are obtained by
combining known static approaches to Closest String with color-coding.
  Next, we note that from a result of Frandsen et al.~[J. ACM'97] one can
easily infer a meta-theorem that provides dynamic data structures for
parameterized string problems with worst-case update time of the form
$\mathcal{O}(\log \log n)$, where $k$ is the parameter in question and $n$ is
the length of the string. We showcase the utility of this meta-theorem by
giving such data structures for problems Disjoint Factors and Edit Distance. We
also give explicit data structures for these problems, with worst-case update
times $\mathcal{O}(k2^{k}\log \log n)$ and $\mathcal{O}(k^2\log \log n)$,
respectively. Finally, we discuss how a lower bound methodology introduced by
Amarilli et al.~[ICALP'21] can be used to show that obtaining update time
$\mathcal{O}(f(k))$ for Disjoint Factors and Edit Distance is unlikely already
for a constant value of the parameter $k$.
</summary>
    <author>
      <name>Jędrzej Olkowski</name>
    </author>
    <author>
      <name>Michał Pilipczuk</name>
    </author>
    <author>
      <name>Mateusz Rychlicki</name>
    </author>
    <author>
      <name>Karol Węgrzycki</name>
    </author>
    <author>
      <name>Anna Zych-Pawlewicz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.00441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.13977">
    <id>http://arxiv.org/abs/2204.13977v2</id>
    <updated>2022-05-05T04:46:58Z</updated>
    <published>2022-04-29T10:00:25Z</published>
    <title>Two-dimensional Fibonacci Words: Tandem Repeats and Factor Complexity</title>
    <summary>  If $x$ is a non-empty string then the repetition $xx$ is called a tandem
repeat. Similarly, a tandem in a two dimensional array $X$ is a configuration
consisting of a same primitive block $W$ that touch each other with one side or
corner. In \cite{Apostolico:2000}, Apostolico and Brimkov have proved various
bounds for the number of tandems in a two dimensional word of size $m \times
n$. Of the two types of tandems considered therein, they also proved that, for
one type, the number of occurrences in an $m \times n$ Fibonacci array attained
the general upper bound, $\mathcal{O}(m^{2}n \hspace{0.1cm} \mbox{log}
\hspace{0.1cm} n)$. In this paper, we derive an expression for the exact number
of tandems in a given finite Fibonacci array $f_{m,n}$. As a required result,
we derive the factor complexities of $f_{m,n}$, $m,n \ge 0$ and that of the
infinite Fibonacci word $f_{\infty, \infty}$. Generations of $f_{\infty,
\infty}$ and $f_{m,n}$, for any given $m,n \ge 1$ using a two-dimensional
homomorphism is also achieved.
</summary>
    <author>
      <name>Sivasankar M</name>
    </author>
    <author>
      <name>Rama R</name>
    </author>
    <link href="http://arxiv.org/abs/2204.13977v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.13977v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2204.14137">
    <id>http://arxiv.org/abs/2204.14137v1</id>
    <updated>2022-04-29T14:47:16Z</updated>
    <published>2022-04-29T14:47:16Z</published>
    <title>Improved Sublinear-Time Edit Distance for Preprocessed Strings</title>
    <summary>  We study the problem of approximating the edit distance of two strings in
sublinear time, in a setting where one or both string(s) are preprocessed, as
initiated by Goldenberg, Rubinstein, Saha (STOC '20). Specifically, in the $(k,
K)$-gap edit distance problem, the goal is to distinguish whether the edit
distance of two strings is at most $k$ or at least $K$. We obtain the following
results:
  * After preprocessing one string in time $n^{1+o(1)}$, we can solve $(k, k
\cdot n^{o(1)})$-gap edit distance in time $(n/k + k) \cdot n^{o(1)}$.
  * After preprocessing both strings separately in time $n^{1+o(1)}$, we can
solve $(k, k \cdot n^{o(1)})$-gap edit distance in time $k \cdot n^{o(1)}$.
  Both results improve upon some previously best known result, with respect to
either the gap or the query time or the preprocessing time.
  Our algorithms build on the framework by Andoni, Krauthgamer and Onak (FOCS
'10) and the recent sublinear-time algorithm by Bringmann, Cassis, Fischer and
Nakos (STOC '22). We replace many complicated parts in their algorithm by
faster and simpler solutions which exploit the preprocessing.
</summary>
    <author>
      <name>Karl Bringmann</name>
    </author>
    <author>
      <name>Alejandro Cassis</name>
    </author>
    <author>
      <name>Nick Fischer</name>
    </author>
    <author>
      <name>Vasileios Nakos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears at ICALP '22</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.14137v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.14137v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1511.00898">
    <id>http://arxiv.org/abs/1511.00898v2</id>
    <updated>2016-01-14T15:35:19Z</updated>
    <published>2015-11-03T13:14:37Z</published>
    <title>Burrows-Wheeler transform for terabases</title>
    <summary>  In order to avoid the reference bias introduced by mapping reads to a
reference genome, bioinformaticians are investigating reference-free methods
for analyzing sequenced genomes. With large projects sequencing thousands of
individuals, this raises the need for tools capable of handling terabases of
sequence data. A key method is the Burrows-Wheeler transform (BWT), which is
widely used for compressing and indexing reads. We propose a practical
algorithm for building the BWT of a large read collection by merging the BWTs
of subcollections. With our 2.4 Tbp datasets, the algorithm can merge 600
Gbp/day on a single system, using 30 gigabytes of memory overhead on top of the
run-length encoded BWTs.
</summary>
    <author>
      <name>Jouni Sirén</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the full version of the paper that was accepted to DCC 2016.
  The implementation is available at https://github.com/jltsiren/bwt-merge</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.00898v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00898v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.06374">
    <id>http://arxiv.org/abs/1707.06374v3</id>
    <updated>2018-11-14T17:28:02Z</updated>
    <published>2017-07-20T05:01:22Z</published>
    <title>Document Listing on Repetitive Collections with Guaranteed Performance</title>
    <summary>  We consider document listing on string collections, that is, finding in which
strings a given pattern appears. In particular, we focus on repetitive
collections: a collection of size $N$ over alphabet $[1,\sigma]$ is composed of
$D$ copies of a string of size $n$, and $s$ edits are applied on ranges of
copies. We introduce the first document listing index with size
$\tilde{O}(n+s)$, precisely $O((n\log\sigma+s\log^2 N)\log D)$ bits, and with
useful worst-case time guarantees: Given a pattern of length $m$, the index
reports the $\ndoc>0$ strings where it appears in time $O(m\log^{1+\epsilon} N
\cdot \ndoc)$, for any constant $\epsilon>0$ (and tells in time $O(m\log N)$ if
$\ndoc=0$). Our technique is to augment a range data structure that is commonly
used on grammar-based indexes, so that instead of retrieving all the pattern
occurrences, it computes useful summaries on them. We show that the idea has
independent interest: we introduce the first grammar-based index that, on a
text $T[1,N]$ with a grammar of size $r$, uses $O(r\log N)$ bits and counts the
number of occurrences of a pattern $P[1,m]$ in time $O(m^2 + m\log^{2+\epsilon}
r)$, for any constant $\epsilon>0$. We also give the first index using
$O(z\log(N/z)\log N)$ bits, where $T$ is parsed by Lempel-Ziv into $z$ phrases,
counting occurrences in time $O(m\log^{2+\epsilon} N)$.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of CPM'17 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.06374v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06374v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.11900">
    <id>http://arxiv.org/abs/1907.11900v1</id>
    <updated>2019-07-27T12:00:01Z</updated>
    <published>2019-07-27T12:00:01Z</published>
    <title>DeepCABAC: A Universal Compression Algorithm for Deep Neural Networks</title>
    <summary>  The field of video compression has developed some of the most sophisticated
and efficient compression algorithms known in the literature, enabling very
high compressibility for little loss of information. Whilst some of these
techniques are domain specific, many of their underlying principles are
universal in that they can be adapted and applied for compressing different
types of data. In this work we present DeepCABAC, a compression algorithm for
deep neural networks that is based on one of the state-of-the-art video coding
techniques. Concretely, it applies a Context-based Adaptive Binary Arithmetic
Coder (CABAC) to the network's parameters, which was originally designed for
the H.264/AVC video coding standard and became the state-of-the-art for
lossless compression. Moreover, DeepCABAC employs a novel quantization scheme
that minimizes the rate-distortion function while simultaneously taking the
impact of quantization onto the accuracy of the network into account.
Experimental results show that DeepCABAC consistently attains higher
compression rates than previously proposed coding techniques for neural network
compression. For instance, it is able to compress the VGG16 ImageNet model by
x63.6 with no loss of accuracy, thus being able to represent the entire network
with merely 8.7MB. The source code for encoding and decoding can be found at
https://github.com/fraunhoferhhi/DeepCABAC.
</summary>
    <author>
      <name>Simon Wiedemann</name>
    </author>
    <author>
      <name>Heiner Kirchoffer</name>
    </author>
    <author>
      <name>Stefan Matlage</name>
    </author>
    <author>
      <name>Paul Haase</name>
    </author>
    <author>
      <name>Arturo Marban</name>
    </author>
    <author>
      <name>Talmaj Marinc</name>
    </author>
    <author>
      <name>David Neumann</name>
    </author>
    <author>
      <name>Tung Nguyen</name>
    </author>
    <author>
      <name>Ahmed Osman</name>
    </author>
    <author>
      <name>Detlev Marpe</name>
    </author>
    <author>
      <name>Heiko Schwarz</name>
    </author>
    <author>
      <name>Thomas Wiegand</name>
    </author>
    <author>
      <name>Wojciech Samek</name>
    </author>
    <link href="http://arxiv.org/abs/1907.11900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0706.1084">
    <id>http://arxiv.org/abs/0706.1084v1</id>
    <updated>2007-06-08T02:58:28Z</updated>
    <published>2007-06-08T02:58:28Z</published>
    <title>Sublinear Algorithms for Approximating String Compressibility</title>
    <summary>  We raise the question of approximating the compressibility of a string with
respect to a fixed compression scheme, in sublinear time. We study this
question in detail for two popular lossless compression schemes: run-length
encoding (RLE) and Lempel-Ziv (LZ), and present sublinear algorithms for
approximating compressibility with respect to both schemes. We also give
several lower bounds that show that our algorithms for both schemes cannot be
improved significantly.
  Our investigation of LZ yields results whose interest goes beyond the initial
questions we set out to study. In particular, we prove combinatorial structural
lemmas that relate the compressibility of a string with respect to Lempel-Ziv
to the number of distinct short substrings contained in it. In addition, we
show that approximating the compressibility with respect to LZ is related to
approximating the support size of a distribution.
</summary>
    <author>
      <name>Sofya Raskhodnikova</name>
    </author>
    <author>
      <name>Dana Ron</name>
    </author>
    <author>
      <name>Ronitt Rubinfeld</name>
    </author>
    <author>
      <name>Adam Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the proceedings of RANDOM 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/0706.1084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.1084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1306.4037">
    <id>http://arxiv.org/abs/1306.4037v1</id>
    <updated>2013-06-17T22:48:15Z</updated>
    <published>2013-06-17T22:48:15Z</published>
    <title>Hybrid Indexes for Repetitive Datasets</title>
    <summary>  Advances in DNA sequencing mean databases of thousands of human genomes will
soon be commonplace. In this paper we introduce a simple technique for reducing
the size of conventional indexes on such highly repetitive texts. Given upper
bounds on pattern lengths and edit distances, we preprocess the text with LZ77
to obtain a filtered text, for which we store a conventional index. Later,
given a query, we find all matches in the filtered text, then use their
positions and the structure of the LZ77 parse to find all matches in the
original text. Our experiments show this also significantly reduces query
times.
</summary>
    <author>
      <name>H. Ferrada</name>
    </author>
    <author>
      <name>T. Gagie</name>
    </author>
    <author>
      <name>T. Hirvola</name>
    </author>
    <author>
      <name>S. J. Puglisi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1098/rsta.2013.0137</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1098/rsta.2013.0137" rel="related"/>
    <link href="http://arxiv.org/abs/1306.4037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.4037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1604.08897">
    <id>http://arxiv.org/abs/1604.08897v2</id>
    <updated>2016-05-24T00:36:55Z</updated>
    <published>2016-04-29T16:05:49Z</published>
    <title>Universal Indexes for Highly Repetitive Document Collections</title>
    <summary>  Indexing highly repetitive collections has become a relevant problem with the
emergence of large repositories of versioned documents, among other
applications. These collections may reach huge sizes, but are formed mostly of
documents that are near-copies of others. Traditional techniques for indexing
these collections fail to properly exploit their regularities in order to
reduce space.
  We introduce new techniques for compressing inverted indexes that exploit
this near-copy regularity. They are based on run-length, Lempel-Ziv, or grammar
compression of the differential inverted lists, instead of the usual practice
of gap-encoding them. We show that, in this highly repetitive setting, our
compression methods significantly reduce the space obtained with classical
techniques, at the price of moderate slowdowns. Moreover, our best methods are
universal, that is, they do not need to know the versioning structure of the
collection, nor that a clear versioning structure even exists.
  We also introduce compressed self-indexes in the comparison. These are
designed for general strings (not only natural language texts) and represent
the text collection plus the index structure (not an inverted index) in
integrated form. We show that these techniques can compress much further, using
a small fraction of the space required by our new inverted indexes. Yet, they
are orders of magnitude slower.
</summary>
    <author>
      <name>Francisco Claude</name>
    </author>
    <author>
      <name>Antonio Fariña</name>
    </author>
    <author>
      <name>Miguel A. Martínez-Prieto</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.is.2016.04.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.is.2016.04.002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Systems, Volume 61, Pages 1-23, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.08897v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08897v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.1428">
    <id>http://arxiv.org/abs/1307.1428v1</id>
    <updated>2013-07-04T17:53:54Z</updated>
    <published>2013-07-04T17:53:54Z</published>
    <title>Lempel-Ziv Parsing in External Memory</title>
    <summary>  For decades, computing the LZ factorization (or LZ77 parsing) of a string has
been a requisite and computationally intensive step in many diverse
applications, including text indexing and data compression. Many algorithms for
LZ77 parsing have been discovered over the years; however, despite the
increasing need to apply LZ77 to massive data sets, no algorithm to date scales
to inputs that exceed the size of internal memory. In this paper we describe
the first algorithm for computing the LZ77 parsing in external memory. Our
algorithm is fast in practice and will allow the next generation of text
indexes to be realised for massive strings and string collections.
</summary>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DCC.2014.78</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DCC.2014.78" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.1428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0909.4341">
    <id>http://arxiv.org/abs/0909.4341v1</id>
    <updated>2009-09-24T18:23:20Z</updated>
    <published>2009-09-24T18:23:20Z</published>
    <title>Lightweight Data Indexing and Compression in External Memory</title>
    <summary>  In this paper we describe algorithms for computing the BWT and for building
(compressed) indexes in external memory. The innovative feature of our
algorithms is that they are lightweight in the sense that, for an input of size
$n$, they use only ${n}$ bits of disk working space while all previous
approaches use $\Th{n \log n}$ bits of disk working space. Moreover, our
algorithms access disk data only via sequential scans, thus they take full
advantage of modern disk features that make sequential disk accesses much
faster than random accesses.
  We also present a scan-based algorithm for inverting the BWT that uses
$\Th{n}$ bits of working space, and a lightweight {\em internal-memory}
algorithm for computing the BWT which is the fastest in the literature when the
available working space is $\os{n}$ bits.
  Finally, we prove {\em lower} bounds on the complexity of computing and
inverting the BWT via sequential scans in terms of the classic product:
internal-memory space $\times$ number of passes over the disk data.
</summary>
    <author>
      <name>Paolo Ferragina</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <link href="http://arxiv.org/abs/0909.4341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.4341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1403.4445">
    <id>http://arxiv.org/abs/1403.4445v1</id>
    <updated>2014-03-18T13:27:46Z</updated>
    <published>2014-03-18T13:27:46Z</published>
    <title>A really simple approximation of smallest grammar</title>
    <summary>  In this paper we present a really simple linear-time algorithm constructing a
context-free grammar of size O(g log (N/g)) for the input string, where N is
the size of the input string and g the size of the optimal grammar generating
this string. The algorithm works for arbitrary size alphabets, but the running
time is linear assuming that the alphabet Sigma of the input string can be
identified with numbers from 1,ldots, N^c for some constant c. Algorithms with
such an approximation guarantee and running time are known, however all of them
were non-trivial and their analyses were involved. The here presented algorithm
computes the LZ77 factorisation and transforms it in phases to a grammar. In
each phase it maintains an LZ77-like factorisation of the word with at most l
factors as well as additional O(l) letters, where l was the size of the
original LZ77 factorisation. In one phase in a greedy way (by a left-to-right
sweep and a help of the factorisation) we choose a set of pairs of consecutive
letters to be replaced with new symbols, i.e. nonterminals of the constructed
grammar. We choose at least 2/3 of the letters in the word and there are O(l)
many different pairs among them. Hence there are O(log N) phases, each of them
introduces O(l) nonterminals to a grammar. A more precise analysis yields a
bound O(l log(N/l)). As l \leq g, this yields the desired bound O(g log(N/g)).
</summary>
    <author>
      <name>Artur Jeż</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for CPM 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.4445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.4445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; F.4.2; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1107.2729">
    <id>http://arxiv.org/abs/1107.2729v1</id>
    <updated>2011-07-14T05:35:09Z</updated>
    <published>2011-07-14T05:35:09Z</published>
    <title>Restructuring Compressed Texts without Explicit Decompression</title>
    <summary>  We consider the problem of {\em restructuring} compressed texts without
explicit decompression. We present algorithms which allow conversions from
compressed representations of a string $T$ produced by any grammar-based
compression algorithm, to representations produced by several specific
compression algorithms including LZ77, LZ78, run length encoding, and some
grammar based compression algorithms. These are the first algorithms that
achieve running times polynomial in the size of the compressed input and output
representations of $T$. Since most of the representations we consider can
achieve exponential compression, our algorithms are theoretically faster in the
worst case, than any algorithm which first decompresses the string for the
conversion.
</summary>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Shirou Maruyama</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1107.2729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.2729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1609.02067">
    <id>http://arxiv.org/abs/1609.02067v1</id>
    <updated>2016-09-07T16:53:40Z</updated>
    <published>2016-09-07T16:53:40Z</published>
    <title>Practical Data Compression for Modern Memory Hierarchies</title>
    <summary>  In this thesis, we describe a new, practical approach to integrating
hardware-based data compression within the memory hierarchy, including on-chip
caches, main memory, and both on-chip and off-chip interconnects. This new
approach is fast, simple, and effective in saving storage space. A key insight
in our approach is that access time (including decompression latency) is
critical in modern memory hierarchies. By combining inexpensive hardware
support with modest OS support, our holistic approach to compression achieves
substantial improvements in performance and energy efficiency across the memory
hierarchy. Using this new approach, we make several major contributions in this
thesis. First, we propose a new compression algorithm, Base-Delta-Immediate
Compression (BDI), that achieves high compression ratio with very low
compression/decompression latency. BDI exploits the existing low dynamic range
of values present in many cache lines to compress them to smaller sizes using
Base+Delta encoding. Second, we observe that the compressed size of a cache
block can be indicative of its reuse. We use this observation to develop a new
cache insertion policy for compressed caches, the Size-based Insertion Policy
(SIP), which uses the size of a compressed block as one of the metrics to
predict its potential future reuse. Third, we propose a new main memory
compression framework, Linearly Compressed Pages (LCP), that significantly
reduces the complexity and power cost of supporting main memory compression. We
demonstrate that any compression algorithm can be adapted to fit the
requirements of LCP, and that LCP can be efficiently integrated with the
existing cache compression designs, avoiding extra compression/decompression.
</summary>
    <author>
      <name>Gennady Pekhimenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD Thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2205.02815">
    <id>http://arxiv.org/abs/2205.02815v1</id>
    <updated>2022-05-05T17:47:22Z</updated>
    <published>2022-05-05T17:47:22Z</published>
    <title>Cut-Down de Bruijn Sequences</title>
    <summary>  A cut-down de Bruijn sequence is a cyclic string of length $L$, where $1 \leq
L \leq k^n$, such that every substring of length $n$ appears at most once.
Etzion [Theor. Comp. Sci 44 (1986)] gives an algorithm to construct binary
cut-down de Bruijn sequences that requires $o(n)$ simple $n$-bit operations per
symbol generated. In this paper, we simplify the algorithm and improve the
running time to $\mathcal{O}(n)$ time per symbol generated using
$\mathcal{O}(n)$ space. We then provide the first successor-rule approach for
constructing a binary cut-down de Bruijn sequence by leveraging recent ranking
algorithms for fixed-density Lyndon words. Finally, we develop an algorithm to
generate cut-down de Bruijn sequences for $k>2$ that runs in $\mathcal{O}(n)$
time per symbol using $\mathcal{O}(n)$ space after some initialization. While
our $k$-ary algorithm is based on our simplified version of Etzion's binary
algorithm, a number of non-trivial adaptations are required to generalize to
larger alphabets.
</summary>
    <author>
      <name>Ben Cameron</name>
    </author>
    <author>
      <name>Aysu Gündoğan</name>
    </author>
    <author>
      <name>Joe Sawada</name>
    </author>
    <link href="http://arxiv.org/abs/2205.02815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.02815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1702.01340">
    <id>http://arxiv.org/abs/1702.01340v1</id>
    <updated>2017-02-04T20:19:26Z</updated>
    <published>2017-02-04T20:19:26Z</published>
    <title>From LZ77 to the Run-Length Encoded Burrows-Wheeler Transform, and Back</title>
    <summary>  The Lempel-Ziv factorization (LZ77) and the Run-Length encoded
Burrows-Wheeler Transform (RLBWT) are two important tools in text compression
and indexing, being their sizes $z$ and $r$ closely related to the amount of
text self-repetitiveness. In this paper we consider the problem of converting
the two representations into each other within a working space proportional to
the input and the output. Let $n$ be the text length. We show that $RLBWT$ can
be converted to $LZ77$ in $\mathcal{O}(n\log r)$ time and $\mathcal{O}(r)$
words of working space. Conversely, we provide an algorithm to convert $LZ77$
to $RLBWT$ in $\mathcal{O}\big(n(\log r + \log z)\big)$ time and
$\mathcal{O}(r+z)$ words of working space. Note that $r$ and $z$ can be
\emph{constant} if the text is highly repetitive, and our algorithms can
operate with (up to) \emph{exponentially} less space than naive solutions based
on full decompression.
</summary>
    <author>
      <name>Alberto Policriti</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1702.01340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1704.05233">
    <id>http://arxiv.org/abs/1704.05233v2</id>
    <updated>2017-10-15T03:11:30Z</updated>
    <published>2017-04-18T08:36:44Z</published>
    <title>A Faster Implementation of Online Run-Length Burrows-Wheeler Transform</title>
    <summary>  Run-length encoding Burrows-Wheeler Transformed strings, resulting in
Run-Length BWT (RLBWT), is a powerful tool for processing highly repetitive
strings. We propose a new algorithm for online RLBWT working in run-compressed
space, which runs in $O(n\lg r)$ time and $O(r\lg n)$ bits of space, where $n$
is the length of input string $S$ received so far and $r$ is the number of runs
in the BWT of the reversed $S$. We improve the state-of-the-art algorithm for
online RLBWT in terms of empirical construction time. Adopting the dynamic list
for maintaining a total order, we can replace rank queries in a dynamic wavelet
tree on a run-length compressed string by the direct comparison of labels in a
dynamic list. The empirical result for various benchmarks show the efficiency
of our algorithm, especially for highly repetitive strings.
</summary>
    <author>
      <name>Tatsuya Ohno</name>
    </author>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proc. IWOCA2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05233v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05233v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1706.10094">
    <id>http://arxiv.org/abs/1706.10094v2</id>
    <updated>2018-01-09T10:45:07Z</updated>
    <published>2017-06-30T10:02:10Z</published>
    <title>Time-Space Trade-Offs for Lempel-Ziv Compressed Indexing</title>
    <summary>  Given a string $S$, the \emph{compressed indexing problem} is to preprocess
$S$ into a compressed representation that supports fast \emph{substring
queries}. The goal is to use little space relative to the compressed size of
$S$ while supporting fast queries. We present a compressed index based on the
Lempel--Ziv 1977 compression scheme. We obtain the following time-space
trade-offs: For constant-sized alphabets; (i) $O(m + occ \lg\lg n)$ time using
$O(z\lg(n/z)\lg\lg z)$ space, or (ii) $O(m(1 + \frac{\lg^\epsilon z}{\lg(n/z)})
+ occ(\lg\lg n + \lg^\epsilon z))$ time using $O(z\lg(n/z))$ space. For integer
alphabets polynomially bounded by $n$; (iii) $O(m(1 + \frac{\lg^\epsilon
z}{\lg(n/z)}) + occ(\lg\lg n + \lg^\epsilon z))$ time using $O(z(\lg(n/z) +
\lg\lg z))$ space, or (iv) $O(m + occ(\lg\lg n + \lg^{\epsilon} z))$ time using
$O(z(\lg(n/z) + \lg^{\epsilon} z))$ space, where $n$ and $m$ are the length of
the input string and query string respectively, $z$ is the number of phrases in
the LZ77 parse of the input string, $occ$ is the number of occurrences of the
query in the input and $\epsilon > 0$ is an arbitrarily small constant. In
particular, (i) improves the leading term in the query time of the previous
best solution from $O(m\lg m)$ to $O(m)$ at the cost of increasing the space by
a factor $\lg \lg z$. Alternatively, (ii) matches the previous best space
bound, but has a leading term in the query time of $O(m(1+\frac{\lg^{\epsilon}
z}{\lg (n/z)}))$. However, for any polynomial compression ratio, i.e., $z =
O(n^{1-\delta})$, for constant $\delta > 0$, this becomes $O(m)$. Our index
also supports extraction of any substring of length $\ell$ in $O(\ell +
\lg(n/z))$ time. Technically, our results are obtained by novel extensions and
combinations of existing data structures of independent interest, including a
new batched variant of weak prefix search.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Mikko Berggren Ettienne</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Hjalte Wedel Vildhøj</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2017.12.021</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2017.12.021" rel="related"/>
    <link href="http://arxiv.org/abs/1706.10094v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.10094v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; E.4; E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1711.08217">
    <id>http://arxiv.org/abs/1711.08217v3</id>
    <updated>2018-04-11T12:55:35Z</updated>
    <published>2017-11-22T10:40:06Z</published>
    <title>Compressed Indexing with Signature Grammars</title>
    <summary>  The compressed indexing problem is to preprocess a string $S$ of length $n$
into a compressed representation that supports pattern matching queries. That
is, given a string $P$ of length $m$ report all occurrences of $P$ in $S$.
  We present a data structure that supports pattern matching queries in $O(m +
occ (\lg\lg n + \lg^\epsilon z))$ time using $O(z \lg(n / z))$ space where $z$
is the size of the LZ77 parse of $S$ and $\epsilon > 0$ is an arbitrarily small
constant, when the alphabet is small or $z = O(n^{1 - \delta})$ for any
constant $\delta > 0$. We also present two data structures for the general
case; one where the space is increased by $O(z\lg\lg z)$, and one where the
query time changes from worst-case to expected. These results improve the
previously best known solutions. Notably, this is the first data structure that
decides if $P$ occurs in $S$ in $O(m)$ time using $O(z\lg(n/z))$ space.
  Our results are mainly obtained by a novel combination of a randomized
grammar construction algorithm with well known techniques relating pattern
matching to 2D-range reporting.
</summary>
    <author>
      <name>Anders Roy Christiansen</name>
    </author>
    <author>
      <name>Mikko Berggren Ettienne</name>
    </author>
    <link href="http://arxiv.org/abs/1711.08217v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08217v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1109.3954">
    <id>http://arxiv.org/abs/1109.3954v6</id>
    <updated>2012-09-26T20:34:19Z</updated>
    <published>2011-09-19T07:10:06Z</published>
    <title>A Faster Grammar-Based Self-Index</title>
    <summary>  To store and search genomic databases efficiently, researchers have recently
started building compressed self-indexes based on grammars. In this paper we
show how, given a straight-line program with $r$ rules for a string (S [1..n])
whose LZ77 parse consists of $z$ phrases, we can store a self-index for $S$ in
$\Oh{r + z \log \log n}$ space such that, given a pattern (P [1..m]), we can
list the $\occ$ occurrences of $P$ in $S$ in $\Oh{m^2 + \occ \log \log n}$
time. If the straight-line program is balanced and we accept a small
probability of building a faulty index, then we can reduce the $\Oh{m^2}$ term
to $\Oh{m \log m}$. All previous self-indexes are larger or slower in the worst
case.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">journal version of LATA '12 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.3954v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.3954v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1101.4065">
    <id>http://arxiv.org/abs/1101.4065v1</id>
    <updated>2011-01-21T02:43:16Z</updated>
    <published>2011-01-21T02:43:16Z</published>
    <title>Self-Index Based on LZ77</title>
    <summary>  We introduce the first self-index based on the Lempel-Ziv 1977 compression
format (LZ77). It is particularly competitive for highly repetitive text
collections such as sequence databases of genomes of related species, software
repositories, versioned document collections, and temporal text databases. Such
collections are extremely compressible but classical self-indexes fail to
capture that source of compressibility. Our self-index takes in practice a few
times the space of the text compressed with LZ77 (as little as 2.6 times),
extracts 1--2 million characters of the text per second, and finds patterns at
a rate of 10--50 microseconds per occurrence. It is smaller (up to one half)
than the best current self-index for repetitive collections, and faster in many
cases.
</summary>
    <author>
      <name>Sebastian Kreft</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1101.4065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.4065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2205.05643">
    <id>http://arxiv.org/abs/2205.05643v1</id>
    <updated>2022-05-11T17:17:08Z</updated>
    <published>2022-05-11T17:17:08Z</published>
    <title>A New Class of String Transformations for Compressed Text Indexing</title>
    <summary>  Introduced about thirty years ago in the field of Data Compression, the
Burrows-Wheeler Transform (BWT) is a string transformation that, besides being
a booster of the performance of memoryless compressors, plays a fundamental
role in the design of efficient self-indexing compressed data structures.
Finding other string transformations with the same remarkable properties of BWT
has been a challenge for many researchers for a long time. Among the known BWT
variants, the only one that has been recently shown to be a valid alternative
to BWT is the Alternating BWT (ABWT), another invertible string transformation
introduced about ten years ago in connection with a generalization of Lyndon
words. In this paper, we introduce a whole class of new string transformations,
called local orderings-based transformations, which have all the myriad virtues
of BWT. We show that this new family is a special case of a much larger class
of transformations, based on context adaptive alphabet orderings, that includes
BWT and ABWT. Although all transformations support pattern search, we show
that, in the general case, the transformations within our larger class may take
quadratic time for inversion and pattern search. As a further result, we show
that the local orderings-based transformations can be used for the construction
of the recently introduced r-index, which makes them suitable also for highly
repetitive collections. In this context, we consider the problem of finding,
for a given string, the BWT variant that minimizes the number of runs in the
transformed string, and we provide an algorithm solving this problem in linear
time.
</summary>
    <author>
      <name>Raffaele Giancarlo</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Antonio Restivo</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1902.01280</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.05643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.05643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.02232">
    <id>http://arxiv.org/abs/2006.02232v1</id>
    <updated>2020-06-01T23:16:28Z</updated>
    <published>2020-06-01T23:16:28Z</published>
    <title>Analysis of Compression Techniques for DNA Sequence Data</title>
    <summary>  Biological data mainly comprises of Deoxyribonucleic acid (DNA) and protein
sequences. These are the biomolecules which are present in all cells of human
beings. Due to the self-replicating property of DNA, it is a key constitute of
genetic material that exist in all breathingcreatures. This biomolecule (DNA)
comprehends the genetic material obligatory for the operational and expansion
of all personified lives. To save DNA data of single person we require
10CD-ROMs.Moreover, this size is increasing constantly, and more and more
sequences are adding in the public databases. This abundant increase in the
sequence data arise challenges in the precise information extraction from this
data. Since many data analyzing and visualization tools do not support
processing of this huge amount of data. To reduce the size of DNA and protein
sequence, many scientists introduced various types of sequence compression
algorithms such as compress or gzip, Context Tree Weighting (CTW), Lampel Ziv
Welch (LZW), arithmetic coding, run-length encoding and substitution method
etc. These techniques have sufficiently contributed to minimizing the volume of
the biological datasets. On the other hand, traditional compression techniques
are also not much suitable for the compression of these types of sequential
data. In this paper, we have explored diverse types of techniques for
compression of large amounts of DNA Sequence Data. In this paper, the analysis
of techniques reveals that efficient techniques not only reduce the size of the
sequence but also avoid any information loss. The review of existing studies
also shows that compression of a DNA sequence is significant for understanding
the critical characteristics of DNA data in addition to improving storage
efficiency and data transmission. In addition, the compression of the protein
sequence is a challenge for the research community. The major parameters for
evaluation of these compression algorithms include compression ratio, running
time complexity etc.
</summary>
    <author>
      <name>Shakeela Bibi</name>
    </author>
    <author>
      <name>Javed Iqbal</name>
    </author>
    <author>
      <name>Adnan Iftekhar</name>
    </author>
    <author>
      <name>Mir Hassan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.14683.00806</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.14683.00806" rel="related"/>
    <link href="http://arxiv.org/abs/2006.02232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1711.07270">
    <id>http://arxiv.org/abs/1711.07270v1</id>
    <updated>2017-11-20T12:00:24Z</updated>
    <published>2017-11-20T12:00:24Z</published>
    <title>A Separation Between Run-Length SLPs and LZ77</title>
    <summary>  In this paper we give an infinite family of strings for which the length of
the Lempel-Ziv'77 parse is a factor $\Omega(\log n/\log\log n)$ smaller than
the smallest run-length grammar.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1711.07270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.06615">
    <id>http://arxiv.org/abs/1605.06615v3</id>
    <updated>2021-04-01T13:41:06Z</updated>
    <published>2016-05-21T10:02:19Z</published>
    <title>Efficient and Compact Representations of Some Non-Canonical Prefix-Free
  Codes</title>
    <summary>  For many kinds of prefix-free codes there are efficient and compact
alternatives to the traditional tree-based representation. Since these put the
codes into canonical form, however, they can only be used when we can choose
the order in which codewords are assigned to symbols. In this paper we first
show how, given a probability distribution over an alphabet of $\sigma$
symbols, we can store an optimal alphabetic prefix-free code in $\Oh{\sigma
\log L}$ bits such that we can encode and decode any codeword of length $\ell$
in $\Oh{\min (\ell, \log L)}$ time, where $L$ is the maximum codeword length.
With $\Oh{2^{L^\epsilon}}$ further bits, for any constant $\epsilon>0$, we can
encode and decode $\Oh{\log \ell}$ time. We then show how to store a nearly
optimal alphabetic prefix-free code in \(o (\sigma)\) bits such that we can
encode and decode in constant time. We also consider a kind of optimal
prefix-free code introduced recently where the codewords' lengths are
non-decreasing if arranged in lexicographic order of their reverses. We reduce
their storage space to $\Oh{\sigma \log L}$ while maintaining encoding and
decoding times in $\Oh{\ell}$. We also show how, with $\Oh{2^{\epsilon L}}$
further bits, we can encode and decode in constant time. All of our results
hold in the word-RAM model.
</summary>
    <author>
      <name>Antonio Fariña</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Alberto Ordóñez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. A preliminary version was
  presented at the 23rd International Symposium on String Processing and
  Information Retrieval (SPIRE '16)</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06615v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06615v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.09789">
    <id>http://arxiv.org/abs/1707.09789v2</id>
    <updated>2018-01-09T13:09:31Z</updated>
    <published>2017-07-31T09:57:46Z</published>
    <title>Relations Between Greedy and Bit-Optimal LZ77 Encodings</title>
    <summary>  This paper investigates the size in bits of the LZ77 encoding, which is the
most popular and efficient variant of the Lempel-Ziv encodings used in data
compression. We prove that, for a wide natural class of variable-length
encoders for LZ77 phrases, the size of the greedily constructed LZ77 encoding
on constant alphabets is within a factor $O(\frac{\log n}{\log\log\log n})$ of
the optimal LZ77 encoding, where $n$ is the length of the processed string. We
describe a series of examples showing that, surprisingly, this bound is tight,
thus improving both the previously known upper and lower bounds. Further, we
obtain a more detailed bound $O(\min\{z, \frac{\log n}{\log\log z}\})$, which
uses the number $z$ of phrases in the greedy LZ77 encoding as a parameter, and
construct a series of examples showing that this bound is tight even for binary
alphabet. We then investigate the problem on non-constant alphabets: we show
that the known $O(\log n)$ bound is tight even for alphabets of logarithmic
size, and provide tight bounds for some other important cases.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09789v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09789v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1502.05937">
    <id>http://arxiv.org/abs/1502.05937v2</id>
    <updated>2015-02-23T10:18:46Z</updated>
    <published>2015-02-20T17:00:26Z</published>
    <title>Composite repetition-aware data structures</title>
    <summary>  In highly repetitive strings, like collections of genomes from the same
species, distinct measures of repetition all grow sublinearly in the length of
the text, and indexes targeted to such strings typically depend only on one of
these measures. We describe two data structures whose size depends on multiple
measures of repetition at once, and that provide competitive tradeoffs between
the time for counting and reporting all the exact occurrences of a pattern, and
the space taken by the structure. The key component of our constructions is the
run-length encoded BWT (RLBWT), which takes space proportional to the number of
BWT runs: rather than augmenting RLBWT with suffix array samples, we combine it
with data structures from LZ77 indexes, which take space proportional to the
number of LZ77 factors, and with the compact directed acyclic word graph
(CDAWG), which takes space proportional to the number of extensions of maximal
repeats. The combination of CDAWG and RLBWT enables also a new representation
of the suffix tree, whose size depends again on the number of extensions of
maximal repeats, and that is powerful enough to support matching statistics and
constant-space traversal.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Fabio Cunial</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Mathieu Raffinot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(the name of the third co-author was inadvertently omitted from
  previous version)</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.05937v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05937v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1504.06712">
    <id>http://arxiv.org/abs/1504.06712v3</id>
    <updated>2015-06-08T19:03:03Z</updated>
    <published>2015-04-25T11:21:56Z</published>
    <title>Faster Lightweight Lempel-Ziv Parsing</title>
    <summary>  We present an algorithm that computes the Lempel-Ziv decomposition in
$O(n(\log\sigma + \log\log n))$ time and $n\log\sigma + \epsilon n$ bits of
space, where $\epsilon$ is a constant rational parameter, $n$ is the length of
the input string, and $\sigma$ is the alphabet size. The $n\log\sigma$ bits in
the space bound are for the input string itself which is treated as read-only.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 5 figures, accepted to MFCS 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.06712v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06712v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1504.06647">
    <id>http://arxiv.org/abs/1504.06647v2</id>
    <updated>2015-09-10T14:59:37Z</updated>
    <published>2015-04-24T21:26:58Z</published>
    <title>Approximating LZ77 via Small-Space Multiple-Pattern Matching</title>
    <summary>  We generalize Karp-Rabin string matching to handle multiple patterns in
$\mathcal{O}(n \log n + m)$ time and $\mathcal{O}(s)$ space, where $n$ is the
length of the text and $m$ is the total length of the $s$ patterns, returning
correct answers with high probability. As a prime application of our algorithm,
we show how to approximate the LZ77 parse of a string of length $n$. If the
optimal parse consists of $z$ phrases, using only $\mathcal{O}(z)$ working
space we can return a parse consisting of at most $(1+\varepsilon)z$ phrases in
$\mathcal{O}(\varepsilon^{-1}n\log n)$ time, for any $\varepsilon\in (0,1]$. As
previous quasilinear-time algorithms for LZ77 use $\Omega(n/\textrm{polylog
}n)$ space, but $z$ can be exponentially small in $n$, these improvements in
space are substantial.
</summary>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preliminary version presented at ESA 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.06647v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06647v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1503.02416">
    <id>http://arxiv.org/abs/1503.02416v1</id>
    <updated>2015-03-09T10:17:30Z</updated>
    <published>2015-03-09T10:17:30Z</published>
    <title>Approximating LZ77 in Small Space</title>
    <summary>  Given a positive \(\epsilon \leq 1\) and read-only access to a string \(S
[1..n]\) whose LZ77 parse consists of $z$ phrases, with high probability we can
build an LZ77-like parse of $S$ that consists of $\Oh{z / \epsilon}$ phrases
using $\Oh{n^{1 + \epsilon}}$ time, $\Oh{n^{1 + \epsilon} / B}$ I/Os (where $B$
is the size of a disk block) and $\Oh{z / \epsilon}$ space.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/1503.02416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1310.1448">
    <id>http://arxiv.org/abs/1310.1448v1</id>
    <updated>2013-10-05T06:10:43Z</updated>
    <published>2013-10-05T06:10:43Z</published>
    <title>Space Efficient Linear Time Lempel-Ziv Factorization on
  Constant~Size~Alphabets</title>
    <summary>  We present a new algorithm for computing the Lempel-Ziv Factorization (LZ77)
of a given string of length $N$ in linear time, that utilizes only $N\log N +
O(1)$ bits of working space, i.e., a single integer array, for constant size
integer alphabets. This greatly improves the previous best space requirement
for linear time LZ77 factorization (K\"arkk\"ainen et al. CPM 2013), which
requires two integer arrays of length $N$. Computational experiments show that
despite the added complexity of the algorithm, the speed of the algorithm is
only around twice as slow as previous fastest linear time algorithms.
</summary>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <link href="http://arxiv.org/abs/1310.1448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1302.1064">
    <id>http://arxiv.org/abs/1302.1064v2</id>
    <updated>2013-02-06T10:15:05Z</updated>
    <published>2013-02-05T15:34:53Z</published>
    <title>Lightweight Lempel-Ziv Parsing</title>
    <summary>  We introduce a new approach to LZ77 factorization that uses O(n/d) words of
working space and O(dn) time for any d >= 1 (for polylogarithmic alphabet
sizes). We also describe carefully engineered implementations of alternative
approaches to lightweight LZ77 factorization. Extensive experiments show that
the new algorithm is superior in most cases, particularly at the lowest memory
levels and for highly repetitive data. As a part of the algorithm, we describe
new methods for computing matching statistics which may be of independent
interest.
</summary>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-38527-8_14</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-38527-8_14" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.1064v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.1064v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1212.2952">
    <id>http://arxiv.org/abs/1212.2952v1</id>
    <updated>2012-12-12T20:35:53Z</updated>
    <published>2012-12-12T20:35:53Z</published>
    <title>Linear Time Lempel-Ziv Factorization: Simple, Fast, Small</title>
    <summary>  Computing the LZ factorization (or LZ77 parsing) of a string is a
computational bottleneck in many diverse applications, including data
compression, text indexing, and pattern discovery. We describe new linear time
LZ factorization algorithms, some of which require only 2n log n + O(log n)
bits of working space to factorize a string of length n. These are the most
space efficient linear time algorithms to date, using n log n bits less space
than any previous linear time algorithm. The algorithms are also practical,
simple to implement, and very fast in practice.
</summary>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-38905-4_19</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-38905-4_19" rel="related"/>
    <link href="http://arxiv.org/abs/1212.2952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1204.5524">
    <id>http://arxiv.org/abs/1204.5524v3</id>
    <updated>2013-05-27T02:34:42Z</updated>
    <published>2012-04-25T01:07:13Z</published>
    <title>Time and Space Efficient Lempel-Ziv Factorization based on Run Length
  Encoding</title>
    <summary>  We propose a new approach for calculating the Lempel-Ziv factorization of a
string, based on run length encoding (RLE). We present a conceptually simple
off-line algorithm based on a variant of suffix arrays, as well as an on-line
algorithm based on a variant of directed acyclic word graphs (DAWGs). Both
algorithms run in $O(N+n\log n)$ time and O(n) extra space, where N is the size
of the string, $n\leq N$ is the number of RLE factors. The time dependency on N
is only in the conversion of the string to RLE, which can be computed very
efficiently in O(N) time and O(1) extra space (excluding the output). When the
string is compressible via RLE, i.e., $n = o(N)$, our algorithms are, to the
best of our knowledge, the first algorithms which require only o(N) extra space
while running in $o(N\log N)$ time.
</summary>
    <author>
      <name>Jun'ichi Yamamoto</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1204.5524v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.5524v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1611.01769">
    <id>http://arxiv.org/abs/1611.01769v2</id>
    <updated>2017-06-15T21:38:21Z</updated>
    <published>2016-11-06T12:47:25Z</published>
    <title>LZ-End Parsing in Compressed Space</title>
    <summary>  We present an algorithm that constructs the LZ-End parsing (a variation of
LZ77) of a given string of length $n$ in $O(n\log\ell)$ expected time and $O(z
+ \ell)$ space, where $z$ is the number of phrases in the parsing and $\ell$ is
the length of the longest phrase. As an option, we can fix $\ell$ (e.g., to the
size of RAM) thus obtaining a reasonable LZ-End approximation with the same
functionality and the length of phrases restricted by $\ell$. This modified
algorithm constructs the parsing in streaming fashion in one left to right pass
on the input string w.h.p. and performs one right to left pass to verify the
correctness of the result. Experimentally comparing this version to other
LZ77-based analogs, we show that it is of practical interest.
</summary>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DCC.2017.73</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DCC.2017.73" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.01769v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01769v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.04312">
    <id>http://arxiv.org/abs/1707.04312v2</id>
    <updated>2017-07-31T10:17:02Z</updated>
    <published>2017-07-13T20:37:25Z</published>
    <title>Lempel-Ziv: a "one-bit catastrophe" but not a tragedy</title>
    <summary>  The so-called "one-bit catastrophe" for the compression algorithm LZ'78 asks
whether the compression ratio of an infinite word can change when a single bit
is added in front of it. We answer positively this open question raised by Lutz
and others: we show that there exists an infinite word $w$ such that
$\rho_{sup}(w)=0$ but $\rho_{inf}(0w)>0$, where $\rho_{sup}$ and $\rho_{inf}$
are respectively the $\limsup$ and the $\liminf$ of the compression ratios
$\rho$ of the prefixes. To that purpose we explore the behaviour of LZ'78 on
finite words and show the following results:
  - There is a constant $C>0$ such that, for any finite word $w$ and any letter
$a$, $\rho(aw)\leq C\sqrt{\rho(w)\log|w|}$. Thus, sufficiently compressible
words ($\rho(w)=o(1/\log|w|)$) remain compressible with a letter in front;
  - The previous result is tight up to a multiplicative constant for any
compression ratio $\rho(w)=O(1/\log|w|)$. In particular, there are infinitely
many words $w$ satisfying $\rho(w)=O(1/\log|w|)$ but $\rho(0w)=\Omega(1)$.
</summary>
    <author>
      <name>Guillaume Lagarde</name>
    </author>
    <author>
      <name>Sylvain Perifel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.04312v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04312v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1501.06619">
    <id>http://arxiv.org/abs/1501.06619v1</id>
    <updated>2015-01-26T23:15:45Z</updated>
    <published>2015-01-26T23:15:45Z</published>
    <title>Constructing LZ78 Tries and Position Heaps in Linear Time for Large
  Alphabets</title>
    <summary>  We present the first worst-case linear-time algorithm to compute the
Lempel-Ziv 78 factorization of a given string over an integer alphabet. Our
algorithm is based on nearest marked ancestor queries on the suffix tree of the
given string. We also show that the same technique can be used to construct the
position heap of a set of strings in worst-case linear time, when the set of
strings is given as a trie.
</summary>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1501.06619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.06619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1504.06954">
    <id>http://arxiv.org/abs/1504.06954v4</id>
    <updated>2016-04-06T06:29:09Z</updated>
    <published>2015-04-27T07:42:53Z</published>
    <title>Dynamic index, LZ factorization, and LCE queries in compressed space</title>
    <summary>  In this paper, we present the following results: (1) We propose a new
\emph{dynamic compressed index} of $O(w)$ space, that supports searching for a
pattern $P$ in the current text in $O(|P| f(M,w) + \log w \log |P| \log^* M
(\log N + \log |P| \log^* M) + \mathit{occ} \log N)$ time and
insertion/deletion of a substring of length $y$ in $O((y+ \log N\log^* M)\log w
\log N \log^* M)$ time, where $N$ is the length of the current text, $M$ is the
maximum length of the dynamic text, $z$ is the size of the Lempel-Ziv77 (LZ77)
factorization of the current text, $f(a,b) = O(\min \{ \frac{\log\log a
\log\log b}{\log\log\log a}, \sqrt{\frac{\log b}{\log\log b}} \})$ and $w = O(z
\log N \log^*M)$. (2) We propose a new space-efficient LZ77 factorization
algorithm for a given text of length $N$, which runs in $O(N f(N,w') + z \log
w' \log^3 N (\log^* N)^2)$ time with $O(w')$ working space, where $w' =O(z \log
N \log^* N)$. (3) We propose a data structure of $O(w)$ space which supports
longest common extension (LCE) queries on the text in $O(\log N + \log \ell
\log^* N)$ time, where $\ell$ is the output LCE length. On top of the above
contributions, we show several applications of our data structures which
improve previous best known results on grammar-compressed string processing.
</summary>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>I Tomohiro</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1504.06954v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06954v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.09558">
    <id>http://arxiv.org/abs/1605.09558v2</id>
    <updated>2016-07-19T08:17:54Z</updated>
    <published>2016-05-31T10:21:59Z</published>
    <title>Dynamic index and LZ factorization in compressed space</title>
    <summary>  In this paper, we propose a new \emph{dynamic compressed index} of $O(w)$
space for a dynamic text $T$, where $w = O(\min(z \log N \log^*M, N))$ is the
size of the signature encoding of $T$, $z$ is the size of the Lempel-Ziv77
(LZ77) factorization of $T$, $N$ is the length of $T$, and $M \geq 3N$ is an
integer that can be handled in constant time under word RAM model. Our index
supports searching for a pattern $P$ in $T$ in $O(|P| f_{\mathcal{A}} + \log w
\log |P| \log^* M (\log N + \log |P| \log^* M) + \mathit{occ} \log N)$ time and
insertion/deletion of a substring of length $y$ in $O((y+ \log N\log^* M)\log w
\log N \log^* M)$ time, where $f_{\mathcal{A}} = O(\min \{ \frac{\log\log M
\log\log w}{\log\log\log M}, \sqrt{\frac{\log w}{\log\log w}} \})$. Also, we
propose a new space-efficient LZ77 factorization algorithm for a given text of
length $N$, which runs in $O(N f_{\mathcal{A}} + z \log w \log^3 N (\log^*
N)^2)$ time with $O(w)$ working space.
</summary>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1605.01488;
  text overlap with arXiv:1504.06954</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09558v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09558v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.01488">
    <id>http://arxiv.org/abs/1605.01488v2</id>
    <updated>2016-06-26T12:46:11Z</updated>
    <published>2016-05-05T04:29:05Z</published>
    <title>Fully dynamic data structure for LCE queries in compressed space</title>
    <summary>  A Longest Common Extension (LCE) query on a text $T$ of length $N$ asks for
the length of the longest common prefix of suffixes starting at given two
positions. We show that the signature encoding $\mathcal{G}$ of size $w =
O(\min(z \log N \log^* M, N))$ [Mehlhorn et al., Algorithmica 17(2):183-198,
1997] of $T$, which can be seen as a compressed representation of $T$, has a
capability to support LCE queries in $O(\log N + \log \ell \log^* M)$ time,
where $\ell$ is the answer to the query, $z$ is the size of the Lempel-Ziv77
(LZ77) factorization of $T$, and $M \geq 4N$ is an integer that can be handled
in constant time under word RAM model. In compressed space, this is the fastest
deterministic LCE data structure in many cases. Moreover, $\mathcal{G}$ can be
enhanced to support efficient update operations: After processing $\mathcal{G}$
in $O(w f_{\mathcal{A}})$ time, we can insert/delete any (sub)string of length
$y$ into/from an arbitrary position of $T$ in $O((y+ \log N\log^* M)
f_{\mathcal{A}})$ time, where $f_{\mathcal{A}} = O(\min \{ \frac{\log\log M
\log\log w}{\log\log\log M}, \sqrt{\frac{\log w}{\log\log w}} \})$. This yields
the first fully dynamic LCE data structure. We also present efficient
construction algorithms from various types of inputs: We can construct
$\mathcal{G}$ in $O(N f_{\mathcal{A}})$ time from uncompressed string $T$; in
$O(n \log\log n \log N \log^* M)$ time from grammar-compressed string $T$
represented by a straight-line program of size $n$; and in $O(z f_{\mathcal{A}}
\log N \log^* M)$ time from LZ77-compressed string $T$ with $z$ factors. On top
of the above contributions, we show several applications of our data structures
which improve previous best known results on grammar-compressed string
processing.
</summary>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1504.06954</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01488v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01488v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1711.02855">
    <id>http://arxiv.org/abs/1711.02855v4</id>
    <updated>2018-04-24T09:59:51Z</updated>
    <published>2017-11-08T07:43:35Z</published>
    <title>A compressed dynamic self-index for highly repetitive text collections</title>
    <summary>  We present a novel compressed dynamic self-index for highly repetitive text
collections. Signature encoding is a compressed dynamic self-index for highly
repetitive texts and has a large disadvantage that the pattern search for short
patterns is slow. We improve this disadvantage for faster pattern search by
leveraging an idea behind truncated suffix tree and present the first
compressed dynamic self-index named TST-index that supports not only fast
pattern search but also dynamic update operation of index for highly repetitive
texts. Experiments using a benchmark dataset of highly repetitive texts show
that the pattern search of TST-index is significantly improved.
</summary>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <link href="http://arxiv.org/abs/1711.02855v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02855v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1702.07458">
    <id>http://arxiv.org/abs/1702.07458v1</id>
    <updated>2017-02-24T03:57:57Z</updated>
    <published>2017-02-24T03:57:57Z</published>
    <title>Small-space encoding LCE data structure with constant-time queries</title>
    <summary>  The \emph{longest common extension} (\emph{LCE}) problem is to preprocess a
given string $w$ of length $n$ so that the length of the longest common prefix
between suffixes of $w$ that start at any two given positions is answered
quickly. In this paper, we present a data structure of $O(z \tau^2 +
\frac{n}{\tau})$ words of space which answers LCE queries in $O(1)$ time and
can be built in $O(n \log \sigma)$ time, where $1 \leq \tau \leq \sqrt{n}$ is a
parameter, $z$ is the size of the Lempel-Ziv 77 factorization of $w$ and
$\sigma$ is the alphabet size. This is an \emph{encoding} data structure, i.e.,
it does not access the input string $w$ when answering queries and thus $w$ can
be deleted after preprocessing. On top of this main result, we obtain further
results using (variants of) our LCE data structure, which include the
following:
  - For highly repetitive strings where the $z\tau^2$ term is dominated by
$\frac{n}{\tau}$, we obtain a \emph{constant-time and sub-linear space} LCE
query data structure.
  - Even when the input string is not well compressible via Lempel-Ziv 77
factorization, we still can obtain a \emph{constant-time and sub-linear space}
LCE data structure for suitable $\tau$ and for $\sigma \leq 2^{o(\log n)}$.
  - The time-space trade-off lower bounds for the LCE problem by Bille et al.
[J. Discrete Algorithms, 25:42-50, 2014] and by Kosolobov [CoRR,
abs/1611.02891, 2016] can be "surpassed" in some cases with our LCE data
structure.
</summary>
    <author>
      <name>Yuka Tanimura</name>
    </author>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1702.07458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1507.07080">
    <id>http://arxiv.org/abs/1507.07080v1</id>
    <updated>2015-07-25T08:42:56Z</updated>
    <published>2015-07-25T08:42:56Z</published>
    <title>Range Predecessor and Lempel-Ziv Parsing</title>
    <summary>  The Lempel-Ziv parsing of a string (LZ77 for short) is one of the most
important and widely-used algorithmic tools in data compression and string
processing. We show that the Lempel-Ziv parsing of a string of length $n$ on an
alphabet of size $\sigma$ can be computed in $O(n\log\log\sigma)$ time ($O(n)$
time if we allow randomization) using $O(n\log\sigma)$ bits of working space;
that is, using space proportional to that of the input string in bits. The
previous fastest algorithm using $O(n\log\sigma)$ space takes
$O(n(\log\sigma+\log\log n))$ time. We also consider the important rightmost
variant of the problem, where the goal is to associate with each phrase of the
parsing its most recent occurrence in the input string. We solve this problem
in $O(n(1 + (\log\sigma/\sqrt{\log n}))$ time, using the same working space as
above. The previous best solution for rightmost parsing uses
$O(n(1+\log\sigma/\log\log n))$ time and $O(n\log n)$ space. As a bonus, in our
solution for rightmost parsing we provide a faster construction method for
efficient 2D orthogonal range reporting, which is of independent interest.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.07080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.07080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1504.02605">
    <id>http://arxiv.org/abs/1504.02605v1</id>
    <updated>2015-04-10T09:34:07Z</updated>
    <published>2015-04-10T09:34:07Z</published>
    <title>Lempel Ziv Computation In Small Space (LZ-CISS)</title>
    <summary>  For both the Lempel Ziv 77- and 78-factorization we propose algorithms
generating the respective factorization using $(1+\epsilon) n \lg n + O(n)$
bits (for any positive constant $\epsilon \le 1$) working space (including the
space for the output) for any text of size \$n\$ over an integer alphabet in
$O(n / \epsilon^{2})$ time.
</summary>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full Version of CPM 2015 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.02605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1406.0263">
    <id>http://arxiv.org/abs/1406.0263v7</id>
    <updated>2015-06-03T04:08:34Z</updated>
    <published>2014-06-02T06:51:41Z</published>
    <title>The "Runs" Theorem</title>
    <summary>  We give a new characterization of maximal repetitions (or runs) in strings
based on Lyndon words. The characterization leads to a proof of what was known
as the "runs" conjecture (Kolpakov \&amp; Kucherov (FOCS '99)), which states that
the maximum number of runs $\rho(n)$ in a string of length $n$ is less than
$n$. The proof is remarkably simple, considering the numerous endeavors to
tackle this problem in the last 15 years, and significantly improves our
understanding of how runs can occur in strings. In addition, we obtain an upper
bound of $3n$ for the maximum sum of exponents $\sigma(n)$ of runs in a string
of length $n$, improving on the best known bound of $4.1n$ by Crochemore et al.
(JDA 2012), as well as other improved bounds on related problems. The
characterization also gives rise to a new, conceptually simple linear-time
algorithm for computing all the runs in a string. A notable characteristic of
our algorithm is that, unlike all existing linear-time algorithms, it does not
utilize the Lempel-Ziv factorization of the string. We also establish a
relationship between runs and nodes of the Lyndon tree, which gives a simple
optimal solution to the 2-Period Query problem that was recently solved by
Kociumaka et al. (SODA 2015).
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <author>
      <name>Kazuya Tsuruta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/15M1011032</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/15M1011032" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">simple proof with some more bounds</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIAM J. Comput., 46(5), 1501-1514, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.0263v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0263v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1610.03421">
    <id>http://arxiv.org/abs/1610.03421v2</id>
    <updated>2017-02-20T14:31:15Z</updated>
    <published>2016-10-11T16:51:46Z</published>
    <title>Computing All Distinct Squares in Linear Time for Integer Alphabets</title>
    <summary>  Given a string on an integer alphabet, we present an algorithm that computes
the set of all distinct squares belonging to this string in time linear to the
string length. As an application, we show how to compute the tree topology of
the minimal augmented suffix tree in linear time. Asides from that, we
elaborate an algorithm computing the longest previous table in a succinct
representation using compressed working space.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <link href="http://arxiv.org/abs/1610.03421v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03421v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1609.07220">
    <id>http://arxiv.org/abs/1609.07220v2</id>
    <updated>2017-02-20T08:00:30Z</updated>
    <published>2016-09-23T03:32:50Z</published>
    <title>Tight bounds on the maximum number of shortest unique substrings</title>
    <summary>  A substring Q of a string S is called a shortest unique substring (SUS) for
interval [s,t] in S, if Q occurs exactly once in S, this occurrence of Q
contains interval [s,t], and every substring of S which contains interval [s,t]
and is shorter than Q occurs at least twice in S. The SUS problem is, given a
string S, to preprocess S so that for any subsequent query interval [s,t] all
the SUSs for interval [s,t] can be answered quickly. When s = t, we call the
SUSs for [s,t] as point SUSs, and when s \leq t, we call the SUSs for [s,t] as
interval SUSs. There exist optimal O(n)-time preprocessing scheme which answers
queries in optimal O(k) time for both point and interval SUSs, where n is the
length of S and k is the number of outputs for a given query. In this paper, we
reveal structural, combinatorial properties underlying the SUS problem: Namely,
we show that the number of intervals in S that correspond to point SUSs for all
query positions in S is less than 1.5n, and show that this is a matching upper
and lower bound. Also, we consider the maximum number of intervals in S that
correspond to interval SUSs for all query intervals in S.
</summary>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07220v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07220v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1609.03668">
    <id>http://arxiv.org/abs/1609.03668v3</id>
    <updated>2017-02-06T05:43:08Z</updated>
    <published>2016-09-13T03:54:52Z</published>
    <title>Longest Common Subsequence in at Least $k$ Length Order-Isomorphic
  Substrings</title>
    <summary>  We consider the longest common subsequence (LCS) problem with the restriction
that the common subsequence is required to consist of at least $k$ length
substrings. First, we show an $O(mn)$ time algorithm for the problem which
gives a better worst-case running time than existing algorithms, where $m$ and
$n$ are lengths of the input strings. Furthermore, we mainly consider the LCS
in at least $k$ length order-isomorphic substrings problem. We show that the
problem can also be solved in $O(mn)$ worst-case time by an easy-to-implement
algorithm.
</summary>
    <author>
      <name>Yohei Ueki</name>
    </author>
    <author>
      <name> Diptarama</name>
    </author>
    <author>
      <name>Masatoshi Kurihara</name>
    </author>
    <author>
      <name>Yoshiaki Matsuoka</name>
    </author>
    <author>
      <name>Kazuyuki Narisawa</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures, contains erratum to Springer's version (SOFSEM
  2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03668v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03668v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1608.05550">
    <id>http://arxiv.org/abs/1608.05550v2</id>
    <updated>2017-07-04T10:23:59Z</updated>
    <published>2016-08-19T09:35:52Z</published>
    <title>Shortest unique palindromic substring queries in optimal time</title>
    <summary>  A palindrome is a string that reads the same forward and backward. A
palindromic substring $P$ of a string $S$ is called a shortest unique
palindromic substring ($\mathit{SUPS}$) for an interval $[x, y]$ in $S$, if $P$
occurs exactly once in $S$, this occurrence of $P$ contains interval $[x, y]$,
and every palindromic substring of $S$ which contains interval $[x, y]$ and is
shorter than $P$ occurs at least twice in $S$. The $\mathit{SUPS}$ problem is,
given a string $S$, to preprocess $S$ so that for any subsequent query interval
$[x, y]$ all the $\mathit{SUPS}\mbox{s}$ for interval $[x, y]$ can be answered
quickly. We present an optimal solution to this problem. Namely, we show how to
preprocess a given string $S$ of length $n$ in $O(n)$ time and space so that
all $\mathit{SUPS}\mbox{s}$ for any subsequent query interval can be answered
in $O(k+1)$ time, where $k$ is the number of outputs.
</summary>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Hiroe Inoue</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05550v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05550v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1601.07670">
    <id>http://arxiv.org/abs/1601.07670v2</id>
    <updated>2016-01-29T08:30:02Z</updated>
    <published>2016-01-28T07:14:19Z</published>
    <title>Deterministic sub-linear space LCE data structures with efficient
  construction</title>
    <summary>  Given a string $S$ of $n$ symbols, a longest common extension query
$\mathsf{LCE}(i,j)$ asks for the length of the longest common prefix of the
$i$th and $j$th suffixes of $S$. LCE queries have several important
applications in string processing, perhaps most notably to suffix sorting.
Recently, Bille et al. (J. Discrete Algorithms 25:42-50, 2014, Proc. CPM 2015:
65-76) described several data structures for answering LCE queries that offers
a space-time trade-off between data structure size and query time. In
particular, for a parameter $1 \leq \tau \leq n$, their best deterministic
solution is a data structure of size $O(n/\tau)$ which allows LCE queries to be
answered in $O(\tau)$ time. However, the construction time for all
deterministic versions of their data structure is quadratic in $n$. In this
paper, we propose a deterministic solution that achieves a similar space-time
trade-off of $O(\tau\min\{\log\tau,\log\frac{n}{\tau}\})$ query time using
$O(n/\tau)$ space, but significantly improve the construction time to
$O(n\tau)$.
</summary>
    <author>
      <name>Yuka Tanimura</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">updated title</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.07670v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07670v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1706.00148">
    <id>http://arxiv.org/abs/1706.00148v2</id>
    <updated>2017-07-25T08:42:32Z</updated>
    <published>2017-06-01T01:58:59Z</published>
    <title>Order preserving pattern matching on trees and DAGs</title>
    <summary>  The order preserving pattern matching (OPPM) problem is, given a pattern
string $p$ and a text string $t$, find all substrings of $t$ which have the
same relative orders as $p$. In this paper, we consider two variants of the
OPPM problem where a set of text strings is given as a tree or a DAG. We show
that the OPPM problem for a single pattern $p$ of length $m$ and a text tree
$T$ of size $N$ can be solved in $O(m+N)$ time if the characters of $p$ are
drawn from an integer alphabet of polynomial size. The time complexity becomes
$O(m \log m + N)$ if the pattern $p$ is over a general ordered alphabet. We
then show that the OPPM problem for a single pattern and a text DAG is
NP-complete.
</summary>
    <author>
      <name>Temma Nakamura</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1706.00148v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00148v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1703.04954">
    <id>http://arxiv.org/abs/1703.04954v1</id>
    <updated>2017-03-15T06:21:59Z</updated>
    <published>2017-03-15T06:21:59Z</published>
    <title>Faster STR-IC-LCS computation via RLE</title>
    <summary>  The constrained LCS problem asks one to find a longest common subsequence of
two input strings $A$ and $B$ with some constraints. The STR-IC-LCS problem is
a variant of the constrained LCS problem, where the solution must include a
given constraint string $C$ as a substring. Given two strings $A$ and $B$ of
respective lengths $M$ and $N$, and a constraint string $C$ of length at most
$\min\{M, N\}$, the best known algorithm for the STR-IC-LCS problem, proposed
by Deorowicz~({\em Inf. Process. Lett.}, 11:423--426, 2012), runs in $O(MN)$
time. In this work, we present an $O(mN + nM)$-time solution to the STR-IC-LCS
problem, where $m$ and $n$ denote the sizes of the run-length encodings of $A$
and $B$, respectively. Since $m \leq M$ and $n \leq N$ always hold, our
algorithm is always as fast as Deorowicz's algorithm, and is faster when input
strings are compressible via RLE.
</summary>
    <author>
      <name>Keita Kuboi</name>
    </author>
    <author>
      <name>Yuta Fujishige</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1701.02836">
    <id>http://arxiv.org/abs/1701.02836v2</id>
    <updated>2018-01-26T06:59:31Z</updated>
    <published>2017-01-11T03:17:23Z</published>
    <title>Computing Abelian regularities on RLE strings</title>
    <summary>  Two strings x and y are said to be Abelian equivalent if x is a permutation
of y, or vice versa. If a string z satisfies z = xy with x and y being Abelian
equivalent, then z is said to be an Abelian square. If a string w can be
factorized into a sequence v_1,...,v_s of strings such that v_1 ,..., v_{s-1}
are all Abelian equivalent and vs is a substring of a permutation of v_1, then
w is said to have a regular Abelian period (p,t) where p = |v_1| and t = |v_s|.
If a substring w_1[i..i+l-1] of a string w_1 and a substring w_2[j..j+l-1] of
another string w_2 are Abelian equivalent, then the substrings are said to be a
common Abelian factor of w_1 and w_2 and if the length l is the maximum of such
then the substrings are said to be a longest common Abelian factor of w_1 and
w_2. We propose efficient algorithms which compute these Abelian regularities
using the run length encoding (RLE) of strings. For a given string w of length
n whose RLE is of size m, we propose algorithms which compute all Abelian
squares occurring in w in O(mn) time, and all regular Abelian periods of w in
O(mn) time. For two given strings w_1 and w_2 of total length n and of total
RLE size m, we propose an algorithm which computes all longest common Abelian
factors in O(m^2n) time.
</summary>
    <author>
      <name>Shiho Sugimoto</name>
    </author>
    <author>
      <name>Naoki Noda</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1701.02836v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02836v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1802.05906">
    <id>http://arxiv.org/abs/1802.05906v6</id>
    <updated>2019-07-04T13:38:02Z</updated>
    <published>2018-02-16T12:19:07Z</published>
    <title>Refining the $r$-index</title>
    <summary>  Gagie, Navarro and Prezza's $r$-index (SODA, 2018) promises to speed up DNA
alignment and variation calling by allowing us to index entire genomic
databases, provided certain obstacles can be overcome. In this paper we first
strengthen and simplify Policriti and Prezza's Toehold Lemma (DCC '16;
Algorithmica, 2017), which inspired the $r$-index and plays an important role
in its implementation. We then show how to update the $r$-index efficiently
after adding a new genome to the database, which is likely to be vital in
practice. As a by-product of this result, we obtain an online version of
Policriti and Prezza's algorithm for constructing the LZ77 parse from a
run-length compressed Burrows-Wheeler Transform. Our experiments demonstrate
the practicality of all three of these results. Finally, we show how to augment
the $r$-index such that, given a new genome and fast random access to the
database, we can quickly compute the matching statistics and maximal exact
matches of the new genome with respect to the database.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extended version of the paper presented at CPM 2018 under the
  title "Online LZ77 parsing and matching statistics with RLBWTs"</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05906v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05906v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1710.03395">
    <id>http://arxiv.org/abs/1710.03395v4</id>
    <updated>2019-02-21T03:29:53Z</updated>
    <published>2017-10-10T03:56:33Z</published>
    <title>Efficient Dynamic Dictionary Matching with DAWGs and AC-automata</title>
    <summary>  The dictionary matching is a task to find all occurrences of patterns in a
set $D$ (called a dictionary) on a text $T$. The Aho-Corasick-automaton
(AC-automaton) is a data structure which enables us to solve the dictionary
matching problem in $O(d\log\sigma)$ preprocessing time and
$O(n\log\sigma+occ)$ matching time, where $d$ is the total length of the
patterns in $D$, $n$ is the length of the text, $\sigma$ is the alphabet size,
and $occ$ is the total number of occurrences of all the patterns in the text.
The dynamic dictionary matching is a variant where patterns may dynamically be
inserted into and deleted from $D$. This problem is called semi-dynamic
dictionary matching if only insertions are allowed. In this paper, we propose
two efficient algorithms. For a pattern of length $m$, our first algorithm
supports insertions in $O(m\log\sigma+\log d/\log\log d)$ time and pattern
matching in $O(n\log\sigma+occ)$ time for the semi-dynamic setting and supports
both insertions and deletions in $O(\sigma m+\log d/\log\log d)$ time and
pattern matching in $O(n(\log d/\log\log d+\log\sigma)+occ(\log d/\log\log d))$
time for the dynamic setting by some modifications. This algorithm is based on
the directed acyclic word graph. Our second algorithm, which is based on the
AC-automaton, supports insertions in $O(m\log \sigma+u_f+u_o)$ time for the
semi-dynamic setting and supports both insertions and deletions in $O(\sigma
m+u_f+u_o)$ time for the dynamic setting, where $u_f$ and $u_o$ respectively
denote the numbers of states in which the failure function and the output
function need to be updated. This algorithm performs pattern matching in
$O(n\log\sigma+occ)$ time for both settings. Our algorithm achieves optimal
update time for AC-automaton based methods over constant-size alphabets, since
any algorithm which explicitly maintains the AC-automaton requires
$\Omega(m+u_f+u_o)$ update time.
</summary>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2018.04.016</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2018.04.016" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.03395v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03395v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2008.08506">
    <id>http://arxiv.org/abs/2008.08506v1</id>
    <updated>2020-08-19T15:33:51Z</updated>
    <published>2020-08-19T15:33:51Z</published>
    <title>Novel Results on the Number of Runs of the Burrows-Wheeler-Transform</title>
    <summary>  The Burrows-Wheeler-Transform (BWT), a reversible string transformation, is
one of the fundamental components of many current data structures in string
processing. It is central in data compression, as well as in efficient query
algorithms for sequence data, such as webpages, genomic and other biological
sequences, or indeed any textual data. The BWT lends itself well to compression
because its number of equal-letter-runs (usually referred to as $r$) is often
considerably lower than that of the original string; in particular, it is well
suited for strings with many repeated factors. In fact, much attention has been
paid to the $r$ parameter as measure of repetitiveness, especially to evaluate
the performance in terms of both space and time of compressed indexing data
structures.
  In this paper, we investigate $\rho(v)$, the ratio of $r$ and of the number
of runs of the BWT of the reverse of $v$. Kempa and Kociumaka [FOCS 2020] gave
the first non-trivial upper bound as $\rho(v) = O(\log^2(n))$, for any string
$v$ of length $n$. However, nothing is known about the tightness of this upper
bound. We present infinite families of binary strings for which $\rho(v) =
\Theta(\log n)$ holds, thus giving the first non-trivial lower bound on
$\rho(n)$, the maximum over all strings of length $n$.
  Our results suggest that $r$ is not an ideal measure of the repetitiveness of
the string, since the number of repeated factors is invariant between the
string and its reverse. We believe that there is a more intricate relationship
between the number of runs of the BWT and the string's combinatorial
properties.
</summary>
    <author>
      <name>Sara Giuliani</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <author>
      <name>Anna Toffanello</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figues</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.08506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.08506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1107.3022">
    <id>http://arxiv.org/abs/1107.3022v1</id>
    <updated>2011-07-15T09:39:57Z</updated>
    <published>2011-07-15T09:39:57Z</published>
    <title>Computing q-gram Non-overlapping Frequencies on SLP Compressed Texts</title>
    <summary>  Length-$q$ substrings, or $q$-grams, can represent important characteristics
of text data, and determining the frequencies of all $q$-grams contained in the
data is an important problem with many applications in the field of data mining
and machine learning. In this paper, we consider the problem of calculating the
{\em non-overlapping frequencies} of all $q$-grams in a text given in
compressed form, namely, as a straight line program (SLP). We show that the
problem can be solved in $O(q^2n)$ time and $O(qn)$ space where $n$ is the size
of the SLP. This generalizes and greatly improves previous work (Inenaga &amp;
Bannai, 2009) which solved the problem only for $q=2$ in $O(n^4\log n)$ time
and $O(n^3)$ space.
</summary>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1107.3022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.3022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1107.3019">
    <id>http://arxiv.org/abs/1107.3019v1</id>
    <updated>2011-07-15T09:22:46Z</updated>
    <published>2011-07-15T09:22:46Z</published>
    <title>Computing q-gram Frequencies on Collage Systems</title>
    <summary>  Collage systems are a general framework for representing outputs of various
text compression algorithms. We consider the all $q$-gram frequency problem on
compressed string represented as a collage system, and present an $O((q+h\log
n)n)$-time $O(qn)$-space algorithm for calculating the frequencies for all
$q$-grams that occur in the string. Here, $n$ and $h$ are respectively the size
and height of the collage system.
</summary>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1107.3019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.3019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1103.3114">
    <id>http://arxiv.org/abs/1103.3114v2</id>
    <updated>2011-07-13T12:42:38Z</updated>
    <published>2011-03-16T07:16:10Z</published>
    <title>Fast $q$-gram Mining on SLP Compressed Strings</title>
    <summary>  We present simple and efficient algorithms for calculating $q$-gram
frequencies on strings represented in compressed form, namely, as a straight
line program (SLP). Given an SLP of size $n$ that represents string $T$, we
present an $O(qn)$ time and space algorithm that computes the occurrence
frequencies of $q$-grams in $T$. Computational experiments show that our
algorithm and its variation are practical for small $q$, actually running
faster on various real string data, compared to algorithms that work on the
uncompressed text. We also discuss applications in data mining and
classification of string data, for which our algorithms can be useful.
</summary>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1103.3114v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.3114v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1211.3642">
    <id>http://arxiv.org/abs/1211.3642v2</id>
    <updated>2013-01-18T08:31:01Z</updated>
    <published>2012-11-15T16:22:40Z</published>
    <title>Simpler and Faster Lempel Ziv Factorization</title>
    <summary>  We present a new, simple, and efficient approach for computing the Lempel-Ziv
(LZ77) factorization of a string in linear time, based on suffix arrays.
Computational experiments on various data sets show that our approach
constantly outperforms the currently fastest algorithm LZ OG (Ohlebusch and Gog
2011), and can be up to 2 to 3 times faster in the processing after obtaining
the suffix array, while requiring the same or a little more space.
</summary>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <link href="http://arxiv.org/abs/1211.3642v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.3642v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1305.6095">
    <id>http://arxiv.org/abs/1305.6095v1</id>
    <updated>2013-05-27T02:35:15Z</updated>
    <published>2013-05-27T02:35:15Z</published>
    <title>Faster Compact On-Line Lempel-Ziv Factorization</title>
    <summary>  We present a new on-line algorithm for computing the Lempel-Ziv factorization
of a string that runs in $O(N\log N)$ time and uses only $O(N\log\sigma)$ bits
of working space, where $N$ is the length of the string and $\sigma$ is the
size of the alphabet. This is a notable improvement compared to the performance
of previous on-line algorithms using the same order of working space but
running in either $O(N\log^3N)$ time (Okanohara &amp; Sadakane 2009) or
$O(N\log^2N)$ time (Starikovskaya 2012). The key to our new algorithm is in the
utilization of an elegant but less popular index structure called Directed
Acyclic Word Graphs, or DAWGs (Blumer et al. 1985). We also present an
opportunistic variant of our algorithm, which, given the run length encoding of
size $m$ of a string of length $N$, computes the Lempel-Ziv factorization
on-line, in $O\left(m \cdot \min \left\{\frac{(\log\log m)(\log \log
N)}{\log\log\log N}, \sqrt{\frac{\log m}{\log \log m}} \right\}\right)$ time
and $O(m\log N)$ bits of space, which is faster and more space efficient when
the string is run-length compressible.
</summary>
    <author>
      <name>Jun'ichi Yamamoto</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1305.6095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.6095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1304.7067">
    <id>http://arxiv.org/abs/1304.7067v1</id>
    <updated>2013-04-26T04:55:07Z</updated>
    <published>2013-04-26T04:55:07Z</published>
    <title>Detecting regularities on grammar-compressed strings</title>
    <summary>  We solve the problems of detecting and counting various forms of regularities
in a string represented as a Straight Line Program (SLP). Given an SLP of size
$n$ that represents a string $s$ of length $N$, our algorithm compute all runs
and squares in $s$ in $O(n^3h)$ time and $O(n^2)$ space, where $h$ is the
height of the derivation tree of the SLP. We also show an algorithm to compute
all gapped-palindromes in $O(n^3h + gnh\log N)$ time and $O(n^2)$ space, where
$g$ is the length of the gap. The key technique of the above solution also
allows us to compute the periods and covers of the string in $O(n^2 h)$ time
and $O(nh(n+\log^2 N))$ time, respectively.
</summary>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Wataru Matsubara</name>
    </author>
    <author>
      <name>Kouji Shimohira</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <author>
      <name>Kazuyuki Narisawa</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <link href="http://arxiv.org/abs/1304.7067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1304.7061">
    <id>http://arxiv.org/abs/1304.7061v1</id>
    <updated>2013-04-26T04:04:54Z</updated>
    <published>2013-04-26T04:04:54Z</published>
    <title>Efficient Lyndon factorization of grammar compressed text</title>
    <summary>  We present an algorithm for computing the Lyndon factorization of a string
that is given in grammar compressed form, namely, a Straight Line Program
(SLP). The algorithm runs in $O(n^4 + mn^3h)$ time and $O(n^2)$ space, where
$m$ is the size of the Lyndon factorization, $n$ is the size of the SLP, and
$h$ is the height of the derivation tree of the SLP. Since the length of the
decompressed string can be exponentially large w.r.t. $n, m$ and $h$, our
result is the first polynomial time solution when the string is given as SLP.
</summary>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CPM 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.7061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1303.3945">
    <id>http://arxiv.org/abs/1303.3945v1</id>
    <updated>2013-03-16T05:07:24Z</updated>
    <published>2013-03-16T05:07:24Z</published>
    <title>Computing convolution on grammar-compressed text</title>
    <summary>  The convolution between a text string $S$ of length $N$ and a pattern string
$P$ of length $m$ can be computed in $O(N \log m)$ time by FFT. It is known
that various types of approximate string matching problems are reducible to
convolution. In this paper, we assume that the input text string is given in a
compressed form, as a \emph{straight-line program (SLP)}, which is a context
free grammar in the Chomsky normal form that derives a single string. Given an
SLP $\mathcal{S}$ of size $n$ describing a text $S$ of length $N$, and an
uncompressed pattern $P$ of length $m$, we present a simple $O(nm \log m)$-time
algorithm to compute the convolution between $S$ and $P$. We then show that
this can be improved to $O(\min\{nm, N-\alpha\} \log m)$ time, where $\alpha
\geq 0$ is a value that represents the amount of redundancy that the SLP
captures with respect to the length-$m$ substrings. The key of the improvement
is our new algorithm that computes the convolution between a trie of size $r$
and a pattern string $P$ of length $m$ in $O(r \log m)$ time.
</summary>
    <author>
      <name>Toshiya Tanaka</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">DCC 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.3945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.3945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.10987">
    <id>http://arxiv.org/abs/1705.10987v1</id>
    <updated>2017-05-31T08:46:22Z</updated>
    <published>2017-05-31T08:46:22Z</published>
    <title>Succinct Partial Sums and Fenwick Trees</title>
    <summary>  We consider the well-studied partial sums problem in succint space where one
is to maintain an array of n k-bit integers subject to updates such that
partial sums queries can be efficiently answered. We present two succint
versions of the Fenwick Tree - which is known for its simplicity and
practicality. Our results hold in the encoding model where one is allowed to
reuse the space from the input data. Our main result is the first that only
requires nk + o(n) bits of space while still supporting sum/update in O(log_b
n) / O(b log_b n) time where 2 &lt;= b &lt;= log^O(1) n. The second result shows how
optimal time for sum/update can be achieved while only slightly increasing the
space usage to nk + o(nk) bits. Beyond Fenwick Trees, the results are primarily
based on bit-packing and sampling - making them very practical - and they also
allow for simple optimal parallelization.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Anders Roy Christiansen</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Frederik Rye Skjoldjensen</name>
    </author>
    <link href="http://arxiv.org/abs/1705.10987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1701.07238">
    <id>http://arxiv.org/abs/1701.07238v1</id>
    <updated>2017-01-25T10:16:30Z</updated>
    <published>2017-01-25T10:16:30Z</published>
    <title>A Framework of Dynamic Data Structures for String Processing</title>
    <summary>  In this paper we present DYNAMIC, an open-source C++ library implementing
dynamic compressed data structures for string manipulation. Our framework
includes useful tools such as searchable partial sums, succinct/gap-encoded
bitvectors, and entropy/run-length compressed strings and FM-indexes. We prove
close-to-optimal theoretical bounds for the resources used by our structures,
and show that our theoretical predictions are empirically tightly verified in
practice. To conclude, we turn our attention to applications. We compare the
performance of four recently-published compression algorithms implemented using
DYNAMIC with those of state-of-the-art tools performing the same task. Our
experiments show that algorithms making use of dynamic compressed data
structures can be up to three orders of magnitude more space-efficient (albeit
slower) than classical ones performing the same tasks.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1701.07238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.07238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.02480">
    <id>http://arxiv.org/abs/2111.02480v1</id>
    <updated>2021-11-03T19:15:54Z</updated>
    <published>2021-11-03T19:15:54Z</published>
    <title>Linear-time Minimization of Wheeler DFAs</title>
    <summary>  Wheeler DFAs (WDFAs) are a sub-class of finite-state automata which is
playing an important role in the emerging field of compressed data structures:
as opposed to general automata, WDFAs can be stored in just $\log\sigma + O(1)$
bits per edge, $\sigma$ being the alphabet's size, and support optimal-time
pattern matching queries on the substring closure of the language they
recognize. An important step to achieve further compression is minimization.
When the input $\mathcal A$ is a general deterministic finite-state automaton
(DFA), the state-of-the-art is represented by the classic Hopcroft's algorithm,
which runs in $O(|\mathcal A|\log |\mathcal A|)$ time. This algorithm stands at
the core of the only existing minimization algorithm for Wheeler DFAs, which
inherits its complexity. In this work, we show that the minimum WDFA equivalent
to a given input WDFA can be computed in linear $O(|\mathcal A|)$ time. When
run on de Bruijn WDFAs built from real DNA datasets, an implementation of our
algorithm reduces the number of nodes from 14% to 51% at a speed of more than 1
million nodes per second.
</summary>
    <author>
      <name>Jarno Alanko</name>
    </author>
    <author>
      <name>Nicola Cotumaccio</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/2111.02480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.02480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.08179">
    <id>http://arxiv.org/abs/2007.08179v1</id>
    <updated>2020-07-16T08:38:14Z</updated>
    <published>2020-07-16T08:38:14Z</published>
    <title>String Sanitization Under Edit Distance: Improved and Generalized</title>
    <summary>  Let $W$ be a string of length $n$ over an alphabet $\Sigma$, $k$ be a
positive integer, and $\mathcal{S}$ be a set of length-$k$ substrings of $W$.
The ETFS problem asks us to construct a string $X_{\mathrm{ED}}$ such that: (i)
no string of $\mathcal{S}$ occurs in $X_{\mathrm{ED}}$; (ii) the order of all
other length-$k$ substrings over $\Sigma$ is the same in $W$ and in
$X_{\mathrm{ED}}$; and (iii) $X_{\mathrm{ED}}$ has minimal edit distance to
$W$. When $W$ represents an individual's data and $\mathcal{S}$ represents a
set of confidential patterns, the ETFS problem asks for transforming $W$ to
preserve its privacy and its utility [Bernardini et al., ECML PKDD 2019].
  ETFS can be solved in $\mathcal{O}(n^2k)$ time [Bernardini et al., CPM 2020].
The same paper shows that ETFS cannot be solved in $\mathcal{O}(n^{2-\delta})$
time, for any $\delta>0$, unless the Strong Exponential Time Hypothesis (SETH)
is false. Our main results can be summarized as follows: (i) an
$\mathcal{O}(n^2\log^2k)$-time algorithm to solve ETFS; and (ii) an
$\mathcal{O}(n^2\log^2n)$-time algorithm to solve AETFS, a generalization of
ETFS in which the elements of $\mathcal{S}$ can have arbitrary lengths. Our
algorithms are thus optimal up to polylogarithmic factors, unless SETH fails.
Let us also stress that our algorithms work under edit distance with arbitrary
weights at no extra cost. As a bonus, we show how to modify some known
techniques, which speed up the standard edit distance computation, to be
applied to our problems. Beyond string sanitization, our techniques may inspire
solutions to other problems related to regular expressions or context-free
grammars.
</summary>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Leen Stougie</name>
    </author>
    <author>
      <name>Michelle Sweering</name>
    </author>
    <link href="http://arxiv.org/abs/2007.08179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.08179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1509.09237">
    <id>http://arxiv.org/abs/1509.09237v1</id>
    <updated>2015-09-30T16:13:06Z</updated>
    <published>2015-09-30T16:13:06Z</published>
    <title>Efficiently Finding All Maximal $α$-gapped Repeats</title>
    <summary>  For $\alpha\geq 1$, an $\alpha$-gapped repeat in a word $w$ is a factor $uvu$
of $w$ such that $|uv|\leq \alpha |u|$; the two factors $u$ in such a repeat
are called arms, while the factor $v$ is called gap. Such a repeat is called
maximal if its arms cannot be extended simultaneously with the same symbol to
the right or, respectively, to the left. In this paper we show that the number
of maximal $\alpha$-gapped repeats that may occur in a word is upper bounded by
$18\alpha n$. This allows us to construct an algorithm finding all the maximal
$\alpha$-gapped repeats of a word in $O(\alpha n)$; this is optimal, in the
worst case, as there are words that have $\Theta(\alpha n)$ maximal
$\alpha$-gapped repeats. Our techniques can be extended to get comparable
results in the case of $\alpha$-gapped palindromes, i.e., factors
$uvu^\mathrm{T}$ with $|uv|\leq \alpha |u|$.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <link href="http://arxiv.org/abs/1509.09237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.09237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1507.07622">
    <id>http://arxiv.org/abs/1507.07622v5</id>
    <updated>2018-07-12T14:35:38Z</updated>
    <published>2015-07-28T02:15:02Z</published>
    <title>Fully-Online Suffix Tree and Directed Acyclic Word Graph Construction
  for Multiple Texts</title>
    <summary>  We consider construction of the suffix tree and the directed acyclic word
graph (DAWG) indexing data structures for a collection $\mathcal{T}$ of texts,
where a new symbol may be appended to any text in $\mathcal{T} = \{T_1, \ldots,
T_K\}$, at any time. This fully-online scenario, which arises in dynamically
indexing multi-sensor data, is a natural generalization of the long solved
semi-online text indexing problem, where texts $T_1, \ldots, T_{k}$ are
permanently fixed before the next text $T_{k+1}$ is processed for each $1 \leq
k &lt; K$. We present fully-online algorithms that construct the suffix tree and
the DAWG for $\mathcal{T}$ in $O(N \log \sigma)$ time and $O(N)$ space, where
$N$ is the total lengths of the strings in $\mathcal{T}$ and $\sigma$ is their
alphabet size. The standard explicit representation of the suffix tree leaf
edges and some DAWG edges must be relaxed in our fully-online scenario, since
too many updates on these edges are required in the worst case. Instead, we
provide access to the updated suffix tree leaf edge labels and the DAWG edges
to be redirected via auxiliary data structures, in $O(\log \sigma)$ time per
added character.
</summary>
    <author>
      <name>Takuya Takagi</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hiroki Arimura</name>
    </author>
    <author>
      <name>Dany Breslauer</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 6 figures, LaTeX</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.07622v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.07622v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1503.04045">
    <id>http://arxiv.org/abs/1503.04045v3</id>
    <updated>2017-02-16T11:18:51Z</updated>
    <published>2015-03-13T13:04:26Z</published>
    <title>Diverse Palindromic Factorization is NP-Complete</title>
    <summary>  We prove that it is NP-complete to decide whether a given string can be
factored into palindromes that are each unique in the factorization.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Juha Karkkainen</name>
    </author>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Marcin Piatkowski</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Shiho Sugimoto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0129054118400014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0129054118400014" rel="related"/>
    <link href="http://arxiv.org/abs/1503.04045v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.04045v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1612.07475">
    <id>http://arxiv.org/abs/1612.07475v1</id>
    <updated>2016-12-22T08:04:21Z</updated>
    <published>2016-12-22T08:04:21Z</published>
    <title>A hardness result and new algorithm for the longest common palindromic
  subsequence problem</title>
    <summary>  The 2-LCPS problem, first introduced by Chowdhury et al. [Fundam. Inform.,
129(4):329-340, 2014], asks one to compute (the length of) a longest
palindromic common subsequence between two given strings $A$ and $B$. We show
that the 2-LCPS problem is at least as hard as the well-studied longest common
subsequence problem for four strings (the 4-LCS problem). Then, we present a
new algorithm which solves the 2-LCPS problem in $O(\sigma M^2 + n)$ time,
where $n$ denotes the length of $A$ and $B$, $M$ denotes the number of matching
positions between $A$ and $B$, and $\sigma$ denotes the number of distinct
characters occurring in both $A$ and $B$. Our new algorithm is faster than
Chowdhury et al.'s sparse algorithm when $\sigma = o(\log^2n \log\log n)$.
</summary>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Heikki Hyyrö</name>
    </author>
    <link href="http://arxiv.org/abs/1612.07475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1609.03000">
    <id>http://arxiv.org/abs/1609.03000v5</id>
    <updated>2019-10-31T05:54:12Z</updated>
    <published>2016-09-10T05:03:01Z</published>
    <title>Efficient computation of longest single-arm-gapped palindromes in a
  string</title>
    <summary>  In this paper, we introduce new types of approximate palindromes called
single-arm-gapped palindromes (shortly SAGPs). A SAGP contains a gap in either
its left or right arm, which is in the form of either $wguc u^R w^R$ or $wuc
u^Rgw^R$, where $w$ and $u$ are non-empty strings, $w^R$ and $u^R$ are
respectively the reversed strings of $w$ and $u$, $g$ is a string called a gap,
and $c$ is either a single character or the empty string. Here we call $wu$ and
$u^R w^R$ the arm of the SAGP, and $|uv|$ the length of the arm. We classify
SAGPs into two groups: those which have $ucu^R$ as a maximal palindrome
(type-1), and the others (type-2). We propose several algorithms to compute
type-1 SAGPs with longest arms occurring in a given string, based on suffix
arrays. Then, we propose a linear-time algorithm to compute all type-1 SAGPs
with longest arms, based on suffix trees. Also, we show how to compute type-2
SAGPs with longest arms in linear time. We also perform some preliminary
experiments to show practical performances of the proposed methods.
</summary>
    <author>
      <name>Shintaro Narisada</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Kazuyuki Narisawa</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2019.10.025</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2019.10.025" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Theoretical Computer Science, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.03000v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03000v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.00422">
    <id>http://arxiv.org/abs/1602.00422v1</id>
    <updated>2016-02-01T08:17:52Z</updated>
    <published>2016-02-01T08:17:52Z</published>
    <title>Packed Compact Tries: A Fast and Efficient Data Structure for Online
  String Processing</title>
    <summary>  In this paper, we present a new data structure called the packed compact trie
(packed c-trie) which stores a set $S$ of $k$ strings of total length $n$ in $n
\log\sigma + O(k \log n)$ bits of space and supports fast pattern matching
queries and updates, where $\sigma$ is the size of an alphabet. Assume that
$\alpha = \log_\sigma n$ letters are packed in a single machine word on the
standard word RAM model, and let $f(k,n)$ denote the query and update times of
the dynamic predecessor/successor data structure of our choice which stores $k$
integers from universe $[1,n]$ in $O(k \log n)$ bits of space. Then, given a
string of length $m$, our packed c-tries support pattern matching queries and
insert/delete operations in $O(\frac{m}{\alpha} f(k,n))$ worst-case time and in
$O(\frac{m}{\alpha} + f(k,n))$ expected time. Our experiments show that our
packed c-tries are faster than the standard compact tries (a.k.a. Patricia
trees) on real data sets. As an application of our packed c-trie, we show that
the sparse suffix tree for a string of length $n$ over prefix codes with $k$
sampled positions, such as evenly-spaced and word delimited sparse suffix
trees, can be constructed online in $O((\frac{n}{\alpha} + k) f(k,n))$
worst-case time and $O(\frac{n}{\alpha} + k f(k,n))$ expected time with $n \log
\sigma + O(k \log n)$ bits of space. When $k = O(\frac{n}{\alpha})$, by using
the state-of-the-art dynamic predecessor/successor data structures, we obtain
sub-linear time construction algorithms using only $O(\frac{n}{\alpha})$ bits
of space in both cases. We also discuss an application of our packed c-tries to
online LZD factorization.
</summary>
    <author>
      <name>Takuya Takagi</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <author>
      <name>Hiroki Arimura</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1587/transfun.E100.A.1785</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1587/transfun.E100.A.1785" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.00422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1606.02465">
    <id>http://arxiv.org/abs/1606.02465v1</id>
    <updated>2016-06-08T09:17:37Z</updated>
    <published>2016-06-08T09:17:37Z</published>
    <title>On the Benefit of Merging Suffix Array Intervals for Parallel Pattern
  Matching</title>
    <summary>  We present parallel algorithms for exact and approximate pattern matching
with suffix arrays, using a CREW-PRAM with $p$ processors. Given a static text
of length $n$, we first show how to compute the suffix array interval of a
given pattern of length $m$ in $O(\frac{m}{p}+ \lg p + \lg\lg p\cdot\lg\lg n)$
time for $p \le m$. For approximate pattern matching with $k$ differences or
mismatches, we show how to compute all occurrences of a given pattern in
$O(\frac{m^k\sigma^k}{p}\max\left(k,\lg\lg n\right)\!+\!(1+\frac{m}{p}) \lg
p\cdot \lg\lg n + \text{occ})$ time, where $\sigma$ is the size of the alphabet
and $p \le \sigma^k m^k$. The workhorse of our algorithms is a data structure
for merging suffix array intervals quickly: Given the suffix array intervals
for two patterns $P$ and $P'$, we present a data structure for computing the
interval of $PP'$ in $O(\lg\lg n)$ sequential time, or in $O(1+\lg_p\lg n)$
parallel time. All our data structures are of size $O(n)$ bits (in addition to
the suffix array).
</summary>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Florian Kurpicz</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1706.03035">
    <id>http://arxiv.org/abs/1706.03035v1</id>
    <updated>2017-06-09T16:49:35Z</updated>
    <published>2017-06-09T16:49:35Z</published>
    <title>Practical Evaluation of Lempel-Ziv-78 and Lempel-Ziv-Welch Tries</title>
    <summary>  We present the first thorough practical study of the Lempel-Ziv-78 and the
Lempel-Ziv-Welch computation based on trie data structures. With a careful
selection of trie representations we can beat well-tuned popular trie data
structures like Judy, m-Bonsai or Cedar.
</summary>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <link href="http://arxiv.org/abs/1706.03035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1802.10355">
    <id>http://arxiv.org/abs/1802.10355v1</id>
    <updated>2018-02-28T10:58:16Z</updated>
    <published>2018-02-28T10:58:16Z</published>
    <title>Improved Upper Bounds on all Maximal $α$-gapped Repeats and
  Palindromes</title>
    <summary>  We show that the number of all maximal $\alpha$-gapped repeats and
palindromes of a word of length $n$ is at most $3(\pi^2/6 + 5/2) \alpha n$ and
$7 (\pi^2 / 6 + 1/2) \alpha n - 5 n - 1$, respectively.
</summary>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <link href="http://arxiv.org/abs/1802.10355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.02759">
    <id>http://arxiv.org/abs/1707.02759v1</id>
    <updated>2017-07-10T09:06:19Z</updated>
    <published>2017-07-10T09:06:19Z</published>
    <title>A succinct data structure for self-indexing ternary relations</title>
    <summary>  The representation of binary relations has been intensively studied and many
different theoretical and practical representations have been proposed to
answer the usual queries in multiple domains. However, ternary relations have
not received as much attention, even though many real-world applications
require the processing of ternary relations. In this paper we present a new
compressed and self-indexed data structure that we call Interleaved $K^2$-tree
(I$K^2$-tree), designed to compactly represent and efficiently query general
ternary relations. The I$K^2$-tree is an evolution of an existing data
structure, the $K^2$-tree, initially designed to represent Web graphs and later
applied to other domains. The I$K^2$-tree is able to extend the $K^2$-tree to
represent a ternary relation, based on the idea of decomposing it into a
collection of binary relations but providing indexing capabilities in all the
three dimensions. We present different ways to use I$K^2$-tree to model
different types of ternary relations using as reference two typical domains:
RDF and Temporal Graphs. We also experimentally evaluate our representations
comparing them in space usage and performance with other solutions of the state
of the art.
</summary>
    <author>
      <name>Sandra Alvarez-Garcia</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jda.2016.10.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jda.2016.10.002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941, Journal of Discrete
  Algorithms (2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.01743">
    <id>http://arxiv.org/abs/1707.01743v3</id>
    <updated>2017-09-01T18:34:23Z</updated>
    <published>2017-07-06T12:15:45Z</published>
    <title>Fast Compressed Self-Indexes with Deterministic Linear-Time Construction</title>
    <summary>  We introduce a compressed suffix array representation that, on a text $T$ of
length $n$ over an alphabet of size $\sigma$, can be built in $O(n)$
deterministic time, within $O(n\log\sigma)$ bits of working space, and counts
the number of occurrences of any pattern $P$ in $T$ in time $O(|P| + \log\log_w
\sigma)$ on a RAM machine of $w=\Omega(\log n)$-bit words. This new index
outperforms all the other compressed indexes that can be built in linear
deterministic time, and some others. The only faster indexes can be built in
linear time only in expectation, or require $\Theta(n\log n)$ bits. We also
show that, by using $O(n\log\sigma)$ bits, we can build in linear time an index
that counts in time $O(|P|/\log_\sigma n + \log n(\log\log n)^2)$, which is
RAM-optimal for $w=\Theta(\log n)$ and sufficiently long patterns.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/1707.01743v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01743v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.02576">
    <id>http://arxiv.org/abs/1803.02576v1</id>
    <updated>2018-03-07T09:57:23Z</updated>
    <published>2018-03-07T09:57:23Z</published>
    <title>Compact Representations of Event Sequences</title>
    <summary>  We introduce a new technique for the efficient management of large sequences
of multidimensional data, which takes advantage of regularities that arise in
real-world datasets and supports different types of aggregation queries. More
importantly, our representation is flexible in the sense that the relevant
dimensions and queries may be used to guide the construction process, easily
providing a space-time tradeoff depending on the relevant queries in the
domain. We provide two alternative representations for sequences of
multidimensional data and describe the techniques to efficiently store the
datasets and to perform aggregation queries over the compressed representation.
We perform experimental evaluation on realistic datasets, showing the space
efficiency and query capabilities of our proposal.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Tirso V. Rodeiro</name>
    </author>
    <author>
      <name>Diego Seco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.02576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.02576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1502.03288">
    <id>http://arxiv.org/abs/1502.03288v2</id>
    <updated>2015-05-14T12:56:29Z</updated>
    <published>2015-02-11T12:53:53Z</published>
    <title>A Compressed-Gap Data-Aware Measure</title>
    <summary>  In this paper, we consider the problem of efficiently representing a set $S$
of $n$ items out of a universe $U=\{0,...,u-1\}$ while supporting a number of
operations on it. Let $G=g_1...g_n$ be the gap stream associated with $S$,
$gap$ its bit-size when encoded with \emph{gap-encoding}, and $H_0(G)$ its
empirical zero-order entropy. We prove that (1) $nH_0(G)\in o(gap)$ if $G$ is
highly compressible, and (2) $nH_0(G) \leq n\log(u/n) + n \leq uH_0(S)$. Let
$d$ be the number of \emph{distinct} gap lengths between elements in $S$. We
firstly propose a new space-efficient zero-order compressed representation of
$S$ taking $n(H_0(G)+1)+\mathcal O(d\log u)$ bits of space. Then, we describe a
fully-indexable dictionary that supports \emph{rank} and \emph{select} queries
in $\mathcal O(\log(u/n)+\log\log u)$ time while requiring asymptotically the
same space as the proposed compressed representation of $S$.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.03288v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03288v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1608.05100">
    <id>http://arxiv.org/abs/1608.05100v11</id>
    <updated>2017-11-01T10:57:39Z</updated>
    <published>2016-08-17T20:54:07Z</published>
    <title>In-Place Sparse Suffix Sorting</title>
    <summary>  Suffix arrays encode the lexicographical order of all suffixes of a text and
are often combined with the Longest Common Prefix array (LCP) to simulate
navigational queries on the suffix tree in reduced space. In space-critical
applications such as sparse and compressed text indexing, only information
regarding the lexicographical order of a size-$b$ subset of all $n$ text
suffixes is often needed. Such information can be stored space-efficiently (in
$b$ words) in the sparse suffix array (SSA). The SSA and its relative sparse
LCP array (SLCP) can be used as a space-efficient substitute of the sparse
suffix tree. Very recently, Gawrychowski and Kociumaka [SODA 2017] showed that
the sparse suffix tree (and therefore SSA and SLCP) can be built in
asymptotically optimal $O(b)$ space with a Monte Carlo algorithm running in
$O(n)$ time. The main reason for using the SSA and SLCP arrays in place of the
sparse suffix tree is, however, their reduced space of $b$ words each. This
leads naturally to the quest for in-place algorithms building these arrays.
Franceschini and Muthukrishnan [ICALP 2007] showed that the full suffix array
can be built in-place and in optimal running time. On the other hand, finding
sub-quadratic in-place algorithms for building the SSA and SLCP for
\emph{general} subsets of suffixes has been an elusive task for decades. In
this paper, we give the first solution to this problem. We provide the first
in-place algorithm building the full LCP array in $O(n\log n)$ expected time
and the first Monte Carlo in-place algorithms building the SSA and SLCP in $O(n
+ b\log^2 n)$ expected time. We moreover describe the first in-place solution
for the suffix selection problem: to compute the $i$-th smallest text suffix.
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM-SIAM Symposium on Discrete Algorithms 2018; arXiv admin note:
  text overlap with arXiv:1607.06660 Comment: new style (lipics); using
  Heath-Brown theorem for number of primes in Z; improved bounds for LCP array
  computation and sparse suffix sorting; added construction of the LCE
  structure using radix sort; added reference to lower bound for LCE query
  times; uploaded version accepted at SODA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05100v11" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05100v11" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.06660">
    <id>http://arxiv.org/abs/1607.06660v1</id>
    <updated>2016-07-22T12:54:30Z</updated>
    <published>2016-07-22T12:54:30Z</published>
    <title>Fast Longest Common Extensions in Small Space</title>
    <summary>  In this paper we address the longest common extension (LCE) problem: to
compute the length $\ell$ of the longest common prefix between any two suffixes
of $T\in \Sigma^n$ with $ \Sigma = \{0, \ldots \sigma-1\} $. We present two
fast and space-efficient solutions based on (Karp-Rabin)
\textit{fingerprinting} and \textit{sampling}. Our first data structure
exploits properties of Mersenne prime numbers when used as moduli of the
Karp-Rabin hash function and takes $n\lceil \log_2\sigma\rceil$ bits of space.
Our second structure works with any prime modulus and takes $n\lceil
\log_2\sigma\rceil + n/w + w\log_2 n$ bits of space ($ w $ memory-word size).
Both structures support $\mathcal O\left(m\log\sigma/w \right)$-time extraction
of any length-$m$ text substring, $\mathcal O(\log\ell)$-time LCE queries with
high probability, and can be built in optimal $\mathcal O(n)$ time. In the
first case, ours is the first result showing that it is possible to answer LCE
queries in $o(n)$ time while using only $\mathcal O(1)$ words on top of the
space required to store the text. Our results improve the state of the art in
space usage, query times, and preprocessing times and are extremely practical:
we present a C++ implementation that is very fast and space-efficient in
practice.
</summary>
    <author>
      <name>Alberto Policriti</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1604.06002">
    <id>http://arxiv.org/abs/1604.06002v2</id>
    <updated>2016-04-21T14:31:16Z</updated>
    <published>2016-04-20T15:30:36Z</published>
    <title>Practical combinations of repetition-aware data structures</title>
    <summary>  Highly-repetitive collections of strings are increasingly being amassed by
genome sequencing and genetic variation experiments, as well as by storing all
versions of human-generated files, like webpages and source code. Existing
indexes for locating all the exact occurrences of a pattern in a
highly-repetitive string take advantage of a single measure of repetition.
However, multiple, distinct measures of repetition all grow sublinearly in the
length of a highly-repetitive string. In this paper we explore the practical
advantages of combining data structures whose size depends on distinct measures
of repetition. The main ingredient of our structures is the run-length encoded
BWT (RLBWT), which takes space proportional to the number of runs in the
Burrows-Wheeler transform of a string. We describe a range of practical
variants that combine RLBWT with the set of boundaries of the Lempel-Ziv 77
factors of a string, which take space proportional to the number of factors.
Such variants use, respectively, the RLBWT of a string and the RLBWT of its
reverse, or just one RLBWT inside a bidirectional index, or just one RLBWT with
support for unidirectional extraction. We also study the practical advantages
of combining RLBWT with the compact directed acyclic word graph of a string, a
data structure that takes space proportional to the number of one-character
extensions of maximal repeats. Our approaches are easy to implement, and
provide competitive tradeoffs on significant datasets.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Fabio Cunial</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Mathieu Raffinot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1502.05937</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06002v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06002v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1504.02671">
    <id>http://arxiv.org/abs/1504.02671v1</id>
    <updated>2015-04-10T13:23:27Z</updated>
    <published>2015-04-10T13:23:27Z</published>
    <title>Longest Common Extensions in Sublinear Space</title>
    <summary>  The longest common extension problem (LCE problem) is to construct a data
structure for an input string $T$ of length $n$ that supports LCE$(i,j)$
queries. Such a query returns the length of the longest common prefix of the
suffixes starting at positions $i$ and $j$ in $T$. This classic problem has a
well-known solution that uses $O(n)$ space and $O(1)$ query time. In this paper
we show that for any trade-off parameter $1 \leq \tau \leq n$, the problem can
be solved in $O(\frac{n}{\tau})$ space and $O(\tau)$ query time. This
significantly improves the previously best known time-space trade-offs, and
almost matches the best known time-space product lower bound.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Mathias Bæk Tejs Knudsen</name>
    </author>
    <author>
      <name>Moshe Lewenstein</name>
    </author>
    <author>
      <name>Hjalte Wedel Vildhøj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extended abstract of this paper has been accepted to CPM 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.02671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1612.01748">
    <id>http://arxiv.org/abs/1612.01748v1</id>
    <updated>2016-12-06T10:55:34Z</updated>
    <published>2016-12-06T10:55:34Z</published>
    <title>Deterministic Indexing for Packed Strings</title>
    <summary>  Given a string $S$ of length $n$, the classic string indexing problem is to
preprocess $S$ into a compact data structure that supports efficient subsequent
pattern queries. In the \emph{deterministic} variant the goal is to solve the
string indexing problem without any randomization (at preprocessing time or
query time). In the \emph{packed} variant the strings are stored with several
character in a single word, giving us the opportunity to read multiple
characters simultaneously. Our main result is a new string index in the
deterministic \emph{and} packed setting. Given a packed string $S$ of length
$n$ over an alphabet $\sigma$, we show how to preprocess $S$ in $O(n)$
(deterministic) time and space $O(n)$ such that given a packed pattern string
of length $m$ we can support queries in (deterministic) time $O\left(m/\alpha +
\log m + \log \log \sigma\right), $ where $\alpha = w / \log \sigma$ is the
number of characters packed in a word of size $w = \Theta(\log n)$. Our query
time is always at least as good as the previous best known bounds and whenever
several characters are packed in a word, i.e., $\log \sigma \ll w$, the query
times are faster.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Frederik Rye Skjoldjensen</name>
    </author>
    <link href="http://arxiv.org/abs/1612.01748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2.2; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1711.00275">
    <id>http://arxiv.org/abs/1711.00275v1</id>
    <updated>2017-11-01T10:31:32Z</updated>
    <published>2017-11-01T10:31:32Z</published>
    <title>Fast Dynamic Arrays</title>
    <summary>  We present a highly optimized implementation of tiered vectors, a data
structure for maintaining a sequence of $n$ elements supporting access in time
$O(1)$ and insertion and deletion in time $O(n^\epsilon)$ for $\epsilon > 0$
while using $o(n)$ extra space. We consider several different implementation
optimizations in C++ and compare their performance to that of vector and
multiset from the standard library on sequences with up to $10^8$ elements. Our
fastest implementation uses much less space than multiset while providing
speedups of $40\times$ for access operations compared to multiset and speedups
of $10.000\times$ compared to vector for insertion and deletion operations
while being competitive with both data structures for all other operations.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Anders Roy Christiansen</name>
    </author>
    <author>
      <name>Mikko Berggren Ettienne</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.ESA.2017.16</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.ESA.2017.16" rel="related"/>
    <link href="http://arxiv.org/abs/1711.00275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1302.3347">
    <id>http://arxiv.org/abs/1302.3347v1</id>
    <updated>2013-02-14T09:29:09Z</updated>
    <published>2013-02-14T09:29:09Z</published>
    <title>Alphabet-Dependent String Searching with Wexponential Search Trees</title>
    <summary>  It is widely assumed that $O(m+\lg \sigma)$ is the best one can do for
finding a pattern of length $m$ in a compacted trie storing strings over an
alphabet of size $\sigma$, if one insists on linear-size data structures and
deterministic worst-case running times [Cole et al., ICALP'06]. In this
article, we first show that a rather straightforward combination of well-known
ideas yields $O(m+\lg\lg \sigma)$ deterministic worst-case searching time for
static tries.
  Then we move on to dynamic tries, where we achieve a worst-case bound of
$O(m+\frac{\lg^{2}\lg\sigma}{\lg\lg\lg\sigma})$ per query or update, which
should again be compared to the previously known $O(m+\lg\sigma)$ deterministic
worst-case bounds [Cole et al., ICALP'06], and to the alphabet
\emph{in}dependent $O(m+\sqrt{\lg n/\lg\lg n})$ deterministic worst-case bounds
[Andersson and Thorup, SODA'01], where $n$ is the number of nodes in the trie.
The basis of our update procedure is a weighted variant of exponential search
trees which, while simple, might be of independent interest.
  As one particular application, the above bounds (static and dynamic) apply to
suffix trees. There, an update corresponds to pre- or appending a letter to the
text, and an additional goal is to do the updates quicker than rematching
entire suffixes. We show how to do this in $O(\lg\lg n +
\frac{\lg^{2}\lg\sigma}{\lg\lg\lg\sigma})$ time, which improves the previously
known $O(\lg n)$ bound [Amir et al., SPIRE'05].
</summary>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <link href="http://arxiv.org/abs/1302.3347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.3347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1305.0674">
    <id>http://arxiv.org/abs/1305.0674v1</id>
    <updated>2013-05-03T11:29:39Z</updated>
    <published>2013-05-03T11:29:39Z</published>
    <title>LZ-Compressed String Dictionaries</title>
    <summary>  We show how to compress string dictionaries using the Lempel-Ziv (LZ78) data
compression algorithm. Our approach is validated experimentally on dictionaries
of up to 1.5 GB of uncompressed text. We achieve compression ratios often
outperforming the existing alternatives, especially on dictionaries containing
many repeated substrings. Our query times remain competitive.
</summary>
    <author>
      <name>Julian Arz</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <link href="http://arxiv.org/abs/1305.0674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.0674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1610.03332">
    <id>http://arxiv.org/abs/1610.03332v2</id>
    <updated>2016-12-05T21:39:22Z</updated>
    <published>2016-10-11T13:46:54Z</published>
    <title>Engineering a Distributed Full-Text Index</title>
    <summary>  We present a distributed full-text index for big data applications in a
distributed environment. Our index can answer different types of pattern
matching queries (existential, counting and enumeration). We perform
experiments on inputs up to 100 GiB using up to 512 processors, and compare our
index with the distributed suffix array by Arroyuelo et al. [Parall. Comput.
40(9): 471--495, 2014]. The result is that our index answers counting queries
up to 5.5 times faster than the distributed suffix array, while using about the
same space. We also provide a succinct variant of our index that uses only one
third of the memory compared with our non-succinct variant, at the expense of
only 20% slower query times.
</summary>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Florian Kurpicz</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <link href="http://arxiv.org/abs/1610.03332v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03332v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1710.01896">
    <id>http://arxiv.org/abs/1710.01896v1</id>
    <updated>2017-10-05T06:48:13Z</updated>
    <published>2017-10-05T06:48:13Z</published>
    <title>Dismantling DivSufSort</title>
    <summary>  We give the first concise description of the fastest known suffix sorting
algorithm in main memory, the DivSufSort by Yuta Mori. We then present an
extension that also computes the LCP-array, which is competitive with the
fastest known LCP-array construction algorithm.
</summary>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Florian Kurpicz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the Prague Stringology Conference 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.01896v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01896v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1702.07578">
    <id>http://arxiv.org/abs/1702.07578v3</id>
    <updated>2017-11-10T14:22:23Z</updated>
    <published>2017-02-24T13:43:20Z</published>
    <title>Simple, Fast and Lightweight Parallel Wavelet Tree Construction</title>
    <summary>  The wavelet tree (Grossi et al. [SODA, 2003]) and wavelet matrix (Claude et
al. [Inf. Syst., 47:15--32, 2015]) are compact indices for texts over an
alphabet $[0,\sigma)$ that support rank, select and access queries in $O(\lg
\sigma)$ time. We first present new practical sequential and parallel
algorithms for wavelet tree construction. Their unifying characteristics is
that they construct the wavelet tree bottomup}, i.e., they compute the last
level first. We also show that this bottom-up construction can easily be
adapted to wavelet matrices. In practice, our best sequential algorithm is up
to twice as fast as the currently fastest sequential wavelet tree construction
algorithm (Shun [DCC, 2015]), simultaneously saving a factor of 2 in space.
This scales up to 32 cores, where we are about equally fast as the currently
fastest parallel wavelet tree construction algorithm (Labeit et al. [DCC,
2016]), but still use only about 75 % of the space. An additional theoretical
result shows how to adapt any wavelet tree construction algorithm to the
wavelet matrix in the same (asymptotic) time, using only little extra space.
</summary>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Florian Kurpicz</name>
    </author>
    <author>
      <name>Marvin Löbel</name>
    </author>
    <link href="http://arxiv.org/abs/1702.07578v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07578v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1510.02882">
    <id>http://arxiv.org/abs/1510.02882v5</id>
    <updated>2016-05-28T11:54:34Z</updated>
    <published>2015-10-10T07:14:09Z</published>
    <title>Lempel-Ziv Computation In Compressed Space (LZ-CICS)</title>
    <summary>  We show that both the Lempel Ziv 77- and the 78-factorization of a text of
length $n$ on an integer alphabet of size $\sigma$ can be computed in $O(n \lg
\lg \sigma)$ time (linear time if we allow randomization) using $O(n \lg
\sigma)$ bits of working space. Given that a compressed representation of the
suffix tree is loaded into RAM, we can compute both factorizations in $O(n)$
time using $z \lg n + O(n)$ bits of space, where $z$ is the number of factors.
</summary>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <link href="http://arxiv.org/abs/1510.02882v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02882v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1509.07417">
    <id>http://arxiv.org/abs/1509.07417v2</id>
    <updated>2018-02-28T14:05:57Z</updated>
    <published>2015-09-24T16:05:25Z</published>
    <title>Deterministic Sparse Suffix Sorting in the Restore Model</title>
    <summary>  Given a text $T$ of length $n$, we propose a deterministic online algorithm
computing the sparse suffix array and the sparse longest common prefix array of
$T$ in $O(c \sqrt{\lg n} + m \lg m \lg n \lg^* n)$ time with $O(m)$ words of
space under the premise that the space of $T$ is rewritable, where $m \le n$ is
the number of suffixes to be sorted (provided online and arbitrarily), and $c$
is the number of characters with $m \le c \le n$ that must be compared for
distinguishing the designated suffixes.
</summary>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <link href="http://arxiv.org/abs/1509.07417v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07417v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1108.3683">
    <id>http://arxiv.org/abs/1108.3683v1</id>
    <updated>2011-08-18T08:31:11Z</updated>
    <published>2011-08-18T08:31:11Z</published>
    <title>Substring Range Reporting</title>
    <summary>  We revisit various string indexing problems with range reporting features,
namely, position-restricted substring searching, indexing substrings with gaps,
and indexing substrings with intervals. We obtain the following main results.
{itemize} We give efficient reductions for each of the above problems to a new
problem, which we call \emph{substring range reporting}. Hence, we unify the
previous work by showing that we may restrict our attention to a single problem
rather than studying each of the above problems individually. We show how to
solve substring range reporting with optimal query time and little space.
Combined with our reductions this leads to significantly improved time-space
trade-offs for the above problems. In particular, for each problem we obtain
the first solutions with optimal time query and $O(n\log^{O(1)} n)$ space,
where $n$ is the length of the indexed string. We show that our techniques for
substring range reporting generalize to \emph{substring range counting} and
\emph{substring range emptiness} variants. We also obtain non-trivial
time-space trade-offs for these problems. {itemize} Our bounds for substring
range reporting are based on a novel combination of suffix trees and range
reporting data structures. The reductions are simple and general and may apply
to other combinations of string indexing with range reporting.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Goertz</name>
    </author>
    <link href="http://arxiv.org/abs/1108.3683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.3683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1211.0270">
    <id>http://arxiv.org/abs/1211.0270v2</id>
    <updated>2013-04-22T07:33:24Z</updated>
    <published>2012-11-01T19:55:11Z</published>
    <title>Time-Space Trade-Offs for Longest Common Extensions</title>
    <summary>  We revisit the longest common extension (LCE) problem, that is, preprocess a
string $T$ into a compact data structure that supports fast LCE queries. An LCE
query takes a pair $(i,j)$ of indices in $T$ and returns the length of the
longest common prefix of the suffixes of $T$ starting at positions $i$ and $j$.
We study the time-space trade-offs for the problem, that is, the space used for
the data structure vs. the worst-case time for answering an LCE query. Let $n$
be the length of $T$. Given a parameter $\tau$, $1 \leq \tau \leq n$, we show
how to achieve either $O(\infrac{n}{\sqrt{\tau}})$ space and $O(\tau)$ query
time, or $O(\infrac{n}{\tau})$ space and $O(\tau \log({|\LCE(i,j)|}/{\tau}))$
query time, where $|\LCE(i,j)|$ denotes the length of the LCE returned by the
query. These bounds provide the first smooth trade-offs for the LCE problem and
almost match the previously known bounds at the extremes when $\tau=1$ or
$\tau=n$. We apply the result to obtain improved bounds for several
applications where the LCE problem is the computational bottleneck, including
approximate string matching and computing palindromes. We also present an
efficient technique to reduce LCE queries on two strings to one string.
Finally, we give a lower bound on the time-space product for LCE data
structures in the non-uniform cell probe model showing that our second
trade-off is nearly optimal.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Goertz</name>
    </author>
    <author>
      <name>Benjamin Sach</name>
    </author>
    <author>
      <name>Hjalte Wedel Vildhøj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary version of this paper appeared in the proceedings of
  the 23rd Annual Symposium on Combinatorial Pattern Matching (CPM 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.0270v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0270v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1207.1135">
    <id>http://arxiv.org/abs/1207.1135v1</id>
    <updated>2012-07-04T22:41:51Z</updated>
    <published>2012-07-04T22:41:51Z</published>
    <title>Sparse Suffix Tree Construction with Small Space</title>
    <summary>  We consider the problem of constructing a sparse suffix tree (or suffix
array) for $b$ suffixes of a given text $T$ of size $n$, using only $O(b)$
words of space during construction time. Breaking the naive bound of
$\Omega(nb)$ time for this problem has occupied many algorithmic researchers
since a different structure, the (evenly spaced) sparse suffix tree, was
introduced by K{\"a}rkk{\"a}inen and Ukkonen in 1996. While in the evenly
spaced sparse suffix tree the suffixes considered must be evenly spaced in $T$,
here there is no constraint on the locations of the suffixes.
  We show that the sparse suffix tree can be constructed in $O(n\log^2b)$ time.
To achieve this we develop a technique, which may be of independent interest,
that allows to efficiently answer $b$ longest common prefix queries on suffixes
of $T$, using only $O(b)$ space. We expect that this technique will prove
useful in many other applications in which space usage is a concern.
Furthermore, additional tradeoffs between the space usage and the construction
time are given.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Benjamin Sach</name>
    </author>
    <author>
      <name>Hjalte Wedel Vildhøj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.1135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.1135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1305.2777">
    <id>http://arxiv.org/abs/1305.2777v2</id>
    <updated>2013-05-16T12:32:19Z</updated>
    <published>2013-05-13T13:50:11Z</published>
    <title>Fingerprints in Compressed Strings</title>
    <summary>  The Karp-Rabin fingerprint of a string is a type of hash value that due to
its strong properties has been used in many string algorithms. In this paper we
show how to construct a data structure for a string $S$ of size $N$ compressed
by a context-free grammar of size $n$ that answers fingerprint queries. That
is, given indices $i$ and $j$, the answer to a query is the fingerprint of the
substring $S[i,j]$. We present the first O(n) space data structures that answer
fingerprint queries without decompressing any characters. For Straight Line
Programs (SLP) we get $O(\log N)$ query time, and for Linear SLPs (an SLP
derivative that captures LZ78 compression and its variations) we get $O(\log
\log N)$ query time. Hence, our data structures has the same time and space
complexity as for random access in SLPs. We utilize the fingerprint data
structures to solve the longest common extension problem in query time $O(\log
N \log \lce)$ and $O(\log \lce \log\log \lce + \log\log N)$ for SLPs and Linear
SLPs, respectively. Here, $\lce$ denotes the length of the LCE.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Patrick Hagge Cording</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Benjamin Sach</name>
    </author>
    <author>
      <name>Hjalte Wedel Vildhøj</name>
    </author>
    <author>
      <name>Søren Vind</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extended abstract of this paper will appear at WADS 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.2777v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.2777v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1304.5702">
    <id>http://arxiv.org/abs/1304.5702v2</id>
    <updated>2014-05-11T08:19:41Z</updated>
    <published>2013-04-21T07:38:50Z</published>
    <title>Tree Compression with Top Trees</title>
    <summary>  We introduce a new compression scheme for labeled trees based on top trees.
Our compression scheme is the first to simultaneously take advantage of
internal repeats in the tree (as opposed to the classical DAG compression that
only exploits rooted subtree repeats) while also supporting fast navigational
queries directly on the compressed representation. We show that the new
compression scheme achieves close to optimal worst-case compression, can
compress exponentially better than DAG compression, is never much worse than
DAG compression, and supports navigational queries in logarithmic time.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Goertz</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extended abstract of this paper appeared at the 40th International
  Colloquium on Automata, Languages and Programming</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.5702v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.5702v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1412.1254">
    <id>http://arxiv.org/abs/1412.1254v3</id>
    <updated>2015-07-09T14:55:22Z</updated>
    <published>2014-12-03T10:02:47Z</published>
    <title>Longest Common Extensions in Trees</title>
    <summary>  The longest common extension (LCE) of two indices in a string is the length
of the longest identical substrings starting at these two indices. The LCE
problem asks to preprocess a string into a compact data structure that supports
fast LCE queries. In this paper we generalize the LCE problem to trees and
suggest a few applications of LCE in trees to tries and XML databases. Given a
labeled and rooted tree $T$ of size $n$, the goal is to preprocess $T$ into a
compact data structure that support the following LCE queries between subpaths
and subtrees in $T$. Let $v_1$, $v_2$, $w_1$, and $w_2$ be nodes of $T$ such
that $w_1$ and $w_2$ are descendants of $v_1$ and $v_2$ respectively.
\begin{itemize} \item $\LCEPP(v_1, w_1, v_2, w_2)$: (path-path $\LCE$) return
the longest common prefix of the paths $v_1 \leadsto w_1$ and $v_2 \leadsto
w_2$. \item $\LCEPT(v_1, w_1, v_2)$: (path-tree $\LCE$) return maximal
path-path LCE of the path $v_1 \leadsto w_1$ and any path from $v_2$ to a
descendant leaf. \item $\LCETT(v_1, v_2)$: (tree-tree $\LCE$) return a maximal
path-path LCE of any pair of paths from $v_1$ and $v_2$ to descendant leaves.
\end{itemize} We present the first non-trivial bounds for supporting these
queries. For $\LCEPP$ queries, we present a linear-space solution with
$O(\log^{*} n)$ query time. For $\LCEPT$ queries, we present a linear-space
solution with $O((\log\log n)^{2})$ query time, and complement this with a
lower bound showing that any path-tree LCE structure of size $O(n \polylog(n))$
must necessarily use $\Omega(\log\log n)$ time to answer queries. For $\LCETT$
queries, we present a time-space trade-off, that given any parameter $\tau$, $1
\leq \tau \leq n$, leads to an $O(n\tau)$ space and $O(n/\tau)$ query-time
solution. This is complemented with a reduction to the the set intersection
problem implying that a fast linear space solution is not likely to exist.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <author>
      <name>Inge Li Goertz</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <link href="http://arxiv.org/abs/1412.1254v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.1254v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1403.1065">
    <id>http://arxiv.org/abs/1403.1065v2</id>
    <updated>2014-06-05T09:37:56Z</updated>
    <published>2014-03-05T10:16:27Z</published>
    <title>Compressed Subsequence Matching and Packed Tree Coloring</title>
    <summary>  We present a new algorithm for subsequence matching in grammar compressed
strings. Given a grammar of size $n$ compressing a string of size $N$ and a
pattern string of size $m$ over an alphabet of size $\sigma$, our algorithm
uses $O(n+\frac{n\sigma}{w})$ space and $O(n+\frac{n\sigma}{w}+m\log N\log
w\cdot occ)$ or $O(n+\frac{n\sigma}{w}\log w+m\log N\cdot occ)$ time. Here $w$
is the word size and $occ$ is the number of occurrences of the pattern. Our
algorithm uses less space than previous algorithms and is also faster for
$occ=o(\frac{n}{\log N})$ occurrences. The algorithm uses a new data structure
that allows us to efficiently find the next occurrence of a given character
after a given position in a compressed string. This data structure in turn is
based on a new data structure for the tree color problem, where the node colors
are packed in bit strings.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Patrick Hagge Cording</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at CPM '14</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.1065v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1065v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1510.08748">
    <id>http://arxiv.org/abs/1510.08748v2</id>
    <updated>2016-01-19T09:42:20Z</updated>
    <published>2015-10-29T15:47:24Z</published>
    <title>Subsequence Automata with Default Transitions</title>
    <summary>  Let $S$ be a string of length $n$ with characters from an alphabet of size
$\sigma$. The \emph{subsequence automaton} of $S$ (often called the
\emph{directed acyclic subsequence graph}) is the minimal deterministic finite
automaton accepting all subsequences of $S$. A straightforward construction
shows that the size (number of states and transitions) of the subsequence
automaton is $O(n\sigma)$ and that this bound is asymptotically optimal.
  In this paper, we consider subsequence automata with \emph{default
transitions}, that is, special transitions to be taken only if none of the
regular transitions match the current character, and which do not consume the
current character. We show that with default transitions, much smaller
subsequence automata are possible, and provide a full trade-off between the
size of the automaton and the \emph{delay}, i.e., the maximum number of
consecutive default transitions followed before consuming a character.
  Specifically, given any integer parameter $k$, $1 &lt; k \leq \sigma$, we
present a subsequence automaton with default transitions of size
$O(nk\log_{k}\sigma)$ and delay $O(\log_k \sigma)$. Hence, with $k = 2$ we
obtain an automaton of size $O(n \log \sigma)$ and delay $O(\log \sigma)$. On
the other extreme, with $k = \sigma$, we obtain an automaton of size $O(n
\sigma)$ and delay $O(1)$, thus matching the bound for the standard subsequence
automaton construction. Finally, we generalize the result to multiple strings.
The key component of our result is a novel hierarchical automata construction
of independent interest.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Frederik Rye Skjoldjensen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected typos</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.08748v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.08748v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1507.02853">
    <id>http://arxiv.org/abs/1507.02853v4</id>
    <updated>2016-11-16T12:29:44Z</updated>
    <published>2015-07-10T11:17:32Z</published>
    <title>Finger Search in Grammar-Compressed Strings</title>
    <summary>  Grammar-based compression, where one replaces a long string by a small
context-free grammar that generates the string, is a simple and powerful
paradigm that captures many popular compression schemes. Given a grammar, the
random access problem is to compactly represent the grammar while supporting
random access, that is, given a position in the original uncompressed string
report the character at that position. In this paper we study the random access
problem with the finger search property, that is, the time for a random access
query should depend on the distance between a specified index $f$, called the
\emph{finger}, and the query index $i$. We consider both a static variant,
where we first place a finger and subsequently access indices near the finger
efficiently, and a dynamic variant where also moving the finger such that the
time depends on the distance moved is supported.
  Let $n$ be the size the grammar, and let $N$ be the size of the string. For
the static variant we give a linear space representation that supports placing
the finger in $O(\log N)$ time and subsequently accessing in $O(\log D)$ time,
where $D$ is the distance between the finger and the accessed index. For the
dynamic variant we give a linear space representation that supports placing the
finger in $O(\log N)$ time and accessing and moving the finger in $O(\log D +
\log \log N)$ time. Compared to the best linear space solution to random
access, we improve a $O(\log N)$ query bound to $O(\log D)$ for the static
variant and to $O(\log D + \log \log N)$ for the dynamic variant, while
maintaining linear space. As an application of our results we obtain an
improved solution to the longest common extension problem in grammar compressed
strings. To obtain our results, we introduce several new techniques of
independent interest, including a novel van Emde Boas style decomposition of
grammars.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Anders Roy Christiansen</name>
    </author>
    <author>
      <name>Patrick Hagge Cording</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <link href="http://arxiv.org/abs/1507.02853v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02853v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1504.07851">
    <id>http://arxiv.org/abs/1504.07851v5</id>
    <updated>2016-09-16T12:53:27Z</updated>
    <published>2015-04-29T13:31:48Z</published>
    <title>Dynamic Relative Compression, Dynamic Partial Sums, and Substring
  Concatenation</title>
    <summary>  Given a static reference string $R$ and a source string $S$, a relative
compression of $S$ with respect to $R$ is an encoding of $S$ as a sequence of
references to substrings of $R$. Relative compression schemes are a classic
model of compression and have recently proved very successful for compressing
highly-repetitive massive data sets such as genomes and web-data. We initiate
the study of relative compression in a dynamic setting where the compressed
source string $S$ is subject to edit operations. The goal is to maintain the
compressed representation compactly, while supporting edits and allowing
efficient random access to the (uncompressed) source string. We present new
data structures that achieve optimal time for updates and queries while using
space linear in the size of the optimal relative compression, for nearly all
combinations of parameters. We also present solutions for restricted and
extended sets of updates. To achieve these results, we revisit the dynamic
partial sums problem and the substring concatenation problem. We present new
optimal or near optimal bounds for these problems. Plugging in our new results
we also immediately obtain new bounds for the string indexing for patterns with
wildcards problem and the dynamic text and static pattern matching problem.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Patrick Hagge Cording</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Frederik Rye Skjoldjensen</name>
    </author>
    <author>
      <name>Hjalte Wedel Vildhøj</name>
    </author>
    <author>
      <name>Søren Vind</name>
    </author>
    <link href="http://arxiv.org/abs/1504.07851v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07851v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1511.07180">
    <id>http://arxiv.org/abs/1511.07180v4</id>
    <updated>2017-10-11T09:40:01Z</updated>
    <published>2015-11-23T11:19:00Z</published>
    <title>Longest Gapped Repeats and Palindromes</title>
    <summary>  A gapped repeat (respectively, palindrome) occurring in a word $w$ is a
factor $uvu$ (respectively, $u^Rvu$) of $w$. In such a repeat (palindrome) $u$
is called the arm of the repeat (respectively, palindrome), while $v$ is called
the gap. We show how to compute efficiently, for every position $i$ of the word
$w$, the longest gapped repeat and palindrome occurring at that position,
provided that the length of the gap is subject to various types of
restrictions. That is, that for each position $i$ we compute the longest prefix
$u$ of $w[i..n]$ such that $uv$ (respectively, $u^Rv$) is a suffix of
$w[1..i-1]$ (defining thus a gapped repeat $uvu$ -- respectively, palindrome
$u^Rvu$), and the length of $v$ is subject to the aforementioned restrictions.
</summary>
    <author>
      <name>Marius Dumitran</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23638/DMTCS-19-4-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23638/DMTCS-19-4-4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extension of the conference papers "Longest
  $\alpha$-Gapped Repeat and Palindrome", presented by the second and third
  authors at FCT 2015, and "Longest Gapped Repeats and Palindromes", presented
  by the first and third authors at MFCS 2015</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Discrete Mathematics &amp; Theoretical Computer Science, Vol. 19 no.
  4, FCT '15, special issue FCT'15 (October 13, 2017) dmtcs:3988</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.07180v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07180v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1511.02612">
    <id>http://arxiv.org/abs/1511.02612v2</id>
    <updated>2016-04-08T07:57:20Z</updated>
    <published>2015-11-09T09:38:47Z</published>
    <title>Optimal Dynamic Strings</title>
    <summary>  In this paper we study the fundamental problem of maintaining a dynamic
collection of strings under the following operations: concat - concatenates two
strings, split - splits a string into two at a given position, compare - finds
the lexicographical order (less, equal, greater) between two strings, LCP -
calculates the longest common prefix of two strings. We present an efficient
data structure for this problem, where an update requires only $O(\log n)$
worst-case time with high probability, with $n$ being the total length of all
strings in the collection, and a query takes constant worst-case time. On the
lower bound side, we prove that even if the only possible query is checking
equality of two strings, either updates or queries take amortized $\Omega(\log
n)$ time; hence our implementation is optimal.
  Such operations can be used as a basic building block to solve other string
problems. We provide two examples. First, we can augment our data structure to
provide pattern matching queries that may locate occurrences of a specified
pattern $p$ in the strings in our collection in optimal $O(|p|)$ time, at the
expense of increasing update time to $O(\log^2 n)$. Second, we show how to
maintain a history of an edited text, processing updates in $O(\log t \log \log
t)$ time, where $t$ is the number of edits, and how to support pattern matching
queries against the whole history in $O(|p| \log t \log \log t)$ time.
  Finally, we note that our data structure can be applied to test dynamic tree
isomorphism and to compare strings generated by dynamic straight-line grammars.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Adam Karczmarz</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Łącki</name>
    </author>
    <author>
      <name>Piotr Sankowski</name>
    </author>
    <link href="http://arxiv.org/abs/1511.02612v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.02612v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1610.03125">
    <id>http://arxiv.org/abs/1610.03125v1</id>
    <updated>2016-10-10T23:19:10Z</updated>
    <published>2016-10-10T23:19:10Z</published>
    <title>Tight Tradeoffs for Real-Time Approximation of Longest Palindromes in
  Streams</title>
    <summary>  We consider computing a longest palindrome in the streaming model, where the
symbols arrive one-by-one and we do not have random access to the input. While
computing the answer exactly using sublinear space is not possible in such a
setting, one can still hope for a good approximation guarantee. Our
contribution is twofold. First, we provide lower bounds on the space
requirements for randomized approximation algorithms processing inputs of
length $n$. We rule out Las Vegas algorithms, as they cannot achieve sublinear
space complexity. For Monte Carlo algorithms, we prove a lower bounds of
$\Omega( M \log\min\{|\Sigma|,M\})$ bits of memory; here $M=n/E$ for
approximating the answer with additive error $E$, and $M= \frac{\log n}{\log
(1+\varepsilon)}$ for approximating the answer with multiplicative error $(1 +
\varepsilon)$. Second, we design three real-time algorithms for this problem.
Our Monte Carlo approximation algorithms for both additive and multiplicative
versions of the problem use $O(M)$ words of memory. Thus the obtained lower
bounds are asymptotically tight up to a logarithmic factor. The third algorithm
is deterministic and finds a longest palindrome exactly if it is short. This
algorithm can be run in parallel with a Monte Carlo algorithm to obtain better
results in practice. Overall, both the time and space complexity of finding a
longest palindrome in a stream are essentially settled.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Oleg Merkurev</name>
    </author>
    <author>
      <name>Arseny M. Shur</name>
    </author>
    <author>
      <name>Przemysław Uznański</name>
    </author>
    <link href="http://arxiv.org/abs/1610.03125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1608.00865">
    <id>http://arxiv.org/abs/1608.00865v1</id>
    <updated>2016-08-02T15:25:52Z</updated>
    <published>2016-08-02T15:25:52Z</published>
    <title>Sparse Suffix Tree Construction in Optimal Time and Space</title>
    <summary>  Suffix tree (and the closely related suffix array) are fundamental structures
capturing all substrings of a given text essentially by storing all its
suffixes in the lexicographical order. In some applications, we work with a
subset of $b$ interesting suffixes, which are stored in the so-called sparse
suffix tree. Because the size of this structure is $\Theta(b)$, it is natural
to seek a construction algorithm using only $O(b)$ words of space assuming
read-only random access to the text. We design a linear-time Monte Carlo
algorithm for this problem, hence resolving an open question explicitly stated
by Bille et al. [TALG 2016]. The best previously known algorithm by I et al.
[STACS 2014] works in $O(n\log b)$ time. Our solution proceeds in $n/b$ rounds;
in the $r$-th round, we consider all suffixes starting at positions congruent
to $r$ modulo $n/b$. By maintaining rolling hashes, we lexicographically sort
all interesting suffixes starting at such positions, and then we merge them
with the already considered suffixes. For efficient merging, we also need to
answer LCE queries in small space. By plugging in the structure of Bille et al.
[CPM 2015] we obtain $O(n+b\log b)$ time complexity. We improve this structure,
which implies a linear-time sparse suffix tree construction algorithm. We
complement our Monte Carlo algorithm with a deterministic verification
procedure. The verification takes $O(n\sqrt{\log b})$ time, which improves upon
the bound of $O(n\log b)$ obtained by I et al. [STACS 2014]. This is obtained
by first observing that the pruning done inside the previous solution has a
rather clean description using the notion of graph spanners with small
multiplicative stretch. Then, we are able to decrease the verification time by
applying difference covers twice. Combined with the Monte Carlo algorithm, this
gives us an $O(n\sqrt{\log b})$-time and $O(b)$-space Las Vegas algorithm.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <link href="http://arxiv.org/abs/1608.00865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1704.01311">
    <id>http://arxiv.org/abs/1704.01311v1</id>
    <updated>2017-04-05T08:45:22Z</updated>
    <published>2017-04-05T08:45:22Z</published>
    <title>Optimal trade-offs for pattern matching with $k$ mismatches</title>
    <summary>  Given a pattern of length $m$ and a text of length $n$, the goal in
$k$-mismatch pattern matching is to compute, for every $m$-substring of the
text, the exact Hamming distance to the pattern or report that it exceeds $k$.
This can be solved in either $\widetilde{O}(n \sqrt{k})$ time as shown by Amir
et al. [J. Algorithms 2004] or $\widetilde{O}((m + k^2) \cdot n/m)$ time due to
a result of Clifford et al. [SODA 2016]. We provide a smooth time trade-off
between these two bounds by designing an algorithm working in time
$\widetilde{O}( (m + k \sqrt{m}) \cdot n/m)$. We complement this with a
matching conditional lower bound, showing that a significantly faster
combinatorial algorithm is not possible, unless the combinatorial matrix
multiplication conjecture fails.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Przemysław Uznański</name>
    </author>
    <link href="http://arxiv.org/abs/1704.01311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1708.00639">
    <id>http://arxiv.org/abs/1708.00639v1</id>
    <updated>2017-08-02T08:22:34Z</updated>
    <published>2017-08-02T08:22:34Z</published>
    <title>Distinct Squares in Circular Words</title>
    <summary>  A circular word, or a necklace, is an equivalence class under conjugation of
a word. A fundamental question concerning regularities in standard words is
bounding the number of distinct squares in a word of length $n$. The famous
conjecture attributed to Fraenkel and Simpson is that there are at most $n$
such distinct squares, yet the best known upper bound is $1.84n$ by Deza et al.
[Discr. Appl. Math. 180, 52-69 (2015)]. We consider a natural generalization of
this question to circular words: how many distinct squares can there be in all
cyclic rotations of a word of length $n$? We prove an upper bound of $3.14n$.
This is complemented with an infinite family of words implying a lower bound of
$1.25n$.
</summary>
    <author>
      <name>Mika Amit</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in SPIRE 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1508.02550">
    <id>http://arxiv.org/abs/1508.02550v3</id>
    <updated>2017-12-15T14:14:30Z</updated>
    <published>2015-08-11T10:34:50Z</published>
    <title>Relative Suffix Trees</title>
    <summary>  Suffix trees are one of the most versatile data structures in stringology,
with many applications in bioinformatics. Their main drawback is their size,
which can be tens of times larger than the input sequence. Much effort has been
put into reducing the space usage, leading ultimately to compressed suffix
trees. These compressed data structures can efficiently simulate the suffix
tree, while using space proportional to a compressed representation of the
sequence. In this work, we take a new approach to compressed suffix trees for
repetitive sequence collections, such as collections of individual genomes. We
compress the suffix trees of individual sequences relative to the suffix tree
of a reference sequence. These relative data structures provide competitive
time/space trade-offs, being almost as small as the smallest compressed suffix
trees for repetitive collections, and competitive in time with the largest and
fastest compressed suffix trees.
</summary>
    <author>
      <name>Andrea Farruggia</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Jouni Sirén</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/comjnl/bxx108</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/comjnl/bxx108" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to The Computer Journal. The implementation is available at
  https://github.com/jltsiren/relative-fm</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.02550v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02550v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1506.03262">
    <id>http://arxiv.org/abs/1506.03262v1</id>
    <updated>2015-06-10T11:38:11Z</updated>
    <published>2015-06-10T11:38:11Z</published>
    <title>Relative Select</title>
    <summary>  Motivated by the problem of storing coloured de Bruijn graphs, we show how,
if we can already support fast select queries on one string, then we can store
a little extra information and support fairly fast select queries on a similar
string.
</summary>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Alexander Bowe</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Jouni Sirén</name>
    </author>
    <link href="http://arxiv.org/abs/1506.03262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1611.08198">
    <id>http://arxiv.org/abs/1611.08198v1</id>
    <updated>2016-11-24T14:43:57Z</updated>
    <published>2016-11-24T14:43:57Z</published>
    <title>Burrows-Wheeler transform and LCP array construction in constant space</title>
    <summary>  In this article we extend the elegant in-place Burrows-Wheeler transform
(BWT) algorithm proposed by Crochemore et al. (Crochemore et al., 2015). Our
extension is twofold: we first show how to compute simultaneously the longest
common prefix (LCP) array as well as the BWT, using constant additional space;
we then show how to build the LCP array directly in compressed representation
using Elias coding, still using constant additional space and with no
asymptotic slowdown. Furthermore, we provide a time/space tradeoff for our
algorithm when additional memory is allowed. Our algorithm runs in quadratic
time, as does Crochemore et al.'s, and is supported by interesting properties
of the BWT and of the LCP array, contributing to our understanding of the
time/space tradeoff curve for building indexing structures.
</summary>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Guilherme P. Telles</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jda.2016.11.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jda.2016.11.003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to JDA</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Discrete Algorithms, 42 (2017) 14-22</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.08198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1611.01835">
    <id>http://arxiv.org/abs/1611.01835v2</id>
    <updated>2018-05-23T16:39:11Z</updated>
    <published>2016-11-06T20:07:59Z</published>
    <title>Compressed Dynamic Range Majority and Minority Data Structures</title>
    <summary>  In the range $\alpha$-majority query problem, we are given a sequence
$S[1..n]$ and a fixed threshold $\alpha \in (0, 1)$, and are asked to
preprocess $S$ such that, given a query range $[i..j]$, we can efficiently
report the symbols that occur more than $\alpha (j-i+1)$ times in $S[i..j]$,
which are called the range $\alpha$-majorities. In this article we first
describe a dynamic data structure that represents $S$ in compressed space ---
$nH_k+ o(n\lg \sigma)$ bits for any $k = o(\log_{\sigma} n)$, where $\sigma$ is
the alphabet size and $H_k \le H_0 \le \lg\sigma $ is the $k$-th order
empirical entropy of $S$ --- and answers queries in $O \left(\frac{\log
n}{\alpha \log \log n} \right)$ time while supporting insertions and deletions
in $S$ in $O \left( \frac{\lg n}{\alpha} \right)$ amortized time. We then show
how to modify our data structure to receive some $\beta \ge \alpha$ at query
time and report the range $\beta$-majorities in $O \left( \frac{\log n}{\beta
\log \log n} \right)$ time, without increasing the asymptotic space or
update-time bounds. The best previous dynamic solution has the same query and
update times as ours, but it occupies $O(n)$ words and cannot take advantage of
being given a larger threshold $\beta$ at query time.
  [ABSTRACT CLIPPED DUE TO LENGTH.]
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Partially supported by Fondecyt grant 1-171058, Chile; NSERC, Canada;
  basal funds FB0001, Conicyt, Chile; and the Millenium Institute for
  Foundational Research on Data, Chile. A preliminary partial version of this
  article appeared in Proc. DCC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.01835v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01835v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1412.1591">
    <id>http://arxiv.org/abs/1412.1591v1</id>
    <updated>2014-12-04T09:11:46Z</updated>
    <published>2014-12-04T09:11:46Z</published>
    <title>Searching and Indexing Genomic Databases via Kernelization</title>
    <summary>  The rapid advance of DNA sequencing technologies has yielded databases of
thousands of genomes. To search and index these databases effectively, it is
important that we take advantage of the similarity between those genomes.
Several authors have recently suggested searching or indexing only one
reference genome and the parts of the other genomes where they differ. In this
paper we survey the twenty-year history of this idea and discuss its relation
to kernelization in parameterized complexity.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <link href="http://arxiv.org/abs/1412.1591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.1591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1411.2718">
    <id>http://arxiv.org/abs/1411.2718v2</id>
    <updated>2014-11-17T12:36:08Z</updated>
    <published>2014-11-11T07:52:23Z</published>
    <title>Variable-Order de Bruijn Graphs</title>
    <summary>  The de Bruijn graph $G_K$ of a set of strings $S$ is a key data structure in
genome assembly that represents overlaps between all the $K$-length substrings
of $S$. Construction and navigation of the graph is a space and time bottleneck
in practice and the main hurdle for assembling large, eukaryote genomes. This
problem is compounded by the fact that state-of-the-art assemblers do not build
the de Bruijn graph for a single order (value of $K$) but for multiple values
of $K$. More precisely, they build $d$ de Bruijn graphs, each with a specific
order, i.e., $G_{K_1}, G_{K_2}, ..., G_{K_d}$. Although, this paradigm
increases the quality of the assembly produced, it increases the memory by a
factor of $d$ in most cases. In this paper, we show how to augment a succinct
de Bruijn graph representation by Bowe et al. (Proc. WABI, 2012) to support new
operations that let us change order on the fly, effectively representing all de
Bruijn graphs of order up to some maximum $K$ in a single data structure. Our
experiments show our variable-order de Bruijn graph only modestly increases
space usage, construction time, and navigation time compared to a single order
graph.
</summary>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Alex Bowe</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference submission, 10 pages, +minor corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.2718v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2718v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1411.2785">
    <id>http://arxiv.org/abs/1411.2785v3</id>
    <updated>2021-12-08T21:12:32Z</updated>
    <published>2014-11-11T12:53:54Z</published>
    <title>Faster Compressed Quadtrees</title>
    <summary>  Real-world point sets tend to be clustered, so using a machine word for each
point is wasteful. In this paper we first show how a compact representation of
quadtrees using $\Oh{1}$ bits per node can break this bound on clustered point
sets, while offering efficient range searches. We then describe a new compact
quadtree representation based on heavy path decompositions, which supports
queries faster than previous compact structures. We present experimental
evidence showing that our structure is competitive in practice.
</summary>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Susana Ladra</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Diego Seco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal version of DCC '15 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.2785v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2785v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1409.6780">
    <id>http://arxiv.org/abs/1409.6780v2</id>
    <updated>2015-10-01T10:57:53Z</updated>
    <published>2014-09-24T00:27:17Z</published>
    <title>Document Counting in Practice</title>
    <summary>  We address the problem of counting the number of strings in a collection
where a given pattern appears, which has applications in information retrieval
and data mining. Existing solutions are in a theoretical stage. We implement
these solutions and develop some new variants, comparing them experimentally on
various datasets. Our results not only show which are the best options for each
situation and help discard practically unappealing solutions, but also uncover
some unexpected compressibility properties of the best data structures. By
taking advantage of these properties, we can reduce the size of the structures
by a factor of 5--400, depending on the dataset.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Aleksi Hartikainen</name>
    </author>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Jouni Sirén</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a slightly extended version of the paper that was presented
  at DCC 2015. The implementations are available at
  http://jltsiren.kapsi.fi/rlcsa and https://github.com/ahartik/succinct</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.6780v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.6780v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1407.0114">
    <id>http://arxiv.org/abs/1407.0114v1</id>
    <updated>2014-07-01T06:38:45Z</updated>
    <published>2014-07-01T06:38:45Z</published>
    <title>Suffix Arrays for Spaced-SNP Databases</title>
    <summary>  Single-nucleotide polymorphisms (SNPs) account for most variations between
human genomes. We show how, if the genomes in a database differ only by a
reasonable number of SNPs and the substrings between those SNPs are unique,
then we can store a fast compressed suffix array for that database.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/1407.0114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1404.4814">
    <id>http://arxiv.org/abs/1404.4814v2</id>
    <updated>2014-05-09T10:49:58Z</updated>
    <published>2014-04-18T15:13:01Z</published>
    <title>Reusing an FM-index</title>
    <summary>  Intuitively, if two strings $S_1$ and $S_2$ are sufficiently similar and we
already have an FM-index for $S_1$ then, by storing a little extra information,
we should be able to reuse parts of that index in an FM-index for $S_2$. We
formalize this intuition and show that it can lead to significant space savings
in practice, as well as to some interesting theoretical problems.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Jouni Sirén</name>
    </author>
    <link href="http://arxiv.org/abs/1404.4814v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4814v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1403.2431">
    <id>http://arxiv.org/abs/1403.2431v2</id>
    <updated>2014-08-07T09:52:23Z</updated>
    <published>2014-03-10T22:18:40Z</published>
    <title>A Subquadratic Algorithm for Minimum Palindromic Factorization</title>
    <summary>  We give an $\mathcal{O}(n \log n)$-time, $\mathcal{O}(n)$-space algorithm for
factoring a string into the minimum number of palindromic substrings. That is,
given a string $S [1..n]$, in $\mathcal{O}(n \log n)$ time our algorithm
returns the minimum number of palindromes $S_1,\ldots, S_\ell$ such that $S =
S_1 \cdots S_\ell$. We also show that the time complexity is $\mathcal{O}(n)$
on average and $\Omega(n\log n)$ in the worst case. The last result is based on
a characterization of the palindromic structure of Zimin words.
</summary>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jda.2014.08.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jda.2014.08.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Journal of Discrete Algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.2431v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.2431v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1408.6182">
    <id>http://arxiv.org/abs/1408.6182v4</id>
    <updated>2015-05-15T17:17:18Z</updated>
    <published>2014-08-26T16:44:53Z</published>
    <title>Wavelet Trees Meet Suffix Trees</title>
    <summary>  We present an improved wavelet tree construction algorithm and discuss its
applications to a number of rank/select problems for integer keys and strings.
  Given a string of length n over an alphabet of size $\sigma\leq n$, our
method builds the wavelet tree in $O(n \log \sigma/ \sqrt{\log{n}})$ time,
improving upon the state-of-the-art algorithm by a factor of $\sqrt{\log n}$.
As a consequence, given an array of n integers we can construct in $O(n
\sqrt{\log n})$ time a data structure consisting of $O(n)$ machine words and
capable of answering rank/select queries for the subranges of the array in
$O(\log n / \log \log n)$ time. This is a $\log \log n$-factor improvement in
query time compared to Chan and P\u{a}tra\c{s}cu and a $\sqrt{\log n}$-factor
improvement in construction time compared to Brodal et al.
  Next, we switch to stringological context and propose a novel notion of
wavelet suffix trees. For a string w of length n, this data structure occupies
$O(n)$ words, takes $O(n \sqrt{\log n})$ time to construct, and simultaneously
captures the combinatorial structure of substrings of w while enabling
efficient top-down traversal and binary search. In particular, with a wavelet
suffix tree we are able to answer in $O(\log |x|)$ time the following two
natural analogues of rank/select queries for suffixes of substrings: for
substrings x and y of w count the number of suffixes of x that are
lexicographically smaller than y, and for a substring x of w and an integer k,
find the k-th lexicographically smallest suffix of x.
  We further show that wavelet suffix trees allow to compute a
run-length-encoded Burrows-Wheeler transform of a substring x of w in $O(s \log
|x|)$ time, where s denotes the length of the resulting run-length encoding.
This answers a question by Cormode and Muthukrishnan, who considered an
analogous problem for Lempel-Ziv compression.
</summary>
    <author>
      <name>Maxim Babenko</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/1.9781611973730.39</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/1.9781611973730.39" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 5 figures; preliminary version published at SODA 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.6182v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.6182v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05 (Primary), 68W05 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1408.5248">
    <id>http://arxiv.org/abs/1408.5248v2</id>
    <updated>2015-06-09T15:17:44Z</updated>
    <published>2014-08-22T10:06:08Z</published>
    <title>Strong inapproximability of the shortest reset word</title>
    <summary>  The \v{C}ern\'y conjecture states that every $n$-state synchronizing
automaton has a reset word of length at most $(n-1)^2$. We study the hardness
of finding short reset words. It is known that the exact version of the
problem, i.e., finding the shortest reset word, is NP-hard and coNP-hard, and
complete for the DP class, and that approximating the length of the shortest
reset word within a factor of $O(\log n)$ is NP-hard [Gerbush and Heeringa,
CIAA'10], even for the binary alphabet [Berlinkov, DLT'13]. We significantly
improve on these results by showing that, for every $\epsilon>0$, it is NP-hard
to approximate the length of the shortest reset word within a factor of
$n^{1-\epsilon}$. This is essentially tight since a simple $O(n)$-approximation
algorithm exists.
</summary>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <author>
      <name>Damian Straszak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended abstract to appear in MFCS 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.5248v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5248v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1406.7716">
    <id>http://arxiv.org/abs/1406.7716v1</id>
    <updated>2014-06-30T13:10:00Z</updated>
    <published>2014-06-30T13:10:00Z</published>
    <title>Weighted ancestors in suffix trees</title>
    <summary>  The classical, ubiquitous, predecessor problem is to construct a data
structure for a set of integers that supports fast predecessor queries. Its
generalization to weighted trees, a.k.a. the weighted ancestor problem, has
been extensively explored and successfully reduced to the predecessor problem.
It is known that any solution for both problems with an input set from a
polynomially bounded universe that preprocesses a weighted tree in O(n
polylog(n)) space requires \Omega(loglogn) query time. Perhaps the most
important and frequent application of the weighted ancestors problem is for
suffix trees. It has been a long-standing open question whether the weighted
ancestors problem has better bounds for suffix trees. We answer this question
positively: we show that a suffix tree built for a text w[1..n] can be
preprocessed using O(n) extra space, so that queries can be answered in O(1)
time. Thus we improve the running times of several applications. Our
improvement is based on a number of data structure tools and a
periodicity-based insight into the combinatorial structure of a suffix tree.
</summary>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <author>
      <name>Moshe Lewenstein</name>
    </author>
    <author>
      <name>Patrick K. Nicholson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, LNCS format. A condensed version will appear in ESA 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.7716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1210.8386">
    <id>http://arxiv.org/abs/1210.8386v3</id>
    <updated>2012-11-15T18:55:18Z</updated>
    <published>2012-10-31T16:45:08Z</published>
    <title>Grammar-Based Construction of Indexes for Binary Jumbled Pattern
  Matching</title>
    <summary>  We show how, given a straight-line program with $g$ rules for a binary string
$B$ of length $n$, in $O(g^{2 / 3} n^{4 / 3})$ time we can build a linear-space
index such that, given $m$ and $c$, in O(1) time we can determine whether there
is a substring of $B$ with length $m$ containing exactly $c$ copies of 1. If we
use $O(n \log n)$ space for the index, then we can list all such substrings
using $O(m)$ time per substring.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/1210.8386v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.8386v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1210.1765">
    <id>http://arxiv.org/abs/1210.1765v2</id>
    <updated>2014-07-13T18:35:30Z</updated>
    <published>2012-10-05T14:01:58Z</published>
    <title>Better Space Bounds for Parameterized Range Majority and Minority</title>
    <summary>  Karpinski and Nekrich (2008) introduced the problem of parameterized range
majority, which asks to preprocess a string of length $n$ such that, given the
endpoints of a range, one can quickly find all the distinct elements whose
relative frequencies in that range are more than a threshold $\tau$. Subsequent
authors have reduced their time and space bounds such that, when $\tau$ is
given at preprocessing time, we need either $\Oh{n \log (1 / \tau)}$ space and
optimal $\Oh{1 / \tau}$ query time or linear space and $\Oh{(1 / \tau) \log
\log \sigma}$ query time, where $\sigma$ is the alphabet size. In this paper we
give the first linear-space solution with optimal $\Oh{1 / \tau}$ query time.
For the case when $\tau$ is given at query time, we significantly improve
previous bounds, achieving either $\Oh{n \log \log \sigma}$ space and optimal
$\Oh{1 / \tau}$ query time or compressed space and $\Oh{(1 / \tau) \log
\frac{\log (1 / \tau)}{\log w}}$ query time. Along the way, we consider the
complementary problem of parameterized range minority that was recently
introduced by Chan et al.\ (2012), who achieved linear space and $\Oh{1 /
\tau}$ query time even for variable $\tau$. We improve their solution to use
either nearly optimally compressed space with no slowdown, or optimally
compressed space with nearly no slowdown. Some of our intermediate results,
such as density-sensitive query time for one-dimensional range counting, may be
of independent interest.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1210.1765v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.1765v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1205.1195">
    <id>http://arxiv.org/abs/1205.1195v1</id>
    <updated>2012-05-06T09:26:45Z</updated>
    <published>2012-05-06T09:26:45Z</published>
    <title>Sequential-Access FM-Indexes</title>
    <summary>  Previous authors have shown how to build FM-indexes efficiently in external
memory, but querying them efficiently remains an open problem. Searching
na\"{i}vely for a pattern $P$ requires (\Theta (|P|)) random access. In this
paper we show how, by storing a few small auxiliary tables, we can access data
only in the order in which they appear on disk, which should be faster.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/1205.1195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1204.1215">
    <id>http://arxiv.org/abs/1204.1215v1</id>
    <updated>2012-04-05T13:07:38Z</updated>
    <published>2012-04-05T13:07:38Z</published>
    <title>On the Value of Multiple Read/Write Streams for Data Compression</title>
    <summary>  We study whether, when restricted to using polylogarithmic memory and
polylogarithmic passes, we can achieve qualitatively better data compression
with multiple read/write streams than we can with only one. We first show how
we can achieve universal compression using only one pass over one stream. We
then show that one stream is not sufficient for us to achieve good
grammar-based compression. Finally, we show that two streams are necessary and
sufficient for us to achieve entropy-only bounds.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/1204.1215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1202.3208">
    <id>http://arxiv.org/abs/1202.3208v1</id>
    <updated>2012-02-15T05:29:25Z</updated>
    <published>2012-02-15T05:29:25Z</published>
    <title>Linear-Space Substring Range Counting over Polylogarithmic Alphabets</title>
    <summary>  Bille and G{\o}rtz (2011) recently introduced the problem of substring range
counting, for which we are asked to store compactly a string $S$ of $n$
characters with integer labels in ([0, u]), such that later, given an interval
([a, b]) and a pattern $P$ of length $m$, we can quickly count the occurrences
of $P$ whose first characters' labels are in ([a, b]). They showed how to store
$S$ in $\Oh{n \log n / \log \log n}$ space and answer queries in $\Oh{m + \log
\log u}$ time. We show that, if $S$ is over an alphabet of size (\polylog (n)),
then we can achieve optimal linear space. Moreover, if (u = n \polylog (n)),
then we can also reduce the time to $\Oh{m}$. Our results give linear space and
time bounds for position-restricted substring counting and the counting
versions of indexing substrings with intervals, indexing substrings with gaps
and aligned pattern matching.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.6462">
    <id>http://arxiv.org/abs/1307.6462v1</id>
    <updated>2013-07-24T15:42:23Z</updated>
    <published>2013-07-24T15:42:23Z</published>
    <title>AliBI: An Alignment-Based Index for Genomic Datasets</title>
    <summary>  With current hardware and software, a standard computer can now hold in RAM
an index for approximate pattern matching on about half a dozen human genomes.
Sequencing technologies have improved so quickly, however, that scientists will
soon demand indexes for thousands of genomes. Whereas most researchers who have
addressed this problem have proposed completely new kinds of indexes, we
recently described a simple technique that scales standard indexes to work on
more genomes. Our main idea was to filter the dataset with LZ77, build a
standard index for the filtered file, and then create a hybrid of that standard
index and an LZ77-based index. In this paper we describe how to our technique
to use alignments instead of LZ77, in order to simplify and speed up both
preprocessing and random access.
</summary>
    <author>
      <name>Hector Ferrada</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Tommi Hirvola</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <link href="http://arxiv.org/abs/1307.6462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1312.3422">
    <id>http://arxiv.org/abs/1312.3422v3</id>
    <updated>2014-03-09T10:40:30Z</updated>
    <published>2013-12-12T09:15:47Z</published>
    <title>Compressed Spaced Suffix Arrays</title>
    <summary>  Spaced seeds are important tools for similarity search in bioinformatics, and
using several seeds together often significantly improves their performance.
With existing approaches, however, for each seed we keep a separate linear-size
data structure, either a hash table or a spaced suffix array (SSA). In this
paper we show how to compress SSAs relative to normal suffix arrays (SAs) and
still support fast random access to them. We first prove a theoretical upper
bound on the space needed to store an SSA when we already have the SA. We then
present experiments indicating that our approach works even better in practice.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Daniel Valenzuela</name>
    </author>
    <link href="http://arxiv.org/abs/1312.3422v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.3422v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1305.3164">
    <id>http://arxiv.org/abs/1305.3164v2</id>
    <updated>2013-07-13T23:33:37Z</updated>
    <published>2013-05-14T14:24:42Z</published>
    <title>Heaviest Induced Ancestors and Longest Common Substrings</title>
    <summary>  Suppose we have two trees on the same set of leaves, in which nodes are
weighted such that children are heavier than their parents. We say a node from
the first tree and a node from the second tree are induced together if they
have a common leaf descendant. In this paper we describe data structures that
efficiently support the following heaviest-induced-ancestor query: given a node
from the first tree and a node from the second tree, find an induced pair of
their ancestors with maximum combined weight. Our solutions are based on a
geometric interpretation that enables us to find heaviest induced ancestors
using range queries. We then show how to use these results to build an
LZ-compressed index with which we can quickly find with high probability a
longest substring common to the indexed string and a given pattern.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/1305.3164v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.3164v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1304.5560">
    <id>http://arxiv.org/abs/1304.5560v1</id>
    <updated>2013-04-19T22:50:32Z</updated>
    <published>2013-04-19T22:50:32Z</published>
    <title>Indexes for Jumbled Pattern Matching in Strings, Trees and Graphs</title>
    <summary>  We consider how to index strings, trees and graphs for jumbled pattern
matching when we are asked to return a match if one exists. For example, we
show how, given a tree containing two colours, we can build a quadratic-space
index with which we can find a match in time proportional to the size of the
match. We also show how we need only linear space if we are content with
approximate matches.
</summary>
    <author>
      <name>Ferdinando Cicalese</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Emanuele Giaquinta</name>
    </author>
    <author>
      <name>Eduardo Sany Laber</name>
    </author>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <author>
      <name>Romeo Rizzi</name>
    </author>
    <author>
      <name>Alexandru I. Tomescu</name>
    </author>
    <link href="http://arxiv.org/abs/1304.5560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.5560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1301.6127">
    <id>http://arxiv.org/abs/1301.6127v3</id>
    <updated>2014-06-28T12:24:53Z</updated>
    <published>2013-01-25T19:06:06Z</published>
    <title>Binary Jumbled Pattern Matching on Trees and Tree-Like Structures</title>
    <summary>  Binary jumbled pattern matching asks to preprocess a binary string $S$ in
order to answer queries $(i,j)$ which ask for a substring of $S$ that is of
length $i$ and has exactly $j$ 1-bits. This problem naturally generalizes to
vertex-labeled trees and graphs by replacing "substring" with "connected
subgraph". In this paper, we give an $O(n^2 / \log^2 n)$-time solution for
trees, matching the currently best bound for (the simpler problem of) strings.
We also give an $\Oh{g^{2 / 3} n^{4 / 3}/(\log n)^{4/3}}$-time solution for
strings that are compressed by a grammar of size $g$. This solution improves
the known bounds when the string is compressible under many popular compression
schemes. Finally, we prove that the problem is fixed-parameter tractable with
respect to the treewidth $w$ of the graph, thus improving the previous best
$n^{O(w)}$ algorithm [ICALP'07].
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Danny Hermelin</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <link href="http://arxiv.org/abs/1301.6127v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.6127v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0911.4981">
    <id>http://arxiv.org/abs/0911.4981v4</id>
    <updated>2012-04-02T02:12:43Z</updated>
    <published>2009-11-25T23:31:23Z</published>
    <title>Efficient Fully-Compressed Sequence Representations</title>
    <summary>  We present a data structure that stores a sequence $s[1..n]$ over alphabet
$[1..\sigma]$ in $n\Ho(s) + o(n)(\Ho(s){+}1)$ bits, where $\Ho(s)$ is the
zero-order entropy of $s$. This structure supports the queries \access, \rank\
and \select, which are fundamental building blocks for many other compressed
data structures, in worst-case time $\Oh{\lg\lg\sigma}$ and average time
$\Oh{\lg \Ho(s)}$. The worst-case complexity matches the best previous results,
yet these had been achieved with data structures using $n\Ho(s)+o(n\lg\sigma)$
bits. On highly compressible sequences the $o(n\lg\sigma)$ bits of the
redundancy may be significant compared to the the $n\Ho(s)$ bits that encode
the data. Our representation, instead, compresses the redundancy as well.
Moreover, our average-case complexity is unprecedented. Our technique is based
on partitioning the alphabet into characters of similar frequency. The
subsequence corresponding to each group can then be encoded using fast
uncompressed representations without harming the overall compression ratios,
even in the redundancy. The result also improves upon the best current
compressed representations of several other data structures. For example, we
achieve $(i)$ compressed redundancy, retaining the best time complexities, for
the smallest existing full-text self-indexes; $(ii)$ compressed permutations
$\pi$ with times for $\pi()$ and $\pii()$ improved to loglogarithmic; and
$(iii)$ the first compressed representation of dynamic collections of disjoint
sets. We also point out various applications to inverted indexes, suffix
arrays, binary relations, and data compressors. ...
</summary>
    <author>
      <name>Jeremy Barbay</name>
    </author>
    <author>
      <name>Francisco Claude</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/0911.4981v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.4981v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0907.0741">
    <id>http://arxiv.org/abs/0907.0741v1</id>
    <updated>2009-07-04T06:18:38Z</updated>
    <published>2009-07-04T06:18:38Z</published>
    <title>Tight Bounds for Online Stable Sorting</title>
    <summary>  Although many authors have considered how many ternary comparisons it takes
to sort a multiset $S$ of size $n$, the best known upper and lower bounds still
differ by a term linear in $n$. In this paper we restrict our attention to
online stable sorting and prove upper and lower bounds that are within (o (n))
not only of each other but also of the best known upper bound for offline
sorting. Specifically, we first prove that if the number of distinct elements
(\sigma = o (n / \log n)), then ((H + 1) n + o (n)) comparisons are sufficient,
where $H$ is the entropy of the distribution of the elements in $S$. We then
give a simple proof that ((H + 1) n - o (n)) comparisons are necessary in the
worst case.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/0907.0741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0905.3107">
    <id>http://arxiv.org/abs/0905.3107v1</id>
    <updated>2009-05-19T14:19:08Z</updated>
    <published>2009-05-19T14:19:08Z</published>
    <title>Fast and Compact Prefix Codes</title>
    <summary>  It is well-known that, given a probability distribution over $n$ characters,
in the worst case it takes (\Theta (n \log n)) bits to store a prefix code with
minimum expected codeword length. However, in this paper we first show that,
for any $0&lt;\epsilon&lt;1/2$ with (1 / \epsilon = \Oh{\polylog{n}}), it takes
$\Oh{n \log \log (1 / \epsilon)}$ bits to store a prefix code with expected
codeword length within $\epsilon$ of the minimum. We then show that, for any
constant (c > 1), it takes $\Oh{n^{1 / c} \log n}$ bits to store a prefix code
with expected codeword length at most $c$ times the minimum. In both cases, our
data structures allow us to encode and decode any character in $\Oh{1}$ time.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/0905.3107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.3107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0903.4726">
    <id>http://arxiv.org/abs/0903.4726v6</id>
    <updated>2010-04-07T12:48:51Z</updated>
    <published>2009-03-27T02:29:01Z</published>
    <title>Range Quantile Queries: Another Virtue of Wavelet Trees</title>
    <summary>  We show how to use a balanced wavelet tree as a data structure that stores a
list of numbers and supports efficient {\em range quantile queries}. A range
quantile query takes a rank and the endpoints of a sublist and returns the
number with that rank in that sublist. For example, if the rank is half the
sublist's length, then the query returns the sublist's median. We also show how
these queries can be used to support space-efficient {\em coloured range
reporting} and {\em document listing}.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Andrew Turpin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-03784-9_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-03784-9_1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added note about generalization to any constant number of dimensions.</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.4726v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.4726v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0902.0133">
    <id>http://arxiv.org/abs/0902.0133v1</id>
    <updated>2009-02-01T16:03:08Z</updated>
    <published>2009-02-01T16:03:08Z</published>
    <title>New Algorithms and Lower Bounds for Sequential-Access Data Compression</title>
    <summary>  This thesis concerns sequential-access data compression, i.e., by algorithms
that read the input one or more times from beginning to end. In one chapter we
consider adaptive prefix coding, for which we must read the input character by
character, outputting each character's self-delimiting codeword before reading
the next one. We show how to encode and decode each character in constant
worst-case time while producing an encoding whose length is worst-case optimal.
In another chapter we consider one-pass compression with memory bounded in
terms of the alphabet size and context length, and prove a nearly tight
tradeoff between the amount of memory we can use and the quality of the
compression we can achieve. In a third chapter we consider compression in the
read/write streams model, which allows us passes and memory both
polylogarithmic in the size of the input. We first show how to achieve
universal compression using only one pass over one stream. We then show that
one stream is not sufficient for achieving good grammar-based compression.
Finally, we show that two streams are necessary and sufficient for achieving
entropy-only bounds.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">draft of PhD thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.0133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.0133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1011.4532">
    <id>http://arxiv.org/abs/1011.4532v1</id>
    <updated>2010-11-19T22:00:50Z</updated>
    <published>2010-11-19T22:00:50Z</published>
    <title>New Algorithms on Wavelet Trees and Applications to Information
  Retrieval</title>
    <summary>  Wavelet trees are widely used in the representation of sequences,
permutations, text collections, binary relations, discrete points, and other
succinct data structures. We show, however, that this still falls short of
exploiting all of the virtues of this versatile data structure. In particular
we show how to use wavelet trees to solve fundamental algorithmic problems such
as {\em range quantile} queries, {\em range next value} queries, and {\em range
intersection} queries. We explore several applications of these queries in
Information Retrieval, in particular {\em document retrieval} in hierarchical
and temporal documents, and in the representation of {\em inverted lists}.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <link href="http://arxiv.org/abs/1011.4532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.4532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1011.3480">
    <id>http://arxiv.org/abs/1011.3480v1</id>
    <updated>2010-11-15T19:30:19Z</updated>
    <published>2010-11-15T19:30:19Z</published>
    <title>Counting Colours in Compressed Strings</title>
    <summary>  Suppose we are asked to preprocess a string \(s [1..n]\) such that later,
given a substring's endpoints, we can quickly count how many distinct
characters it contains. In this paper we give a data structure for this problem
that takes \(n H_0 (s) + \Oh{n} + \oh{n H_0 (s)}\) bits, where \(H_0 (s)\) is
the 0th-order empirical entropy of $s$, and answers queries in $\Oh{\log^{1 +
\epsilon} n}$ time for any constant \(\epsilon > 0\). We also show how our data
structure can be made partially dynamic.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <link href="http://arxiv.org/abs/1011.3480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.3480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1111.1355">
    <id>http://arxiv.org/abs/1111.1355v1</id>
    <updated>2011-11-05T21:53:33Z</updated>
    <published>2011-11-05T21:53:33Z</published>
    <title>A Compressed Self-Index for Genomic Databases</title>
    <summary>  Advances in DNA sequencing technology will soon result in databases of
thousands of genomes. Within a species, individuals' genomes are almost exact
copies of each other; e.g., any two human genomes are 99.9% the same. Relative
Lempel-Ziv (RLZ) compression takes advantage of this property: it stores the
first genome uncompressed or as an FM-index, then compresses the other genomes
with a variant of LZ77 that copies phrases only from the first genome. RLZ
achieves good compression and supports fast random access; in this paper we
show how to support fast search as well, thus obtaining an efficient compressed
self-index.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <link href="http://arxiv.org/abs/1111.1355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.1355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1109.2930">
    <id>http://arxiv.org/abs/1109.2930v4</id>
    <updated>2012-10-31T17:09:31Z</updated>
    <published>2011-09-13T21:10:01Z</published>
    <title>Faster Approximate Pattern Matching in Compressed Repetitive Texts</title>
    <summary>  Motivated by the imminent growth of massive, highly redundant genomic
databases, we study the problem of compressing a string database while
simultaneously supporting fast random access, substring extraction and pattern
matching to the underlying string(s). Bille et al. (2011) recently showed how,
given a straight-line program with $r$ rules for a string $s$ of length $n$, we
can build an $\Oh{r}$-word data structure that allows us to extract any
substring of length $m$ in $\Oh{\log n + m}$ time. They also showed how, given
a pattern $p$ of length $m$ and an edit distance (k \leq m), their data
structure supports finding all \occ approximate matches to $p$ in $s$ in $\Oh{r
(\min (m k, k^4 + m) + \log n) + \occ}$ time. Rytter (2003) and Charikar et al.
(2005) showed that $r$ is always at least the number $z$ of phrases in the LZ77
parse of $s$, and gave algorithms for building straight-line programs with
$\Oh{z \log n}$ rules. In this paper we give a simple $\Oh{z \log n}$-word data
structure that takes the same time for substring extraction but only $\Oh{z
\min (m k, k^4 + m) + \occ}$ time for approximate pattern matching.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Christopher Hoobin</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal version of ISAAC '11 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.2930v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.2930v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1212.4613">
    <id>http://arxiv.org/abs/1212.4613v2</id>
    <updated>2013-01-13T15:14:10Z</updated>
    <published>2012-12-19T10:03:47Z</published>
    <title>New Algorithms for Position Heaps</title>
    <summary>  We present several results about position heaps, a relatively new alternative
to suffix trees and suffix arrays. First, we show that, if we limit the maximum
length of patterns to be sought, then we can also limit the height of the heap
and reduce the worst-case cost of insertions and deletions. Second, we show how
to build a position heap in linear time independent of the size of the
alphabet. Third, we show how to augment a position heap such that it supports
access to the corresponding suffix array, and vice versa. Fourth, we introduce
a variant of a position heap that can be simulated efficiently by a compressed
suffix array with a linear number of extra bits.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Wing-Kai Hon</name>
    </author>
    <author>
      <name>Tsung-Han Ku</name>
    </author>
    <link href="http://arxiv.org/abs/1212.4613v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.4613v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0804.1214">
    <id>http://arxiv.org/abs/0804.1214v1</id>
    <updated>2008-04-08T09:08:37Z</updated>
    <published>2008-04-08T09:08:37Z</published>
    <title>New Lower Bounds for the Maximum Number of Runs in a String</title>
    <summary>  We show a new lower bound for the maximum number of runs in a string. We
prove that for any e > 0, (a -- e)n is an asymptotic lower bound, where a =
56733/60064 = 0.944542. It is superior to the previous bound 0.927 given by
Franek et al. Moreover, our construction of the strings and the proof is much
simpler than theirs.
</summary>
    <author>
      <name>Kazuhiko Kusano</name>
    </author>
    <author>
      <name>Wataru Matsubara</name>
    </author>
    <author>
      <name>Akira Ishino</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <link href="http://arxiv.org/abs/0804.1214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.1214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1507.06866">
    <id>http://arxiv.org/abs/1507.06866v1</id>
    <updated>2015-07-24T14:57:18Z</updated>
    <published>2015-07-24T14:57:18Z</published>
    <title>Compressed Data Structures for Dynamic Sequences</title>
    <summary>  We consider the problem of storing a dynamic string $S$ over an alphabet
$\Sigma=\{\,1,\ldots,\sigma\,\}$ in compressed form. Our representation
supports insertions and deletions of symbols and answers three fundamental
queries: $\mathrm{access}(i,S)$ returns the $i$-th symbol in $S$,
$\mathrm{rank}_a(i,S)$ counts how many times a symbol $a$ occurs among the
first $i$ positions in $S$, and $\mathrm{select}_a(i,S)$ finds the position
where a symbol $a$ occurs for the $i$-th time. We present the first
fully-dynamic data structure for arbitrarily large alphabets that achieves
optimal query times for all three operations and supports updates with
worst-case time guarantees. Ours is also the first fully-dynamic data structure
that needs only $nH_k+o(n\log\sigma)$ bits, where $H_k$ is the $k$-th order
entropy and $n$ is the string length. Moreover our representation supports
extraction of a substring $S[i..i+\ell]$ in optimal $O(\log n/\log\log n +
\ell/\log_{\sigma}n)$ time.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/1507.06866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.06866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1505.00357">
    <id>http://arxiv.org/abs/1505.00357v5</id>
    <updated>2021-03-09T15:58:38Z</updated>
    <published>2015-05-02T17:12:20Z</published>
    <title>Optimal Search Trees with 2-Way Comparisons</title>
    <summary>  In 1971, Knuth gave an $O(n^2)$-time algorithm for the classic problem of
finding an optimal binary search tree. Knuth's algorithm works only for search
trees based on 3-way comparisons, while most modern computers support only
2-way comparisons (e.g., $&lt;, \le, =, \ge$, and $>$). Until this paper, the
problem of finding an optimal search tree using 2-way comparisons remained open
-- poly-time algorithms were known only for restricted variants. We solve the
general case, giving (i) an $O(n^4)$-time algorithm and (ii) an $O(n \log
n)$-time additive-3 approximation algorithm. Also, for finding optimal binary
split trees, we (iii) obtain a linear speedup and (iv) prove some previous work
incorrect.
</summary>
    <author>
      <name>Marek Chrobak</name>
    </author>
    <author>
      <name>Mordecai Golin</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Neal E. Young</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-662-48971-0_7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-662-48971-0_7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ERRATUM: The proof of Theorem 3 of the ISAAC'15 paper (v4 here) is
  incorrect. Version v5 here contains: a full erratum, proofs of the other
  results, and pointers to journal versions expanding those results</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Optimal Search Trees with 2-Way Comparisons. In: Elbassioni K.,
  Makino K. (eds) Algorithms and Computation. ISAAC 2015. Lecture Notes in
  Computer Science, vol 9472 (2105). Springer, Berlin, Heidelberg</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1505.00357v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00357v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P10, 68P30, 68W25, 94A45," scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; G.1.6; G.2.2; H.3.1; I.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1712.07431">
    <id>http://arxiv.org/abs/1712.07431v3</id>
    <updated>2019-07-15T08:15:41Z</updated>
    <published>2017-12-20T11:51:51Z</published>
    <title>Text Indexing and Searching in Sublinear Time</title>
    <summary>  We introduce the first index that can be built in $o(n)$ time for a text of
length $n$, and can also be queried in $o(q)$ time for a pattern of length $q$.
On an alphabet of size $\sigma$, our index uses $O(n\sqrt{\log n\log\sigma})$
bits, is built in $O(n((\log\log n)^2+\sqrt{\log\sigma})/\sqrt{\log_\sigma n})$
deterministic time, and computes the number $\mathrm{occ}$ of occurrences of
the pattern in time $O(q/\log_\sigma n+\log n)$. Each such occurrence can then
be found in $O(\sqrt{\log n\log\sigma})$ time. By slightly increasing the space
and construction time, to $O(n(\sqrt{\log n\log\sigma}+
\log\sigma\log^\varepsilon n))$ and $O(n\log^{3/2}\sigma/\log^{1/2-\varepsilon}
n)$, respectively, for any constant $0&lt;\varepsilon&lt;1/2$, we can find the
$\mathrm{occ}$ pattern occurrences in time $O(q/\log_\sigma n +
\sqrt{\log_\sigma n}\log\log n + \mathrm{occ})$. We build on a novel text
sampling based on difference covers, which enjoys properties that allow us
efficiently computing longest common prefixes in constant time. We extend our
results to the secondary memory model as well, where we give the first
construction in $o(\mathit{Sort}(n))$ I/Os of a data structure with suffix
array functionality; this data structure supports pattern matching queries with
optimal or nearly-optimal cost.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/1712.07431v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07431v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1312.4678">
    <id>http://arxiv.org/abs/1312.4678v2</id>
    <updated>2014-08-23T01:50:57Z</updated>
    <published>2013-12-17T07:54:49Z</published>
    <title>Simple, compact and robust approximate string dictionary</title>
    <summary>  This paper is concerned with practical implementations of approximate string
dictionaries that allow edit errors. In this problem, we have as input a
dictionary $D$ of $d$ strings of total length $n$ over an alphabet of size
$\sigma$. Given a bound $k$ and a pattern $x$ of length $m$, a query has to
return all the strings of the dictionary which are at edit distance at most $k$
from $x$, where the edit distance between two strings $x$ and $y$ is defined as
the minimum-cost sequence of edit operations that transform $x$ into $y$. The
cost of a sequence of operations is defined as the sum of the costs of the
operations involved in the sequence. In this paper, we assume that each of
these operations has unit cost and consider only three operations: deletion of
one character, insertion of one character and substitution of a character by
another. We present a practical implementation of the data structure we
recently proposed and which works only for one error. We extend the scheme to
$2\leq k&lt;m$. Our implementation has many desirable properties: it has a very
fast and space-efficient building algorithm. The dictionary data structure is
compact and has fast and robust query time. Finally our data structure is
simple to implement as it only uses basic techniques from the literature,
mainly hashing (linear probing and hash signatures) and succinct data
structures (bitvectors supporting rank queries).
</summary>
    <author>
      <name>Ibrahim Chegrane</name>
    </author>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to a journal (19 pages, 2 figures)</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.4678v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4678v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1301.4952">
    <id>http://arxiv.org/abs/1301.4952v2</id>
    <updated>2013-04-25T20:00:55Z</updated>
    <published>2013-01-21T18:33:06Z</published>
    <title>Single and multiple consecutive permutation motif search</title>
    <summary>  Let $t$ be a permutation (that shall play the role of the {\em text}) on
$[n]$ and a pattern $p$ be a sequence of $m$ distinct integer(s) of $[n]$,
$m\leq n$. The pattern $p$ occurs in $t$ in position $i$ if and only if $p_1...
p_m$ is order-isomorphic to $t_i... t_{i+m-1}$, that is, for all $1 \leq k&lt;
\ell \leq m$, $p_k>p_\ell$ if and only if $t_{i+k-1}>t_{i+\ell-1}$. Searching
for a pattern $p$ in a text $t$ consists in identifying all occurrences of $p$
in $t$. We first present a forward automaton which allows us to search for $p$
in $t$ in $O(m^2\log \log m +n)$ time. We then introduce a Morris-Pratt
automaton representation of the forward automaton which allows us to reduce
this complexity to $O(m\log \log m +n)$ at the price of an additional amortized
constant term by integer of the text. Both automata occupy $O(m)$ space. We
then extend the problem to search for a set of patterns and exhibit a specific
Aho-Corasick like algorithm. Next we present a sub-linear average case search
algorithm running in $O(\frac{m\log m}{\log\log m}+\frac{n\log m}{m\log\log
m})$ time, that we eventually prove to be optimal on average.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Adeline Pierrot</name>
    </author>
    <author>
      <name>Mathieu Raffinot</name>
    </author>
    <author>
      <name>Stéphane Vialette</name>
    </author>
    <link href="http://arxiv.org/abs/1301.4952v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.4952v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1301.3488">
    <id>http://arxiv.org/abs/1301.3488v1</id>
    <updated>2013-01-15T20:59:43Z</updated>
    <published>2013-01-15T20:59:43Z</published>
    <title>Various improvements to text fingerprinting</title>
    <summary>  Let s = s_1 .. s_n be a text (or sequence) on a finite alphabet \Sigma of
size \sigma. A fingerprint in s is the set of distinct characters appearing in
one of its substrings. The problem considered here is to compute the set {\cal
F} of all fingerprints of all substrings of s in order to answer efficiently
certain questions on this set. A substring s_i .. s_j is a maximal location for
a fingerprint f in F (denoted by &lt;i,j>) if the alphabet of s_i .. s_j is f and
s_{i-1}, s_{j+1}, if defined, are not in f. The set of maximal locations ins is
{\cal L} (it is easy to see that |{\cal L}| \leq n \sigma). Two maximal
locations &lt;i,j> and &lt;k,l> such that s_i .. s_j = s_k .. s_l are named {\em
copies}, and the quotient set of {\cal L} according to the copy relation is
denoted by {\cal L}_C. We present new exact and approximate efficient
algorithms and data structures for the following three problems: (1) to compute
{\cal F}; (2) given f as a set of distinct characters in \Sigma, to answer if f
represents a fingerprint in {\cal F}; (3) given f, to find all maximal
locations of f in s.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Roman Kolpakov</name>
    </author>
    <author>
      <name>Mathieu Raffinot</name>
    </author>
    <link href="http://arxiv.org/abs/1301.3488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1408.3093">
    <id>http://arxiv.org/abs/1408.3093v2</id>
    <updated>2014-08-14T15:20:57Z</updated>
    <published>2014-08-13T19:18:36Z</published>
    <title>Rank, select and access in grammar-compressed strings</title>
    <summary>  Given a string $S$ of length $N$ on a fixed alphabet of $\sigma$ symbols, a
grammar compressor produces a context-free grammar $G$ of size $n$ that
generates $S$ and only $S$. In this paper we describe data structures to
support the following operations on a grammar-compressed string:
$\mbox{rank}_c(S,i)$ (return the number of occurrences of symbol $c$ before
position $i$ in $S$); $\mbox{select}_c(S,i)$ (return the position of the $i$th
occurrence of $c$ in $S$); and $\mbox{access}(S,i,j)$ (return substring
$S[i,j]$). For rank and select we describe data structures of size
$O(n\sigma\log N)$ bits that support the two operations in $O(\log N)$ time. We
propose another structure that uses $O(n\sigma\log (N/n)(\log N)^{1+\epsilon})$
bits and that supports the two queries in $O(\log N/\log\log N)$, where
$\epsilon>0$ is an arbitrary constant. To our knowledge, we are the first to
study the asymptotic complexity of rank and select in the grammar-compressed
setting, and we provide a hardness result showing that significantly improving
the bounds we achieve would imply a major breakthrough on a hard
graph-theoretical problem. Our main result for access is a method that requires
$O(n\log N)$ bits of space and $O(\log N+m/\log_\sigma N)$ time to extract
$m=j-i+1$ consecutive symbols from $S$. Alternatively, we can achieve $O(\log
N/\log\log N+m/\log_\sigma N)$ query time using $O(n\log (N/n)(\log
N)^{1+\epsilon})$ bits of space. This matches a lower bound stated by Verbin
and Yu for strings where $N$ is polynomially related to $n$.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.3093v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.3093v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1511.09229">
    <id>http://arxiv.org/abs/1511.09229v2</id>
    <updated>2015-12-03T13:51:04Z</updated>
    <published>2015-11-30T10:26:49Z</published>
    <title>Efficient Deterministic Single Round Document Exchange for Edit Distance</title>
    <summary>  Suppose that we have two parties that possess each a binary string. Suppose
that the length of the first string (document) is $n$ and that the two strings
(documents) have edit distance (minimal number of deletes, inserts and
substitutions needed to transform one string into the other) at most $k$. The
problem we want to solve is to devise an efficient protocol in which the first
party sends a single message that allows the second party to guess the first
party's string. In this paper we show an efficient deterministic protocol for
this problem. The protocol runs in time $O(n\cdot \mathtt{polylog}(n))$ and has
message size $O(k^2+k\log^2n)$ bits. To the best of our knowledge, ours is the
first efficient deterministic protocol for this problem, if efficiency is
measured in both the message size and the running time. As an immediate
application of our new protocol, we show a new error correcting code that is
efficient even for large numbers of (adversarial) edit errors.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, under submission. This version has some minor corrections,
  clarifications and a simplification of the message size bound</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.09229v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.09229v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1508.02968">
    <id>http://arxiv.org/abs/1508.02968v1</id>
    <updated>2015-08-12T16:01:21Z</updated>
    <published>2015-08-12T16:01:21Z</published>
    <title>Space-efficient detection of unusual words</title>
    <summary>  Detecting all the strings that occur in a text more frequently or less
frequently than expected according to an IID or a Markov model is a basic
problem in string mining, yet current algorithms are based on data structures
that are either space-inefficient or incur large slowdowns, and current
implementations cannot scale to genomes or metagenomes in practice. In this
paper we engineer an algorithm based on the suffix tree of a string to use just
a small data structure built on the Burrows-Wheeler transform, and a stack of
$O(\sigma^2\log^2 n)$ bits, where $n$ is the length of the string and $\sigma$
is the size of the alphabet. The size of the stack is $o(n)$ except for very
large values of $\sigma$. We further improve the algorithm by removing its time
dependency on $\sigma$, by reporting only a subset of the maximal repeats and
of the minimal rare words of the string, and by detecting and scoring candidate
under-represented strings that $\textit{do not occur}$ in the string. Our
algorithms are practical and work directly on the BWT, thus they can be
immediately applied to a number of existing datasets that are available in this
form, returning this string mining problem to a manageable scale.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Fabio Cunial</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1502.06370</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.02968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1404.2677">
    <id>http://arxiv.org/abs/1404.2677v3</id>
    <updated>2014-10-03T22:11:01Z</updated>
    <published>2014-04-10T02:53:26Z</published>
    <title>Optimal Encodings for Range Majority Queries</title>
    <summary>  We study the problem of designing a data structure that reports the positions
of the distinct $\tau$-majorities within any range of an array $A[1,n]$,
without storing $A$. A $\tau$-majority in a range $A[i,j]$, for $0&lt;\tau&lt; 1$, is
an element that occurs more than $\tau(j-i+1)$ times in $A[i,j]$. We show that
$\Omega(n\log(1/\tau))$ bits are necessary for any data structure able just to
count the number of distinct $\tau$-majorities in any range. Then, we design a
structure using $O(n\log(1/\tau))$ bits that returns one position of each
$\tau$-majority of $A[i,j]$ in $O((1/\tau)\log\log_w(1/\tau)\log n)$ time, on a
RAM machine with word size $w$ (it can output any further position where each
$\tau$-majority occurs in $O(1)$ additional time). Finally, we show how to
remove a $\log n$ factor from the time by adding $O(n\log\log n)$ bits of space
to the structure.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Sharma V. Thankachan</name>
    </author>
    <link href="http://arxiv.org/abs/1404.2677v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.2677v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1612.04094">
    <id>http://arxiv.org/abs/1612.04094v1</id>
    <updated>2016-12-13T10:52:03Z</updated>
    <published>2016-12-13T10:52:03Z</published>
    <title>Efficient Representation of Multidimensional Data over Hierarchical
  Domains</title>
    <summary>  We consider the problem of representing multidimensional data where the
domain of each dimension is organized hierarchically, and the queries require
summary information at a different node in the hierarchy of each dimension.
This is the typical case of OLAP databases. A basic approach is to represent
each hierarchy as a one-dimensional line and recast the queries as
multidimensional range queries. This approach can be implemented compactly by
generalizing to more dimensions the $k^2$-treap, a compact representation of
two-dimensional points that allows for efficient summarization queries along
generic ranges. Instead, we propose a more flexible generalization, which
instead of a generic quadtree-like partition of the space, follows the domain
hierarchies across each dimension to organize the partitioning. The resulting
structure is much more efficient than a generic multidimensional structure,
since queries are resolved by aggregating much fewer nodes of the tree.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Ana Cerdeira-Pena</name>
    </author>
    <author>
      <name>Narciso López-López</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Miguel R. Penabad</name>
    </author>
    <author>
      <name>Fernando Silva-Coira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-46049-9_19</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-46049-9_19" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">String Processing and Information Retrieval: 23rd International
  Symposium, SPIRE 2016, Beppu, Japan, October 18-20, 2016, Proceedings.
  Springer International Publishing. pp 191-203. ISBN: 9783319460482</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1612.04094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1612.03308">
    <id>http://arxiv.org/abs/1612.03308v1</id>
    <updated>2016-12-10T15:50:19Z</updated>
    <published>2016-12-10T15:50:19Z</published>
    <title>GraCT: A Grammar based Compressed representation of Trajectories</title>
    <summary>  We present a compressed data structure to store free trajectories of moving
objects (ships over the sea, for example) allowing spatio-temporal queries. Our
method, GraCT, uses a $k^2$-tree to store the absolute positions of all objects
at regular time intervals (snapshots), whereas the positions between snapshots
are represented as logs of relative movements compressed with Re-Pair. Our
experimental evaluation shows important savings in space and time with respect
to a fair baseline.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Adrián Gómez-Brandón</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>José R. Paramá</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-46049-9_21</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-46049-9_21" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">String Processing and Information Retrieval: 23rd International
  Symposium, SPIRE 2016, Beppu, Japan, October 18-20, 2016, Proceedings.
  Springer International Publishing. pp 218-230. ISBN: 9783319460482</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1612.03308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1603.02063">
    <id>http://arxiv.org/abs/1603.02063v2</id>
    <updated>2016-03-30T20:37:53Z</updated>
    <published>2016-03-07T13:45:36Z</published>
    <title>Aggregated 2D Range Queries on Clustered Points</title>
    <summary>  Efficient processing of aggregated range queries on two-dimensional grids is
a common requirement in information retrieval and data mining systems, for
example in Geographic Information Systems and OLAP cubes. We introduce a
technique to represent grids supporting aggregated range queries that requires
little space when the data points in the grid are clustered, which is common in
practice. We show how this general technique can be used to support two
important types of aggregated queries, which are ranked range queries and
counting range queries. Our experimental evaluation shows that this technique
can speed up aggregated queries up to more than an order of magnitude, with a
small space overhead.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Guillermo De Bernardo</name>
    </author>
    <author>
      <name>Roberto Konow</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Diego Seco</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.is.2016.03.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.is.2016.03.004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Systems, Volume 60, Pages 34-49, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.02063v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.02063v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1601.06939">
    <id>http://arxiv.org/abs/1601.06939v2</id>
    <updated>2016-03-23T13:26:34Z</updated>
    <published>2016-01-26T09:19:15Z</published>
    <title>Simple and Efficient Fully-Functional Succinct Trees</title>
    <summary>  The fully-functional succinct tree representation of Navarro and Sadakane
(ACM Transactions on Algorithms, 2014) supports a large number of operations in
constant time using $2n+o(n)$ bits. However, the full idea is hard to
implement. Only a simplified version with $O(\log n)$ operation time has been
implemented and shown to be practical and competitive. We describe a new
variant of the original idea that is much simpler to implement and has
worst-case time $O(\log\log n)$ for the operations. An implementation based on
this version is experimentally shown to be superior to existing
implementations.
</summary>
    <author>
      <name>Joshimar Cordova</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1601.06939v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.06939v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1111.2621">
    <id>http://arxiv.org/abs/1111.2621v2</id>
    <updated>2013-08-23T16:51:05Z</updated>
    <published>2011-11-10T21:51:27Z</published>
    <title>Optimal Lower and Upper Bounds for Representing Sequences</title>
    <summary>  Sequence representations supporting queries $access$, $select$ and $rank$ are
at the core of many data structures. There is a considerable gap between the
various upper bounds and the few lower bounds known for such representations,
and how they relate to the space used. In this article we prove a strong lower
bound for $rank$, which holds for rather permissive assumptions on the space
used, and give matching upper bounds that require only a compressed
representation of the sequence. Within this compressed space, operations
$access$ and $select$ can be solved in constant or almost-constant time, which
is optimal for large alphabets. Our new upper bounds dominate all of the
previous work in the time/space map.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1111.2621v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.2621v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1103.2167">
    <id>http://arxiv.org/abs/1103.2167v3</id>
    <updated>2014-08-21T21:28:27Z</updated>
    <published>2011-03-10T23:25:45Z</published>
    <title>Improved space-time tradeoffs for approximate full-text indexing with
  one edit error</title>
    <summary>  In this paper we are interested in indexing texts for substring matching
queries with one edit error. That is, given a text $T$ of $n$ characters over
an alphabet of size $\sigma$, we are asked to build a data structure that
answers the following query: find all the $occ$ substrings of the text that are
at edit distance at most $1$ from a given string $q$ of length $m$. In this
paper we show two new results for this problem. The first result, suitable for
an unbounded alphabet, uses $O(n\log^\epsilon n)$ (where $\epsilon$ is any
constant such that $0&lt;\epsilon&lt;1$) words of space and answers to queries in
time $O(m+occ)$. This improves simultaneously in space and time over the result
of Cole et al. The second result, suitable only for a constant alphabet, relies
on compressed text indices and comes in two variants: the first variant uses
$O(n\log^{\epsilon} n)$ bits of space and answers to queries in time
$O(m+occ)$, while the second variant uses $O(n\log\log n)$ bits of space and
answers to queries in time $O((m+occ)\log\log n)$. This second result improves
on the previously best results for constant alphabets achieved in Lam et al.
(Algorithmica 2008) and Chan et al. (Algorithmica 2010).
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in a journal (28 pages)</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.2167v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.2167v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1209.5441">
    <id>http://arxiv.org/abs/1209.5441v1</id>
    <updated>2012-09-24T21:57:39Z</updated>
    <published>2012-09-24T21:57:39Z</published>
    <title>Predecessor search with distance-sensitive query time</title>
    <summary>  A predecessor (successor) search finds the largest element $x^-$ smaller than
the input string $x$ (the smallest element $x^+$ larger than or equal to $x$,
respectively) out of a given set $S$; in this paper, we consider the static
case (i.e., $S$ is fixed and does not change over time) and assume that the $n$
elements of $S$ are available for inspection. We present a number of algorithms
that, with a small additional index (usually of O(n log w) bits, where $w$ is
the string length), can answer predecessor/successor queries quickly and with
time bounds that depend on different kinds of distance, improving significantly
several results that appeared in the recent literature. Intuitively, our first
result has a running time that depends on the distance between $x$ and $x^\pm$:
it is especially efficient when the input $x$ is either very close to or very
far from $x^-$ or $x^+$; our second result depends on some global notion of
distance in the set $S$, and is fast when the elements of $S$ are more or less
equally spaced in the universe; finally, for our third result we rely on a
finger (i.e., an element of $S$) to improve upon the first one; its running
time depends on the distance between the input and the finger.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Paolo Boldi</name>
    </author>
    <author>
      <name>Sebastiano Vigna</name>
    </author>
    <link href="http://arxiv.org/abs/1209.5441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.5441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1011.3441">
    <id>http://arxiv.org/abs/1011.3441v2</id>
    <updated>2011-01-14T13:01:03Z</updated>
    <published>2010-11-15T16:37:43Z</published>
    <title>Worst case efficient single and multiple string matching in the Word-RAM
  model</title>
    <summary>  In this paper, we explore worst-case solutions for the problems of single and
multiple matching on strings in the word RAM model with word length w. In the
first problem, we have to build a data structure based on a pattern p of length
m over an alphabet of size sigma such that we can answer to the following
query: given a text T of length n, where each character is encoded using
log(sigma) bits return the positions of all the occurrences of p in T (in the
following we refer by occ to the number of reported occurrences). For the
multi-pattern matching problem we have a set S of d patterns of total length m
and a query on a text T consists in finding all positions of all occurrences in
T of the patterns in S. As each character of the text is encoded using log
sigma bits and we can read w bits in constant time in the RAM model, we assume
that we can read up to (w/log sigma) consecutive characters of the text in one
time step. This implies that the fastest possible query time for both problems
is O((n(log sigma/w)+occ). In this paper we present several different results
for both problems which come close to that best possible query time. We first
present two different linear space data structures for the first and second
problem: the first one answers to single pattern matching queries in time
O(n(1/m+log sigma/w)+occ) while the second one answers to multiple pattern
matching queries to O(n((log d+log y+log log d)/y+log sigma/w)+occ) where y is
the length of the shortest pattern in the case of multiple pattern-matching. We
then show how a simple application of the four russian technique permits to get
data structures with query times independent of the length of the shortest
pattern (the length of the only pattern in case of single string matching) at
the expense of using more space.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-19222-7_10</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-19222-7_10" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of an extended abstract presented at IWOCA 2010
  conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.3441v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.3441v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1001.2860">
    <id>http://arxiv.org/abs/1001.2860v2</id>
    <updated>2010-02-14T21:06:23Z</updated>
    <published>2010-01-16T22:10:57Z</published>
    <title>Succinct Dictionary Matching With No Slowdown</title>
    <summary>  The problem of dictionary matching is a classical problem in string matching:
given a set S of d strings of total length n characters over an (not
necessarily constant) alphabet of size sigma, build a data structure so that we
can match in a any text T all occurrences of strings belonging to S. The
classical solution for this problem is the Aho-Corasick automaton which finds
all occ occurrences in a text T in time O(|T| + occ) using a data structure
that occupies O(m log m) bits of space where m &lt;= n + 1 is the number of states
in the automaton. In this paper we show that the Aho-Corasick automaton can be
represented in just m(log sigma + O(1)) + O(d log(n/d)) bits of space while
still maintaining the ability to answer to queries in O(|T| + occ) time. To the
best of our knowledge, the currently fastest succinct data structure for the
dictionary matching problem uses space O(n log sigma) while answering queries
in O(|T|log log n + occ) time. In this paper we also show how the space
occupancy can be reduced to m(H0 + O(1)) + O(d log(n/d)) where H0 is the
empirical entropy of the characters appearing in the trie representation of the
set S, provided that sigma &lt; m^epsilon for any constant 0 &lt; epsilon &lt; 1. The
query time remains unchanged.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-13509-5_9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-13509-5_9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected typos and other minor errors</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.2860v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.2860v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1211.5353">
    <id>http://arxiv.org/abs/1211.5353v1</id>
    <updated>2012-11-22T18:58:27Z</updated>
    <published>2012-11-22T18:58:27Z</published>
    <title>Faster Compact Top-k Document Retrieval</title>
    <summary>  An optimal index solving top-k document retrieval [Navarro and Nekrich,
SODA12] takes O(m + k) time for a pattern of length m, but its space is at
least 80n bytes for a collection of n symbols. We reduce it to 1.5n to 3n
bytes, with O(m+(k+log log n) log log n) time, on typical texts. The index is
up to 25 times faster than the best previous compressed solutions, and requires
at most 5% more space in practice (and in some cases as little as one half).
Apart from replacing classical by compressed data structures, our main idea is
to replace suffix tree sampling by frequency thresholding to achieve
compression.
</summary>
    <author>
      <name>Roberto Konow</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.5353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1207.5425">
    <id>http://arxiv.org/abs/1207.5425v1</id>
    <updated>2012-07-23T15:34:39Z</updated>
    <published>2012-07-23T15:34:39Z</published>
    <title>Ranked Document Retrieval in (Almost) No Space</title>
    <summary>  Ranked document retrieval is a fundamental task in search engines. Such
queries are solved with inverted indexes that require additional 45%-80% of the
compressed text space, and take tens to hundreds of microseconds per query. In
this paper we show how ranked document retrieval queries can be solved within
tens of milliseconds using essentially no extra space over an in-memory
compressed representation of the document collection. More precisely, we
enhance wavelet trees on bytecodes (WTBCs), a data structure that rearranges
the bytes of the compressed collection, so that they support ranked conjunctive
and disjunctive queries, using just 6%-18% of the compressed text space.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Ana Cerdeira-Pena</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Oscar Pedreira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extended version of the paper that will appear in Proc. of
  SPIRE'2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.5425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.5425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1206.6982">
    <id>http://arxiv.org/abs/1206.6982v2</id>
    <updated>2013-02-01T16:15:43Z</updated>
    <published>2012-06-29T10:43:34Z</published>
    <title>Optimal Dynamic Sequence Representations</title>
    <summary>  We describe a data structure that supports access, rank and select queries,
as well as symbol insertions and deletions, on a string $S[1,n]$ over alphabet
$[1..\sigma]$ in time $O(\lg n/\lg\lg n)$, which is optimal even on binary
sequences and in the amortized sense. Our time is worst-case for the queries
and amortized for the updates. This complexity is better than the best previous
ones by a $\Theta(1+\lg\sigma/\lg\lg n)$ factor. We also design a variant where
times are worst-case, yet rank and updates take $O(\lg n)$ time. Our structure
uses $nH_0(S)+o(n\lg\sigma) + O(\sigma\lg n)$ bits, where $H_0(S)$ is the
zero-order entropy of $S$. Finally, we pursue various extensions and
applications of the result.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/1206.6982v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6982v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1204.4509">
    <id>http://arxiv.org/abs/1204.4509v2</id>
    <updated>2012-04-25T02:18:05Z</updated>
    <published>2012-04-20T00:54:14Z</published>
    <title>Sorted Range Reporting</title>
    <summary>  In this paper we consider a variant of the orthogonal range reporting problem
when all points should be reported in the sorted order of their
$x$-coordinates. We show that reporting two-dimensional points with this
additional condition can be organized (almost) as efficiently as the standard
range reporting.
  Moreover, our results generalize and improve the previously known results for
the orthogonal range successor problem and can be used to obtain better
solutions for some stringology problems.
</summary>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1204.4509v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.4509v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1204.2034">
    <id>http://arxiv.org/abs/1204.2034v1</id>
    <updated>2012-04-10T02:58:52Z</updated>
    <published>2012-04-10T02:58:52Z</published>
    <title>Adaptive Techniques to find Optimal Planar Boxes</title>
    <summary>  Given a set $P$ of $n$ planar points, two axes and a real-valued score
function $f()$ on subsets of $P$, the Optimal Planar Box problem consists in
finding a box (i.e. axis-aligned rectangle) $H$ maximizing $f(H\cap P)$. We
consider the case where $f()$ is monotone decomposable, i.e. there exists a
composition function $g()$ monotone in its two arguments such that
$f(A)=g(f(A_1),f(A_2))$ for every subset $A\subseteq P$ and every partition
$\{A_1,A_2\}$ of $A$. In this context we propose a solution for the Optimal
Planar Box problem which performs in the worst case $O(n^2\lg n)$ score
compositions and coordinate comparisons, and much less on other classes of
instances defined by various measures of difficulty. A side result of its own
interest is a fully dynamic \textit{MCS Splay tree} data structure supporting
insertions and deletions with the \emph{dynamic finger} property, improving
upon previous results [Cort\'es et al., J.Alg. 2009].
</summary>
    <author>
      <name>J. Barbay</name>
    </author>
    <author>
      <name>G. Navarro</name>
    </author>
    <author>
      <name>P. Pérez-Lantero</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1201.3602">
    <id>http://arxiv.org/abs/1201.3602v1</id>
    <updated>2012-01-17T19:57:11Z</updated>
    <published>2012-01-17T19:57:11Z</published>
    <title>Compact Binary Relation Representations with Rich Functionality</title>
    <summary>  Binary relations are an important abstraction arising in many data
representation problems. The data structures proposed so far to represent them
support just a few basic operations required to fit one particular application.
We identify many of those operations arising in applications and generalize
them into a wide set of desirable queries for a binary relation representation.
We also identify reductions among those operations. We then introduce several
novel binary relation representations, some simple and some quite
sophisticated, that not only are space-efficient but also efficiently support a
large subset of the desired queries.
</summary>
    <author>
      <name>Jérémy Barbay</name>
    </author>
    <author>
      <name>Francisco Claude</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.3602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.3602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1311.4394">
    <id>http://arxiv.org/abs/1311.4394v1</id>
    <updated>2013-11-18T14:29:58Z</updated>
    <published>2013-11-18T14:29:58Z</published>
    <title>Encoding Range Minimum Queries</title>
    <summary>  We consider the problem of encoding range minimum queries (RMQs): given an
array A[1..n] of distinct totally ordered values, to pre-process A and create a
data structure that can answer the query RMQ(i,j), which returns the index
containing the smallest element in A[i..j], without access to the array A at
query time. We give a data structure whose space usage is 2n + o(n) bits, which
is asymptotically optimal for worst-case data, and answers RMQs in O(1)
worst-case time. This matches the previous result of Fischer and Heun [SICOMP,
2011], but is obtained in a more natural way. Furthermore, our result can
encode the RMQs of a random array A in 1.919n + o(n) bits in expectation, which
is not known to hold for Fischer and Heun's result. We then generalize our
result to the encoding range top-2 query (RT2Q) problem, which is like the
encoding RMQ problem except that the query RT2Q(i,j) returns the indices of
both the smallest and second-smallest elements of A[i..j]. We introduce a data
structure using 3.272n+o(n) bits that answers RT2Qs in constant time, and also
give lower bounds on the effective entropy} of RT2Q.
</summary>
    <author>
      <name>Pooya Davoodi</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <author>
      <name>S. Srinivasa Rao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.4394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.4394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.6789">
    <id>http://arxiv.org/abs/1307.6789v2</id>
    <updated>2013-07-31T23:59:56Z</updated>
    <published>2013-07-25T15:26:38Z</published>
    <title>Optimal Top-k Document Retrieval</title>
    <summary>  Let $\mathcal{D}$ be a collection of $D$ documents, which are strings over an
alphabet of size $\sigma$, of total length $n$. We describe a data structure
that uses linear space and and reports $k$ most relevant documents that contain
a query pattern $P$, which is a string of length $p$, in time $O(p/\log_\sigma
n+k)$, which is optimal in the RAM model in the general case where $\lg D =
\Theta(\log n)$, and involves a novel RAM-optimal suffix tree search. Our
construction supports an ample set of important relevance measures... [clip]
  When $\lg D = o(\log n)$, we show how to reduce the space of the data
structure from $O(n\log n)$ to $O(n(\log\sigma+\log D+\log\log n))$ bits...
[clip]
  We also consider the dynamic scenario, where documents can be inserted and
deleted from the collection. We obtain linear space and query time
$O(p(\log\log n)^2/\log_\sigma n+\log n + k\log\log k)$, whereas insertions and
deletions require $O(\log^{1+\epsilon} n)$ time per symbol, for any constant
$\epsilon>0$.
  Finally, we consider an extended static scenario where an extra parameter
$par(P,d)$ is defined, and the query must retrieve only documents $d$ such that
$par(P,d)\in [\tau_1,\tau_2]$, where this range is specified at query time. We
solve these queries using linear space and $O(p/\log_\sigma n +
\log^{1+\epsilon} n + k\log^\epsilon n)$ time, for any constant $\epsilon>0$.
  Our technique is to translate these top-$k$ problems into multidimensional
geometric search problems. As an additional bonus, we describe some
improvements to those problems.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/1307.6789v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6789v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1405.1220">
    <id>http://arxiv.org/abs/1405.1220v1</id>
    <updated>2014-05-06T10:36:07Z</updated>
    <published>2014-05-06T10:36:07Z</published>
    <title>Efficient Compressed Wavelet Trees over Large Alphabets</title>
    <summary>  The {\em wavelet tree} is a flexible data structure that permits representing
sequences $S[1,n]$ of symbols over an alphabet of size $\sigma$, within
compressed space and supporting a wide range of operations on $S$. When
$\sigma$ is significant compared to $n$, current wavelet tree representations
incur in noticeable space or time overheads. In this article we introduce the
{\em wavelet matrix}, an alternative representation for large alphabets that
retains all the properties of wavelet trees but is significantly faster. We
also show how the wavelet matrix can be compressed up to the zero-order entropy
of the sequence without sacrificing, and actually improving, its time
performance. Our experimental results show that the wavelet matrix outperforms
all the wavelet tree variants along the space/time tradeoff map.
</summary>
    <author>
      <name>Francisco Claude</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Alberto Ordóñez</name>
    </author>
    <link href="http://arxiv.org/abs/1405.1220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1404.4909">
    <id>http://arxiv.org/abs/1404.4909v2</id>
    <updated>2014-06-30T21:19:40Z</updated>
    <published>2014-04-19T01:09:10Z</published>
    <title>Document Retrieval on Repetitive Collections</title>
    <summary>  Document retrieval aims at finding the most important documents where a
pattern appears in a collection of strings. Traditional pattern-matching
techniques yield brute-force document retrieval solutions, which has motivated
the research on tailored indexes that offer near-optimal performance. However,
an experimental study establishing which alternatives are actually better than
brute force, and which perform best depending on the collection
characteristics, has not been carried out. In this paper we address this
shortcoming by exploring the relationship between the nature of the underlying
collection and the performance of current methods. Via extensive experiments we
show that established solutions are often beaten in practice by brute-force
alternatives. We also design new methods that offer superior time/space
trade-offs, particularly on repetitive collections.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Jouni Sirén</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ESA 2014. Implementation and experiments at
  http://www.cs.helsinki.fi/group/suds/rlcsa/</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.4909v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4909v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0907.2089">
    <id>http://arxiv.org/abs/0907.2089v2</id>
    <updated>2011-10-05T11:09:37Z</updated>
    <published>2009-07-13T03:19:23Z</published>
    <title>Fast In-Memory XPath Search over Compressed Text and Tree Indexes</title>
    <summary>  A large fraction of an XML document typically consists of text data. The
XPath query language allows text search via the equal, contains, and
starts-with predicates. Such predicates can efficiently be implemented using a
compressed self-index of the document's text nodes. Most queries, however,
contain some parts of querying the text of the document, plus some parts of
querying the tree structure. It is therefore a challenge to choose an
appropriate evaluation order for a given query, which optimally leverages the
execution speeds of the text and tree indexes. Here the SXSI system is
introduced; it stores the tree structure of an XML document using a bit array
of opening and closing brackets, and stores the text nodes of the document
using a global compressed self-index. On top of these indexes sits an XPath
query engine that is based on tree automata. The engine uses fast counting
queries of the text index in order to dynamically determine whether to evaluate
top-down or bottom-up with respect to the tree structure. The resulting system
has several advantages over existing systems: (1) on pure tree queries (without
text search) such as the XPathMark queries, the SXSI system performs on par or
better than the fastest known systems MonetDB and Qizx, (2) on queries that use
text search, SXSI outperforms the existing systems by 1--3 orders of magnitude
(depending on the size of the result set), and (3) with respect to memory
consumption, SXSI outperforms all other systems for counting-only queries.
</summary>
    <author>
      <name>A. Arroyuelo</name>
    </author>
    <author>
      <name>F. Claude</name>
    </author>
    <author>
      <name>S. Maneth</name>
    </author>
    <author>
      <name>V. Mäkinen</name>
    </author>
    <author>
      <name>G. Navarro</name>
    </author>
    <author>
      <name>K. Nguyen</name>
    </author>
    <author>
      <name>J. Siren</name>
    </author>
    <author>
      <name>N. Välimäki</name>
    </author>
    <link href="http://arxiv.org/abs/0907.2089v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.2089v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0911.3318">
    <id>http://arxiv.org/abs/0911.3318v1</id>
    <updated>2009-11-17T14:39:36Z</updated>
    <published>2009-11-17T14:39:36Z</published>
    <title>Re-Pair Compression of Inverted Lists</title>
    <summary>  Compression of inverted lists with methods that support fast intersection
operations is an active research topic. Most compression schemes rely on
encoding differences between consecutive positions with techniques that favor
small numbers. In this paper we explore a completely different alternative: We
use Re-Pair compression of those differences. While Re-Pair by itself offers
fast decompression at arbitrary positions in main and secondary memory, we
introduce variants that in addition speed up the operations required for
inverted list intersection. We compare the resulting data structures with
several recent proposals under various list intersection algorithms, to
conclude that our Re-Pair variants offer an interesting time/space tradeoff for
this problem, yet further improvements are required for it to improve upon the
state of the art.
</summary>
    <author>
      <name>Francisco Claude</name>
    </author>
    <author>
      <name>Antonio Farina</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/0911.3318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.3318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0905.0768">
    <id>http://arxiv.org/abs/0905.0768v5</id>
    <updated>2010-09-24T01:49:11Z</updated>
    <published>2009-05-06T07:56:38Z</published>
    <title>Fully-Functional Static and Dynamic Succinct Trees</title>
    <summary>  We propose new succinct representations of ordinal trees, which have been
studied extensively. It is known that any $n$-node static tree can be
represented in $2n + o(n)$ bits and a number of operations on the tree can be
supported in constant time under the word-RAM model. However the data
structures are complicated and difficult to dynamize. We propose a simple and
flexible data structure, called the range min-max tree, that reduces the large
number of relevant tree operations considered in the literature to a few
primitives that are carried out in constant time on sufficiently small trees.
The result is extended to trees of arbitrary size, achieving $2n + O(n
/\polylog(n))$ bits of space. The redundancy is significantly lower than any
previous proposal. Our data structure builds on the range min-max tree to
achieve $2n+O(n/\log n)$ bits of space and $O(\log n)$ time for all the
operations. We also propose an improved data structure using $2n+O(n\log\log
n/\log n)$ bits and improving the time to the optimal $O(\log n/\log \log n)$
for most operations. Furthermore, we support sophisticated operations that
allow attaching and detaching whole subtrees, in time $\Order(\log^{1+\epsilon}
n / \log\log n)$. Our techniques are of independent interest. One allows
representing dynamic bitmaps and sequences supporting rank/select and indels,
within zero-order entropy bounds and optimal time $O(\log n / \log\log n)$ for
all operations on bitmaps and polylog-sized alphabets, and $O(\log n \log
\sigma / (\log\log n)^2)$ on larger alphabet sizes $\sigma$. This improves upon
the best existing bounds for entropy-bounded storage of dynamic sequences,
compressed full-text self-indexes, and compressed-space construction of the
Burrows-Wheeler transform.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <link href="http://arxiv.org/abs/0905.0768v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.0768v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0902.1038">
    <id>http://arxiv.org/abs/0902.1038v1</id>
    <updated>2009-02-06T09:48:32Z</updated>
    <published>2009-02-06T09:48:32Z</published>
    <title>Compressed Representations of Permutations, and Applications</title>
    <summary>  We explore various techniques to compress a permutation $\pi$ over n
integers, taking advantage of ordered subsequences in $\pi$, while supporting
its application $\pi$(i) and the application of its inverse $\pi^{-1}(i)$ in
small time. Our compression schemes yield several interesting byproducts, in
many cases matching, improving or extending the best existing results on
applications such as the encoding of a permutation in order to support iterated
applications $\pi^k(i)$ of it, of integer functions, and of inverted lists and
suffix arrays.
</summary>
    <author>
      <name>Jérémy Barbay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DCC</arxiv:affiliation>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DCC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">STACS 2009 (2009) 111-122</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0902.1038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.1038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1111.4395">
    <id>http://arxiv.org/abs/1111.4395v1</id>
    <updated>2011-11-18T15:30:49Z</updated>
    <published>2011-11-18T15:30:49Z</published>
    <title>Practical Top-K Document Retrieval in Reduced Space</title>
    <summary>  Supporting top-k document retrieval queries on general text databases, that
is, finding the k documents where a given pattern occurs most frequently, has
become a topic of interest with practical applications. While the problem has
been solved in optimal time and linear space, the actual space usage is a
serious concern. In this paper we study various reduced-space structures that
support top-k retrieval and propose new alternatives. Our experimental results
show that our novel algorithms and data structures dominate almost all the
space/time tradeoff.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Daniel Valenzuela</name>
    </author>
    <link href="http://arxiv.org/abs/1111.4395v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.4395v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1112.4578">
    <id>http://arxiv.org/abs/1112.4578v1</id>
    <updated>2011-12-20T06:09:35Z</updated>
    <published>2011-12-20T06:09:35Z</published>
    <title>Self-Index based on LZ77 (thesis)</title>
    <summary>  Domains like bioinformatics, version control systems, collaborative editing
systems (wiki), and others, are producing huge data collections that are very
repetitive. That is, there are few differences between the elements of the
collection. This fact makes the compressibility of the collection extremely
high. For example, a collection with all different versions of a Wikipedia
article can be compressed up to the 0.1% of its original space, using the
Lempel-Ziv 1977 (LZ77) compression scheme.
  Many of these repetitive collections handle huge amounts of text data. For
that reason, we require a method to store them efficiently, while providing the
ability to operate on them. The most common operations are the extraction of
random portions of the collection and the search for all the occurrences of a
given pattern inside the whole collection.
  A self-index is a data structure that stores a text in compressed form and
allows to find the occurrences of a pattern efficiently. On the other hand,
self-indexes can extract any substring of the collection, hence they are able
to replace the original text. One of the main goals when using these indexes is
to store them within main memory.
  In this thesis we present a scheme for random text extraction from text
compressed with a Lempel-Ziv parsing. Additionally, we present a variant of
LZ77, called LZ-End, that efficiently extracts text using space close to that
of LZ77.
  The main contribution of this thesis is the first self-index based on
LZ77/LZ-End and oriented to repetitive texts, which outperforms the state of
the art (the RLCSA self-index) in many aspects. Finally, we present a corpus of
repetitive texts, coming from several application domains. We aim at providing
a standard set of texts for research and experimentation, hence this corpus is
publicly available.
</summary>
    <author>
      <name>Sebastian Kreft</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">advisor</arxiv:affiliation>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">advisor</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1112.4578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.4578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1106.4649">
    <id>http://arxiv.org/abs/1106.4649v2</id>
    <updated>2012-03-31T21:41:24Z</updated>
    <published>2011-06-23T08:15:24Z</published>
    <title>Space-Efficient Data-Analysis Queries on Grids</title>
    <summary>  We consider various data-analysis queries on two-dimensional points. We give
new space/time tradeoffs over previous work on geometric queries such as
dominance and rectangle visibility, and on semigroup and group queries such as
sum, average, variance, minimum and maximum. We also introduce new solutions to
queries less frequently considered in the literature such as two-dimensional
quantiles, majorities, successor/predecessor, mode, and various top-$k$
queries, considering static and dynamic scenarios.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Luís M. S. Russo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 2 figures, submitting</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.4649v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.4649v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1110.4493">
    <id>http://arxiv.org/abs/1110.4493v1</id>
    <updated>2011-10-20T11:03:07Z</updated>
    <published>2011-10-20T11:03:07Z</published>
    <title>Improved Grammar-Based Compressed Indexes</title>
    <summary>  We introduce the first grammar-compressed representation of a sequence that
supports searches in time that depends only logarithmically on the size of the
grammar. Given a text $T[1..u]$ that is represented by a (context-free) grammar
of $n$ (terminal and nonterminal) symbols and size $N$ (measured as the sum of
the lengths of the right hands of the rules), a basic grammar-based
representation of $T$ takes $N\lg n$ bits of space. Our representation requires
$2N\lg n + N\lg u + \epsilon\, n\lg n + o(N\lg n)$ bits of space, for any
$0&lt;\epsilon \le 1$. It can find the positions of the $occ$ occurrences of a
pattern of length $m$ in $T$ in $O((m^2/\epsilon)\lg (\frac{\lg u}{\lg n})
+occ\lg n)$ time, and extract any substring of length $\ell$ of $T$ in time
$O(\ell+h\lg(N/h))$, where $h$ is the height of the grammar tree.
</summary>
    <author>
      <name>Francisco Claude</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1110.4493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.4493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1108.4408">
    <id>http://arxiv.org/abs/1108.4408v1</id>
    <updated>2011-08-22T19:59:03Z</updated>
    <published>2011-08-22T19:59:03Z</published>
    <title>On Compressing Permutations and Adaptive Sorting</title>
    <summary>  Previous compact representations of permutations have focused on adding a
small index on top of the plain data $&lt;\pi(1), \pi(2),...\pi(n)>$, in order to
efficiently support the application of the inverse or the iterated permutation.
  In this paper we initiate the study of techniques that exploit the
compressibility of the data itself, while retaining efficient computation of
$\pi(i)$ and its inverse.
  In particular, we focus on exploiting {\em runs}, which are subsets
(contiguous or not) of the domain where the permutation is monotonic.
  Several variants of those types of runs arise in real applications such as
inverted indexes and suffix arrays.
  Furthermore, our improved results on compressed data structures for
permutations also yield better adaptive sorting algorithms.
</summary>
    <author>
      <name>Jérémy Barbay</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1108.4408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.4408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1101.5506">
    <id>http://arxiv.org/abs/1101.5506v1</id>
    <updated>2011-01-28T11:08:46Z</updated>
    <published>2011-01-28T11:08:46Z</published>
    <title>Compressed String Dictionaries</title>
    <summary>  The problem of storing a set of strings --- a string dictionary --- in
compact form appears naturally in many cases. While classically it has
represented a small part of the whole data to be processed (e.g., for Natural
Language processing or for indexing text collections), more recent applications
in Web engines, Web mining, RDF graphs, Internet routing, Bioinformatics, and
many others, make use of very large string dictionaries, whose size is a
significant fraction of the whole data. Thus novel approaches to compress them
efficiently are necessary. In this paper we experimentally compare time and
space performance of some existing alternatives, as well as new ones we
propose. We show that space reductions of up to 20% of the original size of the
strings is possible while supporting fast dictionary searches.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Rodrigo Cánovas</name>
    </author>
    <author>
      <name>Miguel A. Martínez-Prieto</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1101.5506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.5506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1106.2587">
    <id>http://arxiv.org/abs/1106.2587v2</id>
    <updated>2011-12-09T03:26:13Z</updated>
    <published>2011-06-14T00:53:40Z</published>
    <title>Relative Lempel-Ziv Factorization for Efficient Storage and Retrieval of
  Web Collections</title>
    <summary>  Compression techniques that support fast random access are a core component
of any information system. Current state-of-the-art methods group documents
into fixed-sized blocks and compress each block with a general-purpose adaptive
algorithm such as GZIP. Random access to a specific document then requires
decompression of a block. The choice of block size is critical: it trades
between compression effectiveness and document retrieval times. In this paper
we present a scalable compression method for large document collections that
allows fast random access. We build a representative sample of the collection
and use it as a dictionary in a LZ77-like encoding of the rest of the
collection, relative to the dictionary. We demonstrate on large collections,
that using a dictionary as small as 0.1% of the collection size, our algorithm
is dramatically faster than previous methods, and in general gives much better
compression.
</summary>
    <author>
      <name>Christopher Hoobin</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Justin Zobel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.
  265-273 (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.2587v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.2587v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1104.3810">
    <id>http://arxiv.org/abs/1104.3810v1</id>
    <updated>2011-04-19T17:26:46Z</updated>
    <published>2011-04-19T17:26:46Z</published>
    <title>Fixed Block Compression Boosting in FM-Indexes</title>
    <summary>  A compressed full-text self-index occupies space close to that of the
compressed text and simultaneously allows fast pattern matching and random
access to the underlying text. Among the best compressed self-indexes, in
theory and in practice, are several members of the FM-index family. In this
paper, we describe new FM-index variants that combine nice theoretical
properties, simple implementation and improved practical performance. Our main
result is a new technique called fixed block compression boosting, which is a
simpler and faster alternative to optimal compression boosting and implicit
compression boosting used in previous FM-indexes.
</summary>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <link href="http://arxiv.org/abs/1104.3810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.3810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1302.4064">
    <id>http://arxiv.org/abs/1302.4064v1</id>
    <updated>2013-02-17T13:06:42Z</updated>
    <published>2013-02-17T13:06:42Z</published>
    <title>Order Preserving Matching</title>
    <summary>  We introduce a new string matching problem called order-preserving matching
on numeric strings where a pattern matches a text if the text contains a
substring whose relative orders coincide with those of the pattern.
Order-preserving matching is applicable to many scenarios such as stock price
analysis and musical melody matching in which the order relations should be
matched instead of the strings themselves. Solving order-preserving matching
has to do with representations of order relations of a numeric string. We
define prefix representation and nearest neighbor representation, which lead to
efficient algorithms for order-preserving matching. We present efficient
algorithms for single and multiple pattern cases. For the single pattern case,
we give an O(n log m) time algorithm and optimize it further to obtain O(n + m
log m) time. For the multiple pattern case, we give an O(n log m) time
algorithm.
</summary>
    <author>
      <name>Jinil Kim</name>
    </author>
    <author>
      <name>Peter Eades</name>
    </author>
    <author>
      <name>Rudolf Fleischer</name>
    </author>
    <author>
      <name>Seok-Hee Hong</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Kunsoo Park</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Takeshi Tokuyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages; submitted to Theoretical Computer Science, 5 Dec 2012;
  presented at Theo Murphy International Scientific Meeting of the Royal
  Society on Storage and Indexing of Massive Data, 7 Feb 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.4064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.4064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1611.08898">
    <id>http://arxiv.org/abs/1611.08898v1</id>
    <updated>2016-11-27T19:44:12Z</updated>
    <published>2016-11-27T19:44:12Z</published>
    <title>On the Size of Lempel-Ziv and Lyndon Factorizations</title>
    <summary>  Lyndon factorization and Lempel-Ziv (LZ) factorization are both important
tools for analysing the structure and complexity of strings, but their
combinatorial structure is very different. In this paper, we establish the
first direct connection between the two by showing that while the Lyndon
factorization can be bigger than the non-overlapping LZ factorization (which we
demonstrate by describing a new, non-trivial family of strings) it is never
more than twice the size.
</summary>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Arseny M. Shur</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.STACS.2017.45</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.STACS.2017.45" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.08898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1606.04573">
    <id>http://arxiv.org/abs/1606.04573v2</id>
    <updated>2017-02-23T21:32:38Z</updated>
    <published>2016-06-14T21:21:17Z</published>
    <title>String Inference from the LCP Array</title>
    <summary>  The suffix array, perhaps the most important data structure in modern string
processing, is often augmented with the longest common prefix (LCP) array which
stores the lengths of the LCPs for lexicographically adjacent suffixes of a
string. Together the two arrays are roughly equivalent to the suffix tree with
the LCP array representing the tree shape.
  In order to better understand the combinatorics of LCP arrays, we consider
the problem of inferring a string from an LCP array, i.e., determining whether
a given array of integers is a valid LCP array, and if it is, reconstructing
some string or all strings with that LCP array. There are recent studies of
inferring a string from a suffix tree shape but using significantly more
information (in the form of suffix links) than is available in the LCP array.
  We provide two main results. (1) We describe two algorithms for inferring
strings from an LCP array when we allow a generalized form of LCP array defined
for a multiset of cyclic strings: a linear time algorithm for binary alphabet
and a general algorithm with polynomial time complexity for a constant alphabet
size. (2) We prove that determining whether a given integer array is a valid
LCP array is NP-complete when we require more restricted forms of LCP array
defined for a single cyclic or non-cyclic string or a multiset of non-cyclic
strings. The result holds whether or not the alphabet is restricted to be
binary. In combination, the two results show that the generalized form of LCP
array for a multiset of cyclic strings is fundamentally different from the
other more restricted forms.
</summary>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Marcin Piątkowski</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added algorithm for general alphabets</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04573v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04573v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; G.2.1; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1711.02910">
    <id>http://arxiv.org/abs/1711.02910v3</id>
    <updated>2018-02-26T14:19:58Z</updated>
    <published>2017-11-08T12:01:47Z</published>
    <title>Run Compressed Rank/Select for Large Alphabets</title>
    <summary>  Given a string of length $n$ that is composed of $r$ runs of letters from the
alphabet $\{0,1,\ldots,\sigma{-}1\}$ such that $2 \le \sigma \le r$, we
describe a data structure that, provided $r \le n / \log^{\omega(1)} n$, stores
the string in $r\log\frac{n\sigma}{r} + o(r\log\frac{n\sigma}{r})$ bits and
supports select and access queries in $O(\log\frac{\log(n/r)}{\log\log n})$
time and rank queries in $O(\log\frac{\log(n\sigma/r)}{\log\log n})$ time. We
show that $r\log\frac{n(\sigma-1)}{r} - O(\log\frac{n}{r})$ bits are necessary
for any such data structure and, thus, our solution is succinct. We also
describe a data structure that uses $(1 + \epsilon)r\log\frac{n\sigma}{r} +
O(r)$ bits, where $\epsilon > 0$ is an arbitrary constant, with the same query
times but without the restriction $r \le n / \log^{\omega(1)} n$. By simple
reductions to the colored predecessor problem, we show that the query times are
optimal in the important case $r \ge 2^{\log^\delta n}$, for an arbitrary
constant $\delta > 0$. We implement our solution and compare it with the state
of the art, showing that the closest competitors consume 31-46% more space.
</summary>
    <author>
      <name>José Fuentes-Sepúlveda</name>
    </author>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. 10 pages, 1 figure, 4
  tables; published in DCC'2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.02910v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02910v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1704.05682">
    <id>http://arxiv.org/abs/1704.05682v1</id>
    <updated>2017-04-19T10:45:05Z</updated>
    <published>2017-04-19T10:45:05Z</published>
    <title>m-Bonsai: a Practical Compact Dynamic Trie</title>
    <summary>  We consider the problem of implementing a space-efficient dynamic trie, with
an emphasis on good practical performance. For a trie with $n$ nodes with an
alphabet of size $\sigma$, the information-theoretic lower bound is $n \log
\sigma + O(n)$ bits. The Bonsai data structure is a compact trie proposed by
Darragh et al. (Softw., Pract. Exper. 23(3), 1993, p. 277-291). Its
disadvantages include the user having to specify an upper bound $M$ on the trie
size in advance (which cannot be changed easily after initalization), a space
usage of $M \log \sigma + O(M \log \log M)$ (which is asymptotically
non-optimal for smaller $\sigma$ or if $n \ll M$) and a lack of support for
deletions. It supports traversal and update operations in $O(1/\epsilon)$
expected time (based on assumptions about the behaviour of hash functions),
where $\epsilon = (M-n)/M$ and has excellent speed performance in practice. We
propose an alternative, m-Bonsai, that addresses the above problems, obtaining
a trie that uses $(1+\beta) n (\log \sigma + O(1))$ bits in expectation, and
supports traversal and update operations in $O(1/\beta)$ expected time and
$O(1/\beta^2)$ amortized expected time, for any user-specified parameter $\beta
> 0$ (again based on assumptions about the behaviour of hash functions). We
give an implementation of m-Bonsai which uses considerably less memory and is
slightly faster than the original Bonsai.
</summary>
    <author>
      <name>Andreas Poyias</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal version of SPIRE 2015 paper by Poyias and Raman</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.2; E.4; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2201.06492">
    <id>http://arxiv.org/abs/2201.06492v1</id>
    <updated>2022-01-17T16:07:22Z</updated>
    <published>2022-01-17T16:07:22Z</published>
    <title>Linear Time Construction of Indexable Elastic Founder Graphs</title>
    <summary>  Pattern matching on graphs has been widely studied lately due to its
importance in genomics applications. Unfortunately, even the simplest problem
of deciding if a string appears as a subpath of a graph admits a quadratic
lower bound under the Orthogonal Vectors Hypothesis (Equi et al. ICALP 2019,
SOFSEM 2021). To avoid this bottleneck, the research has shifted towards more
specific graph classes, e.g. those induced from multiple sequence alignments
(MSAs). Consider segmenting $\mathsf{MSA}[1..m,1..n]$ into $b$ blocks
$\mathsf{MSA}[1..m,1..j_1]$, $\mathsf{MSA}[1..m,j_1+1..j_2]$, $\ldots$,
$\mathsf{MSA}[1..m,j_{b-1}+1..n]$. The distinct strings in the rows of the
blocks, after the removal of gap symbols, form the nodes of an elastic founder
graph (EFG) where the edges represent the original connections observed in the
MSA. An EFG is called indexable if a node label occurs as a prefix of only
those paths that start from a node of the same block. Equi et al. (ISAAC 2021)
showed that such EFGs support fast pattern matching and gave an $O(mn \log
m)$-time algorithm for preprocessing the MSA in a way that allows the
construction of indexable EFGs maximizing the number of blocks and,
alternatively, minimizing the maximum length of a block, in $O(n)$ and $O(n
\log\log n)$ time respectively. Using the suffix tree and solving a novel
ancestor problem on trees, we improve the preprocessing to $O(mn)$ time and the
$O(n \log \log n)$-time EFG construction to $O(n)$ time, thus showing that both
types of indexable EFGs can be constructed in time linear in the input size.
</summary>
    <author>
      <name>Nicola Rizzo</name>
    </author>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.06492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.06492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.4; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1011.3491">
    <id>http://arxiv.org/abs/1011.3491v2</id>
    <updated>2011-04-02T12:44:53Z</updated>
    <published>2010-11-15T20:21:33Z</published>
    <title>Pattern Kits</title>
    <summary>  Suppose we have just performed searches in a self-index for two patterns $A$
and $B$ and now we want to search for their concatenation \A B); how can we
best make use of our previous computations? In this paper we consider this
problem and, more generally, how we can store a dynamic library of patterns
that we can easily manipulate in interesting ways. We give a space- and
time-efficient data structure for this problem that is compatible with many of
the best self-indexes.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Kalle Karhu</name>
    </author>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <author>
      <name>Leena Salmela</name>
    </author>
    <link href="http://arxiv.org/abs/1011.3491v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.3491v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0712.3360">
    <id>http://arxiv.org/abs/0712.3360v1</id>
    <updated>2007-12-20T10:42:54Z</updated>
    <published>2007-12-20T10:42:54Z</published>
    <title>Compressed Text Indexes:From Theory to Practice!</title>
    <summary>  A compressed full-text self-index represents a text in a compressed form and
still answers queries efficiently. This technology represents a breakthrough
over the text indexing techniques of the previous decade, whose indexes
required several times the size of the text. Although it is relatively new,
this technology has matured up to a point where theoretical research is giving
way to practical developments. Nonetheless this requires significant
programming skills, a deep engineering effort, and a strong algorithmic
background to dig into the research results. To date only isolated
implementations and focused comparisons of compressed indexes have been
reported, and they missed a common API, which prevented their re-use or
deployment within other applications.
  The goal of this paper is to fill this gap. First, we present the existing
implementations of compressed indexes from a practitioner's point of view.
Second, we introduce the Pizza&amp;Chili site, which offers tuned implementations
and a standardized API for the most successful compressed full-text
self-indexes, together with effective testbeds and scripts for their automatic
validation and test. Third, we show the results of our extensive experiments on
these codes with the aim of demonstrating the practical relevance of this novel
and exciting technology.
</summary>
    <author>
      <name>Paolo Ferragina</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dept. of Computer Science, University of Pisa</arxiv:affiliation>
    </author>
    <author>
      <name>Rodrigo Gonzalez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dept. of Computer Science, University of Chile</arxiv:affiliation>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dept. of Computer Science, University of Chile</arxiv:affiliation>
    </author>
    <author>
      <name>Rossano Venturini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dept. of Computer Science, University of Chile</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0712.3360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.3360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; H.2.1; H.3.2; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2006.13673">
    <id>http://arxiv.org/abs/2006.13673v1</id>
    <updated>2020-06-24T12:44:22Z</updated>
    <published>2020-06-24T12:44:22Z</published>
    <title>Improved Circular $k$-Mismatch Sketches</title>
    <summary>  The shift distance $\mathsf{sh}(S_1,S_2)$ between two strings $S_1$ and $S_2$
of the same length is defined as the minimum Hamming distance between $S_1$ and
any rotation (cyclic shift) of $S_2$. We study the problem of sketching the
shift distance, which is the following communication complexity problem:
Strings $S_1$ and $S_2$ of length $n$ are given to two identical players
(encoders), who independently compute sketches (summaries) $\mathtt{sk}(S_1)$
and $\mathtt{sk}(S_2)$, respectively, so that upon receiving the two sketches,
a third player (decoder) is able to compute (or approximate)
$\mathsf{sh}(S_1,S_2)$ with high probability.
  This paper primarily focuses on the more general $k$-mismatch version of the
problem, where the decoder is allowed to declare a failure if
$\mathsf{sh}(S_1,S_2)>k$, where $k$ is a parameter known to all parties. Andoni
et al. (STOC'13) introduced exact circular $k$-mismatch sketches of size
$\widetilde{O}(k+D(n))$, where $D(n)$ is the number of divisors of $n$. Andoni
et al. also showed that their sketch size is optimal in the class of linear
homomorphic sketches.
  We circumvent this lower bound by designing a (non-linear) exact circular
$k$-mismatch sketch of size $\widetilde{O}(k)$; this size matches
communication-complexity lower bounds. We also design $(1\pm
\varepsilon)$-approximate circular $k$-mismatch sketch of size
$\widetilde{O}(\min(\varepsilon^{-2}\sqrt{k}, \varepsilon^{-1.5}\sqrt{n}))$,
which improves upon an $\widetilde{O}(\varepsilon^{-2}\sqrt{n})$-size sketch of
Crouch and McGregor (APPROX'11).
</summary>
    <author>
      <name>Shay Golan</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <author>
      <name>Przemysław Uznański</name>
    </author>
    <link href="http://arxiv.org/abs/2006.13673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.13673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1709.08900">
    <id>http://arxiv.org/abs/1709.08900v5</id>
    <updated>2021-12-21T12:16:50Z</updated>
    <published>2017-09-26T09:09:17Z</published>
    <title>In-Place Initializable Arrays</title>
    <summary>  An initializable array is an array that supports the read and write
operations for any element and the initialization of the entire array. This
paper proposes a simple in-place algorithm to implement an initializable array
of length $N$ containing $\ell \in O(w)$ bits entries in $N \ell +1$ bits on
the word RAM model with $w$ bits word size, i.e., the proposed array requires
only 1 extra bit on top of a normal array of length $N$ containing $\ell$ bits
entries. Our algorithm supports the all three operations in constant worst-case
time, that is, it runs in-place using at most constant number of words $O(w)$
bits during each operation. The time and space complexities are optimal since
it was already proven that there is no implementation of an initializable array
with no extra bit supporting all the operations in constant worst-case time
[Hagerup and Kammer, ISAAC 2017]. Our algorithm significantly improves upon the
best algorithm presented in the earlier studies [Navarro, CSUR 2014] which uses
$N + o(N)$ extra bits to support all the operations in constant worst-case
time.
</summary>
    <author>
      <name>Takashi Katoh</name>
    </author>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2022.03.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2022.03.004" rel="related"/>
    <link href="http://arxiv.org/abs/1709.08900v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08900v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1703.01009">
    <id>http://arxiv.org/abs/1703.01009v5</id>
    <updated>2019-07-13T08:59:02Z</updated>
    <published>2017-03-03T01:30:36Z</published>
    <title>Optimal Time and Space Construction of Suffix Arrays and LCP Arrays for
  Integer Alphabets</title>
    <summary>  Suffix arrays and LCP arrays are one of the most fundamental data structures
widely used for various kinds of string processing. We consider two problems
for a read-only string of length $N$ over an integer alphabet $[1, \dots,
\sigma]$ for $1 \leq \sigma \leq N$, the string contains $\sigma$ distinct
characters, the construction of the suffix array, and a simultaneous
construction of both the suffix array and LCP array. For the word RAM model, we
propose algorithms to solve both of the problems in $O(N)$ time by using $O(1)$
extra words, which are optimal in time and space. Extra words means the
required space except for the space of the input string and output suffix array
and LCP array. Our contribution improves the previous most efficient
algorithms, $O(N)$ time using $\sigma+O(1)$ extra words by [Nong, TOIS 2013]
and $O(N \log N)$ time using $O(1)$ extra words by [Franceschini and
Muthukrishnan, ICALP 2007], for constructing suffix arrays, and it improves the
previous most efficient solution that runs in $O(N)$ time using $\sigma + O(1)$
extra words for constructing both suffix arrays and LCP arrays through a
combination of [Nong, TOIS 2013] and [Manzini, SWAT 2004].
</summary>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <link href="http://arxiv.org/abs/1703.01009v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01009v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.08015">
    <id>http://arxiv.org/abs/2004.08015v1</id>
    <updated>2020-04-17T01:22:33Z</updated>
    <published>2020-04-17T01:22:33Z</published>
    <title>Efficient Constrained Pattern Mining Using Dynamic Item Ordering for
  Explainable Classification</title>
    <summary>  Learning of interpretable classification models has been attracting much
attention for the last few years. Discovery of succinct and contrasting
patterns that can highlight the differences between the two classes is very
important. Such patterns are useful for human experts, and can be used to
construct powerful classifiers. In this paper, we consider mining of minimal
emerging patterns from high-dimensional data sets under a variety of
constraints in a supervised setting. We focus on an extension in which patterns
can contain negative items that designate the absence of an item. In such a
case, a database becomes highly dense, and it makes mining more challenging
since popular pattern mining techniques such as fp-tree and occurrence deliver
do not efficiently work. To cope with this difficulty, we present an efficient
algorithm for mining minimal emerging patterns by combining two techniques:
dynamic variable-ordering during pattern search for enhancing pruning effect,
and the use of a pointer-based dynamic data structure, called dancing links,
for efficiently maintaining occurrence lists. Experiments on benchmark data
sets showed that our algorithm achieves significant speed-ups over emerging
pattern mining approach based on LCM, a very fast depth-first frequent itemset
miner using static variable-ordering.
</summary>
    <author>
      <name>Hiroaki Iwashita</name>
    </author>
    <author>
      <name>Takuya Takagi</name>
    </author>
    <author>
      <name>Hirofumi Suzuki</name>
    </author>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Kotaro Ohori</name>
    </author>
    <author>
      <name>Hiroki Arimura</name>
    </author>
    <link href="http://arxiv.org/abs/2004.08015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.08015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1611.05359">
    <id>http://arxiv.org/abs/1611.05359v2</id>
    <updated>2016-11-21T01:36:09Z</updated>
    <published>2016-11-16T16:54:04Z</published>
    <title>Longest Common Extensions with Recompression</title>
    <summary>  Given two positions $i$ and $j$ in a string $T$ of length $N$, a longest
common extension (LCE) query asks for the length of the longest common prefix
between suffixes beginning at $i$ and $j$. A compressed LCE data structure is a
data structure that stores $T$ in a compressed form while supporting fast LCE
queries. In this article we show that the recompression technique is a powerful
tool for compressed LCE data structures. We present a new compressed LCE data
structure of size $O(z \lg (N/z))$ that supports LCE queries in $O(\lg N)$
time, where $z$ is the size of Lempel-Ziv 77 factorization without
self-reference of $T$. Given $T$ as an uncompressed form, we show how to build
our data structure in $O(N)$ time and space. Given $T$ as a grammar compressed
form, i.e., an straight-line program of size n generating $T$, we show how to
build our data structure in $O(n \lg (N/n))$ time and $O(n + z \lg (N/z))$
space. Our algorithms are deterministic and always return correct answers.
</summary>
    <author>
      <name>Tomohiro I</name>
    </author>
    <link href="http://arxiv.org/abs/1611.05359v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.05359v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.04446">
    <id>http://arxiv.org/abs/1607.04446v3</id>
    <updated>2016-08-31T02:01:47Z</updated>
    <published>2016-07-15T10:42:20Z</published>
    <title>Online Grammar Compression for Frequent Pattern Discovery</title>
    <summary>  Various grammar compression algorithms have been proposed in the last decade.
A grammar compression is a restricted CFG deriving the string
deterministically. An efficient grammar compression develops a smaller CFG by
finding duplicated patterns and removing them. This process is just a frequent
pattern discovery by grammatical inference. While we can get any frequent
pattern in linear time using a preprocessed string, a huge working space is
required for longer patterns, and the whole string must be loaded into the
memory preliminarily. We propose an online algorithm approximating this problem
within a compressed space. The main contribution is an improvement of the
previously best known approximation ratio $\Omega(\frac{1}{\lg^2m})$ to
$\Omega(\frac{1}{\lg^*N\lg m})$ where $m$ is the length of an optimal pattern
in a string of length $N$ and $\lg^*$ is the iteration of the logarithm base
$2$. For a sufficiently large $N$, $\lg^*N$ is practically constant. The
experimental results show that our algorithm extracts nearly optimal patterns
and achieves a significant improvement in memory consumption compared to the
offline algorithm.
</summary>
    <author>
      <name>Shouhei Fukunaga</name>
    </author>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <author>
      <name>I Tomohiro</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04446v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04446v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.10719">
    <id>http://arxiv.org/abs/1911.10719v2</id>
    <updated>2019-11-28T06:39:01Z</updated>
    <published>2019-11-25T06:35:10Z</published>
    <title>Faster Privacy-Preserving Computation of Edit Distance with Moves</title>
    <summary>  We consider an efficient two-party protocol for securely computing the
similarity of strings w.r.t. an extended edit distance measure. Here, two
parties possessing strings $x$ and $y$, respectively, want to jointly compute
an approximate value for $\mathrm{EDM}(x,y)$, the minimum number of edit
operations including substring moves needed to transform $x$ into $y$, without
revealing any private information. Recently, the first secure two-party
protocol for this was proposed, based on homomorphic encryption, but this
approach is not suitable for long strings due to its high communication and
round complexities. In this paper, we propose an improved algorithm that
significantly reduces the round complexity without sacrificing its
cryptographic strength. We examine the performance of our algorithm for DNA
sequences compared to previous one.
</summary>
    <author>
      <name>Yohei Yoshimoto</name>
    </author>
    <author>
      <name>Masaharu Kataoka</name>
    </author>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Kilho Shin</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in WALCOM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.10719v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.10719v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6, E.3" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6; E.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2202.13590">
    <id>http://arxiv.org/abs/2202.13590v2</id>
    <updated>2022-03-19T06:28:43Z</updated>
    <published>2022-02-28T07:49:07Z</published>
    <title>LCP-dropout: Compression-based Multiple Subword Segmentation for Neural
  Machine Translation</title>
    <summary>  In this study, we propose a simple and effective preprocessing method for
subword segmentation based on a data compression algorithm. Compression-based
subword segmentation has recently attracted significant attention as a
preprocessing method for training data in Neural Machine Translation. Among
them, BPE/BPE-dropout is one of the fastest and most effective method compared
to conventional approaches. However, compression-based approach has a drawback
in that generating multiple segmentations is difficult due to the determinism.
To overcome this difficulty, we focus on a probabilistic string algorithm,
called locally-consistent parsing (LCP), that has been applied to achieve
optimum compression. Employing the probabilistic mechanism of LCP, we propose
LCP-dropout for multiple subword segmentation that improves BPE/BPE-dropout,
and show that it outperforms various baselines in learning from especially
small training data.
</summary>
    <author>
      <name>Keita Nonaka</name>
    </author>
    <author>
      <name>Kazutaka Yamanouchi</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <author>
      <name>Tsuyoshi Okita</name>
    </author>
    <author>
      <name>Kazutaka Shimada</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.13590v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.13590v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2205.12421">
    <id>http://arxiv.org/abs/2205.12421v1</id>
    <updated>2022-05-25T00:25:08Z</updated>
    <published>2022-05-25T00:25:08Z</published>
    <title>Substring Complexities on Run-length Compressed Strings</title>
    <summary>  Let $S_{T}(k)$ denote the set of distinct substrings of length $k$ in a
string $T$, then the $k$-th substring complexity is defined by its cardinality
$|S_{T}(k)|$. Recently, $\delta = \max \{ |S_{T}(k)| / k : k \ge 1 \}$ is shown
to be a good compressibility measure of highly-repetitive strings. In this
paper, given $T$ of length $n$ in the run-length compressed form of size $r$,
we show that $\delta$ can be computed in $\mathit{C}_{\mathsf{sort}}(r, n)$
time and $O(r)$ space, where $\mathit{C}_{\mathsf{sort}}(r, n) = O(\min (r
\lg\lg r, r \lg_{r} n))$ is the time complexity for sorting $r$ $O(\lg n)$-bit
integers in $O(r)$ space in the Word-RAM model with word size $\Omega(\lg n)$.
</summary>
    <author>
      <name>Akiyoshi Kawamoto</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <link href="http://arxiv.org/abs/2205.12421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.12421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1106.3791">
    <id>http://arxiv.org/abs/1106.3791v1</id>
    <updated>2011-06-20T01:10:01Z</updated>
    <published>2011-06-20T01:10:01Z</published>
    <title>Reference Sequence Construction for Relative Compression of Genomes</title>
    <summary>  Relative compression, where a set of similar strings are compressed with
respect to a reference string, is a very effective method of compressing DNA
datasets containing multiple similar sequences. Relative compression is fast to
perform and also supports rapid random access to the underlying data. The main
difficulty of relative compression is in selecting an appropriate reference
sequence. In this paper, we explore using the dictionary of repeats generated
by Comrad, Re-pair and Dna-x algorithms as reference sequences for relative
compression. We show this technique allows better compression and supports
random access just as well. The technique also allows more general repetitive
datasets to be compressed using relative compression.
</summary>
    <author>
      <name>Shanika Kuruppu</name>
    </author>
    <author>
      <name>Simon Puglisi</name>
    </author>
    <author>
      <name>Justin Zobel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures, to appear in the Proceedings of SPIRE2011 as a
  short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.3791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.3791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1609.04618">
    <id>http://arxiv.org/abs/1609.04618v1</id>
    <updated>2016-09-15T13:03:52Z</updated>
    <published>2016-09-15T13:03:52Z</published>
    <title>From H&amp;M to Gap for Lightweight BWT Merging</title>
    <summary>  Recently, Holt and McMillan [Bionformatics 2014, ACM-BCB 2014] have proposed
a simple and elegant algorithm to merge the Burrows-Wheeler transforms of a
family of strings. In this paper we show that the H&amp;M algorithm can be improved
so that, in addition to merging the BWTs, it can also merge the Longest Common
Prefix (LCP) arrays. The new algorithm, called Gap because of how it operates,
has the same asymptotic cost as the H&amp;M algorithm and requires additional space
only for storing the LCP values.
</summary>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1606.05724">
    <id>http://arxiv.org/abs/1606.05724v3</id>
    <updated>2018-12-10T17:10:31Z</updated>
    <published>2016-06-18T07:30:59Z</published>
    <title>A Compact Index for Order-Preserving Pattern Matching</title>
    <summary>  Order-preserving pattern matching was introduced recently but it has already
attracted much attention. Given a reference sequence and a pattern, we want to
locate all substrings of the reference sequence whose elements have the same
relative order as the pattern elements. For this problem we consider the
offline version in which we build an index for the reference sequence so that
subsequent searches can be completed very efficiently. We propose a
space-efficient index that works well in practice despite its lack of good
worst-case time bounds. Our solution is based on the new approach of
decomposing the indexed sequence into an order component, containing ordering
information, and a delta component, containing information on the absolute
values. Experiments show that this approach is viable, faster than the
available alternatives, and it is the first one offering simultaneously small
space usage and fast retrieval.
</summary>
    <author>
      <name>Gianni Decaroli</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages. A preliminary version appeared in the Proc. IEEE Data
  Compression Conference, DCC 2017, Snowbird, UT, USA, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05724v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05724v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; E.4; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1710.10105">
    <id>http://arxiv.org/abs/1710.10105v1</id>
    <updated>2017-10-27T12:37:14Z</updated>
    <published>2017-10-27T12:37:14Z</published>
    <title>Lyndon Array Construction during Burrows-Wheeler Inversion</title>
    <summary>  In this paper we present an algorithm to compute the Lyndon array of a string
$T$ of length $n$ as a byproduct of the inversion of the Burrows-Wheeler
transform of $T$. Our algorithm runs in linear time using only a stack in
addition to the data structures used for Burrows-Wheeler inversion. We compare
our algorithm with two other linear-time algorithms for Lyndon array
construction and show that computing the Burrows-Wheeler transform and then
constructing the Lyndon array is competitive compared to the known approaches.
We also propose a new balanced parenthesis representation for the Lyndon array
that uses $2n+o(n)$ bits of space and supports constant time access. This
representation can be built in linear time using $O(n)$ words of space, or in
$O(n\log n/\log\log n)$ time using asymptotically the same space as $T$.
</summary>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>W. F. Smyth</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Guilherme P. Telles</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jda.2018.08.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jda.2018.08.001" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Discrete Algorithms, 50 (2018), 2-9</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.10105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1712.08573">
    <id>http://arxiv.org/abs/1712.08573v2</id>
    <updated>2018-08-18T07:05:30Z</updated>
    <published>2017-12-22T16:54:53Z</published>
    <title>Longest common substring with approximately $k$ mismatches</title>
    <summary>  In the longest common substring problem, we are given two strings of length
$n$ and must find a substring of maximal length that occurs in both strings. It
is well known that the problem can be solved in linear time, but the solution
is not robust and can vary greatly when the input strings are changed even by
one character. To circumvent this, Leimeister and Morgenstern introduced the
problem of the longest common substring with $k$ mismatches. Lately, this
problem has received a lot of attention in the literature. In this paper, we
first show a conditional lower bound based on the SETH hypothesis implying that
there is little hope to improve existing solutions. We then introduce a new but
closely related problem of the longest common substring with approximately $k$
mismatches and use locality-sensitive hashing to show that it admits a solution
with strongly subquadratic running time. We also apply these results to obtain
a strongly subquadratic-time 2-approximation algorithm for the longest common
substring with $k$ mismatches problem and show conditional hardness of
improving its approximation ratio.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of a paper from CPM 2016 with corrected proofs</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.08573v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08573v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1708.05223">
    <id>http://arxiv.org/abs/1708.05223v2</id>
    <updated>2018-04-09T12:18:28Z</updated>
    <published>2017-08-17T12:13:53Z</published>
    <title>The streaming $k$-mismatch problem</title>
    <summary>  We consider the streaming complexity of a fundamental task in approximate
pattern matching: the $k$-mismatch problem. It asks to compute Hamming
distances between a pattern of length $n$ and all length-$n$ substrings of a
text for which the Hamming distance does not exceed a given threshold $k$. In
our problem formulation, we report not only the Hamming distance but also, on
demand, the full \emph{mismatch information}, that is the list of mismatched
pairs of symbols and their indices. The twin challenges of streaming pattern
matching derive from the need both to achieve small working space and also to
guarantee that every arriving input symbol is processed quickly.
  We present a streaming algorithm for the $k$-mismatch problem which uses
$O(k\log{n}\log\frac{n}{k})$ bits of space and spends \ourcomplexity time on
each symbol of the input stream, which consists of the pattern followed by the
text. The running time almost matches the classic offline solution and the
space usage is within a logarithmic factor of optimal.
  Our new algorithm therefore effectively resolves and also extends an open
problem first posed in FOCS'09. En route to this solution, we also give a
deterministic $O( k (\log \frac{n}{k} + \log |\Sigma|) )$-bit encoding of all
the alignments with Hamming distance at most $k$ of a length-$n$ pattern within
a text of length $O(n)$. This secondary result provides an optimal solution to
a natural communication complexity problem which may be of independent
interest.
</summary>
    <author>
      <name>Raphaël Clifford</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05223v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05223v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32 (Primary) 68W27, 68P30 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; F.2.1; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1704.07625">
    <id>http://arxiv.org/abs/1704.07625v2</id>
    <updated>2017-08-25T12:29:20Z</updated>
    <published>2017-04-25T10:46:49Z</published>
    <title>Indexing Weighted Sequences: Neat and Efficient</title>
    <summary>  In a \emph{weighted sequence}, for every position of the sequence and every
letter of the alphabet a probability of occurrence of this letter at this
position is specified. Weighted sequences are commonly used to represent
imprecise or uncertain data, for example, in molecular biology where they are
known under the name of Position-Weight Matrices. Given a probability threshold
$\frac1z$, we say that a string $P$ of length $m$ occurs in a weighted sequence
$X$ at position $i$ if the product of probabilities of the letters of $P$ at
positions $i,\ldots,i+m-1$ in $X$ is at least $\frac1z$. In this article, we
consider an \emph{indexing} variant of the problem, in which we are to
preprocess a weighted sequence to answer multiple pattern matching queries. We
present an $O(nz)$-time construction of an $O(nz)$-sized index for a weighted
sequence of length $n$ over a constant-sized alphabet that answers pattern
matching queries in optimal, $O(m+Occ)$ time, where $Occ$ is the number of
occurrences reported. The cornerstone of our data structure is a novel
construction of a family of $\lfloor z \rfloor$ special strings that carries
the information about all the strings that occur in the weighted sequence with
a sufficient probability. We obtain a weighted index with the same complexities
as in the most efficient previously known index by Barton et al. (CPM 2016),
but our construction is significantly simpler. The most complex algorithmic
tool required in the basic form of our index is the suffix tree which we use to
develop a new, more straightforward index for the so-called property matching
problem. We provide an implementation of our data structure. Our construction
allows us also to obtain a significant improvement over the complexities of the
approximate variant of the weighted index presented by Biswas et al. (EDBT
2016) and an improvement of the space complexity of their general index.
</summary>
    <author>
      <name>Carl Barton</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Chang Liu</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A new, even simpler version of the index</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.07625v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07625v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1807.11702">
    <id>http://arxiv.org/abs/1807.11702v3</id>
    <updated>2021-06-16T20:03:10Z</updated>
    <published>2018-07-31T08:49:11Z</published>
    <title>Efficient Computation of Sequence Mappability</title>
    <summary>  In the $(k,m)$-mappability problem, for a given sequence $T$ of length $n$,
the goal is to compute a table whose $i$th entry is the number of indices $j
\ne i$ such that the length-$m$ substrings of $T$ starting at positions $i$ and
$j$ have at most $k$ mismatches. Previous works on this problem focused on
heuristics computing a rough approximation of the result or on the case of
$k=1$. We present several efficient algorithms for the general case of the
problem. Our main result is an algorithm that, for $k=\mathcal{O}(1)$, works in
$\mathcal{O}(n)$ space and, with high probability, in $\mathcal{O}(n \cdot
\min\{m^k,\log^k n\})$ time. Our algorithm requires a careful adaptation of the
$k$-errata trees of Cole et al. [STOC 2004] to avoid multiple counting of pairs
of substrings. Our technique can also be applied to solve the all-pairs Hamming
distance problem introduced by Crochemore et al. [WABI 2017]. We further
develop $\mathcal{O}(n^2)$-time algorithms to compute all $(k,m)$-mappability
tables for a fixed $m$ and all $k\in \{0,\ldots,m\}$ or a fixed $k$ and all
$m\in\{k,\ldots,n\}$. Finally, we show that, for $k,m = \Theta(\log n)$, the
$(k,m)$-mappability problem cannot be solved in strongly subquadratic time
unless the Strong Exponential Time Hypothesis fails.
  This is an improved and extended version of a paper that was presented at
SPIRE 2018.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Juliusz Straszyński</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SPIRE 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11702v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11702v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1802.06369">
    <id>http://arxiv.org/abs/1802.06369v1</id>
    <updated>2018-02-18T13:04:21Z</updated>
    <published>2018-02-18T13:04:21Z</published>
    <title>Linear-Time Algorithm for Long LCF with $k$ Mismatches</title>
    <summary>  In the Longest Common Factor with $k$ Mismatches (LCF$_k$) problem, we are
given two strings $X$ and $Y$ of total length $n$, and we are asked to find a
pair of maximal-length factors, one of $X$ and the other of $Y$, such that
their Hamming distance is at most $k$. Thankachan et al. show that this problem
can be solved in $\mathcal{O}(n \log^k n)$ time and $\mathcal{O}(n)$ space for
constant $k$. We consider the LCF$_k$($\ell$) problem in which we assume that
the sought factors have length at least $\ell$, and the LCF$_k$($\ell$) problem
for $\ell=\Omega(\log^{2k+2} n)$, which we call the Long LCF$_k$ problem. We
use difference covers to reduce the Long LCF$_k$ problem to a task involving
$m=\mathcal{O}(n/\log^{k+1}n)$ synchronized factors. The latter can be solved
in $\mathcal{O}(m \log^{k+1}m)$ time, which results in a linear-time algorithm
for Long LCF$_k$. In general, our solution to LCF$_k$($\ell$) for arbitrary
$\ell$ takes $\mathcal{O}(n + n \log^{k+1} n/\sqrt{\ell})$ time.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to CPM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1801.01404">
    <id>http://arxiv.org/abs/1801.01404v1</id>
    <updated>2018-01-04T15:31:53Z</updated>
    <published>2018-01-04T15:31:53Z</published>
    <title>String Periods in the Order-Preserving Model</title>
    <summary>  The order-preserving model (op-model, in short) was introduced quite recently
but has already attracted significant attention because of its applications in
data analysis. We introduce several types of periods in this setting
(op-periods). Then we give algorithms to compute these periods in time $O(n)$,
$O(n\log\log n)$, $O(n \log^2 \log n/\log \log \log n)$, $O(n\log n)$ depending
on the type of periodicity. In the most general variant the number of different
periods can be as big as $\Omega(n^2)$, and a compact representation is needed.
Our algorithms require novel combinatorial insight into the properties of such
periods.
</summary>
    <author>
      <name>Garance Gourdel</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Arseny Shur</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper accepted to STACS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1801.01096">
    <id>http://arxiv.org/abs/1801.01096v1</id>
    <updated>2018-01-03T18:01:29Z</updated>
    <published>2018-01-03T18:01:29Z</published>
    <title>On Periodicity Lemma for Partial Words</title>
    <summary>  We investigate the function $L(h,p,q)$, called here the threshold function,
related to periodicity of partial words (words with holes). The value
$L(h,p,q)$ is defined as the minimum length threshold which guarantees that a
natural extension of the periodicity lemma is valid for partial words with $h$
holes and (strong) periods $p,q$. We show how to evaluate the threshold
function in $O(\log p + \log q)$ time, which is an improvement upon the best
previously known $O(p+q)$-time algorithm. In a series of papers, the formulae
for the threshold function, in terms of $p$ and $q$, were provided for each
fixed $h \le 7$. We demystify the generic structure of such formulae, and for
each value $h$ we express the threshold function in terms of a piecewise-linear
function with $O(h)$ pieces.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper accepted to LATA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1305.2540">
    <id>http://arxiv.org/abs/1305.2540v1</id>
    <updated>2013-05-11T20:33:09Z</updated>
    <published>2013-05-11T20:33:09Z</published>
    <title>Finding Distinct Subpalindromes Online</title>
    <summary>  We exhibit an online algorithm finding all distinct palindromes inside a
given string in time $\Theta(n\log|\Sigma|)$ over an ordered alphabet and in
time $\Theta(n|\Sigma|)$ over an unordered alphabet. Using a reduction from a
dictionary-like data structure, we prove the optimality of this algorithm in
the comparison-based computation model.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Mikhail Rubinchik</name>
    </author>
    <author>
      <name>Arseny M. Shur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.2540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.2540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1412.4471">
    <id>http://arxiv.org/abs/1412.4471v3</id>
    <updated>2015-05-01T12:14:47Z</updated>
    <published>2014-12-15T06:13:22Z</published>
    <title>Online Detection of Repetitions with Backtracking</title>
    <summary>  In this paper we present two algorithms for the following problem: given a
string and a rational $e > 1$, detect in the online fashion the earliest
occurrence of a repetition of exponent $\ge e$ in the string.
  1. The first algorithm supports the backtrack operation removing the last
letter of the input string. This solution runs in $O(n\log m)$ time and $O(m)$
space, where $m$ is the maximal length of a string generated during the
execution of a given sequence of $n$ read and backtrack operations.
  2. The second algorithm works in $O(n\log\sigma)$ time and $O(n)$ space,
where $n$ is the length of the input string and $\sigma$ is the number of
distinct letters. This algorithm is relatively simple and requires much less
memory than the previously known solution with the same working time and space.
a string generated during the execution of a given sequence of $n$ read and
backtrack operations.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures, accepted to CPM 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.4471v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4471v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1411.2022">
    <id>http://arxiv.org/abs/1411.2022v1</id>
    <updated>2014-11-07T20:28:37Z</updated>
    <published>2014-11-07T20:28:37Z</published>
    <title>Online Square Detection</title>
    <summary>  The online square detection problem is to detect the first occurrence of a
square in a string whose characters are provided as input one at a time. Recall
that a square is a string that is a concatenation of two identical strings. In
this paper we present an algorithm solving this problem in $O(n\log\sigma)$
time and linear space on ordered alphabet, where $\sigma$ is the number of
different letters in the input string. Our solution is relatively simple and
does not require much memory unlike the previously known online algorithm with
the same working time. Also we present an algorithm working in $O(n\log n)$
time and linear space on unordered alphabet, though this solution does not
outperform the previously known result with the same time bound.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.2022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1409.5641">
    <id>http://arxiv.org/abs/1409.5641v1</id>
    <updated>2014-09-19T13:11:13Z</updated>
    <published>2014-09-19T13:11:13Z</published>
    <title>Lempel-Ziv Factorization May Be Harder Than Computing All Runs</title>
    <summary>  The complexity of computing the Lempel-Ziv factorization and the set of all
runs (= maximal repetitions) is studied in the decision tree model of
computation over ordered alphabet. It is known that both these problems can be
solved by RAM algorithms in $O(n\log\sigma)$ time, where $n$ is the length of
the input string and $\sigma$ is the number of distinct letters in it. We prove
an $\Omega(n\log\sigma)$ lower bound on the number of comparisons required to
construct the Lempel-Ziv factorization and thereby conclude that a popular
technique of computation of runs using the Lempel-Ziv factorization cannot
achieve an $o(n\log\sigma)$ time bound. In contrast with this, we exhibit an
$O(n)$ decision tree algorithm finding all runs in a string. Therefore, in the
decision tree model the runs problem is easier than the Lempel-Ziv
factorization. Thus we support the conjecture that there is a linear RAM
algorithm finding all runs.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures, submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.5641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1509.01018">
    <id>http://arxiv.org/abs/1509.01018v2</id>
    <updated>2016-04-11T19:52:42Z</updated>
    <published>2015-09-03T10:11:12Z</published>
    <title>Finding the Leftmost Critical Factorization on Unordered Alphabet</title>
    <summary>  We present a linear time and space algorithm computing the leftmost critical
factorization of a given string on an unordered alphabet.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 13 figures (accepted to Theor. Comp. Sci.)</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.01018v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.01018v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1507.01231">
    <id>http://arxiv.org/abs/1507.01231v2</id>
    <updated>2015-11-22T19:57:52Z</updated>
    <published>2015-07-05T15:42:02Z</published>
    <title>Computing Runs on a General Alphabet</title>
    <summary>  We describe a RAM algorithm computing all runs (maximal repetitions) of a
given string of length $n$ over a general ordered alphabet in
$O(n\log^{\frac{2}3} n)$ time and linear space. Our algorithm outperforms all
known solutions working in $\Theta(n\log\sigma)$ time provided $\sigma =
n^{\Omega(1)}$, where $\sigma$ is the alphabet size. We conjecture that there
exists a linear time RAM algorithm finding all runs.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.01231v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01231v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1611.02891">
    <id>http://arxiv.org/abs/1611.02891v5</id>
    <updated>2017-05-11T13:34:29Z</updated>
    <published>2016-11-09T11:07:31Z</published>
    <title>Tight Lower Bounds for the Longest Common Extension Problem</title>
    <summary>  The longest common extension problem is to preprocess a given string of
length $n$ into a data structure that uses $S(n)$ bits on top of the input and
answers in $T(n)$ time the queries $\mathit{LCE}(i,j)$ computing the length of
the longest string that occurs at both positions $i$ and $j$ in the input. We
prove that the trade-off $S(n)T(n) = \Omega(n\log n)$ holds in the non-uniform
cell-probe model provided that the input string is read-only, each letter
occupies a separate memory cell, $S(n) = \Omega(n)$, and the size of the input
alphabet is at least $2^{8\lceil S(n) / n\rceil}$. It is known that this
trade-off is tight.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, accepted to Information Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.02891v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02891v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1604.00054">
    <id>http://arxiv.org/abs/1604.00054v5</id>
    <updated>2017-07-18T16:01:01Z</updated>
    <published>2016-03-31T21:25:42Z</published>
    <title>Detecting One-variable Patterns</title>
    <summary>  Given a pattern $p = s_1x_1s_2x_2\cdots s_{r-1}x_{r-1}s_r$ such that
$x_1,x_2,\ldots,x_{r-1}\in\{x,\overset{{}_{\leftarrow}}{x}\}$, where $x$ is a
variable and $\overset{{}_{\leftarrow}}{x}$ its reversal, and
$s_1,s_2,\ldots,s_r$ are strings that contain no variables, we describe an
algorithm that constructs in $O(rn)$ time a compact representation of all $P$
instances of $p$ in an input string of length $n$ over a polynomially bounded
integer alphabet, so that one can report those instances in $O(P)$ time.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Dirk Nowotka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages (+13 pages of Appendix), 4 figures, accepted to SPIRE 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00054v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00054v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1708.03558">
    <id>http://arxiv.org/abs/1708.03558v3</id>
    <updated>2018-05-23T08:51:19Z</updated>
    <published>2017-08-11T14:30:42Z</published>
    <title>Comparison of LZ77-type Parsings</title>
    <summary>  We investigate the relations between different variants of the LZ77 parsing
existing in the literature. All of them are defined as greedily constructed
parsings encoding each phrase by reference to a string occurring earlier in the
input. They differ by the phrase encodings: encoded by pairs (length + position
of an earlier occurrence) or by triples (length + position of an earlier
occurrence + the letter following the earlier occurring part); and they differ
by allowing or not allowing overlaps between the phrase and its earlier
occurrence. For a given string of length $n$ over an alphabet of size $\sigma$,
denote the numbers of phrases in the parsings allowing (resp., not allowing)
overlaps by $z$ (resp., $\hat{z}$) for "pairs", and by $z_3$ (resp.,
$\hat{z}_3$) for "triples". We prove the following bounds and provide series of
examples showing that these bounds are tight:
  $\bullet$ $z \le \hat{z} \le z \cdot O(\log\frac{n}{z\log_\sigma z})$ and
$z_3 \le \hat{z}_3 \le z_3 \cdot O(\log\frac{n}{z_3\log_\sigma z_3})$;
  $\bullet$ $\frac{1}2\hat{z} &lt; \hat{z}_3 \le \hat{z}$ and $\frac{1}2 z &lt; z_3
\le z$.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <author>
      <name>Arseny M. Shur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.03558v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03558v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2201.02514">
    <id>http://arxiv.org/abs/2201.02514v1</id>
    <updated>2022-01-07T15:56:46Z</updated>
    <published>2022-01-07T15:56:46Z</published>
    <title>The Efficiency of the ANS Entropy Encoding</title>
    <summary>  The Asymmetric Numeral Systems (ANS) is a class of entropy encoders by Duda
that had an immense impact on the data compression, substituting arithmetic and
Huffman coding. The optimality of ANS was studied by Duda et al. but the
precise asymptotic behaviour of its redundancy (in comparison to the entropy)
was not completely understood. In this paper we establish an optimal bound on
the redundancy for the tabled ANS (tANS), the most popular ANS variant. Given a
sequence $a_1,\ldots,a_n$ of letters from an alphabet $\{0,\ldots,\sigma-1\}$
such that each letter $a$ occurs in it $f_a$ times and $n=2^r$, the tANS
encoder using Duda's ``precise initialization'' to fill tANS tables transforms
this sequence into a bit string of length (frequencies are not included in the
encoding size): $$ \sum\limits_{a\in
[0..\sigma)}f_a\cdot\log\frac{n}{f_a}+O(\sigma+r), $$ where $O(\sigma + r)$ can
be bounded by $\sigma\log e+r$. The $r$-bit term is an encoder artifact
indispensable to ANS; the rest incurs a redundancy of $O(\frac{\sigma}{n})$
bits per letter. We complement this bound by a series of examples showing that
an $\Omega(\sigma+r)$ redundancy is necessary when $\sigma > n/3$, where
$\Omega(\sigma + r)$ is at least $\frac{\sigma-1}{4}+r-2$. We argue that
similar examples exist for any methods that distribute letters in tANS tables
using only the knowledge about frequencies. Thus, we refute Duda's conjecture
that the redundancy is $O(\frac{\sigma}{n^2})$ bits per letter.
  We also propose a new variant of range ANS (rANS), called rANS with fixed
accuracy, that is parameterized by $k \ge 1$. In this variant the integer
division, which is unavoidable in rANS, is performed only in cases when its
result belongs to $[2^k..2^{k+1})$. Hence, the division can be computed by
faster methods provided $k$ is small. We bound the redundancy for the rANS with
fixed accuracy $k$ by $\frac{n}{2^k-1}\log e+r$.
</summary>
    <author>
      <name>Dmitry Kosolobov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures, 2 algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.02514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.02514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1505.04019">
    <id>http://arxiv.org/abs/1505.04019v6</id>
    <updated>2015-09-17T18:09:38Z</updated>
    <published>2015-05-15T10:52:50Z</published>
    <title>Linear-Time Superbubble Identification Algorithm for Genome Assembly</title>
    <summary>  DNA sequencing is the process of determining the exact order of the
nucleotide bases of an individual's genome in order to catalogue sequence
variation and understand its biological implications. Whole-genome sequencing
techniques produce masses of data in the form of short sequences known as
reads. Assembling these reads into a whole genome constitutes a major
algorithmic challenge. Most assembly algorithms utilize de Bruijn graphs
constructed from reads for this purpose. A critical step of these algorithms is
to detect typical motif structures in the graph caused by sequencing errors and
genome repeats, and filter them out; one such complex subgraph class is a
so-called superbubble. In this paper, we propose an O(n+m)-time algorithm to
detect all superbubbles in a directed acyclic graph with n nodes and m
(directed) edges, improving the best-known O(m log m)-time algorithm by Sung et
al.
</summary>
    <author>
      <name>Ljiljana Brankovic</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Ritu Kundu</name>
    </author>
    <author>
      <name>Manal Mohamed</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Fatima Vayani</name>
    </author>
    <link href="http://arxiv.org/abs/1505.04019v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.04019v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1610.08111">
    <id>http://arxiv.org/abs/1610.08111v1</id>
    <updated>2016-10-25T22:38:08Z</updated>
    <published>2016-10-25T22:38:08Z</published>
    <title>Efficient Pattern Matching in Elastic-Degenerate Strings</title>
    <summary>  In this paper, we extend the notion of gapped strings to elastic-degenerate
strings. An elastic-degenerate string can been seen as an ordered collection of
k > 1 seeds (substrings/subpatterns) interleaved by elastic-degenerate symbols
such that each elastic-degenerate symbol corresponds to a set of two or more
variable length strings. Here, we present an algorithm for solving the pattern
matching problem with (solid) pattern and elastic-degenerate text, running in
O(N+{\alpha}{\gamma}nm) time; where m is the length of the given pattern; n and
N are the length and total size of the given elastic-degenerate text,
respectively; {\alpha} and {\gamma} are small constants, respectively
representing the maximum number of strings in any elastic-degenerate symbol of
the text and the largest number of elastic-degenerate symbols spanned by any
occurrence of the pattern in the text. The space used by the algorithm is
linear in the size of the input for a constant number of elastic-degenerate
symbols in the text; {\alpha} and {\gamma} are so small in real applications
that the algorithm is expected to work very efficiently in practice.
</summary>
    <author>
      <name>Costas Iliopoulos</name>
    </author>
    <author>
      <name>Ritu Kundu</name>
    </author>
    <author>
      <name>Solon Pissis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages (without references)</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.08111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.08111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1606.08275">
    <id>http://arxiv.org/abs/1606.08275v1</id>
    <updated>2016-06-27T13:49:16Z</updated>
    <published>2016-06-27T13:49:16Z</published>
    <title>Near-Optimal Computation of Runs over General Alphabet via Non-Crossing
  LCE Queries</title>
    <summary>  Longest common extension queries (LCE queries) and runs are ubiquitous in
algorithmic stringology. Linear-time algorithms computing runs and
preprocessing for constant-time LCE queries have been known for over a decade.
However, these algorithms assume a linearly-sortable integer alphabet. A recent
breakthrough paper by Bannai et.\ al.\ (SODA 2015) showed a link between the
two notions: all the runs in a string can be computed via a linear number of
LCE queries. The first to consider these problems over a general ordered
alphabet was Kosolobov (\emph{Inf.\ Process.\ Lett.}, 2016), who presented an
$O(n (\log n)^{2/3})$-time algorithm for answering $O(n)$ LCE queries. This
result was improved by Gawrychowski et.\ al.\ (accepted to CPM 2016) to $O(n
\log \log n)$ time. In this work we note a special \emph{non-crossing} property
of LCE queries asked in the runs computation. We show that any $n$ such
non-crossing queries can be answered on-line in $O(n \alpha(n))$ time, which
yields an $O(n \alpha(n))$-time algorithm for computing runs.
</summary>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Ritu Kundu</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.01116">
    <id>http://arxiv.org/abs/1602.01116v1</id>
    <updated>2016-02-02T21:13:53Z</updated>
    <published>2016-02-02T21:13:53Z</published>
    <title>Efficient Index for Weighted Sequences</title>
    <summary>  The problem of finding factors of a text string which are identical or
similar to a given pattern string is a central problem in computer science. A
generalised version of this problem consists in implementing an index over the
text to support efficient on-line pattern queries. We study this problem in the
case where the text is weighted: for every position of the text and every
letter of the alphabet a probability of occurrence of this letter at this
position is given. Sequences of this type, also called position weight
matrices, are commonly used to represent imprecise or uncertain data. A
weighted sequence may represent many different strings, each with probability
of occurrence equal to the product of probabilities of its letters at
subsequent positions. Given a probability threshold $1/z$, we say that a
pattern string $P$ matches a weighted text at position $i$ if the product of
probabilities of the letters of $P$ at positions $i,\ldots,i+|P|-1$ in the text
is at least $1/z$. In this article, we present an $O(nz)$-time construction of
an $O(nz)$-sized index that can answer pattern matching queries in a weighted
text in optimal time improving upon the state of the art by a factor of $z \log
z$. Other applications of this data structure include an $O(nz)$-time
construction of the weighted prefix table and an $O(nz)$-time computation of
all covers of a weighted sequence, which improve upon the state of the art by
the same factor.
</summary>
    <author>
      <name>Carl Barton</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.01116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1604.08760">
    <id>http://arxiv.org/abs/1604.08760v1</id>
    <updated>2016-04-29T10:12:06Z</updated>
    <published>2016-04-29T10:12:06Z</published>
    <title>Optimal Computation of Avoided Words</title>
    <summary>  The deviation of the observed frequency of a word $w$ from its expected
frequency in a given sequence $x$ is used to determine whether or not the word
is avoided. This concept is particularly useful in DNA linguistic analysis. The
value of the standard deviation of $w$, denoted by $std(w)$, effectively
characterises the extent of a word by its edge contrast in the context in which
it occurs. A word $w$ of length $k>2$ is a $\rho$-avoided word in $x$ if
$std(w) \leq \rho$, for a given threshold $\rho &lt; 0$. Notice that such a word
may be completely absent from $x$. Hence computing all such words na\"{\i}vely
can be a very time-consuming procedure, in particular for large $k$. In this
article, we propose an $O(n)$-time and $O(n)$-space algorithm to compute all
$\rho$-avoided words of length $k$ in a given sequence $x$ of length $n$ over a
fixed-sized alphabet. We also present a time-optimal $O(\sigma n)$-time and
$O(\sigma n)$-space algorithm to compute all $\rho$-avoided words (of any
length) in a sequence of length $n$ over an alphabet of size $\sigma$.
Furthermore, we provide a tight asymptotic upper bound for the number of
$\rho$-avoided words and the expected length of the longest one. We make
available an open-source implementation of our algorithm. Experimental results,
using both real and synthetic data, show the efficiency of our implementation.
</summary>
    <author>
      <name>Yannis Almirantis</name>
    </author>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Jia Gao</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Manal Mohamed</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Dimitris Polychronopoulos</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.04589">
    <id>http://arxiv.org/abs/1705.04589v1</id>
    <updated>2017-05-12T14:27:28Z</updated>
    <published>2017-05-12T14:27:28Z</published>
    <title>How to answer a small batch of RMQs or LCA queries in practice</title>
    <summary>  In the Range Minimum Query (RMQ) problem, we are given an array $A$ of $n$
numbers and we are asked to answer queries of the following type: for indices
$i$ and $j$ between $0$ and $n-1$, query $\text{RMQ}_A(i,j)$ returns the index
of a minimum element in the subarray $A[i..j]$. Answering a small batch of RMQs
is a core computational task in many real-world applications, in particular due
to the connection with the Lowest Common Ancestor (LCA) problem. With small
batch, we mean that the number $q$ of queries is $o(n)$ and we have them all at
hand. It is therefore not relevant to build an $\Omega(n)$-sized data structure
or spend $\Omega(n)$ time to build a more succinct one. It is well-known, among
practitioners and elsewhere, that these data structures for online querying
carry high constants in their pre-processing and querying time. We would thus
like to answer this batch efficiently in practice. With efficiently in
practice, we mean that we (ultimately) want to spend $n + \mathcal{O}(q)$ time
and $\mathcal{O}(q)$ space. We write $n$ to stress that the number of
operations per entry of $A$ should be a very small constant. Here we show how
existing algorithms can be easily modified to satisfy these conditions. The
presented experimental results highlight the practicality of this new scheme.
The most significant improvement obtained is for answering a small batch of LCA
queries. A library implementation of the presented algorithms is made
available.
</summary>
    <author>
      <name>Mai Alzamel</name>
    </author>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IWOCA 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.04022">
    <id>http://arxiv.org/abs/1705.04022v1</id>
    <updated>2017-05-11T05:52:51Z</updated>
    <published>2017-05-11T05:52:51Z</published>
    <title>Faster algorithms for 1-mappability of a sequence</title>
    <summary>  In the k-mappability problem, we are given a string x of length n and
integers m and k, and we are asked to count, for each length-m factor y of x,
the number of other factors of length m of x that are at Hamming distance at
most k from y. We focus here on the version of the problem where k = 1. The
fastest known algorithm for k = 1 requires time O(mn log n/ log log n) and
space O(n). We present two algorithms that require worst-case time O(mn) and
O(n log^2 n), respectively, and space O(n), thus greatly improving the state of
the art. Moreover, we present an algorithm that requires average-case time and
space O(n) for integer alphabets if m = {\Omega}(log n/ log {\sigma}), where
{\sigma} is the alphabet size.
</summary>
    <author>
      <name>Mai Alzamel</name>
    </author>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wing-Kin Sung</name>
    </author>
    <link href="http://arxiv.org/abs/1705.04022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.03385">
    <id>http://arxiv.org/abs/1705.03385v1</id>
    <updated>2017-05-09T15:26:46Z</updated>
    <published>2017-05-09T15:26:46Z</published>
    <title>Optimal Computation of Overabundant Words</title>
    <summary>  The observed frequency of the longest proper prefix, the longest proper
suffix, and the longest infix of a word $w$ in a given sequence $x$ can be used
for classifying $w$ as avoided or overabundant. The definitions used for the
expectation and deviation of $w$ in this statistical model were described and
biologically justified by Brendel et al. (J Biomol Struct Dyn 1986). We have
very recently introduced a time-optimal algorithm for computing all avoided
words of a given sequence over an integer alphabet (Algorithms Mol Biol 2017).
In this article, we extend this study by presenting an $\mathcal{O}(n)$-time
and $\mathcal{O}(n)$-space algorithm for computing all overabundant words in a
sequence $x$ of length $n$ over an integer alphabet. Our main result is based
on a new non-trivial combinatorial property of the suffix tree $\mathcal{T}$ of
$x$: the number of distinct factors of $x$ whose longest infix is the label of
an explicit node of $\mathcal{T}$ is no more than $3n-4$. We further show that
the presented algorithm is time-optimal by proving that $\mathcal{O}(n)$ is a
tight upper bound for the number of overabundant words. Finally, we present
experimental results, using both synthetic and real data, which justify the
effectiveness and efficiency of our approach in practical terms.
</summary>
    <author>
      <name>Yannis Almirantis</name>
    </author>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Jia Gao</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Manal Mohamed</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Dimitris Polychronopoulos</name>
    </author>
    <link href="http://arxiv.org/abs/1705.03385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1703.08931">
    <id>http://arxiv.org/abs/1703.08931v1</id>
    <updated>2017-03-27T05:09:06Z</updated>
    <published>2017-03-27T05:09:06Z</published>
    <title>Palindromic Decompositions with Gaps and Errors</title>
    <summary>  Identifying palindromes in sequences has been an interesting line of research
in combinatorics on words and also in computational biology, after the
discovery of the relation of palindromes in the DNA sequence with the HIV
virus. Efficient algorithms for the factorization of sequences into palindromes
and maximal palindromes have been devised in recent years. We extend these
studies by allowing gaps in decompositions and errors in palindromes, and also
imposing a lower bound to the length of acceptable palindromes.
  We first present an algorithm for obtaining a palindromic decomposition of a
string of length n with the minimal total gap length in time O(n log n * g) and
space O(n g), where g is the number of allowed gaps in the decomposition. We
then consider a decomposition of the string in maximal \delta-palindromes (i.e.
palindromes with \delta errors under the edit or Hamming distance) and g
allowed gaps. We present an algorithm to obtain such a decomposition with the
minimal total gap length in time O(n (g + \delta)) and space O(n g).
</summary>
    <author>
      <name>Michał Adamczyk</name>
    </author>
    <author>
      <name>Mai Alzamel</name>
    </author>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to CSR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1801.04425">
    <id>http://arxiv.org/abs/1801.04425v1</id>
    <updated>2018-01-13T11:55:45Z</updated>
    <published>2018-01-13T11:55:45Z</published>
    <title>Longest Common Prefixes with $k$-Errors and Applications</title>
    <summary>  Although real-world text datasets, such as DNA sequences, are far from being
uniformly random, average-case string searching algorithms perform
significantly better than worst-case ones in most applications of interest. In
this paper, we study the problem of computing the longest prefix of each suffix
of a given string of length $n$ over a constant-sized alphabet that occurs
elsewhere in the string with $k$-errors. This problem has already been studied
under the Hamming distance model. Our first result is an improvement upon the
state-of-the-art average-case time complexity for non-constant $k$ and using
only linear space under the Hamming distance model. Notably, we show that our
technique can be extended to the edit distance model with the same time and
space complexities. Specifically, our algorithms run in $\mathcal{O}(n \log^k n
\log \log n)$ time on average using $\mathcal{O}(n)$ space. We show that our
technique is applicable to several algorithmic problems in computational
biology and elsewhere.
</summary>
    <author>
      <name>Lorraine A. K. Ayad</name>
    </author>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <link href="http://arxiv.org/abs/1801.04425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1104.5517">
    <id>http://arxiv.org/abs/1104.5517v2</id>
    <updated>2012-12-04T16:40:45Z</updated>
    <published>2011-04-28T21:35:40Z</published>
    <title>Dynamic Range Majority Data Structures</title>
    <summary>  Given a set $P$ of coloured points on the real line, we study the problem of
answering range $\alpha$-majority (or "heavy hitter") queries on $P$. More
specifically, for a query range $Q$, we want to return each colour that is
assigned to more than an $\alpha$-fraction of the points contained in $Q$. We
present a new data structure for answering range $\alpha$-majority queries on a
dynamic set of points, where $\alpha \in (0,1)$. Our data structure uses O(n)
space, supports queries in $O((\lg n) / \alpha)$ time, and updates in $O((\lg
n) / \alpha)$ amortized time. If the coordinates of the points are integers,
then the query time can be improved to $O(\lg n / (\alpha \lg \lg n) +
(\lg(1/\alpha))/\alpha))$. For constant values of $\alpha$, this improved query
time matches an existing lower bound, for any data structure with
polylogarithmic update time. We also generalize our data structure to handle
sets of points in d-dimensions, for $d \ge 2$, as well as dynamic arrays, in
which each entry is a colour.
</summary>
    <author>
      <name>Amr Elmasry</name>
    </author>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Patrick K. Nicholson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, Preliminary version appeared in ISAAC 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.5517v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.5517v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1106.5076">
    <id>http://arxiv.org/abs/1106.5076v3</id>
    <updated>2013-05-08T17:46:10Z</updated>
    <published>2011-06-24T22:27:37Z</published>
    <title>Dynamic Range Selection in Linear Space</title>
    <summary>  Given a set $S$ of $n$ points in the plane, we consider the problem of
answering range selection queries on $S$: that is, given an arbitrary $x$-range
$Q$ and an integer $k > 0$, return the $k$-th smallest $y$-coordinate from the
set of points that have $x$-coordinates in $Q$. We present a linear space data
structure that maintains a dynamic set of $n$ points in the plane with real
coordinates, and supports range selection queries in $O((\lg n / \lg \lg n)^2)$
time, as well as insertions and deletions in $O((\lg n / \lg \lg n)^2)$
amortized time. The space usage of this data structure is an $\Theta(\lg n /
\lg \lg n)$ factor improvement over the previous best result, while maintaining
asymptotically matching query and update times. We also present a succinct data
structure that supports range selection queries on a dynamic array of $n$
values drawn from a bounded universe.
</summary>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Patrick K. Nicholson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages (lncs fullpage). This is a corrected version of the
  preliminary version of the paper that appeared in ISAAC 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.5076v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.5076v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1204.4835">
    <id>http://arxiv.org/abs/1204.4835v1</id>
    <updated>2012-04-21T19:36:06Z</updated>
    <published>2012-04-21T19:36:06Z</published>
    <title>Succinct Indices for Range Queries with applications to Orthogonal Range
  Maxima</title>
    <summary>  We consider the problem of preprocessing $N$ points in 2D, each endowed with
a priority, to answer the following queries: given a axis-parallel rectangle,
determine the point with the largest priority in the rectangle. Using the ideas
of the \emph{effective entropy} of range maxima queries and \emph{succinct
indices} for range maxima queries, we obtain a structure that uses O(N) words
and answers the above query in $O(\log N \log \log N)$ time. This is a direct
improvement of Chazelle's result from FOCS 1985 for this problem -- Chazelle
required $O(N/\epsilon)$ words to answer queries in $O((\log N)^{1+\epsilon})$
time for any constant $\epsilon > 0$.
</summary>
    <author>
      <name>Arash Farzan</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICALP 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.4835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.4835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1306.4287">
    <id>http://arxiv.org/abs/1306.4287v1</id>
    <updated>2013-06-18T18:31:27Z</updated>
    <published>2013-06-18T18:31:27Z</published>
    <title>Succinct data structures for representing equivalence classes</title>
    <summary>  Given a partition of an n element set into equivalence classes, we consider
time-space tradeoffs for representing it to support the query that asks whether
two given elements are in the same equivalence class. This has various
applications including for testing whether two vertices are in the same
component in an undirected graph or in the same strongly connected component in
a directed graph.
  We consider the problem in several models.
  -- Concerning labeling schemes where we assign labels to elements and the
query is to be answered just by examining the labels of the queried elements
(without any extra space): if each vertex is required to have a unique label,
then we show that a label space of (\sum_{i=1}^n \lfloor {n \over i} \rfloor)
is necessary and sufficient. In other words, \lg n + \lg \lg n + O(1) bits of
space are necessary and sufficient for representing each of the labels. This
slightly strengthens the known lower bound and is in contrast to the known
necessary and sufficient bound of \lceil \lg n \rceil for the label length, if
each vertex need not get a unique label.
  --Concerning succinct data structures for the problem when the n elements are
to be uniquely assigned labels from label set {1, 2, ...n}, we first show that
\Theta(\sqrt n) bits are necessary and sufficient to represent the equivalence
class information. This space includes the space for implicitly encoding the
vertex labels. We can support the query in such a structure in O(\lg n) time in
the standard word RAM model. We then develop structures resulting in one where
the queries can be supported in constant time using O({\sqrt n} \lg n) bits of
space. We also develop space efficient structures where union operation along
with the equivalence query can be answered fast.
</summary>
    <author>
      <name>Moshe Lewenstein</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Venkatesh Raman</name>
    </author>
    <link href="http://arxiv.org/abs/1306.4287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.4287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1303.6872">
    <id>http://arxiv.org/abs/1303.6872v1</id>
    <updated>2013-03-27T16:13:03Z</updated>
    <published>2013-03-27T16:13:03Z</published>
    <title>Order-Preserving Suffix Trees and Their Algorithmic Applications</title>
    <summary>  Recently Kubica et al. (Inf. Process. Let., 2013) and Kim et al. (submitted
to Theor. Comp. Sci.) introduced order-preserving pattern matching. In this
problem we are looking for consecutive substrings of the text that have the
same "shape" as a given pattern. These results include a linear-time
order-preserving pattern matching algorithm for polynomially-bounded alphabet
and an extension of this result to pattern matching with multiple patterns. We
make one step forward in the analysis and give an
$O(\frac{n\log{n}}{\log\log{n}})$ time randomized algorithm constructing suffix
trees in the order-preserving setting. We show a number of applications of
order-preserving suffix trees to identify patterns and repetitions in time
series.
</summary>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Marcin Kubica</name>
    </author>
    <author>
      <name>Alessio Langiu</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Walen</name>
    </author>
    <link href="http://arxiv.org/abs/1303.6872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.6872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1406.6341">
    <id>http://arxiv.org/abs/1406.6341v2</id>
    <updated>2014-06-28T10:52:31Z</updated>
    <published>2014-06-24T19:24:53Z</published>
    <title>Linear-time Computation of Minimal Absent Words Using Suffix Array</title>
    <summary>  An absent word of a word y of length n is a word that does not occur in y. It
is a minimal absent word if all its proper factors occur in y. Minimal absent
words have been computed in genomes of organisms from all domains of life;
their computation provides a fast alternative for measuring approximation in
sequence comparison. There exists an O(n)-time and O(n)-space algorithm for
computing all minimal absent words on a fixed-sized alphabet based on the
construction of suffix automata (Crochemore et al., 1998). No implementation of
this algorithm is publicly available. There also exists an O(n^2)-time and
O(n)-space algorithm for the same problem based on the construction of suffix
arrays (Pinho et al., 2009). An implementation of this algorithm was also
provided by the authors and is currently the fastest available. In this
article, we bridge this unpleasant gap by presenting an O(n)-time and
O(n)-space algorithm for computing all minimal absent words based on the
construction of suffix arrays. Experimental results using real and synthetic
data show that the respective implementation outperforms the one by Pinho et
al.
</summary>
    <author>
      <name>Carl Barton</name>
    </author>
    <author>
      <name>Alice Heliou</name>
    </author>
    <author>
      <name>Laurent Mouchard</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <link href="http://arxiv.org/abs/1406.6341v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6341v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1406.5480">
    <id>http://arxiv.org/abs/1406.5480v2</id>
    <updated>2016-04-25T18:04:19Z</updated>
    <published>2014-06-20T18:37:14Z</published>
    <title>Average-Case Optimal Approximate Circular String Matching</title>
    <summary>  Approximate string matching is the problem of finding all factors of a text t
of length n that are at a distance at most k from a pattern x of length m.
Approximate circular string matching is the problem of finding all factors of t
that are at a distance at most k from x or from any of its rotations. In this
article, we present a new algorithm for approximate circular string matching
under the edit distance model with optimal average-case search time O(n(k + log
m)/m). Optimal average-case search time can also be achieved by the algorithms
for multiple approximate string matching (Fredriksson and Navarro, 2004) using
x and its rotations as the set of multiple patterns. Here we reduce the
preprocessing time and space requirements compared to that approach.
</summary>
    <author>
      <name>Carl Barton</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <link href="http://arxiv.org/abs/1406.5480v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5480v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1401.0163">
    <id>http://arxiv.org/abs/1401.0163v1</id>
    <updated>2013-12-31T15:56:30Z</updated>
    <published>2013-12-31T15:56:30Z</published>
    <title>Fast Algorithm for Partial Covers in Words</title>
    <summary>  A factor $u$ of a word $w$ is a cover of $w$ if every position in $w$ lies
within some occurrence of $u$ in $w$. A word $w$ covered by $u$ thus
generalizes the idea of a repetition, that is, a word composed of exact
concatenations of $u$. In this article we introduce a new notion of
$\alpha$-partial cover, which can be viewed as a relaxed variant of cover, that
is, a factor covering at least $\alpha$ positions in $w$. We develop a data
structure of $O(n)$ size (where $n=|w|$) that can be constructed in $O(n\log
n)$ time which we apply to compute all shortest $\alpha$-partial covers for a
given $\alpha$. We also employ it for an $O(n\log n)$-time algorithm computing
a shortest $\alpha$-partial cover for each $\alpha=1,2,\ldots,n$.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <link href="http://arxiv.org/abs/1401.0163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1512.01085">
    <id>http://arxiv.org/abs/1512.01085v2</id>
    <updated>2015-12-08T12:34:42Z</updated>
    <published>2015-12-03T14:17:13Z</published>
    <title>Fast Average-Case Pattern Matching on Weighted Sequences</title>
    <summary>  A weighted string over an alphabet of size $\sigma$ is a string in which a
set of letters may occur at each position with respective occurrence
probabilities. Weighted strings, also known as position weight matrices or
uncertain sequences, naturally arise in many contexts. In this article, we
study the problem of weighted string matching with a special focus on
average-case analysis. Given a weighted pattern string $x$ of length $m$, a
text string $y$ of length $n>m$, and a cumulative weight threshold $1/z$,
defined as the minimal probability of occurrence of factors in a weighted
string, we present an algorithm requiring average-case search time $o(n)$ for
pattern matching for weight ratio $\frac{z}{m} &lt; \min\{\frac{1}{\log
z},\frac{\log \sigma}{\log z (\log m + \log \log \sigma)}\}$. For a pattern
string $x$ of length $m$, a weighted text string $y$ of length $n>m$, and a
cumulative weight threshold $1/z$, we present an algorithm requiring
average-case search time $o(\sigma n)$ for the same weight ratio. The
importance of these results lies on the fact that these algorithms work in
average-case sublinear search time in the size of the text, and in linear
preprocessing time and space in the size of the pattern, for these ratios.
</summary>
    <author>
      <name>Carl Barton</name>
    </author>
    <author>
      <name>Chang Liu</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <link href="http://arxiv.org/abs/1512.01085v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01085v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1506.04917">
    <id>http://arxiv.org/abs/1506.04917v2</id>
    <updated>2015-12-22T08:48:14Z</updated>
    <published>2015-06-16T11:04:02Z</published>
    <title>Linear-Time Sequence Comparison Using Minimal Absent Words &amp;
  Applications</title>
    <summary>  Sequence comparison is a prerequisite to virtually all comparative genomic
analyses. It is often realized by sequence alignment techniques, which are
computationally expensive. This has led to increased research into
alignment-free techniques, which are based on measures referring to the
composition of sequences in terms of their constituent patterns. These
measures, such as $q$-gram distance, are usually computed in time linear with
respect to the length of the sequences. In this article, we focus on the
complementary idea: how two sequences can be efficiently compared based on
information that does not occur in the sequences. A word is an {\em absent
word} of some sequence if it does not occur in the sequence. An absent word is
{\em minimal} if all its proper factors occur in the sequence. Here we present
the first linear-time and linear-space algorithm to compare two sequences by
considering {\em all} their minimal absent words. In the process, we present
results of combinatorial interest, and also extend the proposed techniques to
compare circular sequences.
</summary>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Robert Mercaş</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to LATIN 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.04917v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04917v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1104.4203">
    <id>http://arxiv.org/abs/1104.4203v1</id>
    <updated>2011-04-21T08:39:37Z</updated>
    <published>2011-04-21T08:39:37Z</published>
    <title>Pattern matching in Lempel-Ziv compressed strings: fast, simple, and
  deterministic</title>
    <summary>  Countless variants of the Lempel-Ziv compression are widely used in many
real-life applications. This paper is concerned with a natural modification of
the classical pattern matching problem inspired by the popularity of such
compression methods: given an uncompressed pattern s[1..m] and a Lempel-Ziv
representation of a string t[1..N], does s occur in t? Farach and Thorup gave a
randomized O(nlog^2(N/n)+m) time solution for this problem, where n is the size
of the compressed representation of t. We improve their result by developing a
faster and fully deterministic O(nlog(N/n)+m) time algorithm with the same
space complexity. Note that for highly compressible texts, log(N/n) might be of
order n, so for such inputs the improvement is very significant. A (tiny)
fragment of our method can be used to give an asymptotically optimal solution
for the substring hashing problem considered by Farach and Muthukrishnan.
</summary>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.4203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.4203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1102.5682">
    <id>http://arxiv.org/abs/1102.5682v1</id>
    <updated>2011-02-28T15:01:36Z</updated>
    <published>2011-02-28T15:01:36Z</published>
    <title>On minimising automata with errors</title>
    <summary>  The problem of k-minimisation for a DFA M is the computation of a smallest
DFA N (where the size |M| of a DFA M is the size of the domain of the
transition function) such that their recognized languages differ only on words
of length less than k. The previously best algorithm, which runs in time O(|M|
log^2 n) where n is the number of states, is extended to DFAs with partial
transition functions. Moreover, a faster O(|M| log n) algorithm for DFAs that
recognise finite languages is presented. In comparison to the previous
algorithm for total DFAs, the new algorithm is much simpler and allows the
calculation of a k-minimal DFA for each k in parallel. Secondly, it is
demonstrated that calculating the least number of introduced errors is hard:
Given a DFA M and numbers k and m, it is NP-hard to decide whether there exists
a k-minimal DFA N differing from DFA M on at most m words. A similar result
holds for hyper-minimisation of DFAs in general: Given a DFA M and numbers s
and m, it is NP-hard to decide whether there exists a DFA N with at most s
states such that DFA M and N differ on at msot m words.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Artur Jeż</name>
    </author>
    <author>
      <name>Andreas Maletti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages plus 19-page appendix, submitted to a conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1102.5682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.5682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1202.5670">
    <id>http://arxiv.org/abs/1202.5670v1</id>
    <updated>2012-02-25T17:05:53Z</updated>
    <published>2012-02-25T17:05:53Z</published>
    <title>(Really) Tight bounds for dispatching binary methods</title>
    <summary>  We consider binary dispatching problem originating from object oriented
programming. We want to preprocess a hierarchy of classes and collection of
methods so that given a function call in the run-time we are able to retrieve
the most specialized implementation which can be invoked with the actual types
of the arguments. For the binary dispatching, where the methods take exactly
two arguments, logarithmic query time is possible, even if the structure is
allowed to take linear space. Unfortunately, known solutions achieving such
complexity require superlinear time for constructing the structure. Using a
different idea we are able to construct in (deterministic) linear time and
space a structure allowing dispatching binary methods in the same logarithmic
time. Then we show how to improve the query time to slightly sublogarithmic,
which is easily seen to be optimal as a consequence of some already known lower
bounds if we want to keep the size of the resulting structure close to linear.
</summary>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.5670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.5670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1309.6453">
    <id>http://arxiv.org/abs/1309.6453v2</id>
    <updated>2014-03-05T23:05:50Z</updated>
    <published>2013-09-25T10:11:33Z</published>
    <title>Order-preserving pattern matching with k mismatches</title>
    <summary>  We study a generalization of the recently introduced order-preserving pattern
matching, where instead of looking for an exact copy of the pattern, we only
require that the relative order between the elements is the same. In our
variant, we additionally allow up to k mismatches between the pattern and the
text, and the goal is to construct an efficient algorithm for small values of
k. For a pattern of length m and a text of length n, our algorithm detects an
order-preserving occurrence with up to k mismatches in O(n(loglogm + kloglogk))
time.
</summary>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <author>
      <name>Przemyslaw Uznanski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the full version of an extended abstract to appear in CPM'14</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.6453v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.6453v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1309.5618">
    <id>http://arxiv.org/abs/1309.5618v1</id>
    <updated>2013-09-22T17:06:40Z</updated>
    <published>2013-09-22T17:06:40Z</published>
    <title>Substring Suffix Selection</title>
    <summary>  We study the following substring suffix selection problem: given a substring
of a string T of length n, compute its k-th lexicographically smallest suffix.
This a natural generalization of the well-known question of computing the
maximal suffix of a string, which is a basic ingredient in many other problems.
We first revisit two special cases of the problem, introduced by Babenko,
Kolesnichenko and Starikovskaya [CPM'13], in which we are asked to compute the
minimal non-empty and the maximal suffixes of a substring. For the maximal
suffixes problem, we give a linear-space structure with O(1) query time and
linear preprocessing time, i.e., we manage to achieve optimal construction and
optimal query time simultaneously. For the minimal suffix problem, we give a
linear-space data structure with O(\tau) query time and O(n log n / \tau)
preprocessing time, where 1 &lt;= \tau &lt;= log n is a parameter of the data
structure. As a sample application, we show that this data structure can be
used to compute the Lyndon decomposition of any substring of T in O(k \tau)
time, where k is the number of distinct factors in the decomposition.
  Finally, we move to the general case of the substring suffix selection
problem, where using any combinatorial properties seems more difficult.
Nevertheless, we develop a linear-space data structure with O(log^{2+\epsilon}
n) query time.
</summary>
    <author>
      <name>Maxim Babenko</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <link href="http://arxiv.org/abs/1309.5618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1308.6509">
    <id>http://arxiv.org/abs/1308.6509v2</id>
    <updated>2013-09-19T21:20:11Z</updated>
    <published>2013-08-29T16:16:45Z</published>
    <title>Beating O(nm) in approximate LZW-compressed pattern matching</title>
    <summary>  Given an LZW/LZ78 compressed text, we want to find an approximate occurrence
of a given pattern of length m. The goal is to achieve time complexity
depending on the size n of the compressed representation of the text instead of
its length. We consider two specific definitions of approximate matching,
namely the Hamming distance and the edit distance, and show how to achieve
O(nm^0.5k^2) and O(nm^0.5k^3) running time, respectively, where k is the bound
on the distance. Both algorithms use just linear space. Even for very small
values of k, the best previously known solutions required O(nm) time. Our main
contribution is applying a periodicity-based argument in a way that is
computationally effective even if we need to operate on a compressed
representation of a string, while the previous solutions were either based on a
dynamic programming, or a black-box application of tools developed for
uncompressed strings.
</summary>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <author>
      <name>Damian Straszak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the full version of an extended abstract to appear in
  ISAAC'13</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.6509v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.6509v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1410.2847">
    <id>http://arxiv.org/abs/1410.2847v3</id>
    <updated>2015-06-11T08:55:46Z</updated>
    <published>2014-10-10T17:36:52Z</published>
    <title>Encodings of Range Maximum-Sum Segment Queries and Applications</title>
    <summary>  Given an array A containing arbitrary (positive and negative) numbers, we
consider the problem of supporting range maximum-sum segment queries on A:
i.e., given an arbitrary range [i,j], return the subrange [i' ,j' ] \subseteq
[i,j] such that the sum of the numbers in A[i'..j'] is maximized. Chen and Chao
[Disc. App. Math. 2007] presented a data structure for this problem that
occupies {\Theta}(n) words, can be constructed in {\Theta}(n) time, and
supports queries in {\Theta}(1) time. Our first result is that if only the
indices [i',j'] are desired (rather than the maximum sum achieved in that
subrange), then it is possible to reduce the space to {\Theta}(n) bits,
regardless the numbers stored in A, while retaining the same construction and
query time. We also improve the best known space lower bound for any data
structure that supports range maximum-sum segment queries from n bits to
1.89113n - {\Theta}(lg n) bits, for sufficiently large values of n. Finally, we
provide a new application of this data structure which simplifies a previously
known linear time algorithm for finding k-covers: i.e., given an array A of n
numbers and a number k, find k disjoint subranges [i_1 ,j_1 ],...,[i_k ,j_k ],
such that the total sum of all the numbers in the subranges is maximized.
</summary>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <author>
      <name>Patrick K. Nicholson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages + 2 page appendix, 4 figures. A shortened version of this
  paper will appear in CPM 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.2847v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2847v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1509.00622">
    <id>http://arxiv.org/abs/1509.00622v2</id>
    <updated>2015-10-12T08:11:57Z</updated>
    <published>2015-09-02T10:07:06Z</published>
    <title>Testing k-binomial equivalence</title>
    <summary>  Two words $w_1$ and $w_2$ are said to be $k$-binomial equivalent if every
non-empty word $x$ of length at most $k$ over the alphabet of $w_1$ and $w_2$
appears as a scattered factor of $w_1$ exactly as many times as it appears as a
scattered factor of $w_2$. We give two different polynomial-time algorithms
testing the $k$-binomial equivalence of two words. The first one is
deterministic (but the degree of the corresponding polynomial is too high) and
the second one is randomised (it is more direct and more efficient). These are
the first known algorithms for the problem which run in polynomial time.
</summary>
    <author>
      <name>Dominik D. Freydenberger</name>
    </author>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <author>
      <name>Juhani Karhumäki</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">"Multidisciplinary Creativity: homage to Gheorghe Paun on his 65th
  birthday", Pg. 239--248, Ed. Spandugino, Bucharest, Romania, ISBN:
  978-606-8401-63-8, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.00622v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.00622v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.00447">
    <id>http://arxiv.org/abs/1602.00447v2</id>
    <updated>2016-04-07T07:23:47Z</updated>
    <published>2016-02-01T09:47:44Z</published>
    <title>Faster Longest Common Extension Queries in Strings over General
  Alphabets</title>
    <summary>  Longest common extension queries (often called longest common prefix queries)
constitute a fundamental building block in multiple string algorithms, for
example computing runs and approximate pattern matching. We show that a
sequence of $q$ LCE queries for a string of size $n$ over a general ordered
alphabet can be realized in $O(q \log \log n+n\log^*n)$ time making only
$O(q+n)$ symbol comparisons. Consequently, all runs in a string over a general
ordered alphabet can be computed in $O(n \log \log n)$ time making $O(n)$
symbol comparisons. Our results improve upon a solution by Kosolobov
(Information Processing Letters, 2016), who gave an algorithm with $O(n
\log^{2/3} n)$ running time and conjectured that $O(n)$ time is possible. We
make a significant progress towards resolving this conjecture. Our techniques
extend to the case of general unordered alphabets, when the time increases to
$O(q\log n + n\log^*n)$. The main tools are difference covers and the
disjoint-sets data structure.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CPM 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.00447v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00447v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.06011">
    <id>http://arxiv.org/abs/1707.06011v1</id>
    <updated>2017-07-19T10:45:41Z</updated>
    <published>2017-07-19T10:45:41Z</published>
    <title>Better Labeling Schemes for Nearest Common Ancestors through
  Minor-Universal Trees</title>
    <summary>  A labeling scheme for nearest common ancestors assigns a distinct binary
string, called the label, to every node of a tree, so that given the labels of
two nodes (and no further information about the topology of the tree) we can
compute the label of their nearest common ancestor. The goal is to make the
labels as short as possible. Alstrup, Gavoille, Kaplan, and Rauhe [Theor.
Comput. Syst. 37(3):441-456 2004] showed that $O(\log n)$-bit labels are
enough. More recently, Alstrup, Halvorsen, and Larsen [SODA 2014] refined this
to only $2.772\log n$, and provided a lower bound of $1.008\log n$.
  We connect designing a labeling scheme for nearest common ancestors to the
existence of a tree, called a minor-universal tree, that contains every tree on
$n$ nodes as a topological minor. Even though it is not clear if a labeling
scheme must be based on such a notion, we argue that the existing schemes can
be reformulated as such, and it allows us to obtain clean and good bounds on
the length of the labels. As the main upper bound, we show that $2.318\log
n$-bit labels are enough. Surprisingly, the notion of a minor-universal tree
for binary trees on $n$ nodes has been already used in a different context by
Hrubes et al. [CCC 2010], and Young, Chu, and Wong [J. ACM 46(3):416-435, 1999]
introduced a closely related notion of a universal tree. On the lower bound
side, we show that the size of any minor-universal tree for trees on $n$ nodes
is $\Omega(n^{2.174})$. This highlights a natural limitation for all approaches
based on such trees. Our lower bound technique also implies that the size of
any universal tree in the sense of Young et al. is $\Omega(n^{2.185})$, thus
dramatically improves their lower bound of $\Omega(n\log n)$. We complement the
existential results with a generic transformation that decreases the query time
to constant in any scheme based on a minor-universal tree.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Jakub Łopuszański</name>
    </author>
    <link href="http://arxiv.org/abs/1707.06011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1407.0522">
    <id>http://arxiv.org/abs/1407.0522v1</id>
    <updated>2014-07-02T11:22:11Z</updated>
    <published>2014-07-02T11:22:11Z</published>
    <title>Sublinear Space Algorithms for the Longest Common Substring Problem</title>
    <summary>  Given $m$ documents of total length $n$, we consider the problem of finding a
longest string common to at least $d \geq 2$ of the documents. This problem is
known as the \emph{longest common substring (LCS) problem} and has a classic
$O(n)$ space and $O(n)$ time solution (Weiner [FOCS'73], Hui [CPM'92]).
However, the use of linear space is impractical in many applications. In this
paper we show that for any trade-off parameter $1 \leq \tau \leq n$, the LCS
problem can be solved in $O(\tau)$ space and $O(n^2/\tau)$ time, thus providing
the first smooth deterministic time-space trade-off from constant to linear
space. The result uses a new and very simple algorithm, which computes a
$\tau$-additive approximation to the LCS in $O(n^2/\tau)$ time and $O(1)$
space. We also show a time-space trade-off lower bound for deterministic
branching programs, which implies that any deterministic RAM algorithm solving
the LCS problem on documents from a sufficiently large alphabet in $O(\tau)$
space must use $\Omega(n\sqrt{\log(n/(\tau\log n))/\log\log(n/(\tau\log n)})$
time.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <author>
      <name>Hjalte Wedel Vildhøj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to 22nd European Symposium on Algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.0522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1403.1364">
    <id>http://arxiv.org/abs/1403.1364v3</id>
    <updated>2014-09-01T16:09:50Z</updated>
    <published>2014-03-06T07:24:14Z</published>
    <title>A Suffix Tree Or Not A Suffix Tree?</title>
    <summary>  In this paper we study the structure of suffix trees. Given an unlabeled tree
$\tau$ on $n$ nodes and suffix links of its internal nodes, we ask the question
"Is $\tau$ a suffix tree?", i.e., is there a string $S$ whose suffix tree has
the same topological structure as $\tau$? We place no restrictions on $S$, in
particular we do not require that $S$ ends with a unique symbol. This
corresponds to considering the more general definition of implicit or extended
suffix trees. Such general suffix trees have many applications and are for
example needed to allow efficient updates when suffix trees are built online.
We prove that $\tau$ is a suffix tree if and only if it is realized by a string
$S$ of length $n-1$, and we give a linear-time algorithm for inferring $S$ when
the first letter on each edge is known. This generalizes the work of I et al.
[Discrete Appl. Math. 163, 2014].
</summary>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <author>
      <name>Hjalte Wedel Vildhøj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version. An extended abstract has been accepted to IWOCA 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.1364v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1364v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1508.00731">
    <id>http://arxiv.org/abs/1508.00731v2</id>
    <updated>2015-08-27T17:39:45Z</updated>
    <published>2015-08-04T11:07:50Z</published>
    <title>The k-mismatch problem revisited</title>
    <summary>  We revisit the complexity of one of the most basic problems in pattern
matching. In the k-mismatch problem we must compute the Hamming distance
between a pattern of length m and every m-length substring of a text of length
n, as long as that Hamming distance is at most k. Where the Hamming distance is
greater than k at some alignment of the pattern and text, we simply output
"No".
  We study this problem in both the standard offline setting and also as a
streaming problem. In the streaming k-mismatch problem the text arrives one
symbol at a time and we must give an output before processing any future
symbols. Our main results are as follows:
  1) Our first result is a deterministic $O(n k^2\log{k} / m+n \text{polylog}
m)$ time offline algorithm for k-mismatch on a text of length n. This is a
factor of k improvement over the fastest previous result of this form from SODA
2000 by Amihood Amir et al.
  2) We then give a randomised and online algorithm which runs in the same time
complexity but requires only $O(k^2\text{polylog} {m})$ space in total.
  3) Next we give a randomised $(1+\epsilon)$-approximation algorithm for the
streaming k-mismatch problem which uses $O(k^2\text{polylog} m / \epsilon^2)$
space and runs in $O(\text{polylog} m / \epsilon^2)$ worst-case time per
arriving symbol.
  4) Finally we combine our new results to derive a randomised
$O(k^2\text{polylog} {m})$ space algorithm for the streaming k-mismatch problem
which runs in $O(\sqrt{k}\log{k} + \text{polylog} {m})$ worst-case time per
arriving symbol. This improves the best previous space complexity for streaming
k-mismatch from FOCS 2009 by Benny Porat and Ely Porat by a factor of k. We
also improve the time complexity of this previous result by an even greater
factor to match the fastest known offline algorithm (up to logarithmic
factors).
</summary>
    <author>
      <name>Raphaël Clifford</name>
    </author>
    <author>
      <name>Allyx Fontaine</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <author>
      <name>Benjamin Sach</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <link href="http://arxiv.org/abs/1508.00731v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00731v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1504.07406">
    <id>http://arxiv.org/abs/1504.07406v1</id>
    <updated>2015-04-28T10:08:20Z</updated>
    <published>2015-04-28T10:08:20Z</published>
    <title>On Maximal Unbordered Factors</title>
    <summary>  Given a string $S$ of length $n$, its maximal unbordered factor is the
longest factor which does not have a border. In this work we investigate the
relationship between $n$ and the length of the maximal unbordered factor of
$S$. We prove that for the alphabet of size $\sigma \ge 5$ the expected length
of the maximal unbordered factor of a string of length~$n$ is at least $0.99 n$
(for sufficiently large values of $n$). As an application of this result, we
propose a new algorithm for computing the maximal unbordered factor of a
string.
</summary>
    <author>
      <name>Gregory Kucherov</name>
    </author>
    <author>
      <name>Alexander Loptev</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 26th Annual Symposium on Combinatorial Pattern
  Matching (CPM 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.07406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1504.06242">
    <id>http://arxiv.org/abs/1504.06242v1</id>
    <updated>2015-04-23T16:24:15Z</updated>
    <published>2015-04-23T16:24:15Z</published>
    <title>Dictionary matching in a stream</title>
    <summary>  We consider the problem of dictionary matching in a stream. Given a set of
strings, known as a dictionary, and a stream of characters arriving one at a
time, the task is to report each time some string in our dictionary occurs in
the stream. We present a randomised algorithm which takes O(log log(k + m))
time per arriving character and uses O(k log m) words of space, where k is the
number of strings in the dictionary and m is the length of the longest string
in the dictionary.
</summary>
    <author>
      <name>Raphael Clifford</name>
    </author>
    <author>
      <name>Allyx Fontaine</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <author>
      <name>Benjamin Sach</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <link href="http://arxiv.org/abs/1504.06242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.07241">
    <id>http://arxiv.org/abs/1602.07241v1</id>
    <updated>2016-02-23T17:21:26Z</updated>
    <published>2016-02-23T17:21:26Z</published>
    <title>Approximate Hamming distance in a stream</title>
    <summary>  We consider the problem of computing a $(1+\epsilon)$-approximation of the
Hamming distance between a pattern of length $n$ and successive substrings of a
stream. We first look at the one-way randomised communication complexity of
this problem, giving Alice the first half of the stream and Bob the second
half. We show the following: (1) If Alice and Bob both share the pattern then
there is an $O(\epsilon^{-4} \log^2 n)$ bit randomised one-way communication
protocol. (2) If only Alice has the pattern then there is an
$O(\epsilon^{-2}\sqrt{n}\log n)$ bit randomised one-way communication protocol.
  We then go on to develop small space streaming algorithms for
$(1+\epsilon)$-approximate Hamming distance which give worst case running time
guarantees per arriving symbol. (1) For binary input alphabets there is an
$O(\epsilon^{-3} \sqrt{n} \log^{2} n)$ space and $O(\epsilon^{-2} \log{n})$
time streaming $(1+\epsilon)$-approximate Hamming distance algorithm. (2) For
general input alphabets there is an $O(\epsilon^{-5} \sqrt{n} \log^{4} n)$
space and $O(\epsilon^{-4} \log^3 {n})$ time streaming
$(1+\epsilon)$-approximate Hamming distance algorithm.
</summary>
    <author>
      <name>Raphael Clifford</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICALP' 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.05626">
    <id>http://arxiv.org/abs/1607.05626v3</id>
    <updated>2019-04-23T10:49:14Z</updated>
    <published>2016-07-19T15:18:59Z</published>
    <title>Streaming k-mismatch with error correcting and applications</title>
    <summary>  We present a new streaming algorithm for the $k$-Mismatch problem, one of the
most basic problems in pattern matching. Given a pattern and a text, the task
is to find all substrings of the text that are at the Hamming distance at most
$k$ from the pattern. Our algorithm is enhanced with an important new feature
called Error Correcting, and its complexities for $k=1$ and for a general $k$
are comparable to those of the solutions for the $k$-Mismatch problem by Porat
and Porat (FOCS 2009) and Clifford et al. (SODA 2016). In parallel to our
research, a yet more efficient algorithm for the $k$-Mismatch problem with the
Error Correcting feature was developed by Clifford et al. (SODA 2019). Using
the new feature and recent work on streaming Multiple Pattern Matching we
develop a series of streaming algorithms for pattern matching on weighted
strings, which are a commonly used representation of uncertain sequences in
molecular biology. We also show that these algorithms are space-optimal up to
polylog factors.
  A preliminary version of this work was published at DCC 2017 conference.
</summary>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <link href="http://arxiv.org/abs/1607.05626v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05626v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.06606">
    <id>http://arxiv.org/abs/1707.06606v1</id>
    <updated>2017-07-20T16:56:54Z</updated>
    <published>2017-07-20T16:56:54Z</published>
    <title>Improved bounds for testing Dyck languages</title>
    <summary>  In this paper we consider the problem of deciding membership in Dyck
languages, a fundamental family of context-free languages, comprised of
well-balanced strings of parentheses. In this problem we are given a string of
length $n$ in the alphabet of parentheses of $m$ types and must decide if it is
well-balanced. We consider this problem in the property testing setting, where
one would like to make the decision while querying as few characters of the
input as possible.
  Property testing of strings for Dyck language membership for $m=1$, with a
number of queries independent of the input size $n$, was provided in [Alon,
Krivelevich, Newman and Szegedy, SICOMP 2001]. Property testing of strings for
Dyck language membership for $m \ge 2$ was first investigated in [Parnas, Ron
and Rubinfeld, RSA 2003]. They showed an upper bound and a lower bound for
distinguishing strings belonging to the language from strings that are far (in
terms of the Hamming distance) from the language, which are respectively (up to
polylogarithmic factors) the $2/3$ power and the $1/11$ power of the input size
$n$.
  Here we improve the power of $n$ in both bounds. For the upper bound, we
introduce a recursion technique, that together with a refinement of the methods
in the original work provides a test for any power of $n$ larger than $2/5$.
For the lower bound, we introduce a new problem called Truestring Equivalence,
which is easily reducible to the $2$-type Dyck language property testing
problem. For this new problem, we show a lower bound of $n$ to the power of
$1/5$.
</summary>
    <author>
      <name>Eldar Fischer</name>
    </author>
    <author>
      <name>Frédéric Magniez</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <link href="http://arxiv.org/abs/1707.06606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1802.06545">
    <id>http://arxiv.org/abs/1802.06545v1</id>
    <updated>2018-02-19T08:29:40Z</updated>
    <published>2018-02-19T08:29:40Z</published>
    <title>Upper and lower bounds for dynamic data structures on strings</title>
    <summary>  We consider a range of simply stated dynamic data structure problems on
strings. An update changes one symbol in the input and a query asks us to
compute some function of the pattern of length $m$ and a substring of a longer
text. We give both conditional and unconditional lower bounds for variants of
exact matching with wildcards, inner product, and Hamming distance computation
via a sequence of reductions. As an example, we show that there does not exist
an $O(m^{1/2-\varepsilon})$ time algorithm for a large range of these problems
unless the online Boolean matrix-vector multiplication conjecture is false. We
also provide nearly matching upper bounds for most of the problems we consider.
</summary>
    <author>
      <name>Raphael Clifford</name>
    </author>
    <author>
      <name>Allan Grønlund</name>
    </author>
    <author>
      <name>Kasper Green Larsen</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at STACS'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06545v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06545v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1109.4034">
    <id>http://arxiv.org/abs/1109.4034v1</id>
    <updated>2011-09-19T14:20:02Z</updated>
    <published>2011-09-19T14:20:02Z</published>
    <title>Tying up the loose ends in fully LZW-compressed pattern matching</title>
    <summary>  We consider a natural generalization of the classical pattern matching
problem: given compressed representations of a pattern p[1..M] and a text
t[1..N] of sizes m and n, respectively, does p occur in t? We develop an
optimal linear time solution for the case when both p and t are compressed
using the LZW method. This improves the previously known O((n+m)log(n+m)) time
solution of Gasieniec and Rytter, and essentially closes the line of research
devoted to studying LZW-compressed exact pattern matching.
</summary>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.4034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.4034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0906.4692">
    <id>http://arxiv.org/abs/0906.4692v1</id>
    <updated>2009-06-25T13:36:24Z</updated>
    <published>2009-06-25T13:36:24Z</published>
    <title>On optimally partitioning a text to improve its compression</title>
    <summary>  In this paper we investigate the problem of partitioning an input string T in
such a way that compressing individually its parts via a base-compressor C gets
a compressed output that is shorter than applying C over the entire T at once.
This problem was introduced in the context of table compression, and then
further elaborated and extended to strings and trees. Unfortunately, the
literature offers poor solutions: namely, we know either a cubic-time algorithm
for computing the optimal partition based on dynamic programming, or few
heuristics that do not guarantee any bounds on the efficacy of their computed
partition, or algorithms that are efficient but work in some specific scenarios
(such as the Burrows-Wheeler Transform) and achieve compression performance
that might be worse than the optimal-partitioning by a $\Omega(\sqrt{\log n})$
factor. Therefore, computing efficiently the optimal solution is still open. In
this paper we provide the first algorithm which is guaranteed to compute in
$O(n \log_{1+\eps}n)$ time a partition of T whose compressed output is
guaranteed to be no more than $(1+\epsilon)$-worse the optimal one, where
$\epsilon$ may be any positive constant.
</summary>
    <author>
      <name>Paolo Ferragina</name>
    </author>
    <author>
      <name>Igor Nitto</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <link href="http://arxiv.org/abs/0906.4692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.4692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.3872">
    <id>http://arxiv.org/abs/1307.3872v1</id>
    <updated>2013-07-15T10:14:56Z</updated>
    <published>2013-07-15T10:14:56Z</published>
    <title>Bicriteria data compression</title>
    <summary>  The advent of massive datasets (and the consequent design of high-performing
distributed storage systems) have reignited the interest of the scientific and
engineering community towards the design of lossless data compressors which
achieve effective compression ratio and very efficient decompression speed.
Lempel-Ziv's LZ77 algorithm is the de facto choice in this scenario because of
its decompression speed and its flexibility in trading decompression speed
versus compressed-space efficiency. Each of the existing implementations offers
a trade-off between space occupancy and decompression speed, so software
engineers have to content themselves by picking the one which comes closer to
the requirements of the application in their hands. Starting from these
premises, and for the first time in the literature, we address in this paper
the problem of trading optimally, and in a principled way, the consumption of
these two resources by introducing the Bicriteria LZ77-Parsing problem, which
formalizes in a principled way what data-compressors have traditionally
approached by means of heuristics. The goal is to determine an LZ77 parsing
which minimizes the space occupancy in bits of the compressed file, provided
that the decompression time is bounded by a fixed amount (or vice-versa). This
way, the software engineer can set its space (or time) requirements and then
derive the LZ77 parsing which optimizes the decompression speed (or the space
occupancy, respectively). We solve this problem efficiently in O(n log^2 n)
time and optimal linear space within a small, additive approximation, by
proving and deploying some specific structural properties of the weighted graph
derived from the possible LZ77-parsings of the input file. The preliminary set
of experiments shows that our novel proposal dominates all the highly
engineered competitors, hence offering a win-win situation in theory&amp;practice.
</summary>
    <author>
      <name>Andrea Farruggia</name>
    </author>
    <author>
      <name>Paolo Ferragina</name>
    </author>
    <author>
      <name>Antonio Frangioni</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <link href="http://arxiv.org/abs/1307.3872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1101.0080">
    <id>http://arxiv.org/abs/1101.0080v3</id>
    <updated>2011-01-09T21:41:02Z</updated>
    <published>2010-12-30T13:04:56Z</published>
    <title>A Searchable Compressed Edit-Sensitive Parsing</title>
    <summary>  Practical data structures for the edit-sensitive parsing (ESP) are proposed.
Given a string S, its ESP tree is equivalent to a context-free grammar G
generating just S, which is represented by a DAG. Using the succinct data
structures for trees and permutations, G is decomposed to two LOUDS bit strings
and single array in (1+\epsilon)n\log n+4n+o(n) bits for any 0&lt;\epsilon &lt;1 and
the number n of variables in G. The time to count occurrences of P in S is in
O(\frac{1}{\epsilon}(m\log n+occ_c(\log m\log u)), whereas m = |P|, u = |S|,
and occ_c is the number of occurrences of a maximal common subtree in ESPs of P
and S. The efficiency of the proposed index is evaluated by the experiments
conducted on several benchmarks complying with the other compressed indexes.
</summary>
    <author>
      <name>Naoya Kishiue</name>
    </author>
    <author>
      <name>Masaya Nakahara</name>
    </author>
    <author>
      <name>Shirou Maruyama</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.0080v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.0080v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1408.0467">
    <id>http://arxiv.org/abs/1408.0467v2</id>
    <updated>2014-08-26T05:56:42Z</updated>
    <published>2014-08-03T07:48:52Z</published>
    <title>Online Pattern Matching for String Edit Distance with Moves</title>
    <summary>  Edit distance with moves (EDM) is a string-to-string distance measure that
includes substring moves in addition to ordinal editing operations to turn one
string to the other. Although optimizing EDM is intractable, it has many
applications especially in error detections. Edit sensitive parsing (ESP) is an
efficient parsing algorithm that guarantees an upper bound of parsing
discrepancies between different appearances of the same substrings in a string.
ESP can be used for computing an approximate EDM as the L1 distance between
characteristic vectors built by node labels in parsing trees. However, ESP is
not applicable to a streaming text data where a whole text is unknown in
advance. We present an online ESP (OESP) that enables an online pattern
matching for EDM. OESP builds a parse tree for a streaming text and computes
the L1 distance between characteristic vectors in an online manner. For the
space-efficient computation of EDM, OESP directly encodes the parse tree into a
succinct representation by leveraging the idea behind recent results of a
dynamic succinct tree. We experimentally test OESP on the ability to compute
EDM in an online manner on benchmark datasets, and we show OESP's efficiency.
</summary>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted to the 21st edition of the International
  Symposium on String Processing and Information Retrieval (SPIRE2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.0467v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.0467v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1404.4972">
    <id>http://arxiv.org/abs/1404.4972v2</id>
    <updated>2014-04-28T03:08:52Z</updated>
    <published>2014-04-19T17:08:29Z</published>
    <title>Improved ESP-index: a practical self-index for highly repetitive texts</title>
    <summary>  While several self-indexes for highly repetitive texts exist, developing a
practical self-index applicable to real world repetitive texts remains a
challenge. ESP-index is a grammar-based self-index on the notion of
edit-sensitive parsing (ESP), an efficient parsing algorithm that guarantees
upper bounds of parsing discrepancies between different appearances of the same
subtexts in a text. Although ESP-index performs efficient top-down searches of
query texts, it has a serious issue on binary searches for finding appearances
of variables for a query text, which resulted in slowing down the query
searches. We present an improved ESP-index (ESP-index-I) by leveraging the idea
behind succinct data structures for large alphabets. While ESP-index-I keeps
the same types of efficiencies as ESP-index about the top-down searches, it
avoid the binary searches using fast rank/select operations. We experimentally
test ESP-index-I on the ability to search query texts and extract subtexts from
real world repetitive texts on a large-scale, and we show that ESP-index-I
performs better that other possible approaches.
</summary>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the full version of a proceeding accepted to the 11th
  International Symposium on Experimental Algorithms (SEA2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.4972v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4972v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1507.00805">
    <id>http://arxiv.org/abs/1507.00805v2</id>
    <updated>2015-07-06T11:50:02Z</updated>
    <published>2015-07-03T03:15:01Z</published>
    <title>Online Self-Indexed Grammar Compression</title>
    <summary>  Although several grammar-based self-indexes have been proposed thus far,
their applicability is limited to offline settings where whole input texts are
prepared, thus requiring to rebuild index structures for given additional
inputs, which is often the case in the big data era. In this paper, we present
the first online self-indexed grammar compression named OESP-index that can
gradually build the index structure by reading input characters one-by-one.
Such a property is another advantage which enables saving a working space for
construction, because we do not need to store input texts in memory. We
experimentally test OESP-index on the ability to build index structures and
search query texts, and we show OESP-index's efficiency, especially
space-efficiency for building index structures.
</summary>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proceedings of the 22nd edition of the International
  Symposium on String Processing and Information Retrieval (SPIRE2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.00805v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.00805v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.06688">
    <id>http://arxiv.org/abs/1602.06688v2</id>
    <updated>2016-04-08T05:23:27Z</updated>
    <published>2016-02-22T09:02:44Z</published>
    <title>siEDM: an efficient string index and search algorithm for edit distance
  with moves</title>
    <summary>  Although several self-indexes for highly repetitive text collections exist,
developing an index and search algorithm with editing operations remains a
challenge. Edit distance with moves (EDM) is a string-to-string distance
measure that includes substring moves in addition to ordinal editing operations
to turn one string into another. Although the problem of computing EDM is
intractable, it has a wide range of potential applications, especially in
approximate string retrieval. Despite the importance of computing EDM, there
has been no efficient method for indexing and searching large text collections
based on the EDM measure. We propose the first algorithm, named string index
for edit distance with moves (siEDM), for indexing and searching strings with
EDM. The siEDM algorithm builds an index structure by leveraging the idea
behind the edit sensitive parsing (ESP), an efficient algorithm enabling
approximately computing EDM with guarantees of upper and lower bounds for the
exact EDM. siEDM efficiently prunes the space for searching query strings by
the proposed method, which enables fast query searches with the same guarantee
as ESP. We experimentally tested the ability of siEDM to index and search
strings on benchmark datasets, and we showed siEDM's efficiency.
</summary>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <author>
      <name>Kenta Nakashima</name>
    </author>
    <author>
      <name>Tetsuji Kuboyama</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.06688v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06688v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1103.2613">
    <id>http://arxiv.org/abs/1103.2613v1</id>
    <updated>2011-03-14T09:49:10Z</updated>
    <published>2011-03-14T09:49:10Z</published>
    <title>Linear pattern matching on sparse suffix trees</title>
    <summary>  Packing several characters into one computer word is a simple and natural way
to compress the representation of a string and to speed up its processing.
Exploiting this idea, we propose an index for a packed string, based on a {\em
sparse suffix tree} \cite{KU-96} with appropriately defined suffix links.
Assuming, under the standard unit-cost RAM model, that a word can store up to
$\log_{\sigma}n$ characters ($\sigma$ the alphabet size), our index takes
$O(n/\log_{\sigma}n)$ space, i.e. the same space as the packed string itself.
The resulting pattern matching algorithm runs in time $O(m+r^2+r\cdot occ)$,
where $m$ is the length of the pattern, $r$ is the actual number of characters
stored in a word and $occ$ is the number of pattern occurrences.
</summary>
    <author>
      <name>Roman Kolpakov</name>
    </author>
    <author>
      <name>Gregory Kucherov</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CCP.2011.45</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CCP.2011.45" rel="related"/>
    <link href="http://arxiv.org/abs/1103.2613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.2613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1202.5233">
    <id>http://arxiv.org/abs/1202.5233v4</id>
    <updated>2012-11-30T10:05:56Z</updated>
    <published>2012-02-23T16:50:51Z</published>
    <title>Computing Lempel-Ziv Factorization Online</title>
    <summary>  We present an algorithm which computes the Lempel-Ziv factorization of a word
$W$ of length $n$ on an alphabet $\Sigma$ of size $\sigma$ online in the
following sense: it reads $W$ starting from the left, and, after reading each
$r = O(\log_{\sigma} n)$ characters of $W$, updates the Lempel-Ziv
factorization. The algorithm requires $O(n \log \sigma)$ bits of space and O(n
\log^2 n) time. The basis of the algorithm is a sparse suffix tree combined
with wavelet trees.
</summary>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-32589-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-32589-2" rel="related"/>
    <link href="http://arxiv.org/abs/1202.5233v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.5233v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1202.4076">
    <id>http://arxiv.org/abs/1202.4076v1</id>
    <updated>2012-02-18T13:31:36Z</updated>
    <published>2012-02-18T13:31:36Z</published>
    <title>Cross-Document Pattern Matching</title>
    <summary>  We study a new variant of the string matching problem called cross-document
string matching, which is the problem of indexing a collection of documents to
support an efficient search for a pattern in a selected document, where the
pattern itself is a substring of another document. Several variants of this
problem are considered, and efficient linear-space solutions are proposed with
query time bounds that either do not depend at all on the pattern size or
depend on it in a very limited way (doubly logarithmic). As a side result, we
propose an improved solution to the weighted level ancestor problem.
</summary>
    <author>
      <name>Gregory Kucherov</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-31265-6_16</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-31265-6_16" rel="related"/>
    <link href="http://arxiv.org/abs/1202.4076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0802.0835">
    <id>http://arxiv.org/abs/0802.0835v1</id>
    <updated>2008-02-06T16:31:54Z</updated>
    <published>2008-02-06T16:31:54Z</published>
    <title>Bit-Optimal Lempel-Ziv compression</title>
    <summary>  One of the most famous and investigated lossless data-compression scheme is
the one introduced by Lempel and Ziv about 40 years ago. This compression
scheme is known as "dictionary-based compression" and consists of squeezing an
input string by replacing some of its substrings with (shorter) codewords which
are actually pointers to a dictionary of phrases built as the string is
processed. Surprisingly enough, although many fundamental results are nowadays
known about upper bounds on the speed and effectiveness of this compression
process and references therein), ``we are not aware of any parsing scheme that
achieves optimality when the LZ77-dictionary is in use under any constraint on
the codewords other than being of equal length'' [N. Rajpoot and C. Sahinalp.
Handbook of Lossless Data Compression, chapter Dictionary-based data
compression. Academic Press, 2002. pag. 159]. Here optimality means to achieve
the minimum number of bits in compressing each individual input string, without
any assumption on its generating source. In this paper we provide the first
LZ-based compressor which computes the bit-optimal parsing of any input string
in efficient time and optimal space, for a general class of variable-length
codeword encodings which encompasses most of the ones typically used in data
compression and in the design of search engines and compressed indexes.
</summary>
    <author>
      <name>Paolo Ferragina</name>
    </author>
    <author>
      <name>Igor Nitto</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <link href="http://arxiv.org/abs/0802.0835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.0835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0801.2378">
    <id>http://arxiv.org/abs/0801.2378v1</id>
    <updated>2008-01-15T20:54:18Z</updated>
    <published>2008-01-15T20:54:18Z</published>
    <title>String algorithms and data structures</title>
    <summary>  The string-matching field has grown at a such complicated stage that various
issues come into play when studying it: data structure and algorithmic design,
database principles, compression techniques, architectural features, cache and
prefetching policies. The expertise nowadays required to design good string
data structures and algorithms is therefore transversal to many computer
science fields and much more study on the orchestration of known, or novel,
techniques is needed to make progress in this fascinating topic. This survey is
aimed at illustrating the key ideas which should constitute, in our opinion,
the current background of every index designer. We also discuss the positive
features and drawback of known indexing schemes and algorithms, and devote much
attention to detail research issues and open problems both on the theoretical
and the experimental side.
</summary>
    <author>
      <name>Paolo Ferragina</name>
    </author>
    <link href="http://arxiv.org/abs/0801.2378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.2378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0912.0850">
    <id>http://arxiv.org/abs/0912.0850v4</id>
    <updated>2010-02-06T02:09:00Z</updated>
    <published>2009-12-04T13:17:22Z</published>
    <title>Grammar-Based Compression in a Streaming Model</title>
    <summary>  We show that, given a string $s$ of length $n$, with constant memory and
logarithmic passes over a constant number of streams we can build a
context-free grammar that generates $s$ and only $s$ and whose size is within
an $\Oh{\min (g \log g, \sqrt{n \log g})}$-factor of the minimum $g$. This
stands in contrast to our previous result that, with polylogarithmic memory and
polylogarithmic passes over a single stream, we cannot build such a grammar
whose size is within any polynomial of $g$.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Section on recent work added, sketching how to improve bounds and
  support random access</arxiv:comment>
    <link href="http://arxiv.org/abs/0912.0850v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.0850v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1610.03337">
    <id>http://arxiv.org/abs/1610.03337v1</id>
    <updated>2016-10-11T13:51:00Z</updated>
    <published>2016-10-11T13:51:00Z</published>
    <title>String Cadences</title>
    <summary>  We say a string has a cadence if a certain character is repeated at regular
intervals, possibly with intervening occurrences of that character. We call the
cadence anchored if the first interval must be the same length as the others.
We give a sub-quadratic algorithm for determining whether a string has any
cadence consisting of at least three occurrences of a character, and a nearly
linear algorithm for finding all anchored cadences.
</summary>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Alberto Apostolico</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <link href="http://arxiv.org/abs/1610.03337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1610.02865">
    <id>http://arxiv.org/abs/1610.02865v2</id>
    <updated>2017-02-17T21:51:33Z</updated>
    <published>2016-10-10T11:47:05Z</published>
    <title>An Encoding for Order-Preserving Matching</title>
    <summary>  Encoding data structures store enough information to answer the queries they
are meant to support but not enough to recover their underlying datasets. In
this paper we give the first encoding data structure for the challenging
problem of order-preserving pattern matching. This problem was introduced only
a few years ago but has already attracted significant attention because of its
applications in data analysis. Two strings are said to be an order-preserving
match if the {\em relative order} of their characters is the same: e.g., $4, 1,
3, 2$ and $10, 3, 7, 5$ are an order-preserving match. We show how, given a
string $S [1..n]$ over an arbitrary alphabet and a constant $c \geq 1$, we can
build an $O (n \log \log n)$-bit encoding such that later, given a pattern $P
[1..m]$ with $m \leq \lg^c n$, we can return the number of order-preserving
occurrences of $P$ in $S$ in $O (m)$ time. Within the same time bound we can
also return the starting position of some order-preserving match for $P$ in $S$
(if such a match exists). We prove that our space bound is within a constant
factor of optimal; our query time is optimal if $\log \sigma = \Omega(\log n)$.
Our space bound contrasts with the $\Omega (n \log n)$ bits needed in the worst
case to store $S$ itself, an index for order-preserving pattern matching with
no restrictions on the pattern length, or an index for standard pattern
matching even with restrictions on the pattern length. Moreover, we can build
our encoding knowing only how each character compares to $O (\lg^c n)$
neighbouring characters.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02865v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02865v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.04909">
    <id>http://arxiv.org/abs/1607.04909v2</id>
    <updated>2016-07-19T21:21:33Z</updated>
    <published>2016-07-17T19:34:43Z</published>
    <title>Fully Dynamic de Bruijn Graphs</title>
    <summary>  We present a space- and time-efficient fully dynamic implementation de Bruijn
graphs, which can also support fixed-length jumbled pattern matching.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <author>
      <name>Marco Previtali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 23rd edition of the International Symposium on
  String Processing and Information Retrieval (SPIRE 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04909v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04909v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1606.04495">
    <id>http://arxiv.org/abs/1606.04495v1</id>
    <updated>2016-06-14T18:47:01Z</updated>
    <published>2016-06-14T18:47:01Z</published>
    <title>Range Majorities and Minorities in Arrays</title>
    <summary>  Karpinski and Nekrich (2008) introduced the problem of parameterized range
majority, which asks us to preprocess a string of length $n$ such that, given
the endpoints of a range, one can quickly find all the distinct elements whose
relative frequencies in that range are more than a threshold $\tau$. Subsequent
authors have reduced their time and space bounds such that, when $\tau$ is
fixed at preprocessing time, we need either $O(n \log (1 / \tau))$ space and
optimal $O(1 / \tau)$ query time or linear space and $O((1 / \tau) \log \log
\sigma)$ query time, where $\sigma$ is the alphabet size. In this paper we give
the first linear-space solution with optimal $O(1 / \tau)$ query time, even
with variable $\tau$ (i.e., specified with the query). For the case when
$\sigma$ is polynomial on the computer word size, our space is optimally
compressed according to the symbol frequencies in the string. Otherwise, either
the compressed space is increased by an arbitrarily small constant factor or
the time rises to any function in $(1/\tau)\cdot\omega(1)$. We obtain the same
results on the complementary problem of parameterized range minority introduced
by Chan et al. (2015), who had achieved linear space and $O(1 / \tau)$ query
time with variable $\tau$.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1210.1765</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.09362">
    <id>http://arxiv.org/abs/1605.09362v3</id>
    <updated>2017-05-18T21:06:43Z</updated>
    <published>2016-05-30T19:40:18Z</published>
    <title>Document Retrieval on Repetitive String Collections</title>
    <summary>  Most of the fastest-growing string collections today are repetitive, that is,
most of the constituent documents are similar to many others. As these
collections keep growing, a key approach to handling them is to exploit their
repetitiveness, which can reduce their space usage by orders of magnitude. We
study the problem of indexing repetitive string collections in order to perform
efficient document retrieval operations on them. Document retrieval problems
are routinely solved by search engines on large natural language collections,
but the techniques are less developed on generic string collections. The case
of repetitive string collections is even less understood, and there are very
few existing solutions. We develop two novel ideas, {\em interleaved LCPs} and
{\em precomputed document lists}, that yield highly compressed indexes solving
the problem of document listing (find all the documents where a string
appears), top-$k$ document retrieval (find the $k$ documents where a string
appears most often), and document counting (count the number of documents where
a string appears). We also show that a classical data structure supporting the
latter query becomes highly compressible on repetitive data. Finally, we show
how the tools we developed can be combined to solve ranked conjunctive and
disjunctive multi-term queries under the simple tf-idf model of relevance. We
thoroughly evaluate the resulting techniques in various real-life
repetitiveness scenarios, and recommend the best choices for each case.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Aleksi Hartikainen</name>
    </author>
    <author>
      <name>Kalle Karhu</name>
    </author>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Jouni Sirén</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. Accepted to the Information
  Retrieval Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09362v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09362v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.04421">
    <id>http://arxiv.org/abs/1605.04421v1</id>
    <updated>2016-05-14T13:49:27Z</updated>
    <published>2016-05-14T13:49:27Z</published>
    <title>RLZAP: Relative Lempel-Ziv with Adaptive Pointers</title>
    <summary>  Relative Lempel-Ziv (RLZ) is a popular algorithm for compressing databases of
genomes from individuals of the same species when fast random access is
desired. With Kuruppu et al.'s (SPIRE 2010) original implementation, a
reference genome is selected and then the other genomes are greedily parsed
into phrases exactly matching substrings of the reference. Deorowicz and
Grabowski (Bioinformatics, 2011) pointed out that letting each phrase end with
a mismatch character usually gives better compression because many of the
differences between individuals' genomes are single-nucleotide substitutions.
Ferrada et al. (SPIRE 2014) then pointed out that also using relative pointers
and run-length compressing them usually gives even better compression. In this
paper we generalize Ferrada et al.'s idea to handle well also short insertions,
deletions and multi-character substitutions. We show experimentally that our
generalization achieves better compression than Ferrada et al.'s implementation
with comparable random-access times.
</summary>
    <author>
      <name>Anthony J. Cox</name>
    </author>
    <author>
      <name>Andrea Farruggia</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Jouni Sirén</name>
    </author>
    <link href="http://arxiv.org/abs/1605.04421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1710.01952">
    <id>http://arxiv.org/abs/1710.01952v1</id>
    <updated>2017-10-05T10:30:27Z</updated>
    <published>2017-10-05T10:30:27Z</published>
    <title>Efficient Compression and Indexing of Trajectories</title>
    <summary>  We present a new compressed representation of free trajectories of moving
objects. It combines a partial-sums-based structure that retrieves in constant
time the position of the object at any instant, with a hierarchical
minimum-bounding-boxes representation that allows determining if the object is
seen in a certain rectangular area during a time period. Combined with spatial
snapshots at regular intervals, the representation is shown to outperform
classical ones by orders of magnitude in space, and also to outperform previous
compressed representations in time performance, when using the same amount of
space.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Adrián Gómez-Brandón</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>José R. Paramá</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-67428-5_10</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-67428-5_10" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">String Processing and Information Retrieval: 24th International
  Symposium, SPIRE 2017, Palermo, Italy, September 26-29, 2017, Proceedings.
  Springer International Publishing. pp 103-115. ISBN: 9783319674278</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.01952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1702.01280">
    <id>http://arxiv.org/abs/1702.01280v2</id>
    <updated>2017-02-14T12:24:33Z</updated>
    <published>2017-02-04T12:15:06Z</published>
    <title>Fast and Simple Jumbled Indexing for Binary RLE Strings</title>
    <summary>  Important papers have appeared recently on the problem of indexing binary
strings for jumbled pattern matching, and further lowering the time bounds in
terms of the input size would now be a breakthrough with broad implications. We
can still make progress on the problem, however, by considering other natural
parameters. Badkobeh et al.\ (IPL, 2013) and Amir et al.\ (TCS, 2016) gave
algorithms that index a binary string in $O (n + \rho^2 \log \rho)$ time, where
$n$ is the length and $\rho$ is the number of runs, and Giaquinta and Grabowski
(IPL, 2013) gave one that runs in $O (n + \rho^2)$ time. In this paper we
propose a new and very simple algorithm that also runs in $O(n + \rho^2)$ time
and can be extended either so that the index returns the position of a match
(if there is one), or so that the algorithm uses only $O (n)$ bits of space.
</summary>
    <author>
      <name>Luís Cunha</name>
    </author>
    <author>
      <name>Simone Dantas</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Roland Wittler</name>
    </author>
    <author>
      <name>Luis Kowada</name>
    </author>
    <author>
      <name>Jens Stoye</name>
    </author>
    <link href="http://arxiv.org/abs/1702.01280v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01280v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2205.10156">
    <id>http://arxiv.org/abs/2205.10156v1</id>
    <updated>2022-05-20T13:05:49Z</updated>
    <published>2022-05-20T13:05:49Z</published>
    <title>A note on the maximum number of $k$-powers in a finite word</title>
    <summary>  A \emph{power} is a word of the form $\underbrace{uu...u}_{k \;
\text{times}}$, where $u$ is a word and $k$ is a positive integer; the power is
also called a {\em $k$-power} and $k$ is its {\em exponent}. We prove that for
any $k \ge 2$, the maximum number of different non-empty $k$-power factors in a
word of length $n$ is between $\frac{n}{k-1}-\Theta(\sqrt{n})$ and
$\frac{n-1}{k-1}$. We also show that the maximum number of different non-empty
power factors of exponent at least 2 in a length-$n$ word is at most $n-1$.
Both upper bounds generalize the recent upper bound of $n-1$ on the maximum
number of different square factors in a length-$n$ word by Brlek and Li (2022).
</summary>
    <author>
      <name>Shuo Li</name>
    </author>
    <author>
      <name>Jakub Pachocki</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <link href="http://arxiv.org/abs/2205.10156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.10156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="cs/0512061">
    <id>http://arxiv.org/abs/cs/0512061v3</id>
    <updated>2007-12-07T08:40:11Z</updated>
    <published>2005-12-15T10:28:04Z</published>
    <title>Matching Subsequences in Trees</title>
    <summary>  Given two rooted, labeled trees $P$ and $T$ the tree path subsequence problem
is to determine which paths in $P$ are subsequences of which paths in $T$. Here
a path begins at the root and ends at a leaf. In this paper we propose this
problem as a useful query primitive for XML data, and provide new algorithms
improving the previously best known time and space bounds.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Goertz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor correction of typos, etc</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0512061v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0512061v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="cs/0609085">
    <id>http://arxiv.org/abs/cs/0609085v2</id>
    <updated>2007-05-03T11:07:06Z</updated>
    <published>2006-09-15T07:36:25Z</published>
    <title>Improved Approximate String Matching and Regular Expression Matching on
  Ziv-Lempel Compressed Texts</title>
    <summary>  We study the approximate string matching and regular expression matching
problem for the case when the text to be searched is compressed with the
Ziv-Lempel adaptive dictionary compression schemes. We present a time-space
trade-off that leads to algorithms improving the previously known complexities
for both problems. In particular, we significantly improve the space bounds,
which in practical applications are likely to be a bottleneck.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Rolf Fagerberg</name>
    </author>
    <author>
      <name>Inge Li Goertz</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0609085v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609085v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="cs/0608124">
    <id>http://arxiv.org/abs/cs/0608124v5</id>
    <updated>2011-01-18T15:38:52Z</updated>
    <published>2006-08-31T12:23:37Z</published>
    <title>The Tree Inclusion Problem: In Linear Space and Faster</title>
    <summary>  Given two rooted, ordered, and labeled trees $P$ and $T$ the tree inclusion
problem is to determine if $P$ can be obtained from $T$ by deleting nodes in
$T$. This problem has recently been recognized as an important query primitive
in XML databases. Kilpel\"ainen and Mannila [\emph{SIAM J. Comput. 1995}]
presented the first polynomial time algorithm using quadratic time and space.
Since then several improved results have been obtained for special cases when
$P$ and $T$ have a small number of leaves or small depth. However, in the worst
case these algorithms still use quadratic time and space. Let $n_S$, $l_S$, and
$d_S$ denote the number of nodes, the number of leaves, and the %maximum depth
of a tree $S \in \{P, T\}$. In this paper we show that the tree inclusion
problem can be solved in space $O(n_T)$ and time: O(\min(l_Pn_T, l_Pl_T\log
\log n_T + n_T, \frac{n_Pn_T}{\log n_T} + n_{T}\log n_{T})). This improves or
matches the best known time complexities while using only linear space instead
of quadratic. This is particularly important in practical applications, such as
XML databases, where the space is likely to be a bottleneck.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Goertz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor updates from last time</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0608124v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0608124v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="cs/0606116">
    <id>http://arxiv.org/abs/cs/0606116v1</id>
    <updated>2006-06-28T10:51:39Z</updated>
    <published>2006-06-28T10:51:39Z</published>
    <title>New Algorithms for Regular Expression Matching</title>
    <summary>  In this paper we revisit the classical regular expression matching problem,
namely, given a regular expression $R$ and a string $Q$, decide if $Q$ matches
one of the strings specified by $R$. Let $m$ and $n$ be the length of $R$ and
$Q$, respectively. On a standard unit-cost RAM with word length $w \geq \log
n$, we show that the problem can be solved in $O(m)$ space with the following
running times: \begin{equation*} \begin{cases}
  O(n\frac{m \log w}{w} + m \log w) &amp; \text{if $m > w$} \\
  O(n\log m + m\log m) &amp; \text{if $\sqrt{w} &lt; m \leq w$} \\
  O(\min(n+ m^2, n\log m + m\log m)) &amp; \text{if $m \leq \sqrt{w}$.} \end{cases}
\end{equation*} This improves the best known time bound among algorithms using
$O(m)$ space. Whenever $w \geq \log^2 n$ it improves all known time bounds
regardless of how much space is used.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0606116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0708.4288">
    <id>http://arxiv.org/abs/0708.4288v1</id>
    <updated>2007-08-31T08:07:32Z</updated>
    <published>2007-08-31T08:07:32Z</published>
    <title>Pattern Matching in Trees and Strings</title>
    <summary>  We study the design of efficient algorithms for combinatorial pattern
matching. More concretely, we study algorithms for tree matching, string
matching, and string matching in compressed texts.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD dissertation, 140 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.4288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.4288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0811.3490">
    <id>http://arxiv.org/abs/0811.3490v2</id>
    <updated>2011-03-17T21:11:16Z</updated>
    <published>2008-11-21T08:52:59Z</published>
    <title>Faster Approximate String Matching for Short Patterns</title>
    <summary>  We study the classical approximate string matching problem, that is, given
strings $P$ and $Q$ and an error threshold $k$, find all ending positions of
substrings of $Q$ whose edit distance to $P$ is at most $k$. Let $P$ and $Q$
have lengths $m$ and $n$, respectively. On a standard unit-cost word RAM with
word size $w \geq \log n$ we present an algorithm using time $$ O(nk \cdot
\min(\frac{\log^2 m}{\log n},\frac{\log^2 m\log w}{w}) + n) $$ When $P$ is
short, namely, $m = 2^{o(\sqrt{\log n})}$ or $m = 2^{o(\sqrt{w/\log w})}$ this
improves the previously best known time bounds for the problem. The result is
achieved using a novel implementation of the Landau-Vishkin algorithm based on
tabulation and word-level parallelism.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Theory of Computing Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.3490v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.3490v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0911.0577">
    <id>http://arxiv.org/abs/0911.0577v2</id>
    <updated>2010-09-08T19:16:42Z</updated>
    <published>2009-11-03T13:35:41Z</published>
    <title>Fast Arc-Annotated Subsequence Matching in Linear Space</title>
    <summary>  An arc-annotated string is a string of characters, called bases, augmented
with a set of pairs, called arcs, each connecting two bases. Given
arc-annotated strings $P$ and $Q$ the arc-preserving subsequence problem is to
determine if $P$ can be obtained from $Q$ by deleting bases from $Q$. Whenever
a base is deleted any arc with an endpoint in that base is also deleted.
Arc-annotated strings where the arcs are ``nested'' are a natural model of RNA
molecules that captures both the primary and secondary structure of these. The
arc-preserving subsequence problem for nested arc-annotated strings is basic
primitive for investigating the function of RNA molecules. Gramm et al. [ACM
Trans. Algorithms 2006] gave an algorithm for this problem using $O(nm)$ time
and space, where $m$ and $n$ are the lengths of $P$ and $Q$, respectively. In
this paper we present a new algorithm using $O(nm)$ time and $O(n + m)$ space,
thereby matching the previous time bound while significantly reducing the space
from a quadratic term to linear. This is essential to process large RNA
molecules where the space is likely to be a bottleneck. To obtain our result we
introduce several novel ideas which may be of independent interest for related
problems on arc-annotated strings.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Goertz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Algoritmica</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.0577v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0577v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0907.3135">
    <id>http://arxiv.org/abs/0907.3135v2</id>
    <updated>2010-09-07T19:43:47Z</updated>
    <published>2009-07-17T19:29:13Z</published>
    <title>Fast Searching in Packed Strings</title>
    <summary>  Given strings $P$ and $Q$ the (exact) string matching problem is to find all
positions of substrings in $Q$ matching $P$. The classical Knuth-Morris-Pratt
algorithm [SIAM J. Comput., 1977] solves the string matching problem in linear
time which is optimal if we can only read one character at the time. However,
most strings are stored in a computer in a packed representation with several
characters in a single word, giving us the opportunity to read multiple
characters simultaneously. In this paper we study the worst-case complexity of
string matching on strings given in packed representation. Let $m \leq n$ be
the lengths $P$ and $Q$, respectively, and let $\sigma$ denote the size of the
alphabet. On a standard unit-cost word-RAM with logarithmic word size we
present an algorithm using time $$ O\left(\frac{n}{\log_\sigma n} + m +
\occ\right). $$ Here $\occ$ is the number of occurrences of $P$ in $Q$. For $m
= o(n)$ this improves the $O(n)$ bound of the Knuth-Morris-Pratt algorithm.
Furthermore, if $m = O(n/\log_\sigma n)$ our algorithm is optimal since any
algorithm must spend at least $\Omega(\frac{(n+m)\log
  \sigma}{\log n} + \occ) = \Omega(\frac{n}{\log_\sigma n} + \occ)$ time to
read the input and report all occurrences. The result is obtained by a novel
automaton construction based on the Knuth-Morris-Pratt algorithm combined with
a new compact representation of subautomata allowing an optimal
tabulation-based simulation.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Journal of Discrete Algorithms. Special Issue on CPM
  2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.3135v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.3135v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1110.5236">
    <id>http://arxiv.org/abs/1110.5236v2</id>
    <updated>2012-09-06T13:35:12Z</updated>
    <published>2011-10-24T13:57:08Z</published>
    <title>String Indexing for Patterns with Wildcards</title>
    <summary>  We consider the problem of indexing a string $t$ of length $n$ to report the
occurrences of a query pattern $p$ containing $m$ characters and $j$ wildcards.
Let $occ$ be the number of occurrences of $p$ in $t$, and $\sigma$ the size of
the alphabet. We obtain the following results.
  - A linear space index with query time $O(m+\sigma^j \log \log n + occ)$.
This significantly improves the previously best known linear space index by Lam
et al. [ISAAC 2007], which requires query time $\Theta(jn)$ in the worst case.
  - An index with query time $O(m+j+occ)$ using space $O(\sigma^{k^2} n \log^k
\log n)$, where $k$ is the maximum number of wildcards allowed in the pattern.
This is the first non-trivial bound with this query time.
  - A time-space trade-off, generalizing the index by Cole et al. [STOC 2004].
  We also show that these indexes can be generalized to allow variable length
gaps in the pattern. Our results are obtained using a novel combination of
well-known and new techniques, which could be of independent interest.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Goertz</name>
    </author>
    <author>
      <name>Hjalte Wedel Vildhøj</name>
    </author>
    <author>
      <name>Søren Vind</name>
    </author>
    <link href="http://arxiv.org/abs/1110.5236v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.5236v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1110.2893">
    <id>http://arxiv.org/abs/1110.2893v1</id>
    <updated>2011-10-13T11:13:48Z</updated>
    <published>2011-10-13T11:13:48Z</published>
    <title>String Matching with Variable Length Gaps</title>
    <summary>  We consider string matching with variable length gaps. Given a string $T$ and
a pattern $P$ consisting of strings separated by variable length gaps
(arbitrary strings of length in a specified range), the problem is to find all
ending positions of substrings in $T$ that match $P$. This problem is a basic
primitive in computational biology applications. Let $m$ and $n$ be the lengths
of $P$ and $T$, respectively, and let $k$ be the number of strings in $P$. We
present a new algorithm achieving time $O(n\log k + m +\alpha)$ and space $O(m
+ A)$, where $A$ is the sum of the lower bounds of the lengths of the gaps in
$P$ and $\alpha$ is the total number of occurrences of the strings in $P$
within $T$. Compared to the previous results this bound essentially achieves
the best known time and space complexities simultaneously. Consequently, our
algorithm obtains the best known bounds for almost all combinations of $m$,
$n$, $k$, $A$, and $\alpha$. Our algorithm is surprisingly simple and
straightforward to implement. We also present algorithms for finding and
encoding the positions of all strings in $P$ for every match of the pattern.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Goertz</name>
    </author>
    <author>
      <name>Hjalte Wedel Vildhøj</name>
    </author>
    <author>
      <name>David Kofoed Wind</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">draft of full version, extended abstract at SPIRE 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.2893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.2893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="cs/0611099">
    <id>http://arxiv.org/abs/cs/0611099v1</id>
    <updated>2006-11-21T02:06:31Z</updated>
    <published>2006-11-21T02:06:31Z</published>
    <title>On the space complexity of one-pass compression</title>
    <summary>  We study how much memory one-pass compression algorithms need to compete with
the best multi-pass algorithms. We call a one-pass algorithm an (f (n,
\ell))-footprint compressor if, given $n$, $\ell$ and an $n$-ary string $S$, it
stores $S$ in ((\rule{0ex}{2ex} O (H_\ell (S)) + o (\log n)) |S| + O (n^{\ell +
1} \log n)) bits -- where (H_\ell (S)) is the $\ell$th-order empirical entropy
of $S$ -- while using at most (f (n, \ell)) bits of memory. We prove that, for
any (\epsilon > 0) and some (f (n, \ell) \in O (n^{\ell + \epsilon} \log n)),
there is an (f (n, \ell))-footprint compressor; on the other hand, there is no
(f (n, \ell))-footprint compressor for (f (n, \ell) \in o (n^\ell \log n)).
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0611099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0711.3338">
    <id>http://arxiv.org/abs/0711.3338v2</id>
    <updated>2008-04-19T14:54:21Z</updated>
    <published>2007-11-21T10:32:07Z</published>
    <title>Bounds for Compression in Streaming Models</title>
    <summary>  Compression algorithms and streaming algorithms are both powerful tools for
dealing with massive data sets, but many of the best compression algorithms --
e.g., those based on the Burrows-Wheeler Transform -- at first seem
incompatible with streaming. In this paper we consider several popular
streaming models and ask in which, if any, we can compress as well as we can
with the BWT. We first prove a nearly tight tradeoff between memory and
redundancy for the Standard, Multipass and W-Streams models, demonstrating a
bound that is achievable with the BWT but unachievable in those models. We then
show we can compute the related Schindler Transform in the StreamSort model and
the BWT in the Read-Write model and, thus, achieve that bound.
</summary>
    <author>
      <name>Travis Gagie</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">corresponding author</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">added reduction from sorting to the Burrows-Wheeler Transform; thus,
  Grohe and Schweikardt's lower bound for short-sorting implies the same lower
  bound for the BWT</arxiv:comment>
    <link href="http://arxiv.org/abs/0711.3338v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.3338v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0708.2084">
    <id>http://arxiv.org/abs/0708.2084v1</id>
    <updated>2007-08-15T19:00:10Z</updated>
    <published>2007-08-15T19:00:10Z</published>
    <title>Empirical entropy in context</title>
    <summary>  We trace the history of empirical entropy, touching briefly on its relation
to Markov processes, normal numbers, Shannon entropy, the Chomsky hierarchy,
Kolmogorov complexity, Ziv-Lempel compression, de Bruijn sequences and
stochastic complexity.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A survey of some results related to empirical entropy, written in the
  spring of 2007 as part of an introduction to a PhD thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.2084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.2084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0708.1877">
    <id>http://arxiv.org/abs/0708.1877v1</id>
    <updated>2007-08-14T12:50:41Z</updated>
    <published>2007-08-14T12:50:41Z</published>
    <title>A nearly tight memory-redundancy trade-off for one-pass compression</title>
    <summary>  Let $s$ be a string of length $n$ over an alphabet of constant size $\sigma$
and let $c$ and $\epsilon$ be constants with (1 \geq c \geq 0) and (\epsilon >
0). Using (O (n)) time, (O (n^c)) bits of memory and one pass we can always
encode $s$ in (n H_k (s) + O (\sigma^k n^{1 - c + \epsilon})) bits for all
integers (k \geq 0) simultaneously. On the other hand, even with unlimited
time, using (O (n^c)) bits of memory and one pass we cannot always encode $s$
in (O (n H_k (s) + \sigma^k n^{1 - c - \epsilon})) bits for, e.g., (k = \lceil
(c + \epsilon / 2) \log_\sigma n \rceil).
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/0708.1877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.1877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0812.3306">
    <id>http://arxiv.org/abs/0812.3306v1</id>
    <updated>2008-12-17T14:47:34Z</updated>
    <published>2008-12-17T14:47:34Z</published>
    <title>Worst-Case Optimal Adaptive Prefix Coding</title>
    <summary>  A common complaint about adaptive prefix coding is that it is much slower
than static prefix coding. Karpinski and Nekrich recently took an important
step towards resolving this: they gave an adaptive Shannon coding algorithm
that encodes each character in (O (1)) amortized time and decodes it in (O
(\log H)) amortized time, where $H$ is the empirical entropy of the input
string $s$. For comparison, Gagie's adaptive Shannon coder and both Knuth's and
Vitter's adaptive Huffman coders all use (\Theta (H)) amortized time for each
character. In this paper we give an adaptive Shannon coder that both encodes
and decodes each character in (O (1)) worst-case time. As with both previous
adaptive Shannon coders, we store $s$ in at most ((H + 1) |s| + o (|s|)) bits.
We also show that this encoding length is worst-case optimal up to the lower
order term.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/0812.3306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.3306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0812.2868">
    <id>http://arxiv.org/abs/0812.2868v2</id>
    <updated>2009-01-28T13:45:39Z</updated>
    <published>2008-12-15T17:15:51Z</published>
    <title>Minimax Trees in Linear Time</title>
    <summary>  A minimax tree is similar to a Huffman tree except that, instead of
minimizing the weighted average of the leaves' depths, it minimizes the maximum
of any leaf's weight plus its depth. Golumbic (1976) introduced minimax trees
and gave a Huffman-like, $\Oh{n \log n}$-time algorithm for building them.
Drmota and Szpankowski (2002) gave another $\Oh{n \log n}$-time algorithm,
which checks the Kraft Inequality in each step of a binary search. In this
paper we show how Drmota and Szpankowski's algorithm can be made to run in
linear time on a word RAM with (\Omega (\log n))-bit words. We also discuss how
our solution applies to problems in data compression, group testing and circuit
design.
</summary>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/0812.2868v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.2868v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0811.3602">
    <id>http://arxiv.org/abs/0811.3602v1</id>
    <updated>2008-11-21T18:23:00Z</updated>
    <published>2008-11-21T18:23:00Z</published>
    <title>Low-Memory Adaptive Prefix Coding</title>
    <summary>  In this paper we study the adaptive prefix coding problem in cases where the
size of the input alphabet is large. We present an online prefix coding
algorithm that uses $O(\sigma^{1 / \lambda + \epsilon}) $ bits of space for any
constants $\eps>0$, $\lambda>1$, and encodes the string of symbols in $O(\log
\log \sigma)$ time per symbol \emph{in the worst case}, where $\sigma$ is the
size of the alphabet. The upper bound on the encoding length is $\lambda n H
(s) +(\lambda \ln 2 + 2 + \epsilon) n + O (\sigma^{1 / \lambda} \log^2 \sigma)$
bits.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Marek Karpinski</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.3602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.3602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0810.5064">
    <id>http://arxiv.org/abs/0810.5064v1</id>
    <updated>2008-10-28T15:59:55Z</updated>
    <published>2008-10-28T15:59:55Z</published>
    <title>A New Algorithm for Building Alphabetic Minimax Trees</title>
    <summary>  We show how to build an alphabetic minimax tree for a sequence (W = w_1,
>..., w_n) of real weights in (O (n d \log \log n)) time, where $d$ is the
number of distinct integers (\lceil w_i \rceil). We apply this algorithm to
building an alphabetic prefix code given a sample.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in preparation</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.5064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.5064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0912.5079">
    <id>http://arxiv.org/abs/0912.5079v1</id>
    <updated>2009-12-27T14:39:30Z</updated>
    <published>2009-12-27T14:39:30Z</published>
    <title>A Lower Bound on the Complexity of Approximating the Entropy of a Markov
  Source</title>
    <summary>  Suppose that, for any (k \geq 1), (\epsilon > 0) and sufficiently large
$\sigma$, we are given a black box that allows us to sample characters from a
$k$th-order Markov source over the alphabet (\{0, ..., \sigma - 1\}). Even if
we know the source has entropy either 0 or at least (\log (\sigma - k)), there
is still no algorithm that, with probability bounded away from (1 / 2), guesses
the entropy correctly after sampling at most ((\sigma - k)^{k / 2 - \epsilon})
characters.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/0912.5079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.5079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="cs/0509069">
    <id>http://arxiv.org/abs/cs/0509069v3</id>
    <updated>2008-09-22T08:27:28Z</updated>
    <published>2005-09-22T13:30:20Z</published>
    <title>Fast and Compact Regular Expression Matching</title>
    <summary>  We study 4 problems in string matching, namely, regular expression matching,
approximate regular expression matching, string edit distance, and subsequence
indexing, on a standard word RAM model of computation that allows
logarithmic-sized words to be manipulated in constant time. We show how to
improve the space and/or remove a dependency on the alphabet size for each
problem using either an improved tabulation technique of an existing algorithm
or by combining known algorithms in a new way.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Martin Farach-Colton</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0509069v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0509069v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; F.2.0; F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1502.06370">
    <id>http://arxiv.org/abs/1502.06370v1</id>
    <updated>2015-02-23T10:18:16Z</updated>
    <published>2015-02-23T10:18:16Z</published>
    <title>A framework for space-efficient string kernels</title>
    <summary>  String kernels are typically used to compare genome-scale sequences whose
length makes alignment impractical, yet their computation is based on data
structures that are either space-inefficient, or incur large slowdowns. We show
that a number of exact string kernels, like the $k$-mer kernel, the substrings
kernels, a number of length-weighted kernels, the minimal absent words kernel,
and kernels with Markovian corrections, can all be computed in $O(nd)$ time and
in $o(n)$ bits of space in addition to the input, using just a
$\mathtt{rangeDistinct}$ data structure on the Burrows-Wheeler transform of the
input strings, which takes $O(d)$ time per element in its output. The same
bounds hold for a number of measures of compositional complexity based on
multiple value of $k$, like the $k$-mer profile and the $k$-th order empirical
entropy, and for calibrating the value of $k$ using the data.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Fabio Cunial</name>
    </author>
    <link href="http://arxiv.org/abs/1502.06370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.06370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1609.06378">
    <id>http://arxiv.org/abs/1609.06378v1</id>
    <updated>2016-09-20T22:51:19Z</updated>
    <published>2016-09-20T22:51:19Z</published>
    <title>Linear-time string indexing and analysis in small space</title>
    <summary>  The field of succinct data structures has flourished over the last 16 years.
Starting from the compressed suffix array (CSA) by Grossi and Vitter (STOC
2000) and the FM-index by Ferragina and Manzini (FOCS 2000), a number of
generalizations and applications of string indexes based on the Burrows-Wheeler
transform (BWT) have been developed, all taking an amount of space that is
close to the input size in bits. In many large-scale applications, the
construction of the index and its usage need to be considered as one unit of
computation. Efficient string indexing and analysis in small space lies also at
the core of a number of primitives in the data-intensive field of
high-throughput DNA sequencing. We report the following advances in string
indexing and analysis. We show that the BWT of a string $T\in
\{1,\ldots,\sigma\}^n$ can be built in deterministic $O(n)$ time using just
$O(n\log{\sigma})$ bits of space, where $\sigma \leq n$. Within the same time
and space budget, we can build an index based on the BWT that allows one to
enumerate all the internal nodes of the suffix tree of $T$. Many fundamental
string analysis problems can be mapped to such enumeration, and can thus be
solved in deterministic $O(n)$ time and in $O(n\log{\sigma})$ bits of space
from the input string. We also show how to build many of the existing indexes
based on the BWT, such as the CSA, the compressed suffix tree (CST), and the
bidirectional BWT index, in randomized $O(n)$ time and in $O(n\log{\sigma})$
bits of space. The previously fastest construction algorithms for BWT, CSA and
CST, which used $O(n\log{\sigma})$ bits of space, took $O(n\log{\log{\sigma}})$
time for the first two structures, and $O(n\log^{\epsilon}n)$ time for the
third, where $\epsilon$ is any positive constant. Contrary to the state of the
art, our bidirectional BWT index supports every operation in constant time per
element in its output.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Fabio Cunial</name>
    </author>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal submission (52 pages, 2 figures)</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.06378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.04200">
    <id>http://arxiv.org/abs/1607.04200v1</id>
    <updated>2016-07-14T16:38:17Z</updated>
    <published>2016-07-14T16:38:17Z</published>
    <title>Edit Distance: Sketching, Streaming and Document Exchange</title>
    <summary>  We show that in the document exchange problem, where Alice holds $x \in
\{0,1\}^n$ and Bob holds $y \in \{0,1\}^n$, Alice can send Bob a message of
size $O(K(\log^2 K+\log n))$ bits such that Bob can recover $x$ using the
message and his input $y$ if the edit distance between $x$ and $y$ is no more
than $K$, and output "error" otherwise. Both the encoding and decoding can be
done in time $\tilde{O}(n+\mathsf{poly}(K))$. This result significantly
improves the previous communication bounds under polynomial encoding/decoding
time. We also show that in the referee model, where Alice and Bob hold $x$ and
$y$ respectively, they can compute sketches of $x$ and $y$ of sizes
$\mathsf{poly}(K \log n)$ bits (the encoding), and send to the referee, who can
then compute the edit distance between $x$ and $y$ together with all the edit
operations if the edit distance is no more than $K$, and output "error"
otherwise (the decoding). To the best of our knowledge, this is the first
result for sketching edit distance using $\mathsf{poly}(K \log n)$ bits.
Moreover, the encoding phase of our sketching algorithm can be performed by
scanning the input string in one pass. Thus our sketching algorithm also
implies the first streaming algorithm for computing edit distance and all the
edits exactly using $\mathsf{poly}(K \log n)$ bits of space.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Qin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of an article to be presented at the 57th Annual IEEE
  Symposium on Foundations of Computer Science (FOCS 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.00329">
    <id>http://arxiv.org/abs/1602.00329v1</id>
    <updated>2016-01-31T21:43:40Z</updated>
    <published>2016-01-31T21:43:40Z</published>
    <title>Lempel-Ziv Decoding in External Memory</title>
    <summary>  Simple and fast decoding is one of the main advantages of LZ77-type text
encoding used in many popular file compressors such as gzip and 7zip. With the
recent introduction of external memory algorithms for Lempel-Ziv factorization
there is a need for external memory LZ77 decoding but the standard algorithm
makes random accesses to the text and cannot be trivially modified for external
memory computation. We describe the first external memory algorithms for LZ77
decoding, prove that their I/O complexity is optimal, and demonstrate that they
are very fast in practice, only about three times slower than in-memory
decoding (when reading input and writing output is included in the time).
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Juha Kärkkäinen</name>
    </author>
    <author>
      <name>Dominik Kempa</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-38851-9_5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-38851-9_5" rel="related"/>
    <link href="http://arxiv.org/abs/1602.00329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.08197">
    <id>http://arxiv.org/abs/1707.08197v2</id>
    <updated>2017-09-26T05:12:24Z</updated>
    <published>2017-07-25T20:16:50Z</published>
    <title>Fast Label Extraction in the CDAWG</title>
    <summary>  The compact directed acyclic word graph (CDAWG) of a string $T$ of length $n$
takes space proportional just to the number $e$ of right extensions of the
maximal repeats of $T$, and it is thus an appealing index for highly repetitive
datasets, like collections of genomes from similar species, in which $e$ grows
significantly more slowly than $n$. We reduce from $O(m\log{\log{n}})$ to
$O(m)$ the time needed to count the number of occurrences of a pattern of
length $m$, using an existing data structure that takes an amount of space
proportional to the size of the CDAWG. This implies a reduction from
$O(m\log{\log{n}}+\mathtt{occ})$ to $O(m+\mathtt{occ})$ in the time needed to
locate all the $\mathtt{occ}$ occurrences of the pattern. We also reduce from
$O(k\log{\log{n}})$ to $O(k)$ the time needed to read the $k$ characters of the
label of an edge of the suffix tree of $T$, and we reduce from
$O(m\log{\log{n}})$ to $O(m)$ the time needed to compute the matching
statistics between a query of length $m$ and $T$, using an existing
representation of the suffix tree based on the CDAWG. All such improvements
derive from extracting the label of a vertex or of an arc of the CDAWG using a
straight-line program induced by the reversed CDAWG.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Fabio Cunial</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 1 figure. In proceedings of the 24th International
  Symposium on String Processing and Information Retrieval (SPIRE 2017). arXiv
  admin note: text overlap with arXiv:1705.08640</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.08197v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08197v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.08640">
    <id>http://arxiv.org/abs/1705.08640v1</id>
    <updated>2017-05-24T07:42:36Z</updated>
    <published>2017-05-24T07:42:36Z</published>
    <title>Representing the suffix tree with the CDAWG</title>
    <summary>  Given a string $T$, it is known that its suffix tree can be represented using
the compact directed acyclic word graph (CDAWG) with $e_T$ arcs, taking overall
$O(e_T+e_{{\overline{T}}})$ words of space, where ${\overline{T}}$ is the
reverse of $T$, and supporting some key operations in time between $O(1)$ and
$O(\log{\log{n}})$ in the worst case. This representation is especially
appealing for highly repetitive strings, like collections of similar genomes or
of version-controlled documents, in which $e_T$ grows sublinearly in the length
of $T$ in practice. In this paper we augment such representation, supporting a
number of additional queries in worst-case time between $O(1)$ and $O(\log{n})$
in the RAM model, without increasing space complexity asymptotically. Our
technique, based on a heavy path decomposition of the suffix tree, enables also
a representation of the suffix array, of the inverse suffix array, and of $T$
itself, that takes $O(e_T)$ words of space, and that supports random access in
$O(\log{n})$ time. Furthermore, we establish a connection between the reversed
CDAWG of $T$ and a context-free grammar that produces $T$ and only $T$, which
might have independent interest.
</summary>
    <author>
      <name>Djamal Belazzougui</name>
    </author>
    <author>
      <name>Fabio Cunial</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 1 figure. Presented at the 28th Annual Symposium on
  Combinatorial Pattern Matching (CPM 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="cs/0506056">
    <id>http://arxiv.org/abs/cs/0506056v3</id>
    <updated>2006-03-09T19:00:02Z</updated>
    <published>2005-06-14T03:08:57Z</published>
    <title>Large Alphabets and Incompressibility</title>
    <summary>  We briefly survey some concepts related to empirical entropy -- normal
numbers, de Bruijn sequences and Markov processes -- and investigate how well
it approximates Kolmogorov complexity. Our results suggest $\ell$th-order
empirical entropy stops being a reasonable complexity metric for almost all
strings of length $m$ over alphabets of size $n$ about when $n^\ell$ surpasses
$m$.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ipl.2006.04.008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ipl.2006.04.008" rel="related"/>
    <link href="http://arxiv.org/abs/cs/0506056v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506056v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="cs/0506027">
    <id>http://arxiv.org/abs/cs/0506027v1</id>
    <updated>2005-06-08T22:15:18Z</updated>
    <published>2005-06-08T22:15:18Z</published>
    <title>Sorting a Low-Entropy Sequence</title>
    <summary>  We give the first sorting algorithm with bounds in terms of higher-order
entropies: let $S$ be a sequence of length $m$ containing $n$ distinct elements
and let (H_\ell (S)) be the $\ell$th-order empirical entropy of $S$, with
(n^{\ell + 1} \log n \in O (m)); our algorithm sorts $S$ using ((H_\ell (S) + O
(1)) m) comparisons.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0506027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; E.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="cs/0506016">
    <id>http://arxiv.org/abs/cs/0506016v1</id>
    <updated>2005-06-06T12:28:33Z</updated>
    <published>2005-06-06T12:28:33Z</published>
    <title>Compressing Probability Distributions</title>
    <summary>  We show how to store good approximations of probability distributions in
small space.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ipl.2005.10.006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0506016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="cs/0503085">
    <id>http://arxiv.org/abs/cs/0503085v1</id>
    <updated>2005-03-30T13:51:46Z</updated>
    <published>2005-03-30T13:51:46Z</published>
    <title>Dynamic Shannon Coding</title>
    <summary>  We present a new algorithm for dynamic prefix-free coding, based on Shannon
coding. We give a simple analysis and prove a better upper bound on the length
of the encoding produced than the corresponding bound for dynamic Huffman
coding. We show how our algorithm can be modified for efficient
length-restricted coding, alphabetic coding and coding with unequal letter
costs.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; conference version presented at ESA 2004; journal version
  submitted to IEEE Transactions on Information Theory</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0503085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1703.08940">
    <id>http://arxiv.org/abs/1703.08940v1</id>
    <updated>2017-03-27T05:48:36Z</updated>
    <published>2017-03-27T05:48:36Z</published>
    <title>Tree Edit Distance Cannot be Computed in Strongly Subcubic Time (unless
  APSP can)</title>
    <summary>  The edit distance between two rooted ordered trees with $n$ nodes labeled
from an alphabet~$\Sigma$ is the minimum cost of transforming one tree into the
other by a sequence of elementary operations consisting of deleting and
relabeling existing nodes, as well as inserting new nodes. Tree edit distance
is a well known generalization of string edit distance. The fastest known
algorithm for tree edit distance runs in cubic $O(n^3)$ time and is based on a
similar dynamic programming solution as string edit distance. In this paper we
show that a truly subcubic $O(n^{3-\varepsilon})$ time algorithm for tree edit
distance is unlikely: For $|\Sigma| = \Omega(n)$, a truly subcubic algorithm
for tree edit distance implies a truly subcubic algorithm for the all pairs
shortest paths problem. For $|\Sigma| = O(1)$, a truly subcubic algorithm for
tree edit distance implies an $O(n^{k-\varepsilon})$ algorithm for finding a
maximum weight $k$-clique.
  Thus, while in terms of upper bounds string edit distance and tree edit
distance are highly related, in terms of lower bounds string edit distance
exhibits the hardness of the strong exponential time hypothesis [Backurs, Indyk
STOC'15] whereas tree edit distance exhibits the hardness of all pairs shortest
paths. Our result provides a matching conditional lower bound for one of the
last remaining classic dynamic programming problems.
</summary>
    <author>
      <name>Karl Bringmann</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Shay Mozes</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <link href="http://arxiv.org/abs/1703.08940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1703.04814">
    <id>http://arxiv.org/abs/1703.04814v1</id>
    <updated>2017-03-14T23:06:33Z</updated>
    <published>2017-03-14T23:06:33Z</published>
    <title>Near-Optimal Compression for the Planar Graph Metric</title>
    <summary>  The Planar Graph Metric Compression Problem is to compactly encode the
distances among $k$ nodes in a planar graph of size $n$. Two na\"ive solutions
are to store the graph using $O(n)$ bits, or to explicitly store the distance
matrix with $O(k^2 \log{n})$ bits. The only lower bounds are from the seminal
work of Gavoille, Peleg, Prennes, and Raz [SODA'01], who rule out compressions
into a polynomially smaller number of bits, for {\em weighted} planar graphs,
but leave a large gap for unweighted planar graphs. For example, when
$k=\sqrt{n}$, the upper bound is $O(n)$ and their constructions imply an
$\Omega(n^{3/4})$ lower bound. This gap is directly related to other major open
questions in labelling schemes, dynamic algorithms, and compact routing.
  Our main result is a new compression of the planar graph metric into
$\tilde{O}(\min (k^2 , \sqrt{k\cdot n}))$ bits, which is optimal up to log
factors. Our data structure breaks an $\Omega(k^2)$ lower bound of Krauthgamer,
Nguyen, and Zondiner [SICOMP'14] for compression using minors, and the lower
bound of Gavoille et al. for compression of weighted planar graphs. This is an
unexpected and decisive proof that weights can make planar graphs inherently
more complex. Moreover, we design a new {\em Subset Distance Oracle} for planar
graphs with $\tilde O(\sqrt{k\cdot n})$ space, and $\tilde O(n^{3/4})$ query
time.
  Our work carries strong messages to related fields. In particular, the famous
$O(n^{1/2})$ vs. $\Omega(n^{1/3})$ gap for distance labelling schemes in planar
graphs {\em cannot} be resolved with the current lower bound techniques.
</summary>
    <author>
      <name>Amir Abboud</name>
    </author>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <author>
      <name>Shay Mozes</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.00348">
    <id>http://arxiv.org/abs/2010.00348v1</id>
    <updated>2020-10-01T12:25:52Z</updated>
    <published>2020-10-01T12:25:52Z</published>
    <title>Counting 4-Patterns in Permutations Is Equivalent to Counting 4-Cycles
  in Graphs</title>
    <summary>  Permutation $\sigma$ appears in permutation $\pi$ if there exists a
subsequence of $\pi$ that is order-isomorphic to $\sigma$. The natural question
is to check if $\sigma$ appears in $\pi$, and if so count the number of
occurrences. We know that for any fixed length~k, we can check if a given
pattern of length k appears in a permutation of length n in time linear in n,
but being able to count all such occurrences in $f(k)\cdot n^{o(k/\log k)}$
time would refute the exponential time hypothesis (ETH). This motivates a
systematic study of the complexity of counting occurrences for different
patterns of fixed small length k. We investigate this question for k=4. Very
recently, Even-Zohar and Leng [arXiv 2019] identified two types of 4-patterns.
For the first type they designed an $\~O(n)$ time algorithm, while for the
second they were able to provide an $\~O(n^{1.5})$ time algorithm. This brings
up the question whether the permutations of the second type are inherently
harder than the first type.
  We establish a connection between counting 4-patterns of the second type and
counting 4-cycles in a sparse undirected graph. By designing two-way reductions
we show that the complexities of both problems are the same, up to
polylogarithmic factors. This allows us to provide a reasonable argument for
why there is a difference in the complexities for counting 4-patterns of the
two types. In particular, even for the simpler problem of detecting a 4-cycle
in a graph on m edges, the best known algorithm works in $O(m^{4/3})$ time. Our
reductions imply that an $O(n^{4/3-\varepsilon})$ time algorithm for counting
occurrences would imply an exciting breakthrough for counting (and hence also
detecting) 4-cycles. In the other direction, by plugging in the fastest known
algorithm for counting 4-cycles, we obtain an algorithm for counting
occurrences of any 4-pattern in $O(n^{1.48})$ time.
</summary>
    <author>
      <name>Bartłomiej Dudek</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <link href="http://arxiv.org/abs/2010.00348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.00348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="cs/0610001">
    <id>http://arxiv.org/abs/cs/0610001v1</id>
    <updated>2006-09-29T23:52:09Z</updated>
    <published>2006-09-29T23:52:09Z</published>
    <title>Practical Entropy-Compressed Rank/Select Dictionary</title>
    <summary>  Rank/Select dictionaries are data structures for an ordered set $S \subset
\{0,1,...,n-1\}$ to compute $\rank(x,S)$ (the number of elements in $S$ which
are no greater than $x$), and $\select(i,S)$ (the $i$-th smallest element in
$S$), which are the fundamental components of \emph{succinct data structures}
of strings, trees, graphs, etc. In those data structures, however, only
asymptotic behavior has been considered and their performance for real data is
not satisfactory. In this paper, we propose novel four Rank/Select
dictionaries, esp, recrank, vcode and sdarray, each of which is small if the
number of elements in $S$ is small, and indeed close to $nH_0(S)$ ($H_0(S) \leq
1$ is the zero-th order \textit{empirical entropy} of $S$) in practice, and its
query time is superior to the previous ones. Experimental results reveal the
characteristics of our data structures and also show that these data structures
are superior to existing implementations in both size and query time.
</summary>
    <author>
      <name>Daisuke Okanohara</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0610001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0610001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1011.1708">
    <id>http://arxiv.org/abs/1011.1708v2</id>
    <updated>2012-02-18T09:08:29Z</updated>
    <published>2010-11-08T04:10:42Z</published>
    <title>CRAM: Compressed Random Access Memory</title>
    <summary>  We present a new data structure called the \emph{Compressed Random Access
Memory} (CRAM) that can store a dynamic string $T$ of characters, e.g.,
representing the memory of a computer, in compressed form while achieving
asymptotically almost-optimal bounds (in terms of empirical entropy) on the
compression ratio. It allows short substrings of $T$ to be decompressed and
retrieved efficiently and, significantly, characters at arbitrary positions of
$T$ to be modified quickly during execution \emph{without decompressing the
entire string}. This can be regarded as a new type of data compression that can
update a compressed file directly. Moreover, at the cost of slightly increasing
the time spent per operation, the CRAM can be extended to also support
insertions and deletions. Our key observation that the empirical entropy of a
string does not change much after a small change to the string, as well as our
simple yet efficient method for maintaining an array of variable-length blocks
under length modifications, may be useful for many other applications as well.
</summary>
    <author>
      <name>Jesper Jansson</name>
    </author>
    <author>
      <name>Kunihiko Sadakane</name>
    </author>
    <author>
      <name>Wing-Kin Sung</name>
    </author>
    <link href="http://arxiv.org/abs/1011.1708v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.1708v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1208.3663">
    <id>http://arxiv.org/abs/1208.3663v5</id>
    <updated>2014-06-25T14:08:38Z</updated>
    <published>2012-08-17T19:39:34Z</published>
    <title>Space-Time Trade-offs for Stack-Based Algorithms</title>
    <summary>  In memory-constrained algorithms we have read-only access to the input, and
the number of additional variables is limited. In this paper we introduce the
compressed stack technique, a method that allows to transform algorithms whose
space bottleneck is a stack into memory-constrained algorithms. Given an
algorithm \alg\ that runs in O(n) time using $\Theta(n)$ variables, we can
modify it so that it runs in $O(n^2/s)$ time using a workspace of O(s)
variables (for any $s\in o(\log n)$) or $O(n\log n/\log p)$ time using $O(p\log
n/\log p)$ variables (for any $2\leq p\leq n$). We also show how the technique
can be applied to solve various geometric problems, namely computing the convex
hull of a simple polygon, a triangulation of a monotone polygon, the shortest
path between two points inside a monotone polygon, 1-dimensional pyramid
approximation of a 1-dimensional vector, and the visibility profile of a point
inside a simple polygon. Our approach exceeds or matches the best-known results
for these problems in constant-workspace models (when they exist), and gives
the first trade-off between the size of the workspace and running time. To the
best of our knowledge, this is the first general framework for obtaining
memory-constrained algorithms.
</summary>
    <author>
      <name>Luis Barba</name>
    </author>
    <author>
      <name>Matias Korman</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <author>
      <name>Kunikiko Sadakane</name>
    </author>
    <author>
      <name>Rodrigo Silveira</name>
    </author>
    <link href="http://arxiv.org/abs/1208.3663v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.3663v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="cs/0601081">
    <id>http://arxiv.org/abs/cs/0601081v1</id>
    <updated>2006-01-18T21:20:10Z</updated>
    <published>2006-01-18T21:20:10Z</published>
    <title>An O(1) Solution to the Prefix Sum Problem on a Specialized Memory
  Architecture</title>
    <summary>  In this paper we study the Prefix Sum problem introduced by Fredman.
  We show that it is possible to perform both update and retrieval in O(1) time
simultaneously under a memory model in which individual bits may be shared by
several words.
  We also show that two variants (generalizations) of the problem can be solved
optimally in $\Theta(\lg N)$ time under the comparison based model of
computation.
</summary>
    <author>
      <name>Andrej Brodnik</name>
    </author>
    <author>
      <name>Johan Karlsson</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Andreas Nilsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0601081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0601081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1005.4652">
    <id>http://arxiv.org/abs/1005.4652v2</id>
    <updated>2010-06-24T02:13:02Z</updated>
    <published>2010-05-25T18:27:47Z</published>
    <title>Succinct Representations of Dynamic Strings</title>
    <summary>  The rank and select operations over a string of length n from an alphabet of
size $\sigma$ have been used widely in the design of succinct data structures.
In many applications, the string itself need be maintained dynamically,
allowing characters of the string to be inserted and deleted. Under the word
RAM model with word size $w=\Omega(\lg n)$, we design a succinct representation
of dynamic strings using $nH_0 + o(n)\lg\sigma + O(w)$ bits to support rank,
select, insert and delete in $O(\frac{\lg n}{\lg\lg n}(\frac{\lg \sigma}{\lg\lg
n}+1))$ time. When the alphabet size is small, i.e. when $\sigma = O(\polylog
(n))$, including the case in which the string is a bit vector, these operations
are supported in $O(\frac{\lg n}{\lg\lg n})$ time. Our data structures are more
efficient than previous results on the same problem, and we have applied them
to improve results on the design and construction of space-efficient text
indexes.
</summary>
    <author>
      <name>Meng He</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <link href="http://arxiv.org/abs/1005.4652v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.4652v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1002.3511">
    <id>http://arxiv.org/abs/1002.3511v1</id>
    <updated>2010-02-18T13:29:47Z</updated>
    <published>2010-02-18T13:29:47Z</published>
    <title>Range Reporting for Moving Points on a Grid</title>
    <summary>  In this paper we describe a new data structure that supports orthogonal range
reporting queries on a set of points that move along linear trajectories on a
$U\times U$ grid. The assumption that points lie on a $U\times U$ grid enables
us to significantly decrease the query time in comparison to the standard
kinetic model. Our data structure answers queries in $O(\sqrt{\log U/\log \log
U}+k)$ time, where $k$ denotes the number of points in the answer. The above
improves over the $\Omega(\log n)$ lower bound that is valid in the
infinite-precision kinetic model. The methods used in this paper could be also
of independent interest.
</summary>
    <author>
      <name>Marek Karpinski</name>
    </author>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.3511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.3511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1108.1983">
    <id>http://arxiv.org/abs/1108.1983v1</id>
    <updated>2011-08-09T17:01:12Z</updated>
    <published>2011-08-09T17:01:12Z</published>
    <title>Succinct Representations of Permutations and Functions</title>
    <summary>  We investigate the problem of succinctly representing an arbitrary
permutation, \pi, on {0,...,n-1} so that \pi^k(i) can be computed quickly for
any i and any (positive or negative) integer power k. A representation taking
(1+\epsilon) n lg n + O(1) bits suffices to compute arbitrary powers in
constant time, for any positive constant \epsilon &lt;= 1. A representation taking
the optimal \ceil{\lg n!} + o(n) bits can be used to compute arbitrary powers
in O(lg n / lg lg n) time.
  We then consider the more general problem of succinctly representing an
arbitrary function, f: [n] \rightarrow [n] so that f^k(i) can be computed
quickly for any i and any integer power k. We give a representation that takes
(1+\epsilon) n lg n + O(1) bits, for any positive constant \epsilon &lt;= 1, and
computes arbitrary positive powers in constant time. It can also be used to
compute f^k(i), for any negative integer k, in optimal O(1+|f^k(i)|) time.
  We place emphasis on the redundancy, or the space beyond the
information-theoretic lower bound that the data structure uses in order to
support operations efficiently. A number of lower bounds have recently been
shown on the redundancy of data structures. These lower bounds confirm the
space-time optimality of some of our solutions. Furthermore, the redundancy of
one of our structures "surpasses" a recent lower bound by Golynski [Golynski,
SODA 2009], thus demonstrating the limitations of this lower bound.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <author>
      <name>Venkatesh Raman</name>
    </author>
    <author>
      <name>S. Srinivasa Rao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary versions of these results have appeared in the
  Proceedings of ICALP 2003 and 2004. However, all results in this version are
  improved over the earlier conference version</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.1983v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.1983v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.04380">
    <id>http://arxiv.org/abs/2206.04380v1</id>
    <updated>2022-06-09T09:47:52Z</updated>
    <published>2022-06-09T09:47:52Z</published>
    <title>Hinted Dictionaries: Efficient Functional Ordered Sets and Maps</title>
    <summary>  This article introduces hinted dictionaries for expressing efficient ordered
sets and maps functionally. As opposed to the traditional ordered dictionaries
with logarithmic operations, hinted dictionaries can achieve better performance
by using cursor-like objects referred to as hints. Hinted dictionaries unify
the interfaces of imperative ordered dictionaries (e.g., C++ maps) and
functional ones (e.g., Adams' sets). We show that such dictionaries can use
sorted arrays, unbalanced trees, and balanced trees as their underlying
representations. Throughout the article, we use Scala to present the different
components of hinted dictionaries. We also provide a C++ implementation to
evaluate the effectiveness of hinted dictionaries. Hinted dictionaries provide
superior performance for set-set operations in comparison with the standard
library of C++. Also, they show a competitive performance in comparison with
the SciPy library for sparse vector operations.
</summary>
    <author>
      <name>Amir Shaikhha</name>
    </author>
    <author>
      <name>Mahdi Ghorbani</name>
    </author>
    <author>
      <name>Hesam Shahrokhi</name>
    </author>
    <link href="http://arxiv.org/abs/2206.04380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.04380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.03242">
    <id>http://arxiv.org/abs/2206.03242v1</id>
    <updated>2022-06-07T12:56:56Z</updated>
    <published>2022-06-07T12:56:56Z</published>
    <title>Fast Exact String to D-Texts Alignments</title>
    <summary>  In recent years, aligning a sequence to a pangenome has become a central
problem in genomics and pangenomics. A fast and accurate solution to this
problem can serve as a toolkit to many crucial tasks such as read-correction,
Multiple Sequences Alignment (MSA), genome assemblies, variant calling, just to
name a few. In this paper we propose a new, fast and exact method to align a
string to a D-string, the latter possibly representing an MSA, a pan-genome or
a partial assembly. An implementation of our tool dsa is publicly available at
https://github.com/urbanslug/dsa
</summary>
    <author>
      <name>Njagi Moses Mwaniki</name>
    </author>
    <author>
      <name>Erik Garrison</name>
    </author>
    <author>
      <name>Nadia Pisanti</name>
    </author>
    <link href="http://arxiv.org/abs/2206.03242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.03242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.01776">
    <id>http://arxiv.org/abs/2206.01776v1</id>
    <updated>2022-06-03T18:43:46Z</updated>
    <published>2022-06-03T18:43:46Z</published>
    <title>Properties of a Ternary Infinite Word</title>
    <summary>  We study the properties of the ternary infinite word p =
012102101021012101021012 ... , that is, the fixed point of the map h:0->01,
1->21, 2->0. We determine its factor complexity, critical exponent, and prove
that it is 2-balanced. We compute its abelian complexity and determine the
lengths of its bispecial factors. Finally, we give a characterization of p in
terms of avoided factors.
</summary>
    <author>
      <name>James Currie</name>
    </author>
    <author>
      <name>Pascal Ochem</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2206.01776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.01776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.01688">
    <id>http://arxiv.org/abs/2206.01688v1</id>
    <updated>2022-06-03T17:05:09Z</updated>
    <published>2022-06-03T17:05:09Z</published>
    <title>L-systems for Measuring Repetitiveness*</title>
    <summary>  An L-system (for lossless compression) is a CPD0L-system extended with two
parameters $d$ and $n$, which determines unambiguously a string $w =
\tau(\varphi^d(s))[1:n]$, where $\varphi$ is the morphism of the system, $s$ is
its axiom, and $\tau$ is its coding. The length of the shortest description of
an L-system generating $w$ is known as $\ell$, and is arguably a relevant
measure of repetitiveness that builds on the self-similarities that arise in
the sequence.
  In this paper we deepen the study of the measure $\ell$ and its relation with
$\delta$, a better established lower bound that builds on substring complexity.
Our results show that $\ell$ and $\delta$ are largely orthogonal, in the sense
that one can be much larger than the other depending on the case. This suggests
that both sources of repetitiveness are mostly unrelated. We also show that the
recently introduced NU-systems, which combine the capabilities of L-systems
with bidirectional macro-schemes, can be asymptotically strictly smaller than
both mechanisms, which makes the size $\nu$ of the smallest NU-system the
unique smallest reachable repetitiveness measure to date.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Chile</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CeBiB</arxiv:affiliation>
    </author>
    <author>
      <name>Cristian Urbina</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Chile</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CeBiB</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Funded in part by Basal Funds FB0001, Fondecyt Grant 1-200038, and a
  Conicyt Doctoral Scholarship, ANID, Chile</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.01688v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.01688v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.00995">
    <id>http://arxiv.org/abs/2206.00995v2</id>
    <updated>2022-07-08T21:23:21Z</updated>
    <published>2022-06-02T11:32:53Z</published>
    <title>On the Lie complexity of Sturmian words</title>
    <summary>  Bell and Shallit recently introduced the Lie complexity of an infinite word
$s$ as the function counting for each length the number of conjugacy classes of
words whose elements are all factors of $s$. They proved, using algebraic
techniques, that the Lie complexity is bounded above by the first difference of
the factor complexity plus one; hence, it is uniformly bounded for words with
linear factor complexity, and, in particular, it is at most 2 for Sturmian
words, which are precisely the words with factor complexity $n+1$ for every
$n$. In this note, we provide an elementary combinatorial proof of the result
of Bell and Shallit and give an exact formula for the Lie complexity of any
Sturmian word.
</summary>
    <author>
      <name>Alessandro De Luca</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, submitted to Theoretical Computer Science</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.00995v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.00995v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.00781">
    <id>http://arxiv.org/abs/2206.00781v1</id>
    <updated>2022-06-01T22:06:37Z</updated>
    <published>2022-06-01T22:06:37Z</published>
    <title>Near-Optimal Search Time in $δ$-Optimal Space</title>
    <summary>  Two recent lower bounds on the compressiblity of repetitive sequences,
$\delta \le \gamma$, have received much attention. It has been shown that a
string $S[1..n]$ can be represented within the optimal
$O(\delta\log\frac{n}{\delta})$ space, and further, that within that space one
can find all the $occ$ occurrences in $S$ of any pattern of length $m$ in time
$O(m\log n + occ \log^\epsilon n)$ for any constant $\epsilon>0$. Instead, the
near-optimal search time $O(m+(occ+1)\log^\epsilon n)$ was achieved only within
$O(\gamma\log\frac{n}{\gamma})$ space. Both results are based on considerably
different locally consistent parsing techniques. The question of whether the
better search time could be obtained within the $\delta$-optimal space was
open. In this paper we prove that both techniques can indeed be combined in
order to obtain the best of both worlds, $O(m+(occ+1)\log^\epsilon n)$ search
time within $O(\delta\log\frac{n}{\delta})$ space.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <author>
      <name>Francisco Olivares</name>
    </author>
    <link href="http://arxiv.org/abs/2206.00781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.00781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.01149">
    <id>http://arxiv.org/abs/2206.01149v1</id>
    <updated>2022-06-02T16:59:51Z</updated>
    <published>2022-06-02T16:59:51Z</published>
    <title>Engineering Compact Data Structures for Rank and Select Queries on Bit
  Vectors</title>
    <summary>  Bit vectors are fundamental building blocks of many succinct data structures.
They can be used to represent graphs, are an important part of many text
indices in the form of the wavelet tree, and can be used to encode ordered
sequences of integers as Elias-Fano codes. To do so, two queries have to be
answered: namely rank and select queries. Given a position in the bit vector, a
rank query returns the number of 1-bits before that position. A select query,
given a parameter $j$, returns the position of the $j$-th 1-bit. On a length-n
bit vector, both queries can be answered in $O(1)$ time and require $o(n)$ bits
of additional space. In practice, the smallest (uncompressed) rank and select
data structure cs-poppy has a space overhead of $\approx$ 3.51% [Zhou et al.,
SEA 13]. In this paper, we present an improved rank and select data structure
that has the same space overhead but can answer queries up to 8% (rank) and
16.5% (select) faster compared with cs-poppy.
</summary>
    <author>
      <name>Florian Kurpicz</name>
    </author>
    <link href="http://arxiv.org/abs/2206.01149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.01149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.00376">
    <id>http://arxiv.org/abs/2206.00376v1</id>
    <updated>2022-06-01T10:22:59Z</updated>
    <published>2022-06-01T10:22:59Z</published>
    <title>String Attractors and Infinite Words</title>
    <summary>  The notion of string attractor has been introduced in [Kempa and Prezza,
2018] in the context of Data Compression and it represents a set of positions
of a finite word in which all of its factors can be "attracted". The smallest
size $\gamma^*$ of a string attractor for a finite word is a lower bound for
several repetitiveness measures associated with the most common compression
schemes, including BWT-based and LZ-based compressors. The combinatorial
properties of the measure $\gamma^*$ have been studied in [Mantaci et al.,
2021]. Very recently, a complexity measure, called string attractor profile
function, has been introduced for infinite words, by evaluating $\gamma^*$ on
each prefix. Such a measure has been studied for automatic sequences and
linearly recurrent infinite words [Schaeffer and Shallit, 2021]. In this paper,
we study the relationship between such a complexity measure and other
well-known combinatorial notions related to repetitiveness in the context of
infinite words, such as the factor complexity and the recurrence. Furthermore,
we introduce new string attractor-based complexity measures, in which the
structure and the distribution of positions in a string attractor of the
prefixes of infinite words are considered. We show that such measures provide a
finer classification of some infinite families of words.
</summary>
    <author>
      <name>Antonio Restivo</name>
    </author>
    <author>
      <name>Giuseppe Romana</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <link href="http://arxiv.org/abs/2206.00376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.00376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2205.15912">
    <id>http://arxiv.org/abs/2205.15912v2</id>
    <updated>2022-06-02T14:07:47Z</updated>
    <published>2022-05-31T16:00:09Z</published>
    <title>Efficient Algorithms for Sorting in Trees</title>
    <summary>  Sorting is a foundational problem in computer science that is typically
employed on sequences or total orders. More recently, a more general form of
sorting on partially ordered sets (or posets), where some pairs of elements are
incomparable, has been studied. General poset sorting algorithms have a
lower-bound query complexity of $\Omega(wn + n \log n)$, where $w$ is the width
of the poset.
  We consider the problem of sorting in trees, a particular case of partial
orders, and parametrize the complexity with respect to $d$, the maximum degree
of an element in the tree, as $d$ is usually much smaller than $w$ in trees.
For example, in complete binary trees, $d = \Theta(1), w = \Theta(n)$. We
present a randomized algorithm for sorting a tree poset in worst-case expected
$O(dn\log n)$ query and time complexity. This improves the previous upper bound
of $O(dn \log^2 n)$. Our algorithm is the first to be optimal for
bounded-degree trees. We also provide a new lower bound of $\Omega(dn + n \log
n)$ for the worst-case query complexity of sorting a tree poset. Finally, we
present the first deterministic algorithm for sorting tree posets that has
lower total complexity than existing algorithms for sorting general partial
orders.
</summary>
    <author>
      <name>Jishnu Roychoudhury</name>
    </author>
    <author>
      <name>Jatin Yadav</name>
    </author>
    <link href="http://arxiv.org/abs/2205.15912v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.15912v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2205.13752">
    <id>http://arxiv.org/abs/2205.13752v1</id>
    <updated>2022-05-27T03:31:31Z</updated>
    <published>2022-05-27T03:31:31Z</published>
    <title>On Lyndon-Word Representable Graphs</title>
    <summary>  In this short note, we first associate a new simple undirected graph with a
given word over an ordered alphabet of $n$-letters. We will call it the Lyndon
graph of that word. Then, we introduce the concept of the Lyndon-word
representable graph as a graph isomorphic to a Lyndon graph of some word. Then,
we introduce the generalized Stirling cycle number $S(N;n,k)$ as the number
words of length $N$ with $k$ distinct Lydon words in their Lyndon factorization
over an ordered alphabet of $n$-letters . Finally, we conclude the paper with
several interesting open questions and conjectures for interested audiences.
</summary>
    <author>
      <name>Hossein Teimoori Faal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.13752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.13752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.14273">
    <id>http://arxiv.org/abs/2206.14273v1</id>
    <updated>2022-06-28T20:04:24Z</updated>
    <published>2022-06-28T20:04:24Z</published>
    <title>Asymptotic bounds for the number of closed and privileged words</title>
    <summary>  A word $w$ has a border $u$ if $u$ is a non-empty proper prefix and suffix of
$u$. A word $w$ is said to be \emph{closed} if $|w| \leq 1$ or if $w$ has a
border that occurs exactly twice in $w$. A word $w$ is said to be
\emph{privileged} if $|w| \leq 1$ or if $w$ has a privileged border that occurs
exactly twice in $w$. Let $C_k(n)$ (resp. $P_k(n)$) be the number of length-$n$
closed (resp. privileged) words over a $k$-letter alphabet. In this paper we
improve existing upper and lower bounds on $C_k(n)$ and $P_k(n)$. We prove that
$C_k(n) \in \Theta(\frac{k^n}{n})$. Let $\log_k^{\circ 0}(n) = n$ and
$\log_k^{\circ j}(n) = \log_k(\log_k^{\circ j-1}(n))$ for $j\geq 1$. We also
prove that for all $j\geq 0$ there exist constants $N_j$, $c_j$, and $c_j'$
such that \[c_j\frac{k^n}{n\log_k^{\circ j}(n)\prod_{i=1}^j\log_k^{\circ
i}(n)}\leq P_k(n) \leq c_j'\frac{k^n}{n\prod_{i=1}^j\log_k^{\circ i}(n)}\] for
all $n>N_j$.
</summary>
    <author>
      <name>Daniel Gabric</name>
    </author>
    <link href="http://arxiv.org/abs/2206.14273v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.14273v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.13896">
    <id>http://arxiv.org/abs/2206.13896v1</id>
    <updated>2022-06-28T11:27:34Z</updated>
    <published>2022-06-28T11:27:34Z</published>
    <title>Subsequences With Gap Constraints: Complexity Bounds for Matching and
  Analysis Problems</title>
    <summary>  We consider subsequences with gap constraints, i.e., length-k subsequences p
that can be embedded into a string w such that the induced gaps (i.e., the
factors of w between the positions to which p is mapped to) satisfy given gap
constraints $gc = (C_1, C_2, ..., C_{k-1})$; we call p a gc-subsequence of w.
In the case where the gap constraints gc are defined by lower and upper length
bounds $C_i = (L^-_i, L^+_i) \in \mathbb{N}^2$ and/or regular languages $C_i
\in REG$, we prove tight (conditional on the orthogonal vectors (OV)
hypothesis) complexity bounds for checking whether a given p is a
gc-subsequence of a string w. We also consider the whole set of all
gc-subsequences of a string, and investigate the complexity of the
universality, equivalence and containment problems for these sets of
gc-subsequences.
</summary>
    <author>
      <name>Joel D. Day</name>
    </author>
    <author>
      <name>Maria Kosche</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Markus L. Schmid</name>
    </author>
    <link href="http://arxiv.org/abs/2206.13896v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.13896v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.12110">
    <id>http://arxiv.org/abs/2206.12110v1</id>
    <updated>2022-06-24T07:08:49Z</updated>
    <published>2022-06-24T07:08:49Z</published>
    <title>Learning Augmented Binary Search Trees</title>
    <summary>  A treap is a classic randomized binary search tree data structure that is
easy to implement and supports O(\log n) expected time access. However, classic
treaps do not take advantage of the input distribution or patterns in the
input. Given recent advances in algorithms with predictions, we propose pairing
treaps with machine advice to form a learning-augmented treap. We are the first
to propose a learning-augmented data structure that supports binary search tree
operations such as range-query and successor functionalities. With the
assumption that we have access to advice from a frequency estimation oracle, we
assign learned priorities to the nodes to better improve the treap's structure.
We theoretically analyze the learning-augmented treap's performance under
various input distributions and show that under those circumstances, our
learning-augmented treap has stronger guarantees than classic treaps and other
classic tree-based data structures. Further, we experimentally evaluate our
learned treap on synthetic datasets and demonstrate a performance advantage
over other search tree data structures. We also present experiments on real
world datasets with known frequency estimation oracles and show improvements as
well.
</summary>
    <author>
      <name>Honghao Lin</name>
    </author>
    <author>
      <name>Tian Luo</name>
    </author>
    <author>
      <name>David P. Woodruff</name>
    </author>
    <link href="http://arxiv.org/abs/2206.12110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.12110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.12600">
    <id>http://arxiv.org/abs/2206.12600v1</id>
    <updated>2022-06-25T08:56:09Z</updated>
    <published>2022-06-25T08:56:09Z</published>
    <title>PalFM-index: FM-index for Palindrome Pattern Matching</title>
    <summary>  Palindrome pattern matching (pal-matching) problem is a generalized pattern
matching problem, which searches for the occurrences of substrings in a text
that have the same palindromic structure with a pattern. Given a text $T$ of
length $n$ over an alphabet of size $\sigma$, an index for pal-matching is to
support, given a pattern $P$ of length $m$, counting queries that compute the
number $\mathsf{occ}$ of the occurrences of $P$ and locating queries that
compute the occurrences of $P$. The authors in~[I et al., Theor. Comput. Sci.,
2013] proposed $O(n \lg n)$-bit data structure to support counting queries in
$O(m \lg \sigma)$ time and locating queries in $O(m \lg \sigma + \mathsf{occ})$
time. In this paper, we propose an FM-index type index for pal-matching
problem, which we call PalFM-index, that occupies $2n \lg \min(\sigma, \lg n) +
2n + o(n)$ bits of space and supports counting queries in $O(m)$ time.
PalFM-index can support locating queries in $O(m + \Delta \mathsf{occ})$ time
by adding $\frac{n}{\Delta} \lg n + n + o(n)$ bits of space, where $\Delta$ is
a parameter chosen from $1 \le \Delta \le n$.
</summary>
    <author>
      <name>Shinya Nagashita</name>
    </author>
    <author>
      <name>Tomohiro I</name>
    </author>
    <link href="http://arxiv.org/abs/2206.12600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.12600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.13027">
    <id>http://arxiv.org/abs/2206.13027v1</id>
    <updated>2022-06-27T03:04:48Z</updated>
    <published>2022-06-27T03:04:48Z</published>
    <title>Balancing Run-Length Straight-Line Programs*</title>
    <summary>  It was recently proved that any SLP generating a given string $w$ can be
transformed in linear time into an equivalent balanced SLP of the same
asymptotic size. We show that this result also holds for RLSLPs, which are SLPs
extended with run-length rules of the form $A \rightarrow B^t$ for $t>2$,
deriving $\texttt{exp}(A) = \texttt{exp}(B)^t$. An immediate consequence is the
simplification of the algorithm for extracting substrings of an
RLSLP-compressed string. We also show that several problems like answering RMQs
and computing Karp-Rabin fingerprints on substrings can be solved in
$\mathcal{O}(g_{rl})$ space and $\mathcal{O}(\log n)$ time, $g_{rl}$ being the
size of the smallest RLSLP generating the string, of length $n$. We extend the
result to solving more general operations on string ranges, in
$\mathcal{O}(g_{rl})$ space and $\mathcal{O}(\log n)$ applications of the
operation. In general, the smallest RLSLP can be asymptotically smaller than
the smallest SLP by up to an $\mathcal{O}(\log n)$ factor, so our results can
make a difference in terms of the space needed for computing these operations
efficiently for some string families.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Chile</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CeBiB</arxiv:affiliation>
    </author>
    <author>
      <name>Francisco Olivares</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Chile</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CeBiB</arxiv:affiliation>
    </author>
    <author>
      <name>Cristian Urbina</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Chile</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CeBiB</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Funded in part by Basal Funds FB0001, Fondecyt Grant 1-200038, and
  two Conicyt Doctoral Scholarships, ANID, Chile</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.13027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.13027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.12222">
    <id>http://arxiv.org/abs/2206.12222v1</id>
    <updated>2022-06-24T11:43:56Z</updated>
    <published>2022-06-24T11:43:56Z</published>
    <title>On the Optimisation of the GSACA Suffix Array Construction Algorithm</title>
    <summary>  The suffix array is arguably one of the most important data structures in
sequence analysis and consequently there is a multitude of suffix sorting
algorithms. However, to this date the \texttt{GSACA} algorithm introduced in
2015 is the only known non-recursive linear-time suffix array construction
algorithm (SACA). Despite its interesting theoretical properties, there has
been little effort in improving the algorithm's subpar real-world performance.
There is a super-linear algorithm \texttt{DSH} which relies on the same sorting
principle and is faster than \texttt{DivSufSort}, the fastest SACA for over a
decade. This paper is concerned with analysing the sorting principle used in
\texttt{GSACA} and \texttt{DSH} and exploiting its properties in order to give
an optimised linear-time algorithm. Our resulting algorithm is not only
significantly faster than \texttt{GSACA} but also
outperforms\texttt{DivSufSort} and \texttt{DSH}.
</summary>
    <author>
      <name>Jannik Olbrich</name>
    </author>
    <author>
      <name>Enno Ohlebusch</name>
    </author>
    <author>
      <name>Thomas Büchler</name>
    </author>
    <link href="http://arxiv.org/abs/2206.12222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.12222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.11726">
    <id>http://arxiv.org/abs/2206.11726v1</id>
    <updated>2022-06-23T14:22:50Z</updated>
    <published>2022-06-23T14:22:50Z</published>
    <title>Longest Common Subsequence: Tabular vs. Closed-Form Equation Computation
  of Subsequence Probability</title>
    <summary>  The Longest Common Subsequence Problem (LCS) deals with finding the longest
subsequence among a given set of strings. The LCS problem is an NP-hard problem
which makes it a target for lots of effort to find a better solution with
heuristics methods. The baseline for most famous heuristics functions is a
tabular random, probabilistic approach. This approach approximates the length
of the LCS in each iteration. The combination of beam search and tabular
probabilistic-based heuristics has led to a large number of proposals and
achievements in algorithms for solving the LCS problem. In this work, we
introduce a closed-form equation of the probabilistic table calculation for the
first time. Moreover, we present other corresponding forms of the closed-form
equation and prove all of them. The closed-form equation opens new ways for
analysis and further approximations. Using the theorems and beam search, we
propose an analytic method for estimating the length of the LCS of the
remaining subsequence. Furthermore, we present another heuristic function based
on the Coefficient of Variation. The results show that our proposed methods
outperform the state-of-the-art methods on the LCS problem.
</summary>
    <author>
      <name>Alireza Abdi</name>
    </author>
    <author>
      <name>Mohsen Hooshmand</name>
    </author>
    <link href="http://arxiv.org/abs/2206.11726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.11726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.10383">
    <id>http://arxiv.org/abs/2206.10383v1</id>
    <updated>2022-06-21T13:32:06Z</updated>
    <published>2022-06-21T13:32:06Z</published>
    <title>The Complexity of the Co-Occurrence Problem</title>
    <summary>  Let $S$ be a string of length $n$ over an alphabet $\Sigma$ and let $Q$ be a
subset of $\Sigma$ of size $q \geq 2$. The 'co-occurrence problem' is to
construct a compact data structure that supports the following query: given an
integer $w$ return the number of length-$w$ substrings of $S$ that contain each
character of $Q$ at least once. This is a natural string problem with
applications to, e.g., data mining, natural language processing, and DNA
analysis. The state of the art is an $O(\sqrt{nq})$ space data structure that
$\unicode{x2014}$ with some minor additions $\unicode{x2014}$ supports queries
in $O(\log\log n)$ time [CPM 2021].
  Our contributions are as follows. Firstly, we analyze the problem in terms of
a new, natural parameter $d$, giving a simple data structure that uses $O(d)$
space and supports queries in $O(\log\log n)$ time. The preprocessing algorithm
does a single pass over $S$, runs in expected $O(n)$ time, and uses $O(d)$
space in addition to the input. Furthermore, we show that $O(d)$ space is
optimal and that $O(\log\log n)$-time queries are optimal given optimal space.
Secondly, we bound $d = O(\sqrt{nq})$, giving clean bounds in terms of $n$ and
$q$ that match the state of the art. Furthermore, we prove that
$\Omega(\sqrt{nq})$ bits of space is necessary in the worst case, meaning that
the $O(\sqrt{nq})$ upper bound is tight to within polylogarithmic factors. All
of our results are based on simple and intuitive combinatorial ideas that
simplify the state of the art.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Tord Stordalen</name>
    </author>
    <link href="http://arxiv.org/abs/2206.10383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.10383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.05597">
    <id>http://arxiv.org/abs/2206.05597v1</id>
    <updated>2022-06-11T19:46:42Z</updated>
    <published>2022-06-11T19:46:42Z</published>
    <title>Lower Bounds for Sorting 16, 17, and 18 Elements</title>
    <summary>  It is a long-standing open question to determine the minimum number of
comparisons $S(n)$ that suffice to sort an array of $n$ elements. Indeed,
before this work $S(n)$ has been known only for $n\leq 22$ with the exception
for $n=16$, $17$, and $18$. In this work, we fill that gap by proving that
sorting $n=16$, $17$, and $18$ elements requires $46$, $50$, and $54$
comparisons respectively. This fully determines $S(n)$ for these values and
disproves a conjecture by Knuth that $S(16) = 45$. Moreover, we show that for
sorting $28$ elements at least 99 comparisons are needed. We obtain our result
via an exhaustive computer search which extends previous work by Wells (1965)
and Peczarski (2002, 2004, 2007, 2012). Our progress is both based on advances
in hardware and on novel algorithmic ideas such as applying a bidirectional
search to this problem.
</summary>
    <author>
      <name>Florian Stober</name>
    </author>
    <author>
      <name>Armin Weiß</name>
    </author>
    <link href="http://arxiv.org/abs/2206.05597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.05597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.06053">
    <id>http://arxiv.org/abs/2206.06053v1</id>
    <updated>2022-06-13T11:30:15Z</updated>
    <published>2022-06-13T11:30:15Z</published>
    <title>KATKA: A KRAKEN-like tool with $k$ given at query time</title>
    <summary>  We describe a new tool, KATKA, that stores a phylogenetic tree $T$ such that
later, given a query $P$ and an integer $k$, it can quickly return the root of
the smallest subtree of $T$ containing all the genomes in which the $k$-mer $P
[i..i + k - 1]$ occurs, for $1 \leq i \leq m - k + 1$. This is similar to
KRAKEN's functionality but with $k$ given at query time instead of at
construction time.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Sana Kashgouli</name>
    </author>
    <link href="http://arxiv.org/abs/2206.06053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.06053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.04304">
    <id>http://arxiv.org/abs/2207.04304v1</id>
    <updated>2022-07-09T17:17:47Z</updated>
    <published>2022-07-09T17:17:47Z</published>
    <title>Enumerating and Locating the Subwords of the Two-dimensional Infinite
  Fibonacci Word</title>
    <summary>  Given an infinite word, enumerating its subwords is an important exercise for
understanding the structure of the word. The process of finding all the
subwords is quite tricky for two-dimensional words. In this paper we enumerate
the subwords of the two-dimensional infinite Fibonacci word,
$f_{\infty,\infty}$, in a few possible ways. In addition, we extend a method
for locating the subwords of the one-dimensional infinite Fibonacci word
$f_{\infty}$ to locate the positions of the subwords of $f_{\infty,\infty}$.
</summary>
    <author>
      <name>Sivasankar Mohankumar</name>
    </author>
    <author>
      <name>Rama Raghavan</name>
    </author>
    <link href="http://arxiv.org/abs/2207.04304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.04304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.04194">
    <id>http://arxiv.org/abs/2207.04194v1</id>
    <updated>2022-07-09T04:54:19Z</updated>
    <published>2022-07-09T04:54:19Z</published>
    <title>Online algorithms for finding distinct substrings with length and
  multiple prefix and suffix conditions</title>
    <summary>  Let two static sequences of strings $P$ and $S$, representing prefix and
suffix conditions respectively, be given as input for preprocessing. For the
query, let two positive integers $k_1$ and $k_2$ be given, as well as a string
$T$ given in an online manner, such that $T_i$ represents the length-$i$ prefix
of $T$ for $1 \leq i \leq |T|$. In this paper we are interested in computing
the set $\mathit{ans_i}$ of distinct substrings $w$ of $T_i$ such that $k_1
\leq |w| \leq k_2$, and $w$ contains some $p \in P$ as a prefix and some $s \in
S$ as a suffix. Let $\sigma$ denote the alphabet size. Then, we show that after
$O((\Vert P\Vert +\Vert S\Vert)\log\sigma)$-time preprocessing, the counting
problem of outputting $|\mathit{ans_i}|$ on each iteration $i$ can be solved in
$O(|T_i| \log\sigma)$ cumulative time, while the reporting problem can be
solved in $O(|T_i| \log\sigma + |\mathit{ans_i}|)$ cumulative time, with both
problems requiring only $O(|T_i|+\Vert P\Vert + \Vert S\Vert)$ working space.
The preprocessing time can be reduced to $O(\Vert P\Vert +\Vert S\Vert)$ for
integer alphabets of size polynomial with regard to $\Vert P\Vert +\Vert
S\Vert$. Here, for a sequence of strings $A$, $\Vert A\Vert=\sum_{u\in A}|u|$.
Our algorithms have possible applications to network traffic classification.
</summary>
    <author>
      <name>Laurentius Leonard</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.04194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.04194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.02571">
    <id>http://arxiv.org/abs/2207.02571v2</id>
    <updated>2022-07-12T10:09:48Z</updated>
    <published>2022-07-06T10:36:44Z</published>
    <title>Computing NP-hard Repetitiveness Measures via MAX-SAT</title>
    <summary>  Repetitiveness measures reveal profound characteristics of datasets, and give
rise to compressed data structures and algorithms working in compressed space.
Alas, the computation of some of these measures is NP-hard, and
straight-forward computation is infeasible for datasets of even small sizes.
Three such measures are the smallest size of a string attractor, the smallest
size of a bidirectional macro scheme, and the smallest size of a straight-line
program. While a vast variety of implementations for heuristically computing
approximations exist, exact computation of these measures has received little
to no attention. In this paper, we present MAX-SAT formulations that provide
the first non-trivial implementations for exact computation of smallest string
attractors, smallest bidirectional macro schemes, and smallest straight-line
programs. Computational experiments show that our implementations work for
texts of length up to a few hundred for straight-line programs and
bidirectional macro schemes, and texts even over a million for string
attractors.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Masakazu Ishihata</name>
    </author>
    <author>
      <name>Shunsuke Kanda</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Takaaki Nishimoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">paper accepted to ESA 2022 (plus Appendix); corrected attribution of
  Python program for computing https://oeis.org/A339391</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.02571v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.02571v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.00915">
    <id>http://arxiv.org/abs/2207.00915v1</id>
    <updated>2022-07-02T22:59:34Z</updated>
    <published>2022-07-02T22:59:34Z</published>
    <title>Approximating Dynamic Time Warping Distance Between Run-Length Encoded
  Strings</title>
    <summary>  Dynamic Time Warping (DTW) is a widely used similarity measure for comparing
strings that encode time series data, with applications to areas including
bioinformatics, signature verification, and speech recognition. The standard
dynamic-programming algorithm for DTW takes $O(n^2)$ time, and there are
conditional lower bounds showing that no algorithm can do substantially better.
  In many applications, however, the strings $x$ and $y$ may contain long runs
of repeated letters, meaning that they can be compressed using run-length
encoding. A natural question is whether the DTW-distance between these
compressed strings can be computed efficiently in terms of the lengths $k$ and
$\ell$ of the compressed strings. Recent work has shown how to achieve
$O(k\ell^2 + \ell k^2)$ time, leaving open the question of whether a
near-quadratic $\tilde{O}(k\ell)$-time algorithm might exist.
  We show that, if a small approximation loss is permitted, then a
near-quadratic time algorithm is indeed possible: our algorithm computes a $(1
+ \epsilon)$-approximation for $DTW(x, y)$ in $\tilde{O}(k\ell / \epsilon^3)$
time, where $k$ and $\ell$ are the number of runs in $x$ and $y$. Our algorithm
allows for $DTW$ to be computed over any metric space $(\Sigma, \delta)$ in
which distances are $O(log(n))$-bit integers. Surprisingly, the algorithm also
works even if $\delta$ does not induce a metric space on $\Sigma$ (e.g.,
$\delta$ need not satisfy the triangle inequality).
</summary>
    <author>
      <name>Zoe Xi</name>
    </author>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A shorter version of this paper will be published in ESA 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.00915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.00915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.00972">
    <id>http://arxiv.org/abs/2207.00972v1</id>
    <updated>2022-07-03T07:23:07Z</updated>
    <published>2022-07-03T07:23:07Z</published>
    <title>Suffix sorting via matching statistics</title>
    <summary>  We introduce a new algorithm for constructing the generalized suffix array of
a collection of highly similar strings. As a first step, we construct a
compressed representation of the matching statistics of the collection with
respect to a reference string. We then use this data structure to distribute
suffixes into a partial order, and subsequently to speed up suffix comparisons
to complete the generalized suffix array. Our experimental evidence with a
prototype implementation (a tool we call sacamats) shows that on string
collections with highly similar strings we can construct the suffix array in
time competitive with or faster than the fastest available methods. Along the
way, we describe a heuristic for fast computation of the matching statistics of
two strings, which may be of independent interest.
</summary>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <author>
      <name>Francesco Masillo</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures; accepted at WABI 2022 (Workshop on Algorithms in
  Bioinformatics, Sept. 5-9, 2022, Potsdam, Germany)</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.00972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.00972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.13851">
    <id>http://arxiv.org/abs/2206.13851v1</id>
    <updated>2022-06-28T09:30:08Z</updated>
    <published>2022-06-28T09:30:08Z</published>
    <title>Which arithmetic operations can be performed in constant time in the RAM
  model with addition?</title>
    <summary>  In the literature of algorithms, the specific computation model is often not
explicit as it is assumed that the model of computation is the RAM (Random
Access Machine) model. However, the RAM model itself is ill-founded in the
literature, with disparate definitions and no unified results.
  The ambition of this paper is to found the RAM model from scratch by
exhibiting a RAM model that enjoys interesting algorithmic properties and the
robustness of its complexity classes, notably LIN, the class of linear-time
computable problems, or the now well-known CONST-DELAY-lin class of enumeration
problems computable with constant delay after linear-time preprocessing,
  The computation model that we define is a RAM whose contents and addresses of
registers are $O(N)$, where $N$ is the size (number of registers) of the input,
and where the time cost of each instruction is 1 (unit cost criterion). The key
to the foundation of our RAM model will be to prove that even if addition is
the only primitive operation, such a RAM can still compute all the basic
arithmetic operations in constant time after a linear-time preprocessing.
Moreover, while the RAM handles only $O(N)$ integers in each register, we will
show that our RAM can handle $O(N^d)$ integers, for any fixed d, by storing
them on $O(d)$ registers and we will have surprising algorithms that computes
many operations acting on these "polynomial" integers -- addition, subtraction,
multiplication, division, exponential, integer logarithm, integer square root
(or $c$-th root, for any integer $c$), bitwise logical operations, and, more
generally, any operation computable in linear time on a cellular automaton --
in constant time after a linear-time preprocessing.
</summary>
    <author>
      <name>Étienne Grandjean</name>
    </author>
    <author>
      <name>Louis Jachiet</name>
    </author>
    <link href="http://arxiv.org/abs/2206.13851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.13851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; F.1.3; F.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.15097">
    <id>http://arxiv.org/abs/2206.15097v1</id>
    <updated>2022-06-30T07:55:50Z</updated>
    <published>2022-06-30T07:55:50Z</published>
    <title>Prefix-free parsing for building large tunnelled Wheeler graphs</title>
    <summary>  We propose a new technique for creating a space-efficient index for large
repetitive text collections, such as pangenomic databases containing sequences
of many individuals from the same species. We combine two recent techniques
from this area: Wheeler graphs (Gagie et al., 2017) and prefix-free parsing
(PFP, Boucher et al., 2019). Wheeler graphs (WGs) are a general framework
encompassing several indexes based on the Burrows-Wheeler transform (BWT), such
as the FM-index. Wheeler graphs admit a succinct representation which can be
further compacted by employing the idea of tunnelling, which exploits
redundancies in the form of parallel, equally-labelled paths called blocks that
can be merged into a single path. The problem of finding the optimal set of
blocks for tunnelling, i.e. the one that minimizes the size of the resulting
WG, is known to be NP-complete and remains the most computationally challenging
part of the tunnelling process.
  To find an adequate set of blocks in less time, we propose a new method based
on the prefix-free parsing (PFP). The idea of PFP is to divide the input text
into phrases of roughly equal sizes that overlap by a fixed number of
characters. The original text is represented by a sequence of phrase ranks (the
parse) and a list of all used phrases (the dictionary). In repetitive texts,
the PFP of the text is generally much shorter than the original. To speed up
the block selection for tunnelling, we apply the PFP to obtain the parse and
the dictionary of the text, tunnel the WG of the parse using existing
heuristics and subsequently use this tunnelled parse to construct a compact WG
of the original text. Compared with constructing a WG from the original text
without PFP, our method is much faster and uses less memory on collections of
pangenomic sequences. Therefore, our method enables the use of WGs as a
pangenomic reference for real-world datasets.
</summary>
    <author>
      <name>Adrián Goga</name>
    </author>
    <author>
      <name>Andrej Baláž</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures, 2 tables, to be published in the WABI (Workshop
  on Algorithms in Bioinformatics) 2022 conference proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.15097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.15097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.15319">
    <id>http://arxiv.org/abs/2206.15319v1</id>
    <updated>2022-06-30T14:45:18Z</updated>
    <published>2022-06-30T14:45:18Z</published>
    <title>On extended boundary sequences of morphic and Sturmian words</title>
    <summary>  Generalizing the notion of the boundary sequence introduced by Chen and Wen,
the $n$th term of the $\ell$-boundary sequence of an infinite word is the
finite set of pairs $(u,v)$ of prefixes and suffixes of length $\ell$ appearing
in factors $uyv$ of length $n+\ell$ ($n\ge \ell\ge 1$). Otherwise stated, for
increasing values of $n$, one looks for all pairs of factors of length $\ell$
separated by $n-\ell$ symbols.
  For the large class of addable numeration systems $U$, we show that if an
infinite word is $U$-automatic, then the same holds for its $\ell$-boundary
sequence. In particular, they are both morphic (or generated by an HD0L
system). We also provide examples of numeration systems and $U$-automatic words
with a boundary sequence that is not $U$-automatic. In the second part of the
paper, we study the $\ell$-boundary sequence of a Sturmian word. We show that
it is obtained through a sliding block code from the characteristic Sturmian
word of the same slope. We also show that it is the image under a morphism of
some other characteristic Sturmian word.
</summary>
    <author>
      <name>Michel Rigo</name>
    </author>
    <author>
      <name>Manon Stipulanti</name>
    </author>
    <author>
      <name>Markus A. Whiteland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A short version to appear in the proceedings of MFCS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.15319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.15319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; G.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.15100">
    <id>http://arxiv.org/abs/2206.15100v1</id>
    <updated>2022-06-30T08:04:17Z</updated>
    <published>2022-06-30T08:04:17Z</published>
    <title>Computing the Parameterized Burrows--Wheeler Transform Online</title>
    <summary>  Parameterized strings are a generalization of strings in that their
characters are drawn from two different alphabets, where one is considered to
be the alphabet of static characters and the other to be the alphabet of
parameter characters. Two parameterized strings are a parameterized match if
there is a bijection over all characters such that the bijection transforms one
string to the other while keeping the static characters (i.e., it behaves as
the identity on the static alphabet). Ganguly et al.~[SODA'2017] proposed the
parameterized Burrows--Wheeler transform (PBWT) as a variant of the
Burrows--Wheeler transform for space-efficient parameterized pattern matching.
In this paper, we propose an algorithm for computing the PBWT online by reading
the characters of a given input string one-by-one from right to left. Our
algorithm works in $O(|\Pi| \log n / \log \log n)$ amortized time for each
input character, where $n$, $\Sigma$, and $\Pi$ denote the size of the input
string, and the alphabets of the static and parameter characters, respectively.
</summary>
    <author>
      <name>Daiki Hashimoto</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.15100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.15100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.14592">
    <id>http://arxiv.org/abs/2207.14592v1</id>
    <updated>2022-07-29T10:21:05Z</updated>
    <published>2022-07-29T10:21:05Z</published>
    <title>Pattern matching algorithms in Blockchain for network fees reduction</title>
    <summary>  Blockchain received a vast amount of attention in recent years and is still
growing. The second generation of blockchain, such as Ethereum, allows
execution of almost any program in Ethereum Virtual Machine (EVM), making it a
global protocol for distributed applications. The code deployment and each
operation performed in EVM cost the network fee called gas, which price varies
and can be significant. That is why code optimization and well-chosen
algorithms are crucial in programming on the blockchain. This paper evaluates
the gas usage of several exact pattern matching algorithms on the Ethereum
Virtual Machine. We also propose an efficient implementation of the algorithms
in the Solidity/YUL language. We evaluate the gas fees of all the algorithms
for different parameters (such as pattern length, alphabet size, and text
size). We show a significant gas fee and execution time reduction with up to
22-fold lower gas usage and 55-fold speed-up comparing to StringUtils (a
popular Solidity string library).
</summary>
    <author>
      <name>Robert Susik</name>
    </author>
    <author>
      <name>Robert Nowotniak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Source codes, and datasets:
  https://github.com/rsusik/pattern-matching-in-blockchain</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.14592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.14592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32, 68M14" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.0; J.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.13870">
    <id>http://arxiv.org/abs/2207.13870v1</id>
    <updated>2022-07-28T03:43:18Z</updated>
    <published>2022-07-28T03:43:18Z</published>
    <title>Engineering faster double-array Aho-Corasick automata</title>
    <summary>  Multiple pattern matching in strings is a fundamental problem in text
processing applications such as regular expressions or tokenization. This paper
studies efficient implementations of \emph{double-array Aho--Corasick automata
(DAACs)}, data structures for quickly performing the multiple pattern matching.
The practical performance of DAACs is improved by carefully designing the data
structure, and many implementation techniques have been proposed thus far. A
problem in DAACs is that their ideas are not aggregated. Since comprehensive
descriptions and experimental analyses are unavailable, engineers face
difficulties in implementing an efficient DAAC.
  In this paper, we review implementation techniques for DAACs and provide a
comprehensive description of them. We also propose several new techniques for
further improvement. We conduct exhaustive experiments through real-world
datasets and reveal the best combination of techniques to achieve a higher
performance in DAACs. The best combination is different from those used in the
most popular libraries of DAACs, which demonstrates that their performance can
be further enhanced. On the basis of our experimental analysis, we developed a
new Rust library for fast multiple pattern matching using DAACs, named
\emph{Daachorse}, as open-source software at
\url{https://github.com/daac-tools/daachorse}. Experiments demonstrate that
Daachorse outperforms other AC-automaton implementations, indicating its
suitability as a fast alternative for multiple pattern matching in many
applications.
</summary>
    <author>
      <name>Shunsuke Kanda</name>
    </author>
    <author>
      <name>Koichi Akabe</name>
    </author>
    <author>
      <name>Yusuke Oda</name>
    </author>
    <link href="http://arxiv.org/abs/2207.13870v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.13870v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.11954">
    <id>http://arxiv.org/abs/2207.11954v1</id>
    <updated>2022-07-25T07:43:07Z</updated>
    <published>2022-07-25T07:43:07Z</published>
    <title>Simple O(1) Query Algorithm for Level Ancestors</title>
    <summary>  This note describes a very simple O(1) query time algorithm for finding level
ancestors. This is basically a serial (re)-implementation of the parallel
algorithm.
  Earlier, Menghani and Matani described another simple algorithm; however,
their algorithm takes O(log n) time to answer queries.
  Although the basic algorithm has preprocessing time of O(n\log n), by having
additional levels, the preprocessing time can be reduced to almost linear or
linear.
</summary>
    <author>
      <name>Sanjeev Saxena</name>
    </author>
    <link href="http://arxiv.org/abs/2207.11954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.11954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.10556">
    <id>http://arxiv.org/abs/2207.10556v2</id>
    <updated>2022-07-25T11:28:02Z</updated>
    <published>2022-07-21T15:56:16Z</published>
    <title>Tight Bounds for Monotone Minimal Perfect Hashing</title>
    <summary>  The monotone minimal perfect hash function (MMPHF) problem is the following
indexing problem. Given a set $S= \{s_1,\ldots,s_n\}$ of $n$ distinct keys from
a universe $U$ of size $u$, create a data structure $DS$ that answers the
following query:
  \[ RankOp(q) = \text{rank of } q \text{ in } S \text{ for all } q\in S
  ~\text{ and arbitrary answer otherwise.}
  \]
  Solutions to the MMPHF problem are in widespread use in both theory and
practice.
  The best upper bound known for the problem encodes $DS$ in $O(n\log\log\log
u)$ bits and performs queries in $O(\log u)$ time. It has been an open problem
to either improve the space upper bound or to show that this somewhat odd
looking bound is tight.
  In this paper, we show the latter: specifically that any data structure
(deterministic or randomized) for monotone minimal perfect hashing of any
collection of $n$ elements from a universe of size $u$ requires $\Omega(n \cdot
\log\log\log{u})$ expected bits to answer every query correctly.
  We achieve our lower bound by defining a graph $\mathbf{G}$ where the nodes
are the possible ${u \choose n}$ inputs and where two nodes are adjacent if
they cannot share the same $DS$. The size of $DS$ is then lower bounded by the
log of the chromatic number of $\mathbf{G}$. Finally, we show that the
fractional chromatic number (and hence the chromatic number) of $\mathbf{G}$ is
lower bounded by $2^{\Omega(n \log\log\log u)}$.
</summary>
    <author>
      <name>Sepehr Assadi</name>
    </author>
    <author>
      <name>Martin Farach-Colton</name>
    </author>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <link href="http://arxiv.org/abs/2207.10556v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.10556v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.10171">
    <id>http://arxiv.org/abs/2207.10171v1</id>
    <updated>2022-07-20T19:49:15Z</updated>
    <published>2022-07-20T19:49:15Z</published>
    <title>Pseudoperiodic Words and a Question of Shevelev</title>
    <summary>  We generalize the familiar notion of periodicity in sequences to a new kind
of pseudoperiodicity, and we prove some basic results about it. We revisit the
results of a 2012 paper of Shevelev and reprove his results in a simpler and
more unified manner, and provide a complete answer to one of his previously
unresolved questions. We consider finding words with specific pseudoperiods and
having the smallest possible critical exponent. Finally, we consider the
problem of determining whether a finite word is pseudoperiodic of a given size,
and show that it is NP-complete.
</summary>
    <author>
      <name>Joseph Meleshko</name>
    </author>
    <author>
      <name>Pascal Ochem</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <author>
      <name>Sonja Linghui Shan</name>
    </author>
    <link href="http://arxiv.org/abs/2207.10171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.10171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.09937">
    <id>http://arxiv.org/abs/2207.09937v1</id>
    <updated>2022-07-20T14:24:56Z</updated>
    <published>2022-07-20T14:24:56Z</published>
    <title>Abelian Combinatorics on Words: a Survey</title>
    <summary>  We survey known results and open problems in abelian combinatorics on words.
Abelian combinatorics on words is the extension to the commutative setting of
the classical theory of combinatorics on words, i.e., the extension based on
the equivalence relation defined in the set of words by having the same Parikh
vector, that is, the same number of occurrences of each letter of the alphabet
-- called \emph{abelian equivalence}. In the past few years, there was a lot of
research on abelian analogues of classical definitions and properties in
combinatorics on words. This survey aims to gather these results.
</summary>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Svetlana Puzynina</name>
    </author>
    <link href="http://arxiv.org/abs/2207.09937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.09937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.09201">
    <id>http://arxiv.org/abs/2207.09201v2</id>
    <updated>2022-09-22T13:00:01Z</updated>
    <published>2022-07-19T11:22:32Z</published>
    <title>Subsequences in Bounded Ranges: Matching and Analysis Problems</title>
    <summary>  In this paper, we consider a variant of the classical algorithmic problem of
checking whether a given word $v$ is a subsequence of another word $w$. More
precisely, we consider the problem of deciding, given a number $p$ (defining a
range-bound) and two words $v$ and $w$, whether there exists a factor
$w[i:i+p-1]$ (or, in other words, a range of length $p$) of $w$ having $v$ as
subsequence (i.\,e., $v$ occurs as a subsequence in the bounded range
$w[i:i+p-1]$). We give matching upper and lower quadratic bounds for the time
complexity of this problem. Further, we consider a series of algorithmic
problems in this setting, in which, for given integers $k$, $p$ and a word $w$,
we analyse the set $p$-Subseq$_{k}(w)$ of all words of length $k$ which occur
as subsequence of some factor of length $p$ of $w$. Among these, we consider
the $k$-universality problem, the $k$-equivalence problem, as well as problems
related to absent subsequences. Surprisingly, unlike the case of the classical
model of subsequences in words where such problems have efficient solutions in
general, we show that most of these problems become intractable in the new
setting when subsequences in bounded ranges are considered. Finally, we provide
an example of how some of our results can be applied to subsequence matching
problems for circular words.
</summary>
    <author>
      <name>Maria Kosche</name>
    </author>
    <author>
      <name>Tore Koß</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Viktoriya Pak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper which will appear in the proceedings of
  the 16th International Conference on Reachability Problems, RP 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.09201v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.09201v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.08120">
    <id>http://arxiv.org/abs/2207.08120v1</id>
    <updated>2022-07-17T09:08:13Z</updated>
    <published>2022-07-17T09:08:13Z</published>
    <title>On the Practical Power of Automata in Pattern Matching</title>
    <summary>  The classical pattern matching paradigm is that of seeking occurrences of one
string - the pattern, in another - the text, where both strings are drawn from
an alphabet set $\Sigma$. Assuming the text length is $n$ and the pattern
length is $m$, this problem can naively be solved in time $O(nm)$. In Knuth,
Morris and Pratt's seminal paper of 1977, an automaton, was developed that
allows solving this problem in time $O(n)$ for any alphabet.
  This automaton, which we will refer to as the {\em KMP-automaton}, has proven
useful in solving many other problems. A notable example is the {\em
parameterized pattern matching} model. In this model, a consistent renaming of
symbols from $\Sigma$ is allowed in a match. The parameterized matching
paradigm has proven useful in problems in software engineering, computer
vision, and other applications.
  It has long been suspected that for texts where the symbols are uniformly
random, the naive algorithm will perform as well as the KMP algorithm. In this
paper we examine the practical efficiency of the KMP algorithm vs. the naive
algorithm on a randomly generated text. We analyse the time under various
parameters, such as alphabet size, pattern length, and the distribution of
pattern occurrences in the text. We do this for both the original exact
matching problem and parameterized matching. While the folklore wisdom is
vindicated by these findings for the exact matching case, surprisingly, the KMP
algorithm works significantly faster than the naive in the parameterized
matching case.
  We check this hypothesis for DNA texts, and observe a similar behaviour as in
the random text. We also show a very structured case where the automaton is
much more efficient.
</summary>
    <author>
      <name>Ora Amir</name>
    </author>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Aviezri Fraenkel</name>
    </author>
    <author>
      <name>David Sarne</name>
    </author>
    <link href="http://arxiv.org/abs/2207.08120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.08120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.07477">
    <id>http://arxiv.org/abs/2207.07477v1</id>
    <updated>2022-07-15T13:46:16Z</updated>
    <published>2022-07-15T13:46:16Z</published>
    <title>Matching Patterns with Variables Under Edit Distance</title>
    <summary>  A pattern $\alpha$ is a string of variables and terminal letters. We say that
$\alpha$ matches a word $w$, consisting only of terminal letters, if $w$ can be
obtained by replacing the variables of $\alpha$ by terminal words. The matching
problem, i.e., deciding whether a given pattern matches a given word, was
heavily investigated: it is NP-complete in general, but can be solved
efficiently for classes of patterns with restricted structure. If we are
interested in what is the minimum Hamming distance between $w$ and any word $u$
obtained by replacing the variables of $\alpha$ by terminal words (so matching
under Hamming distance), one can devise efficient algorithms and matching
conditional lower bounds for the class of regular patterns (in which no
variable occurs twice), as well as for classes of patterns where we allow
unbounded repetitions of variables, but restrict the structure of the pattern,
i.e., the way the occurrences of different variables can be interleaved.
Moreover, under Hamming distance, if a variable occurs more than once and its
occurrences can be interleaved arbitrarily with those of other variables, even
if each of these occurs just once, the matching problem is intractable. In this
paper, we consider the problem of matching patterns with variables under edit
distance. We still obtain efficient algorithms and matching conditional lower
bounds for the class of regular patterns, but show that the problem becomes, in
this case, intractable already for unary patterns, consisting of repeated
occurrences of a single variable interleaved with terminals.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Stefan Siemer</name>
    </author>
    <link href="http://arxiv.org/abs/2207.07477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.07477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1801.07449">
    <id>http://arxiv.org/abs/1801.07449v3</id>
    <updated>2018-09-06T11:56:57Z</updated>
    <published>2018-01-23T09:25:19Z</published>
    <title>Sliding Suffix Tree using LCA</title>
    <summary>  We consider a sliding window $W$ over a stream of characters from some
alphabet of constant size. The user wants to perform deterministic substring
matching on the current sliding window content and obtain positions of the
matches. We present an indexed version of the sliding window using the suffix
tree, the link tree and the lowest common ancestor. The data structure of size
$\Theta(|W|)$ has optimal time queries $\Theta(m+occ)$ and amortized constant
time updates, where $m$ is the length of the query string and $occ$ is the
number of its occurrences.
</summary>
    <author>
      <name>Andrej Brodnik</name>
    </author>
    <author>
      <name>Matevž Jekovec</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A simplified version with improved maintenance and without using the
  LCA is available at https://doi.org/10.3390/a11080118</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.07449v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.07449v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.09351">
    <id>http://arxiv.org/abs/2208.09351v1</id>
    <updated>2022-08-19T14:02:53Z</updated>
    <published>2022-08-19T14:02:53Z</published>
    <title>Merging Sorted Lists of Similar Strings</title>
    <summary>  Merging $T$ sorted, non-redundant lists containing $M$ elements into a single
sorted, non-redundant result of size $N \ge M/T$ is a classic problem typically
solved practically in $O(M \log T)$ time with a priority-queue data structure
the most basic of which is the simple *heap*. We revisit this problem in the
situation where the list elements are *strings* and the lists contain many
*identical or nearly identical elements*. By keeping simple auxiliary
information with each heap node, we devise an $O(M \log T+S)$ worst-case method
that performs no more character comparisons than the sum of the lengths of all
the strings $S$, and another $O(M \log (T/ \bar e)+S)$ method that becomes
progressively more efficient as a function of the fraction of equal elements
$\bar e = M/N$ between input lists, reaching linear time when the lists are all
identical. The methods perform favorably in practice versus an alternate
formulation based on a trie.
</summary>
    <author>
      <name>Gene Myers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages. Associated code at
  https://github.com/thegenemyers/STRING.HEAP</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.09351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W40 (Primary) 68P05, 68P10, 68W05, 68W32 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.08913">
    <id>http://arxiv.org/abs/2208.08913v1</id>
    <updated>2022-08-18T15:29:11Z</updated>
    <published>2022-08-18T15:29:11Z</published>
    <title>Walking on Words</title>
    <summary>  Take any word over some alphabet. If it is non-empty, go to any position and
print out the letter being scanned. Now repeat the following any number of
times (possibly zero): either stay at the current letter, or move one letter
leftwards (if possible) or move one letter rightwards (if possible); then print
out the letter being scanned. In effect, we are going for a walk on the input
word. Let u be the infix of the input word comprising the visited positions,
and w the word printed out (empty if the input word is). Since any unvisited
prefix or suffix of the input word cannot influence w, we may as well discard
them, and say that u generates w. We ask: given a word w, what words u generate
it? The answer is surprising. Call u a primitive generator of w if u generates
w and is not generated by any word shorter than u. We show that, excepting some
degenerate cases, every word has precisely two primitive generators.
</summary>
    <author>
      <name>Ian Pratt-Hartmann</name>
    </author>
    <link href="http://arxiv.org/abs/2208.08913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.08913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q42" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.2; G.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.08823">
    <id>http://arxiv.org/abs/2208.08823v1</id>
    <updated>2022-08-18T13:29:29Z</updated>
    <published>2022-08-18T13:29:29Z</published>
    <title>Algorithm to derive shortest edit script using Levenshtein distance
  algorithm</title>
    <summary>  String similarity, longest common subsequence and shortest edit scripts are
the triplets of problem that related to each other. There are different
algorithms exist to generate edit script by solving longest common subsequence
problem. This paper proposes an algorithm that uses string similarity problem
to generate shortest edit script. For this we use the famous Levenshtein
distance algorithm, which computes a numerical value that represents similarity
between the strings from 0 to n, where n is the length of longest input string,
and produce the shortest edit script which contains instructions of Insert,
Delete and Substitute.
</summary>
    <author>
      <name>P. Prakash Maria Liju</name>
    </author>
    <link href="http://arxiv.org/abs/2208.08823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.08823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.08915">
    <id>http://arxiv.org/abs/2208.08915v1</id>
    <updated>2022-08-18T15:35:06Z</updated>
    <published>2022-08-18T15:35:06Z</published>
    <title>Approximate Circular Pattern Matching</title>
    <summary>  We consider approximate circular pattern matching (CPM, in short) under the
Hamming and edit distance, in which we are given a length-$n$ text $T$, a
length-$m$ pattern $P$, and a threshold $k>0$, and we are to report all
starting positions of fragments of $T$ (called occurrences) that are at
distance at most $k$ from some cyclic rotation of $P$. In the decision version
of the problem, we are to check if any such occurrence exists. All previous
results for approximate CPM were either average-case upper bounds or
heuristics, except for the work of Charalampopoulos et al. [CKP$^+$, JCSS'21],
who considered only the Hamming distance. For the reporting version of the
approximate CPM problem, under the Hamming distance we improve upon the main
algorithm of [CKP$^+$, JCSS'21] from ${\cal O}(n+(n/m)\cdot k^4)$ to ${\cal
O}(n+(n/m)\cdot k^3\log\log k)$ time; for the edit distance, we give an ${\cal
O}(nk^2)$-time algorithm. We also consider the decision version of the
approximate CPM problem. Under the Hamming distance, we obtain an ${\cal
O}(n+(n/m)\cdot k^2\log k/\log\log k)$-time algorithm, which nearly matches the
algorithm by Chan et al. [CGKKP, STOC'20] for the standard counterpart of the
problem. Under the edit distance, the ${\cal O}(nk\log^3 k)$ runtime of our
algorithm nearly matches the ${\cal O}(nk)$ runtime of the Landau-Vishkin
algorithm [LV, J. Algorithms'89]. As a stepping stone, we propose ${\cal
O}(nk\log^3 k)$-time algorithm for the Longest Prefix $k'$-Approximate Match
problem, proposed by Landau et al. [LMS, SICOMP'98], for all $k'\in
\{1,\dots,k\}$. We give a conditional lower bound that suggests a polynomial
separation between approximate CPM under the Hamming distance over the binary
alphabet and its non-circular counterpart. We also show that a strongly
subquadratic-time algorithm for the decision version of approximate CPM under
edit distance would refute SETH.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ESA 2022. Abstract abridged to meet arXiv requirements</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.08915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.08915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.07800">
    <id>http://arxiv.org/abs/2208.07800v1</id>
    <updated>2022-08-16T15:30:22Z</updated>
    <published>2022-08-16T15:30:22Z</published>
    <title>New Parallel Order Maintenance Data Structure</title>
    <summary>  The \emph{Order-Maintenance} (OM) data structure maintains a total order list
of items for insertions, deletions, and comparisons. As a basic data structure,
OM has many applications, such as maintaining the topological order, core
numbers, and truss in graphs, and maintaining ordered sets in Unified Modeling
Language (UML) Specification. The prevalence of multicore machines suggests
parallelizing such a basic data structure. This paper proposes a new parallel
OM data structure that supports insertions, deletions, and comparisons in
parallel. Specifically, parallel insertions and deletions are synchronized by
using locks efficiently, which achieve up to $7$x and $5.6$x speedups with $64$
workers. One big advantage is that the comparisons are lock-free so that they
can execute highly in parallel with other insertions and deletions, which
achieve up to $34.4$x speedups with $64$ workers. Typical real applications
maintain order lists that always have a much larger portion of comparisons than
insertions and deletions. For example, in core maintenance, the number of
comparisons is up to 297 times larger compared with insertions and deletions in
certain graphs. This is why the lock-free order comparison is a breakthrough in
practice.
</summary>
    <author>
      <name>Bin Guo</name>
    </author>
    <author>
      <name>Emil Sekerinski</name>
    </author>
    <link href="http://arxiv.org/abs/2208.07800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.07800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.06025">
    <id>http://arxiv.org/abs/2208.06025v1</id>
    <updated>2022-08-11T20:12:29Z</updated>
    <published>2022-08-11T20:12:29Z</published>
    <title>Automatic Sequences in Negative Bases and Proofs of Some Conjectures of
  Shevelev</title>
    <summary>  We discuss the use of negative bases in automatic sequences. Recently the
theorem-prover Walnut has been extended to allow the use of base (-k) to
express variables, thus permitting quantification over Z instead of N. This
enables us to prove results about two-sided (bi-infinite) automatic sequences.
We first explain the theory behind negative bases in Walnut. Next, we use this
new version of Walnut to give a very simple proof of a strengthened version of
a theorem of Shevelev. We use our ideas to resolve two open problems of
Shevelev from 2017. We also reprove a 2000 result of Shur involving bi-infinite
binary words.
</summary>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <author>
      <name>Sonja Linghui Shan</name>
    </author>
    <author>
      <name>Kai Hsiang Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2208.06025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.06025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.04931">
    <id>http://arxiv.org/abs/2208.04931v1</id>
    <updated>2022-08-09T17:51:20Z</updated>
    <published>2022-08-09T17:51:20Z</published>
    <title>Co-lexicographically ordering automata and regular languages. Part I</title>
    <summary>  In the present work, we lay out a new theory showing that all automata can
always be co-lexicographically partially ordered, and an intrinsic measure of
their complexity can be defined and effectively determined, namely, the minimum
width $p$ of one of their admissible co-lex partial orders - dubbed here the
automaton's co-lex width. We first show that this new measure captures at once
the complexity of several seemingly-unrelated hard problems on automata. Any
NFA of co-lex width $p$: (i) has an equivalent powerset DFA whose size is
exponential in $p$ rather than (as a classic analysis shows) in the NFA's size;
(ii) can be encoded using just $\Theta(\log p)$ bits per transition; (iii)
admits a linear-space data structure solving regular expression matching
queries in time proportional to $p^2$ per matched character. Some consequences
of this new parameterization of automata are that PSPACE-hard problems such as
NFA equivalence are FPT in $p$, and quadratic lower bounds for the regular
expression matching problem do not hold for sufficiently small $p$. Having
established that the co-lex width of an automaton is a fundamental complexity
measure, we proceed by (i) determining its computational complexity and (ii)
extending this notion from automata to regular languages by studying their
smallest-width accepting NFAs and DFAs. In this work we focus on the
deterministic case and prove that a canonical minimum-width DFA accepting a
language $\mathcal L$ - dubbed the Hasse automaton $\mathcal H$ of $\mathcal L$
- can be exhibited. Finally, we explore the relationship between two
conflicting objectives: minimizing the width and minimizing the number of
states of a DFA. In this context, we provide an analogous of the Myhill-Nerode
Theorem for co-lexicographically ordered regular languages.
</summary>
    <author>
      <name>Nicola Cotumaccio</name>
    </author>
    <author>
      <name>Giovanna D'Agostino</name>
    </author>
    <author>
      <name>Alberto Policriti</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2106.02309</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.04931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.04931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.04380">
    <id>http://arxiv.org/abs/2208.04380v1</id>
    <updated>2022-08-08T19:36:01Z</updated>
    <published>2022-08-08T19:36:01Z</published>
    <title>Almost optimal searching of maximal subrepetitions in a word</title>
    <summary>  For $0&lt;\delta &lt;1$ a $\delta$-subrepetition in a word is a factor which
exponent is less than~2 but is not less than $1+\delta$ (the exponent of the
factor is the ratio of the factor length to its minimal period). The
$\delta$-subrepetition is maximal if it cannot be extended to the left or to
the right by at least one letter with preserving its minimal period. In the
paper we propose an algorithm for searching all maximal $\delta$-subrepetitions
in a word of length~$n$ in $O(\frac{n}{\delta}\log\frac{1}{\delta})$ time (the
lower bound for this time is $\Omega (\frac{n}{\delta})$).
</summary>
    <author>
      <name>Roman Kolpakov</name>
    </author>
    <link href="http://arxiv.org/abs/2208.04380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.04380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.03451">
    <id>http://arxiv.org/abs/2208.03451v1</id>
    <updated>2022-08-06T06:32:36Z</updated>
    <published>2022-08-06T06:32:36Z</published>
    <title>Fast detection of specific fragments against a set of sequences</title>
    <summary>  We design alignment-free techniques for comparing a sequence or word, called
a target, against a set of words, called a reference. A target-specific factor
of a target $T$ against a reference $R$ is a factor $w$ of a word in $T$ which
is not a factor of a word of $R$ and such that any proper factor of $w$ is a
factor of a word of $R$. We first address the computation of the set of
target-specific factors of a target $T$ against a reference $R$, where $T$ and
$R$ are finite sets of sequences. The result is the construction of an
automaton accepting the set of all considered target-specific factors. The
construction algorithm runs in linear time according to the size of $T\cup R$.
The second result consists in the design of an algorithm to compute all the
occurrences in a single sequence $T$ of its target-specific factors against a
reference $R$. The algorithm runs in real-time on the target sequence,
independently of the number of occurrences of target-specific factors.
</summary>
    <author>
      <name>Marie-Pierre Béal</name>
    </author>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <link href="http://arxiv.org/abs/2208.03451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.03451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.03123">
    <id>http://arxiv.org/abs/2208.03123v1</id>
    <updated>2022-08-05T12:28:13Z</updated>
    <published>2022-08-05T12:28:13Z</published>
    <title>Watson-Crick conjugates of words and languages</title>
    <summary>  This paper is a theoretical study of notions in combinatorics of words
motivated by information being encoded as DNA strands in DNA computing. We
study Watson-Crick conjugates or \theta-conjugates, a generalization of the
classical notions of conjugates of a word, inspired by biomolecular computing.
The Watson-Crick mapping \theta is an involution that is also an antimorphism.
We study some combinatorial properties of \theta-conjugates of a word. We
characterize words that have the same set of \theta-conjugates. We investigate
whether or not the family of certain languages is closed under \theta-conjugate
operation. We also study the iterated \theta-conjugates. We then discuss the
concept of \theta-conjugate-free language and some decidability problems for
\theta-conjugate-freeness for different language classes.
</summary>
    <author>
      <name>Kalpana Mahalingam</name>
    </author>
    <author>
      <name>Anuran Maity</name>
    </author>
    <link href="http://arxiv.org/abs/2208.03123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.03123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.00158">
    <id>http://arxiv.org/abs/2209.00158v1</id>
    <updated>2022-08-31T23:53:47Z</updated>
    <published>2022-08-31T23:53:47Z</published>
    <title>Space-efficient data structure for next/previous larger/smaller value
  queries</title>
    <summary>  Given an array of size $n$ from a total order, we consider the problem of
constructing a data structure that supports various queries (range
minimum/maximum queries with their variants and next/previous larger/smaller
queries) efficiently. In the encoding model (i.e., the queries can be answered
without the input array), we propose a $(3.701n + o(n))$-bit data structure,
which supports all these queries in $O(\log^{(\ell)}n)$ time, for any positive
integer $\ell$ (here, $\log^{(1)} n = \log n$, and for $\ell > 1$,
$\log^{(\ell)} n = \log ({\log^{(\ell-1)}} n)$). The space of our data
structure matches the current best upper bound of Tsur (Inf. Process. Lett.,
2019), which does not support the queries efficiently. Also, we show that at
least $3.16n-\Theta(\log n)$ bits are necessary for answering all the queries.
Our result is obtained by generalizing Gawrychowski and Nicholson's $(3n -
\Theta(\log n))$-bit lower bound (ICALP 15) for answering range minimum and
maximum queries on a permutation of size $n$.
</summary>
    <author>
      <name>Seungbum Jo</name>
    </author>
    <author>
      <name>Geunho Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2209.00158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.00158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.00271">
    <id>http://arxiv.org/abs/2209.00271v1</id>
    <updated>2022-09-01T07:18:12Z</updated>
    <published>2022-09-01T07:18:12Z</published>
    <title>Maximal Closed Substrings</title>
    <summary>  A string is closed if it has length 1 or has a nonempty border without
internal occurrences. In this paper we introduce the definition of a maximal
closed substring (MCS), which is an occurrence of a closed substring that
cannot be extended to the left nor to the right into a longer closed substring.
MCSs with exponent at least $2$ are commonly called runs; those with exponent
smaller than $2$, instead, are particular cases of maximal gapped repeats. We
show that a string of length $n$ contains $\mathcal O(n^{1.5})$ MCSs. We also
provide an output-sensitive algorithm that, given a string of length $n$ over a
constant-size alphabet, locates all $m$ MCSs the string contains in $\mathcal
O(n\log n + m)$ time.
</summary>
    <author>
      <name>Golnaz Badkobeh</name>
    </author>
    <author>
      <name>Alessandro De Luca</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Simon Puglisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted in SPIRE '22</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.00271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.00271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.14722">
    <id>http://arxiv.org/abs/2208.14722v1</id>
    <updated>2022-08-31T09:28:43Z</updated>
    <published>2022-08-31T09:28:43Z</published>
    <title>Combinatorial Algorithms for Subsequence Matching: A Survey</title>
    <summary>  In this paper we provide an overview of a series of recent results regarding
algorithms for searching for subsequences in words or for the analysis of the
sets of subsequences occurring in a word.
</summary>
    <author>
      <name>Maria Kosche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Göttingen University, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Tore Koß</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Göttingen University, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Florin Manea</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Göttingen University, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Stefan Siemer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Göttingen University, Germany</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.367.2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.367.2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings NCMA 2022, arXiv:2208.13015. arXiv admin note: text
  overlap with arXiv:2206.13896</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 367, 2022, pp. 11-27</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2208.14722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.14722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.14669">
    <id>http://arxiv.org/abs/2208.14669v1</id>
    <updated>2022-08-31T07:54:35Z</updated>
    <published>2022-08-31T07:54:35Z</published>
    <title>Pattern matching under DTW distance</title>
    <summary>  In this work, we consider the problem of pattern matching under the dynamic
time warping (DTW) distance motivated by potential applications in the analysis
of biological data produced by the third generation sequencing. To measure the
DTW distance between two strings, one must "warp" them, that is, double some
letters in the strings to obtain two equal-lengths strings, and then sum the
distances between the letters in the corresponding positions. When the
distances between letters are integers, we show that for a pattern P with m
runs and a text T with n runs: 1. There is an O(m + n)-time algorithm that
computes all locations where the DTW distance from P to T is at most 1; 2.
There is an O(kmn)-time algorithm that computes all locations where the DTW
distance from P to T is at most k. As a corollary of the second result, we also
derive an approximation algorithm for general metrics on the alphabet.
</summary>
    <author>
      <name>Garance Gourdel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA, DI-ENS</arxiv:affiliation>
    </author>
    <author>
      <name>Anne Driemel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Peterlongo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA</arxiv:affiliation>
    </author>
    <author>
      <name>Tatiana Starikovskaya</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DI-ENS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SPIRE: String Processing and Information Retrieval, Nov 2022,
  Concepci{\'o}n, Chile</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2208.14669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.14669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.14787">
    <id>http://arxiv.org/abs/2208.14787v1</id>
    <updated>2022-08-31T11:52:58Z</updated>
    <published>2022-08-31T11:52:58Z</published>
    <title>Computing all-vs-all MEMs in run-length encoded collections of HiFi
  reads</title>
    <summary>  We describe an algorithm to find maximal exact matches (MEMs) among HiFi
reads with homopolymer errors. The main novelty in our work is that we resort
to run-length compression to help deal with errors. Our method receives as
input a run-length-encoded string collection containing the HiFi reads along
with their reverse complements. Subsequently, it splits the encoding into two
arrays, one storing the sequence of symbols for equal-symbol runs and another
storing the run lengths. The purpose of the split is to get the BWT of the run
symbols and reorder their lengths accordingly. We show that this special BWT,
as it encodes the HiFi reads and their reverse complements, supports
bi-directional queries for the HiFi reads. Then, we propose a variation of the
MEM algorithm of Belazzougui et al. (2013) that exploits the run-length
encoding and the implicit bi-directional property of our BWT to compute
approximate MEMs. Concretely, if the algorithm finds that two substrings, $a_1
\ldots a_p$ and $b_1 \ldots b_p$, have a MEM, then it reports the MEM only if
their corresponding length sequences, $\ell^{a}_1 \ldots \ell^{a}_p$ and
$\ell^{b}_1 \ldots \ell^{b}_p$, do not differ beyond an input threshold. We use
a simple metric to calculate the similarity of the length sequences that we
call the {\em run-length excess}. Our technique facilitates the detection of
MEMs with homopolymer errors as it does not require dynamic programming to find
approximate matches where the only edits are the lengths of the equal-symbol
runs. Finally, we present a method that relies on a geometric data structure to
report the text occurrences of the MEMs detected by our algorithm.
</summary>
    <author>
      <name>Diego Díaz-Domínguez</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Leena Salmela</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in SPIRE'22</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.14787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.14787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.14315">
    <id>http://arxiv.org/abs/2208.14315v1</id>
    <updated>2022-08-30T14:53:16Z</updated>
    <published>2022-08-30T14:53:16Z</published>
    <title>Sorting Genomes by Prefix Double-Cut-and-Joins</title>
    <summary>  In this paper, we study the problem of sorting unichromosomal linear genomes
by prefix double-cut-and-joins (or DCJs) in both the signed and the unsigned
settings. Prefix DCJs cut the leftmost segment of a genome and any other
segment, and recombine the severed endpoints in one of two possible ways: one
of these options corresponds to a prefix reversal, which reverses the order of
elements between the two cuts (as well as their signs in the signed case).
Depending on whether we consider both options or reversals only, our main
results are:
  (1) new structural lower bounds based on the breakpoint graph for sorting by
unsigned prefix reversals, unsigned prefix DCJs, or signed prefix DCJs;
  (2) a polynomial-time algorithm for sorting by signed prefix DCJs, thus
answering an open question in [8];
  (3) a 3/2-approximation for sorting by unsigned prefix DCJs, which is, to the
best of our knowledge, the first sorting by {\em prefix} rearrangements problem
that admits an approximation ratio strictly smaller than 2 (with the obvious
exception of the polynomial-time solvable problems); and finally,
  (4) an FPT algorithm for sorting by unsigned prefix DCJs parameterised by the
number of breakpoints in the genome.
</summary>
    <author>
      <name>Guillaume Fertin</name>
    </author>
    <author>
      <name>Géraldine Jean</name>
    </author>
    <author>
      <name>Anthony Labarre</name>
    </author>
    <link href="http://arxiv.org/abs/2208.14315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.14315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.11791">
    <id>http://arxiv.org/abs/2208.11791v1</id>
    <updated>2022-08-24T22:53:13Z</updated>
    <published>2022-08-24T22:53:13Z</published>
    <title>A Simpler Proof that Pairing Heaps Take O(1) Amortized Time per
  Insertion</title>
    <summary>  The pairing heap is a simple "self-adjusting" implementation of a heap
(priority queue). Inserting an item into a pairing heap or decreasing the key
of an item takes O(1) time worst-case, as does melding two heaps. But deleting
an item of minimum key can take time linear in the heap size in the worst case.
The paper that introduced the pairing heap proved an O(log n) amortized time
bound for each heap operation, where n is the number of items in the heap or
heaps involved in the operation, by charging all but O(log n) of the time for
each deletion to non-deletion operations, O(log n) to each. Later Iacono found
a way to reduce the amortized time per insertion to O(1) and that of meld to
zero while preserving the O(log n) amortized time bound for the other update
operations. We give a simpler proof of Iacono's result with significantly
smaller constant factors. Our analysis uses the natural representation of
pairing heaps instead of the conversion to a binary tree used in the original
analysis and in Iacono's.
</summary>
    <author>
      <name>Corwin Sinnamon</name>
    </author>
    <author>
      <name>Robert Tarjan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure, submitted to SOSA 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.11791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.11791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05 (Primary) 68W40 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; E.1; E.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.11371">
    <id>http://arxiv.org/abs/2208.11371v1</id>
    <updated>2022-08-24T08:43:47Z</updated>
    <published>2022-08-24T08:43:47Z</published>
    <title>Hierarchical Relative Lempel-Ziv Compression</title>
    <summary>  Relative Lempel-Ziv (RLZ) parsing is a dictionary compression method in which
a string $S$ is compressed relative to a second string $R$ (called the
reference) by parsing $S$ into a sequence of substrings that occur in $R$. RLZ
is particularly effective at compressing sets of strings that have a high
degree of similarity to the reference string, such as a set of genomes of
individuals from the same species. With the now cheap cost of DNA sequencing,
such data sets have become extremely abundant and are rapidly growing. In this
paper, instead of using a single reference string for the entire collection, we
investigate the use of different reference strings for subsets of the
collection, with the aim of improving compression. In particular, we form a
rooted tree (or hierarchy) on the strings and then compressed each string using
RLZ with parent as reference, storing only the root of the tree in plain text.
To decompress, we traverse the tree in BFS order starting at the root,
decompressing children with respect to their parent. We show that this approach
leads to a twofold improvement in compression on bacterial genome data sets,
with negligible effect on decompression time compared to the standard single
reference approach. We show that an effective hierarchy for a given set of
strings can be constructed by computing the optimal arborescence of a completed
weighted digraph of the strings, with weights as the number of phrases in the
RLZ parsing of the source and destination vertices. We further show that
instead of computing the complete graph, a sparse graph derived using locality
sensitive hashing can significantly reduce the cost of computing a good
hierarchy, without adversely effecting compression performance.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Simon J. Puglisi</name>
    </author>
    <author>
      <name>Simon R. Tarnow</name>
    </author>
    <link href="http://arxiv.org/abs/2208.11371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.11371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.09809">
    <id>http://arxiv.org/abs/2208.09809v1</id>
    <updated>2022-08-21T05:49:04Z</updated>
    <published>2022-08-21T05:49:04Z</published>
    <title>A Work-Efficient Parallel Algorithm for Longest Increasing Subsequence</title>
    <summary>  This paper studies parallel algorithms for the longest increasing subsequence
(LIS) problem. Let $n$ be the input size and $k$ be the LIS length of the
input. Sequentially, LIS is a simple textbook problem that can be solved using
dynamic programming (DP) in $O(n\log n)$ work. However, parallelizing LIS is a
long-standing challenge. We are unaware of any parallel LIS algorithm that has
optimal $O(n\log n)$ work and non-trivial parallelism (i.e., $\tilde{O}(k)$ or
$o(n)$ span). Here, the work of a parallel algorithm is the total number of
operations, and the span is the longest dependent instructions.
  This paper proposes a parallel LIS algorithm that costs $O(n\log k)$ work,
$\tilde{O}(k)$ span, and $O(n)$ space, and is \emph{much simpler} than the
previous parallel LIS algorithms. We also generalize the algorithm to a
weighted version of LIS, which maximizes the weighted sum for all objects in an
increasing subsequence. Our weighted LIS algorithm has $O(n\log^2 n)$ work and
$\tilde{O}(k)$ span.
  We also implemented our parallel LIS algorithms. Due to simplicity, our
implementation is light-weighted, efficient, and scalable. On input size
$10^9$, our LIS algorithm outperforms a highly-optimized sequential algorithm
(with $O(n\log k)$ cost) on inputs with $k\le 3\times 10^5$. Our algorithm is
also much faster than the best existing parallel implementation by Shen et al.
on all input instances.
</summary>
    <author>
      <name>Yan Gu</name>
    </author>
    <author>
      <name>Zheqi Shen</name>
    </author>
    <author>
      <name>Yihan Sun</name>
    </author>
    <author>
      <name>Zijin Wan</name>
    </author>
    <link href="http://arxiv.org/abs/2208.09809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.09840">
    <id>http://arxiv.org/abs/2208.09840v1</id>
    <updated>2022-08-21T08:23:57Z</updated>
    <published>2022-08-21T08:23:57Z</published>
    <title>Teaching the Burrows-Wheeler Transform via the Positional
  Burrows-Wheeler Transform</title>
    <summary>  The Burrows-Wheeler Transform (BWT) is often taught in undergraduate courses
on algorithmic bioinformatics, because it underlies the FM-index and thus
important tools such as Bowtie and BWA. Its admirers consider the BWT a thing
of beauty but, despite thousands of pages being written about it over nearly
thirty years, to undergraduates seeing it for the first time it still often
seems like magic. Some who persevere are later shown the Positional BWT (PBWT),
which was published twenty years after the BWT. In this paper we argue that the
PBWT should be taught {\em before} the BWT.
  We first use the PBWT's close relation to a right-to-left radix sort to
explain how to use it as a fast and space-efficient index for {\em positional
search} on a set of strings (that is, given a pattern and a position, quickly
list the strings containing that pattern starting in that position). We then
observe that {\em prefix search} (listing all the strings that start with the
pattern) is an easy special case of positional search, and that prefix search
on the suffixes of a single string is equivalent to {\em substring search} in
that string (listing all the starting positions of occurrences of the pattern
in the string).
  Storing na\"ively a PBWT of the suffixes of a string is space-{\em
inefficient} but, in even reasonably small examples, most of its columns are
nearly the same. It is not difficult to show that if we store a PBWT of the
cyclic shifts of the string, instead of its suffixes, then all the columns are
exactly the same -- and equal to the BWT of the string. Thus we can teach the
BWT and the FM-index via the PBWT.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <link href="http://arxiv.org/abs/2208.09840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.05686">
    <id>http://arxiv.org/abs/2209.05686v1</id>
    <updated>2022-09-13T01:51:02Z</updated>
    <published>2022-09-13T01:51:02Z</published>
    <title>Software-Hardware Codesign for Efficient In-Memory Regular Pattern
  Matching</title>
    <summary>  Regular pattern matching is used in numerous application domains, including
text processing, bioinformatics, and network security. Patterns are typically
expressed with an extended syntax of regular expressions that include the
computationally challenging construct of bounded iteration or counting, which
describes the repetition of a pattern a fixed number of times. We develop a
design for a specialized in-memory hardware architecture for NFA execution that
integrates counter and bit vector elements. The design is inspired by the
theoretical model of nondeterministic counter automata (NCA). A key feature of
our approach is that we statically analyze regular expressions to determine
bounds on the amount of memory needed for the occurrences of counting. The
results of this analysis are used by a regex-to-hardware compiler in order to
make an appropriate selection of counter or bit vector elements. We evaluate
the performance of our hardware implementation on a simulator based on circuit
parameters collected by SPICE simulation using a TSMC 28nm process. We find the
usage of counter and bit vector quickly outperforms unfolding solutions by
orders of magnitude with small counting quantifiers. Experiments concerning
realistic workloads show up to 76% energy reduction and 58% area reduction in
comparison to traditional in-memory NFA processors.
</summary>
    <author>
      <name>Lingkun Kong</name>
    </author>
    <author>
      <name>Qixuan Yu</name>
    </author>
    <author>
      <name>Agnishom Chattopadhyay</name>
    </author>
    <author>
      <name>Alexis Le Glaunec</name>
    </author>
    <author>
      <name>Yi Huang</name>
    </author>
    <author>
      <name>Konstantinos Mamouras</name>
    </author>
    <author>
      <name>Kaiyuan Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3519939.3523456</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3519939.3523456" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been published in PLDI 2022</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">43rd ACM SIGPLAN Conference on Programming Language Design and
  Implementation (PLDI 2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2209.05686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.05686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.01095">
    <id>http://arxiv.org/abs/2209.01095v1</id>
    <updated>2022-09-02T14:54:17Z</updated>
    <published>2022-09-02T14:54:17Z</published>
    <title>Elastic-Degenerate String Matching with 1 Error</title>
    <summary>  An elastic-degenerate string is a sequence of $n$ finite sets of strings of
total length $N$, introduced to represent a set of related DNA sequences, also
known as a pangenome. The ED string matching (EDSM) problem consists in
reporting all occurrences of a pattern of length $m$ in an ED text. This
problem has recently received some attention by the combinatorial pattern
matching community, culminating in an
$\tilde{\mathcal{O}}(nm^{\omega-1})+\mathcal{O}(N)$-time algorithm [Bernardini
et al., SIAM J. Comput. 2022], where $\omega$ denotes the matrix multiplication
exponent and the $\tilde{\mathcal{O}}(\cdot)$ notation suppresses polylog
factors. In the $k$-EDSM problem, the approximate version of EDSM, we are asked
to report all pattern occurrences with at most $k$ errors. $k$-EDSM can be
solved in $\mathcal{O}(k^2mG+kN)$ time, under edit distance, or
$\mathcal{O}(kmG+kN)$ time, under Hamming distance, where $G$ denotes the total
number of strings in the ED text [Bernardini et al., Theor. Comput. Sci. 2020].
Unfortunately, $G$ is only bounded by $N$, and so even for $k=1$, the existing
algorithms run in $\Omega(mN)$ time in the worst case. In this paper we show
that $1$-EDSM can be solved in $\mathcal{O}((nm^2 + N)\log m)$ or
$\mathcal{O}(nm^3 + N)$ time under edit distance. For the decision version, we
present a faster $\mathcal{O}(nm^2\sqrt{\log m} + N\log\log m)$-time algorithm.
We also show that $1$-EDSM can be solved in $\mathcal{O}(nm^2 + N\log m)$ time
under Hamming distance. Our algorithms for edit distance rely on non-trivial
reductions from $1$-EDSM to special instances of classic computational geometry
problems (2d rectangle stabbing or 2d range emptiness), which we show how to
solve efficiently. In order to obtain an even faster algorithm for Hamming
distance, we rely on employing and adapting the $k$-errata trees for indexing
with errors [Cole et al., STOC 2004].
</summary>
    <author>
      <name>Giulia Bernardini</name>
    </author>
    <author>
      <name>Estéban Gabory</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Leen Stougie</name>
    </author>
    <author>
      <name>Michelle Sweering</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extended version of a paper accepted at LATIN 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.01095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.01095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.14481">
    <id>http://arxiv.org/abs/2208.14481v1</id>
    <updated>2022-08-30T18:13:42Z</updated>
    <published>2022-08-30T18:13:42Z</published>
    <title>Unbalancing Binary Trees</title>
    <summary>  Assuming Zipf's Law to be accurate, we show that existing techniques for
partially optimizing binary trees produce results that are approximately 10%
worse than true optimal. We present a new approximate optimization technique
that runs in O(n log n) time and produces trees approximately 1% worse than
optimal. The running time is comparable to that of the Garsia-Wachs algorithm
but the technique can be applied to the more useful case where the node being
searched for is expected to be contained in the tree as opposed to outside of
it.
</summary>
    <author>
      <name>Matthew L. Ginsberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.14481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.14481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1007.1361">
    <id>http://arxiv.org/abs/1007.1361v2</id>
    <updated>2010-10-18T10:58:22Z</updated>
    <published>2010-07-08T12:51:03Z</published>
    <title>Top-K Color Queries for Document Retrieval</title>
    <summary>  In this paper we describe a new efficient (in fact optimal) data structure
for the {\em top-$K$ color problem}. Each element of an array $A$ is assigned a
color $c$ with priority $p(c)$. For a query range $[a,b]$ and a value $K$, we
have to report $K$ colors with the highest priorities among all colors that
occur in $A[a..b]$, sorted in reverse order by their priorities. We show that
such queries can be answered in $O(K)$ time using an $O(N\log \sigma)$ bits
data structure, where $N$ is the number of elements in the array and $\sigma$
is the number of colors. Thus our data structure is asymptotically optimal with
respect to the worst-case query time and space. As an immediate application of
our results, we obtain optimal time solutions for several document retrieval
problems. The method of the paper could be also of independent interest.
</summary>
    <author>
      <name>Marek Karpinski</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/1007.1361v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.1361v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1106.6261">
    <id>http://arxiv.org/abs/1106.6261v1</id>
    <updated>2011-06-30T15:11:57Z</updated>
    <published>2011-06-30T15:11:57Z</published>
    <title>External Memory Orthogonal Range Reporting with Fast Updates</title>
    <summary>  In this paper we describe data structures for orthogonal range reporting in
external memory that support fast update operations. The query costs either
match the query costs of the best previously known data structures or differ by
a small multiplicative factor.
</summary>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/1106.6261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.6261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1109.3890">
    <id>http://arxiv.org/abs/1109.3890v1</id>
    <updated>2011-09-18T17:17:01Z</updated>
    <published>2011-09-18T17:17:01Z</published>
    <title>A Dynamic Stabbing-Max Data Structure with Sub-Logarithmic Query Time</title>
    <summary>  In this paper we describe a dynamic data structure that answers
one-dimensional stabbing-max queries in optimal $O(\log n/\log\log n)$ time.
Our data structure uses linear space and supports insertions and deletions in
$O(\log n)$ and $O(\log n/\log \log n)$ amortized time respectively.
  We also describe a $O(n(\log n/\log\log n)^{d-1})$ space data structure that
answers $d$-dimensional stabbing-max queries in $O((\log n/\log\log n)^{d})$
time. Insertions and deletions are supported in $O((\log n/\log\log
n)^d\log\log n)$ and $O((\log n/\log\log n)^d)$ amortized time respectively.
</summary>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper accepted to ISAAC 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.3890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.3890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1306.5029">
    <id>http://arxiv.org/abs/1306.5029v1</id>
    <updated>2013-06-21T01:35:36Z</updated>
    <published>2013-06-21T01:35:36Z</published>
    <title>Optimal Color Range Reporting in One Dimension</title>
    <summary>  Color (or categorical) range reporting is a variant of the orthogonal range
reporting problem in which every point in the input is assigned a \emph{color}.
While the answer to an orthogonal point reporting query contains all points in
the query range $Q$, the answer to a color reporting query contains only
distinct colors of points in $Q$. In this paper we describe an O(N)-space data
structure that answers one-dimensional color reporting queries in optimal
$O(k+1)$ time, where $k$ is the number of colors in the answer and $N$ is the
number of points in the data structure. Our result can be also dynamized and
extended to the external memory model.
</summary>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Jeffrey Scott Vitter</name>
    </author>
    <link href="http://arxiv.org/abs/1306.5029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.5029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1401.0625">
    <id>http://arxiv.org/abs/1401.0625v1</id>
    <updated>2014-01-03T11:08:34Z</updated>
    <published>2014-01-03T11:08:34Z</published>
    <title>Space-Efficient String Indexing for Wildcard Pattern Matching</title>
    <summary>  In this paper we describe compressed indexes that support pattern matching
queries for strings with wildcards. For a constant size alphabet our data
structure uses $O(n\log^{\varepsilon}n)$ bits for any $\varepsilon>0$ and
reports all $\mathrm{occ}$ occurrences of a wildcard string in $O(m+\sigma^g
\cdot\mu(n) + \mathrm{occ})$ time, where $\mu(n)=o(\log\log\log n)$, $\sigma$
is the alphabet size, $m$ is the number of alphabet symbols and $g$ is the
number of wildcard symbols in the query string. We also present an $O(n)$-bit
index with $O((m+\sigma^g+\mathrm{occ})\log^{\varepsilon}n)$ query time and an
$O(n(\log\log n)^2)$-bit index with $O((m+\sigma^g+\mathrm{occ})\log\log n)$
query time. These are the first non-trivial data structures for this problem
that need $o(n\log n)$ bits of space.
</summary>
    <author>
      <name>Moshe Lewenstein</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Jeffrey Scott Vitter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, extended version of the STACS paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.0625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.11094">
    <id>http://arxiv.org/abs/2007.11094v1</id>
    <updated>2020-07-21T21:22:25Z</updated>
    <published>2020-07-21T21:22:25Z</published>
    <title>New Data Structures for Orthogonal Range Reporting and Range Minima
  Queries</title>
    <summary>  In this paper we present new data structures for two extensively studied
variants of the orthogonal range searching problem.
  First, we describe a data structure that supports two-dimensional orthogonal
range minima queries in $O(n)$ space and $O(\log^{\varepsilon} n)$ time, where
$n$ is the number of points in the data structure and $\varepsilon$ is an
arbitrarily small positive constant. Previously known linear-space solutions
for this problem require $O(\log^{1+\varepsilon} n)$ (Chazelle, 1988) or
$O(\log n\log \log n)$ time (Farzan et al., 2012). A modification of our data
structure uses space $O(n\log \log n)$ and supports range minima queries in
time $O(\log \log n)$. Both results can be extended to support
three-dimensional five-sided reporting queries.
  Next, we turn to the four-dimensional orthogonal range reporting problem and
present a data structure that answers queries in optimal $O(\log n/\log \log n
+ k)$ time, where $k$ is the number of points in the answer. This is the first
data structure that achieves the optimal query time for this problem.
  Our results are obtained by exploiting the properties of three-dimensional
shallow cuttings.
</summary>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/2007.11094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.11094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1405.0189">
    <id>http://arxiv.org/abs/1405.0189v1</id>
    <updated>2014-05-01T15:30:37Z</updated>
    <published>2014-05-01T15:30:37Z</published>
    <title>On Hardness of Jumbled Indexing</title>
    <summary>  Jumbled indexing is the problem of indexing a text $T$ for queries that ask
whether there is a substring of $T$ matching a pattern represented as a Parikh
vector, i.e., the vector of frequency counts for each character. Jumbled
indexing has garnered a lot of interest in the last four years. There is a
naive algorithm that preprocesses all answers in $O(n^2|\Sigma|)$ time allowing
quick queries afterwards, and there is another naive algorithm that requires no
preprocessing but has $O(n\log|\Sigma|)$ query time. Despite a tremendous
amount of effort there has been little improvement over these running times.
  In this paper we provide good reason for this. We show that, under a
3SUM-hardness assumption, jumbled indexing for alphabets of size $\omega(1)$
requires $\Omega(n^{2-\epsilon})$ preprocessing time or $\Omega(n^{1-\delta})$
query time for any $\epsilon,\delta>0$. In fact, under a stronger 3SUM-hardness
assumption, for any constant alphabet size $r\ge 3$ there exist describable
fixed constant $\epsilon_r$ and $\delta_r$ such that jumbled indexing requires
$\Omega(n^{2-\epsilon_r})$ preprocessing time or $\Omega(n^{1-\delta_r})$ query
time.
</summary>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Timothy Chan</name>
    </author>
    <author>
      <name>Moshe Lewenstein</name>
    </author>
    <author>
      <name>Noa Lewenstein</name>
    </author>
    <link href="http://arxiv.org/abs/1405.0189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.0189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1408.2350">
    <id>http://arxiv.org/abs/1408.2350v1</id>
    <updated>2014-08-11T08:35:29Z</updated>
    <published>2014-08-11T08:35:29Z</published>
    <title>Dictionary Matching with One Gap</title>
    <summary>  The dictionary matching with gaps problem is to preprocess a dictionary $D$
of $d$ gapped patterns $P_1,\ldots,P_d$ over alphabet $\Sigma$, where each
gapped pattern $P_i$ is a sequence of subpatterns separated by bounded
sequences of don't cares. Then, given a query text $T$ of length $n$ over
alphabet $\Sigma$, the goal is to output all locations in $T$ in which a
pattern $P_i\in D$, $1\leq i\leq d$, ends. There is a renewed current interest
in the gapped matching problem stemming from cyber security. In this paper we
solve the problem where all patterns in the dictionary have one gap with at
least $\alpha$ and at most $\beta$ don't cares, where $\alpha$ and $\beta$ are
given parameters. Specifically, we show that the dictionary matching with a
single gap problem can be solved in either $O(d\log d + |D|)$ time and
$O(d\log^{\varepsilon} d + |D|)$ space, and query time $O(n(\beta -\alpha
)\log\log d \log ^2 \min \{ d, \log |D| \} + occ)$, where $occ$ is the number
of patterns found, or preprocessing time and space: $O(d^2 + |D|)$, and query
time $O(n(\beta -\alpha ) + occ)$, where $occ$ is the number of patterns found.
As far as we know, this is the best solution for this setting of the problem,
where many overlaps may exist in the dictionary.
</summary>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Avivit Levy</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <author>
      <name>B. Riva Shalom</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary version was published at CPM 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.2350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.2350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1503.07563">
    <id>http://arxiv.org/abs/1503.07563v2</id>
    <updated>2015-07-10T03:31:44Z</updated>
    <published>2015-03-25T22:02:54Z</published>
    <title>Mind the Gap</title>
    <summary>  We examine the complexity of the online Dictionary Matching with One Gap
Problem (DMOG) which is the following. Preprocess a dictionary $D$ of $d$
patterns, where each pattern contains a special gap symbol that can match any
string, so that given a text that arrives online, a character at a time, we can
report all of the patterns from $D$ that are suffixes of the text that has
arrived so far, before the next character arrives. In more general versions the
gap symbols are associated with bounds determining the possible lengths of
matching strings. Finding efficient algorithmic solutions for (online) DMOG has
proven to be a difficult algorithmic challenge. We demonstrate that the
difficulty in obtaining efficient solutions for the DMOG problem even, in the
offline setting, can be traced back to the infamous 3SUM conjecture.
Interestingly, our reduction deviates from the known reduction paths that
follow from 3SUM. In particular, most reductions from 3SUM go through the
set-disjointness problem, which corresponds to the problem of preprocessing a
graph to answer edge-triangles queries. We use a new path of reductions by
considering the complementary, although structurally very different,
vertex-triangles queries. Using this new path we show a conditional lower bound
of $\Omega(\delta(G_D)+op)$ time per text character, where $G_D$ is a bipartite
graph that captures the structure of $D$, $\delta(G_D)$ is the degeneracy of
this graph, and $op$ is the output size. We also provide matching upper-bounds
(up to sub-polynomial factors) for the vertex-triangles problem, and then
extend these techniques to the online DMOG problem. In particular, we introduce
algorithms whose time cost depends linearly on $\delta(G_D)$. Our algorithms
make use of graph orientations, together with some additional techniques.
Finally, when $\delta(G_D)$ is large we are able to obtain even more efficient
solutions.
</summary>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Avivit Levy</name>
    </author>
    <author>
      <name>Seth Pettie</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <author>
      <name>B. Riva Shalom</name>
    </author>
    <link href="http://arxiv.org/abs/1503.07563v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.07563v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1706.10061">
    <id>http://arxiv.org/abs/1706.10061v2</id>
    <updated>2017-11-10T05:00:34Z</updated>
    <published>2017-06-30T08:36:57Z</published>
    <title>Compaction of Church Numerals for Higher-Order Compression</title>
    <summary>  In this study, we address the problem of compacting Church numerals. Church
numerals appear as a representation of the repetitive part of data in
higher-order compression. We propose a novel decomposition scheme for a natural
number using tetration, which leads to a compact representation of
$\lambda$-terms equivalent to the original Church numerals. For natural number
$n$, we prove that the size of the $\lambda$-term obtained by the proposed
method is $O(({\rm slog}_{2}n)^{\log n/ \log \log n})$. Moreover, we
quantitatively confirmed experimentally that the proposed method outperforms a
binary expression of Church numerals when $n$ is less than approximately 10000.
</summary>
    <author>
      <name>Isamu Furuya</name>
    </author>
    <author>
      <name>Takuya Kida</name>
    </author>
    <link href="http://arxiv.org/abs/1706.10061v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.10061v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.12405">
    <id>http://arxiv.org/abs/2209.12405v1</id>
    <updated>2022-09-26T04:05:05Z</updated>
    <published>2022-09-26T04:05:05Z</published>
    <title>Inferring strings from position heaps in linear time</title>
    <summary>  Position heaps are index structures of text strings used for the exact string
matching problem. They are rooted trees whose nodes and edges are both labeled.
This paper is concerned with variants of the inverse problem of position heap
construction and gives linear-time algorithms for those problems. The basic
problem is to restore a text string from a rooted tree with labeled nodes and
edges. The input trees may miss edge labels or node labels in the variant
problems.
</summary>
    <author>
      <name>Koshiro Kumagai</name>
    </author>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.12405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.12405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.09598">
    <id>http://arxiv.org/abs/2209.09598v1</id>
    <updated>2022-09-20T10:22:13Z</updated>
    <published>2022-09-20T10:22:13Z</published>
    <title>Complement Avoidance in Binary Words</title>
    <summary>  The complement $\overline{x}$ of a binary word $x$ is obtained by changing
each $0$ in $x$ to $1$ and vice versa. We study infinite binary words $\bf w$
that avoid sufficiently large complementary factors; that is, if $x$ is a
factor of $\bf w$ then $\overline{x}$ is not a factor of $\bf w$. In
particular, we classify such words according to their critical exponents.
</summary>
    <author>
      <name>James Currie</name>
    </author>
    <author>
      <name>Pascal Ochem</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2209.09598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.08926">
    <id>http://arxiv.org/abs/2209.08926v2</id>
    <updated>2022-09-28T21:12:41Z</updated>
    <published>2022-09-19T11:17:46Z</published>
    <title>Convergence of the number of period sets in strings</title>
    <summary>  Consider words of length $n$. The set of all periods of a word of length $n$
is a subset of $\{0,1,2,\ldots,n-1\}$. However, any subset of
$\{0,1,2,\ldots,n-1\}$ is not necessarily a valid set of periods. In a seminal
paper in 1981, Guibas and Odlyzko have proposed to encode the set of periods of
a word into an $n$ long binary string, called an autocorrelation, where a one
at position $i$ denotes a period of $i$. They considered the question of
recognizing a valid period set, and also studied the number of valid period
sets for length $n$, denoted $\kappa_n$. They conjectured that $\ln(\kappa_n)$
asymptotically converges to a constant times $\ln^2(n)$. If improved lower
bounds for $\ln(\kappa_n)/\ln^2(n)$ were proposed in 2001, the question of a
tight upper bound has remained opened since Guibas and Odlyzko's paper. Here,
we exhibit an upper bound for this fraction, which implies its convergence and
closes this long standing conjecture. Moreover, we extend our result to find
similar bounds for the number of correlations: a generalization of
autocorrelations which encodes the overlaps between two strings.
</summary>
    <author>
      <name>Eric Rivals</name>
    </author>
    <author>
      <name>Michelle Sweering</name>
    </author>
    <author>
      <name>Pengfei Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 1 figure, 2 tables, 12 bibliographic references; version 2:
  added a Related works section with additional references</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.08926v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.08926v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="MCS: 05-06" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.09223">
    <id>http://arxiv.org/abs/2209.09223v1</id>
    <updated>2022-09-19T17:47:53Z</updated>
    <published>2022-09-19T17:47:53Z</published>
    <title>Antisquares and Critical Exponents</title>
    <summary>  The complement $\bar{x}$ of a binary word $x$ is obtained by changing each
$0$ in $x$ to $1$ and vice versa. An antisquare is a nonempty word of the form
$x\, \bar{x}$. In this paper, we study infinite binary words that do not
contain arbitrarily large antisquares. For example, we show that the repetition
threshold for the language of infinite binary words containing exactly two
distinct antisquares is $(5+\sqrt{5})/2$. We also study repetition thresholds
for related classes, where "two" in the previous sentence is replaced by a
large number.
  We say a binary word is good if the only antisquares it contains are $01$ and
$10$. We characterize the minimal antisquares, that is, those words that are
antisquares but all proper factors are good. We determine the the growth rate
of the number of good words of length $n$ and determine the repetition
threshold between polynomial and exponential growth for the number of good
words.
</summary>
    <author>
      <name>Aseem Baranwal</name>
    </author>
    <author>
      <name>James Currie</name>
    </author>
    <author>
      <name>Lucas Mol</name>
    </author>
    <author>
      <name>Pascal Ochem</name>
    </author>
    <author>
      <name>Narad Rampersad</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2209.09223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.09218">
    <id>http://arxiv.org/abs/2209.09218v1</id>
    <updated>2022-09-19T17:37:16Z</updated>
    <published>2022-09-19T17:37:16Z</published>
    <title>MARIA: Multiple-alignment $r$-index with aggregation</title>
    <summary>  There now exist compact indexes that can efficiently list all the occurrences
of a pattern in a dataset consisting of thousands of genomes, or even all the
occurrences of all the pattern's maximal exact matches (MEMs) with respect to
the dataset. Unless we are lucky and the pattern is specific to only a few
genomes, however, we could be swamped by hundreds of matches -- or even
hundreds per MEM -- only to discover that most or all of the matches are to
substrings that occupy the same few columns in a multiple alignment. To address
this issue, in this paper we present a simple and compact data index MARIA that
stores a multiple alignment such that, given the position of one match of a
pattern (or a MEM or other substring of a pattern) and its length, we can
quickly list all the distinct columns of the multiple alignment where matches
start.
</summary>
    <author>
      <name>Adrián Goga</name>
    </author>
    <author>
      <name>Andrej Baláž</name>
    </author>
    <author>
      <name>Alessia Petescia</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/2209.09218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.09166">
    <id>http://arxiv.org/abs/2209.09166v1</id>
    <updated>2022-09-19T16:35:28Z</updated>
    <published>2022-09-19T16:35:28Z</published>
    <title>Cache-Oblivious Representation of B-Tree Structures</title>
    <summary>  We present a data structure CORoBTS for storing a search tree with all leaves
at the same depth and vertices of arity between chosen constants $a$ and $b$ in
a cache-oblivious way. It provides operations for inserting an $a$-ary subtree
and removing a subtree; both have an amortized I/O complexity
$\mathcal{O}(S\cdot(\log^2 N)/ B + \log_B N \cdot \log\log S + 1)$ and
amortized time complexity $\mathcal{O}(S\cdot\log^2 N)$, where $S$ is the size
of the subtree and $N$ size of the whole stored tree. The tree allows searching
with an optimal I/O complexity $\mathcal{O}(\log_B{N})$ and is stored in a
linear space.
  We use the data structure as a top space-time tree in the cache-oblivious
partially persistent array proposed by Davoodi et al. [DFI\"O14]. The space
complexity of the persistent array is then improved from $\mathcal{O}(U^{\log_2
3} + V \log U)$ to $\mathcal{O}(U + V \log U)$, where $U$ is the maximal size
of the array and $V$ is the number of versions. The data locality and I/O
complexity of both present and persistent reads are kept unchanged; I/O
complexity of writes is worsened by a polylogarithmic factor.
</summary>
    <author>
      <name>Lukáš Ondráček</name>
    </author>
    <author>
      <name>Ondřej Mička</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages + 7 pages of algorithms, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.09166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.07100">
    <id>http://arxiv.org/abs/2209.07100v1</id>
    <updated>2022-09-15T07:28:38Z</updated>
    <published>2022-09-15T07:28:38Z</published>
    <title>Concurrent Size</title>
    <summary>  The size of a data structure (i.e., the number of elements in it) is a widely
used property of a data set. However, for concurrent programs, obtaining a
correct size efficiently is non-trivial. In fact, the literature does not offer
a mechanism to obtain a correct (linearizable) size of a concurrent data set
without resorting to inefficient solutions, such as taking a full snapshot of
the data structure to count the elements, or acquiring one global lock in all
update and size operations. This paper presents a methodology for adding a
concurrent linearizable size operation to sets and dictionaries with a
relatively low performance overhead. Theoretically, the proposed size operation
is wait-free with asymptotic complexity linear in the number of threads
(independently of data-structure size). Practically, we evaluated the
performance overhead by adding size to various concurrent data structures in
Java$-$a skip list, a hash table and a tree. The proposed linearizable size
operation executes faster by orders of magnitude compared to the existing
option of taking a snapshot, while incurring a throughput loss of $1\%-20\%$ on
the original data structure's operations.
</summary>
    <author>
      <name>Gal Sela</name>
    </author>
    <author>
      <name>Erez Petrank</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3563300</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3563300" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code: https://github.com/galysela/ConcurrentSize</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the ACM on Programming Languages 6, OOPSLA2,
  Article 137 (October 2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2209.07100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.07100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.06909">
    <id>http://arxiv.org/abs/2209.06909v1</id>
    <updated>2022-09-14T20:06:30Z</updated>
    <published>2022-09-14T20:06:30Z</published>
    <title>Multiway Powersort</title>
    <summary>  Powersort (Munro &amp; Wild, ESA2018) has recently replaced Timsort's suboptimal
merge policy in the CPython reference implementation of Python, as well as in
PyPy and further libraries. We present a stable mergesort variant, Multiway
Powersort, that exploits existing runs and finds nearly-optimal merging orders
for k-way merges with negligible overhead. As observed with Multiway Quicksort
(Kushagra et al., ALENEX 2014; Aum\"uller &amp; Dietzfelbinger, TALG 2016; Wild,
PhD thesis 2016) and the inclusion of Dual-Pivot Quicksort in the Java runtime
library, memory transfers increasingly determine the cost of internal sorting.
We demonstrate that our 4-way Powersort implementation can achieve substantial
speedups over standard (2-way) Powersort and other stable sorting methods
without compromising the optimally run-adaptive performance of Powersort.
</summary>
    <author>
      <name>William Cawley Gelling</name>
    </author>
    <author>
      <name>Markus E. Nebel</name>
    </author>
    <author>
      <name>Benjamin Smith</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages; accompanying source code at
  https://github.com/sebawild/powersort</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.06909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.06909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.07524">
    <id>http://arxiv.org/abs/2209.07524v1</id>
    <updated>2022-09-15T17:59:52Z</updated>
    <published>2022-09-15T17:59:52Z</published>
    <title>$\tilde{O}(n+\mathrm{poly}(k))$-time Algorithm for Bounded Tree Edit
  Distance</title>
    <summary>  Computing the edit distance of two strings is one of the most basic problems
in computer science and combinatorial optimization. Tree edit distance is a
natural generalization of edit distance in which the task is to compute a
measure of dissimilarity between two (unweighted) rooted trees with node
labels. Perhaps the most notable recent application of tree edit distance is in
NoSQL big databases, such as MongoDB, where each row of the database is a JSON
document represented as a labeled rooted tree, and finding dissimilarity
between two rows is a basic operation. Until recently, the fastest algorithm
for tree edit distance ran in cubic time (Demaine, Mozes, Rossman, Weimann;
TALG'10); however, Mao (FOCS'21) broke the cubic barrier for the tree edit
distance problem using fast matrix multiplication.
  Given a parameter $k$ as an upper bound on the distance, an $O(n+k^2)$-time
algorithm for edit distance has been known since the 1980s due to the works of
Myers (Algorithmica'86) and Landau and Vishkin (JCSS'88). The existence of an
$\tilde{O}(n+\mathrm{poly}(k))$-time algorithm for tree edit distance has been
posed as an open question, e.g., by Akmal and Jin (ICALP'21), who gave a
state-of-the-art $\tilde{O}(nk^2)$-time algorithm. In this paper, we answer
this question positively.
</summary>
    <author>
      <name>Debarati Das</name>
    </author>
    <author>
      <name>Jacob Gilbert</name>
    </author>
    <author>
      <name>MohammadTaghi Hajiaghayi</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Barna Saha</name>
    </author>
    <author>
      <name>Hamed Saleh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper accepted to FOCS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.07524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.07524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1006.4093">
    <id>http://arxiv.org/abs/1006.4093v1</id>
    <updated>2010-06-21T15:26:40Z</updated>
    <published>2010-06-21T15:26:40Z</published>
    <title>Dynamic Range Reporting in External Memory</title>
    <summary>  In this paper we describe a dynamic external memory data structure that
supports range reporting queries in three dimensions in $O(\log_B^2 N +
\frac{k}{B})$ I/O operations, where $k$ is the number of points in the answer
and $B$ is the block size. This is the first dynamic data structure that
answers three-dimensional range reporting queries in $\log_B^{O(1)} N +
O(\frac{k}{B})$ I/Os.
</summary>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/1006.4093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.4093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.09914">
    <id>http://arxiv.org/abs/2210.09914v2</id>
    <updated>2022-10-20T22:18:19Z</updated>
    <published>2022-10-18T14:52:42Z</published>
    <title>Computing MEMs on Repetitive Text Collections</title>
    <summary>  We consider the problem of computing the Maximal Exact Matches (MEMs) of a
given pattern $P[1..m]$ on a large repetitive text collection $T[1..n]$, which
is represented as a (hopefully much smaller) run-length context-free grammar of
size $g_{rl}$. We show that the problem can be solved in time $O(m^2
\log^\epsilon n)$, for any constant $\epsilon > 0$, on a data structure of size
$O(g_{rl})$. Further, on a locally consistent grammar of size
$O(\delta\log\frac{n}{\delta})$, the time decreases to $O(m\log m(\log m +
\log^\epsilon n))$. The value $\delta$ is a function of the substring
complexity of $T$ and $\Omega(\delta\log\frac{n}{\delta})$ is a tight lower
bound on the compressibility of repetitive texts $T$, so our structure has
optimal size in terms of $n$ and $\delta$.
</summary>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/2210.09914v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.09914v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.07979">
    <id>http://arxiv.org/abs/2210.07979v1</id>
    <updated>2022-10-14T17:24:29Z</updated>
    <published>2022-10-14T17:24:29Z</published>
    <title>Space-Efficient STR-IC-LCS Computation</title>
    <summary>  One of the most fundamental method for comparing two given strings $A$ and
$B$ is the longest common subsequence (LCS), where the task is to find (the
length) of the longest common subsequence. In this paper, we address the
STR-IC-LCS problem which is one of the constrained LCS problems proposed by
Chen and Chao [J. Comb. Optim, 2011]. A string $Z$ is said to be an STR-IC-LCS
of three given strings $A$, $B$, and $P$, if $Z$ is one of the longest common
subsequences of $A$ and $B$ that contains $P$ as a substring. We present a
space efficient solution for the STR-IC-LCS problem. Our algorithm computes the
length of an STR-IC-LCS in $O(n^2)$ time and $O((\ell+1)(n-\ell+1))$ space
where $\ell$ is the length of a longest common subsequence of $A$ and $B$ of
length $n$. When $\ell = O(1)$ or $n-\ell = O(1)$, then our algorithm uses only
linear $O(n)$ space.
</summary>
    <author>
      <name>Yuuki Yonemoto</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <link href="http://arxiv.org/abs/2210.07979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.07979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.01560">
    <id>http://arxiv.org/abs/2210.01560v2</id>
    <updated>2022-11-08T16:32:57Z</updated>
    <published>2022-10-04T12:31:47Z</published>
    <title>SicHash - Small Irregular Cuckoo Tables for Perfect Hashing</title>
    <summary>  A Perfect Hash Function (PHF) is a hash function that has no collisions on a
given input set. PHFs can be used for space efficient storage of data in an
array, or for determining a compact representative of each object in the set.
In this paper, we present the PHF construction algorithm SicHash - Small
Irregular Cuckoo Tables for Perfect Hashing. At its core, SicHash uses a known
technique: It places objects in a cuckoo hash table and then stores the final
hash function choice of each object in a retrieval data structure. We combine
the idea with irregular cuckoo hashing, where each object has a different
number of hash functions. Additionally, we use many small tables that we
overload beyond their asymptotic maximum load factor. The most space efficient
competitors often use brute force methods to determine the PHFs. SicHash
provides a more direct construction algorithm that only rarely needs to
recompute parts. Our implementation improves the state of the art in terms of
space usage versus construction time for a wide range of configurations. At the
same time, it provides very fast queries.
</summary>
    <author>
      <name>Hans-Peter Lehmann</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <author>
      <name>Stefan Walzer</name>
    </author>
    <link href="http://arxiv.org/abs/2210.01560v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.01560v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.05982">
    <id>http://arxiv.org/abs/2210.05982v1</id>
    <updated>2022-10-12T07:44:07Z</updated>
    <published>2022-10-12T07:44:07Z</published>
    <title>A nearly optimal randomized algorithm for explorable heap selection</title>
    <summary>  Explorable heap selection is the problem of selecting the $n$th smallest
value in a binary heap. The key values can only be accessed by traversing
through the underlying infinite binary tree, and the complexity of the
algorithm is measured by the total distance traveled in the tree (each edge has
unit cost). This problem was originally proposed as a model to study search
strategies for the branch-and-bound algorithm with storage restrictions by
Karp, Saks and Widgerson (FOCS '86), who gave deterministic and randomized
$n\cdot \exp(O(\sqrt{\log{n}}))$ time algorithms using $O(\log(n)^{2.5})$ and
$O(\sqrt{\log n})$ space respectively. We present a new randomized algorithm
with running time $O(n\log(n)^3)$ using $O(\log n)$ space, substantially
improving the previous best randomized running time at the expense of slightly
increased space usage. We also show an $\Omega(\log(n)n/\log(\log(n)))$ for any
algorithm that solves the problem in the same amount of space, indicating that
our algorithm is nearly optimal.
</summary>
    <author>
      <name>Sander Borst</name>
    </author>
    <author>
      <name>Daniel Dadush</name>
    </author>
    <author>
      <name>Sophie Huiberts</name>
    </author>
    <author>
      <name>Danish Kashaev</name>
    </author>
    <link href="http://arxiv.org/abs/2210.05982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.05982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.02000">
    <id>http://arxiv.org/abs/2210.02000v2</id>
    <updated>2022-10-06T03:43:16Z</updated>
    <published>2022-10-05T03:17:49Z</published>
    <title>Internal Longest Palindrome Queries in Optimal Time</title>
    <summary>  Palindromes are strings that read the same forward and backward. Problems of
computing palindromic structures in strings have been studied for many years
with a motivation of their application to biology. The longest palindrome
problem is one of the most important and classical problems regarding
palindromic structures, that is, to compute the longest palindrome appearing in
a string $T$ of length $n$. The problem can be solved in $\mathcal{O}(n)$ time
by the famous algorithm of Manacher [Journal of the ACM, 1975]. In this paper,
we consider the problem in the internal model. The internal longest palindrome
query is, given a substring $T[i..j]$ of $T$ as a query, to compute the longest
palindrome appearing in $T[i.. j]$. The best known data structure for this
problem is the one proposed by Amir et al. [Algorithmica, 2020], which can
answer any query in $\mathcal{O}(\log n)$ time. In this paper, we propose a
linear-size data structure that can answer any internal longest palindrome
query in constant time. Also, given the input string $T$, our data structure
can be constructed in $\mathcal{O}(n)$ time.
</summary>
    <author>
      <name>Kazuki Mitani</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Kazuhisa Seto</name>
    </author>
    <author>
      <name>Takashi Horiyama</name>
    </author>
    <link href="http://arxiv.org/abs/2210.02000v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.02000v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.02067">
    <id>http://arxiv.org/abs/2210.02067v2</id>
    <updated>2022-10-28T11:46:42Z</updated>
    <published>2022-10-05T07:40:55Z</published>
    <title>Computing maximal generalized palindromes</title>
    <summary>  Palindromes are popular and important objects in textual data processing,
bioinformatics, and combinatorics on words. Let $S = XaY$ be a string, where
$X$ and $Y$ are of the same length and $a$ is either a single character or the
empty string. Then, there exist two alternative definitions for palindromes:
$S$ is said to be a palindrome if: Reversal-based definition: $S$ is equal to
its reversal $S^R$; Symmetry-based definition: its left-arm $X$ is equal to the
reversal of its right-arm $Y^R$. It is clear that if the "equality" ($\approx$)
used in both definitions is exact character matching ($=$), then the two
definitions are the same. However, if we apply other string-equality criteria
$\approx$, including the complementary model for biological sequences, the
parameterized model [Baker, JCSS 1996], the order-preserving model [Kim et al.,
TCS 2014], the Cartesian-tree model [Park et al., TCS 2020], and the
palindromic-structure model [I et al., TCS 2013], then are the reversal-based
palindromes and the symmetry-based palindromes the same? To the best of our
knowledge, no previous work has considered or answered this natural question.
In this paper, we first provide answers to this question, and then present
efficient algorithms for computing all maximal generalized palindromes that
occur in a given string. After confirming that Gusfield's offline suffix-tree
based algorithm for computing maximal symmetry-based palindromes can be readily
extended to the aforementioned matching models, we show how to extend
Manacher's online algorithm for computing maximal reversal-based palindromes in
linear time for all the aforementioned matching models.
</summary>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Masayuki Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/2210.02067v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.02067v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.02292">
    <id>http://arxiv.org/abs/2210.02292v2</id>
    <updated>2022-10-06T15:56:34Z</updated>
    <published>2022-10-05T14:28:46Z</published>
    <title>Double-Ended Palindromic Trees: A Linear-Time Data Structure and Its
  Applications</title>
    <summary>  The palindromic tree (a.k.a. eertree) is a linear-size data structure that
provides access to all palindromic substrings of a string. In this paper, we
propose a generalized version of eertree, called double-ended eertree, which
supports linear-time online double-ended queue operations on the stored string.
At the heart of our construction, is a class of substrings, called surfaces, of
independent interest. Namely, surfaces are neither prefixes nor suffixes of any
other palindromic substrings and characterize the link structure of all
palindromic substrings in the eertree.
  As an application, we develop a framework for range queries involving
palindromes on strings, including counting distinct palindromic substrings, and
finding the longest palindromic substring, shortest unique palindromic
substring and shortest absent palindrome of any substring. In particular,
offline queries only use linear space. Apart from range queries, we enumerate
palindromic rich strings with a given word in linear time on the length of the
given word.
</summary>
    <author>
      <name>Qisheng Wang</name>
    </author>
    <author>
      <name>Ming Yang</name>
    </author>
    <author>
      <name>Xinrui Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor corrections and modifications. 67 pages, 3 tables, 15
  algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.02292v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.02292v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.01475">
    <id>http://arxiv.org/abs/2210.01475v1</id>
    <updated>2022-10-04T08:57:50Z</updated>
    <published>2022-10-04T08:57:50Z</published>
    <title>Designing a parallel suffix sort</title>
    <summary>  Suffix sort plays a critical role in various computational algorithms
including genomics as well as in frequently used day to day software
applications. The sorting algorithm becomes tricky when we have lot of repeated
characters in the string for a given radix. Various innovative implementations
are available in this area e.g., Manber Myers. We present here an analysis that
uses a concept around generalized polynomial factorization to sort these
suffixes. The initial generation of these substring specific polynomial can be
efficiently done using parallel threads and shared memory. The set of distinct
factors and their order are known beforehand, and this helps us to sort the
polynomials (equivalent of strings) accordingly.
</summary>
    <author>
      <name>Kunal Chowdhury</name>
    </author>
    <link href="http://arxiv.org/abs/2210.01475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.01475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.00342">
    <id>http://arxiv.org/abs/2210.00342v1</id>
    <updated>2022-10-01T18:56:08Z</updated>
    <published>2022-10-01T18:56:08Z</published>
    <title>Finding binary words with a given number of subsequences</title>
    <summary>  We relate binary words with a given number of subsequences to continued
fractions of rational numbers with a given denominator. We deduce that there
are binary strings of length $O(\log n \log \log n)$ with exactly $n$
subsequences; this can be improved to $O(\log n)$ under assumption of Zaremba's
conjecture.
</summary>
    <author>
      <name>Radosław Żak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2022.03.032</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2022.03.032" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">\.Zak R., Finding binary words with a given number of
  subsequences, Theoretical Computer Science 919 (2022), pp. 75-79</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2210.00342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.00342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.12301">
    <id>http://arxiv.org/abs/2209.12301v1</id>
    <updated>2022-09-25T19:14:28Z</updated>
    <published>2022-09-25T19:14:28Z</published>
    <title>Constant-delay enumeration for SLP-compressed documents</title>
    <summary>  We study the problem of enumerating results from a query over a compressed
document. The model we use for compression are straight-line programs (SLPs),
which are defined by a context-free grammar that produces a single string. For
our queries we use a model called Annotated Automata, an extension of regular
automata that allows annotations on letters. This model extends the notion of
Regular Spanners as it allows arbitrarily long outputs. Our main result is an
algorithm which evaluates such a query by enumerating all results with
output-linear delay after a preprocessing phase which takes linear time on the
size of the SLP, and cubic time over the size of the automaton. This is an
improvement over Schmid and Schweikardt's result, which, with the same
preprocessing time, enumerates with a delay which is logarithmic on the size of
the uncompressed document. We achieve this through a persistent data structure
named Enumerable Compact Sets with Shifts which guarantees output-linear delay
under certain restrictions. These results imply constant-delay enumeration
algorithms in the context of regular spanners. Further, we use an extension of
annotated automata which utilizes succinctly encoded annotations to save an
exponential factor from previous results that dealt with constant-delay
enumeration over vset automata. Lastly, we extend our results in the same
fashion Schmid and Schweikardt did to allow complex document editing while
maintaining the constant-delay guarantee.
</summary>
    <author>
      <name>Martín Muñoz</name>
    </author>
    <author>
      <name>Cristian Riveros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.12301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.12301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.05460">
    <id>http://arxiv.org/abs/2211.05460v1</id>
    <updated>2022-11-10T10:11:44Z</updated>
    <published>2022-11-10T10:11:44Z</published>
    <title>Polyominoes and graphs built from Fibonacci words</title>
    <summary>  We introduce the $k$-bonacci polyominoes, a new family of polyominoes
associated with the binary words avoiding $k$ consecutive $1$'s, also called
generalized $k$-bonacci words. The polyominoes are very entrancing objects,
considered in combinatorics and computer science. The study of polyominoes
generates a rich source of combinatorial ideas. In this paper we study some
properties of $k$-bonacci polyominoes. Specifically, we determine their
recursive structure and, using this structure, we enumerate them according to
their area, semiperimeter, and length of the corresponding words. We also
introduce the $k$-bonacci graphs, then we obtain the generating functions for
the total number of vertices and edges, the distribution of the degrees, and
the total number of $k$-bonacci graphs that have a Hamiltonian cycle.
</summary>
    <author>
      <name>Sergey Kirgizov</name>
    </author>
    <author>
      <name>José Luis Ramírez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.05460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.05460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.03161">
    <id>http://arxiv.org/abs/2211.03161v1</id>
    <updated>2022-11-06T16:05:29Z</updated>
    <published>2022-11-06T16:05:29Z</published>
    <title>4D Range Reporting in the Pointer Machine Model in Almost-Optimal Time</title>
    <summary>  In the orthogonal range reporting problem we must pre-process a set $P$ of
multi-dimensional points, so that for any axis-parallel query rectangle $q$ all
points from $q\cap P$ can be reported efficiently. In this paper we study the
query complexity of multi-dimensional orthogonal range reporting in the pointer
machine model. We present a data structure that answers four-dimensional
orthogonal range reporting queries in almost-optimal time $O(\log n\log\log n +
k)$ and uses $O(n\log^4 n)$ space, where $n$ is the number of points in $P$ and
$k$ is the number of points in $q\cap P$ . This is the first data structure
with nearly-linear space usage that achieves almost-optimal query time in 4d.
This result can be immediately generalized to $d\ge 4$ dimensions: we show that
there is a data structure supporting $d$-dimensional range reporting queries in
time $O(\log^{d-3} n\log\log n+k)$ for any constant $d\ge 4$.
</summary>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Saladi Rahul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in SODA'23</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.03161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.03161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.03995">
    <id>http://arxiv.org/abs/2211.03995v2</id>
    <updated>2022-11-10T04:37:24Z</updated>
    <published>2022-11-08T04:24:53Z</published>
    <title>Computing palindromes on a trie in linear time</title>
    <summary>  A trie $\mathcal{T}$ is a rooted tree such that each edge is labeled by a
single character from the alphabet, and the labels of out-going edges from the
same node are mutually distinct. Given a trie $\mathcal{T}$ with $n$ edges, we
show how to compute all distinct palindromes and all maximal palindromes on
$\mathcal{T}$ in $O(n)$ time, in the case of integer alphabets of size
polynomial in $n$. This improves the state-of-the-art $O(n \log h)$-time
algorithms by Funakoshi et al. [PCS 2019], where $h$ is the height of
$\mathcal{T}$. Using our new algorithms, the eertree with suffix links for a
given trie $\mathcal{T}$ can readily be obtained in $O(n)$ time. Further, our
trie-based $O(n)$-space data structure allows us to report all distinct
palindromes and maximal palindromes in a query string represented in the trie
$\mathcal{T}$, in output optimal time. This is an improvement over an existing
(na\"ive) solution that precomputes and stores all distinct palindromes and
maximal palindromes for each and every string in the trie $\mathcal{T}$
separately, using a total $O(n^2)$ preprocessing time and space, and reports
them in output optimal time upon query.
</summary>
    <author>
      <name>Takuya Mieno</name>
    </author>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to ISAAC 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.03995v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.03995v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.04024">
    <id>http://arxiv.org/abs/2211.04024v1</id>
    <updated>2022-11-08T05:58:34Z</updated>
    <published>2022-11-08T05:58:34Z</published>
    <title>Comparing Two Counting Methods for Estimating the Probabilities of
  Strings</title>
    <summary>  There are two methods for counting the number of occurrences of a string in
another large string. One is to count the number of places where the string is
found. The other is to determine how many pieces of string can be extracted
without overlapping. The difference between the two becomes apparent when the
string is part of a periodic pattern. This research reports that the difference
is significant in estimating the occurrence probability of a pattern.
  In this study, the strings used in the experiments are approximated from
time-series data. The task involves classifying strings by estimating the
probability or computing the information quantity. First, the frequencies of
all substrings of a string are computed. Each counting method may sometimes
produce different frequencies for an identical string. Second, the probability
of the most probable segmentation is selected. The probability of the string is
the product of all probabilities of substrings in the selected segmentation.
The classification results demonstrate that the difference in counting methods
is statistically significant, and that the method without overlapping is
better.
</summary>
    <author>
      <name>Ayaka Takamoto</name>
    </author>
    <author>
      <name>Mitsuo Yoshida</name>
    </author>
    <author>
      <name>Kyoji Umemura</name>
    </author>
    <link href="http://arxiv.org/abs/2211.04024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.04024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.01660">
    <id>http://arxiv.org/abs/2211.01660v2</id>
    <updated>2022-11-11T10:53:47Z</updated>
    <published>2022-11-03T09:03:29Z</published>
    <title>String attractors of episturmian sequences</title>
    <summary>  In this paper, we describe string attractors of all factors of episturmian
sequences and show that their size is equal to the number of distinct letters
contained in the factor.
</summary>
    <author>
      <name>Lubomíra Dvořáková</name>
    </author>
    <link href="http://arxiv.org/abs/2211.01660v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.01660v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.14719">
    <id>http://arxiv.org/abs/2210.14719v1</id>
    <updated>2022-10-22T16:41:11Z</updated>
    <published>2022-10-22T16:41:11Z</published>
    <title>The appearance function for paper-folding words</title>
    <summary>  We provide a complete characterisation of the appearance function for
paper-folding sequences for factors of any length. We make use of the software
package {\tt Walnut} to establish these results.
</summary>
    <author>
      <name>Rob Burns</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.14719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.14719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11B85" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.14762">
    <id>http://arxiv.org/abs/2210.14762v1</id>
    <updated>2022-10-20T07:50:43Z</updated>
    <published>2022-10-20T07:50:43Z</published>
    <title>On word-representability of simplified de Bruijn graphs</title>
    <summary>  A graph $G=(V,E)$ is word-representable if there exists a word $w$ over the
alphabet $V$ such that letters $x$ and $y$ alternate in $w$ if and only if
$xy\in E$. Word-representable graphs generalize several important classes of
graphs such as $3$-colorable graphs, circle graphs, and comparability graphs.
There is a long line of research in the literature dedicated to
word-representable graphs. In this paper, we study word-representability of
simplified de Bruijn graphs. The simplified de Bruijn graph $S(n,k)$ is a
simple graph obtained from the de Bruijn graph $B(n,k)$ by removing
orientations and loops and replacing multiple edges between a pair of vertices
by a single edge. De Bruijn graphs are a key object in combinatorics on words
that found numerous applications, in particular, in genome assembly. We show
that binary simplified de Bruijn graphs (i.e.\ $S(n,2)$) are word-representable
for any $n\geq 1$, while $S(2,k)$ and $S(3,k)$ are non-word-representable for
$k\geq 3$. We conjecture that all simplified de Bruijn graphs $S(n,k)$ are
non-word-rerpesentable for $n\geq 4$ and $k\geq 3$.
</summary>
    <author>
      <name>Anthony V. Petyuk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:2110.05405 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.14762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.14762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.13097">
    <id>http://arxiv.org/abs/2210.13097v1</id>
    <updated>2022-10-24T10:29:49Z</updated>
    <published>2022-10-24T10:29:49Z</published>
    <title>Locality-Preserving Minimal Perfect Hashing of k-mers</title>
    <summary>  Minimal perfect hashing is the problem of mapping a static set of $n$
distinct keys into the address space $\{1,\ldots,n\}$ bijectively. It is
well-known that $n\log_2 e$ bits are necessary to specify a minimal perfect
hash function $f$, when no additional knowledge of the input keys is to be
used. However, it is often the case in practice that the input keys have
intrinsic relationships that we can exploit to lower the bit complexity of $f$.
For example, consider a string and the set of all its distinct sub-strings of
length $k$ - the so-called $k$-mers of the string. Two consecutive $k$-mers in
the string have a strong intrinsic relationship in that they share an overlap
of $k-1$ symbols. Hence, it seems intuitively possible to beat the classic
$\log_2 e$ bits/key barrier in this case. Moreover, we would like $f$ to map
consecutive $k$-mers to consecutive addresses, as to preserve as much as
possible the relationships between the keys also in the co-domain
$\{1,\ldots,n\}$. This is a useful feature in practice as it guarantees a
certain degree of locality of reference for $f$, resulting in a better
evaluation time when querying consecutive $k$-mers from a string. Motivated by
these premises, we initiate the study of a new type of locality-preserving
minimal perfect hash functions designed for $k$-mers extracted consecutively
from a string (or collections of strings). We show a theoretic lower bound on
the bit complexity of any $(1-\varepsilon)$-locality-preserving MPHF, for a
parameter $0 &lt; \varepsilon &lt; 1$. The complexity is lower than $n\log_2 e$ bits
for sufficiently small $\varepsilon$. We propose a construction that approaches
the theoretic minimum space for growing $k$ and present a practical
implementation of the method.
</summary>
    <author>
      <name>Giulio Ermanno Pibiri</name>
    </author>
    <author>
      <name>Yoshihiro Shibuya</name>
    </author>
    <author>
      <name>Antoine Limasset</name>
    </author>
    <link href="http://arxiv.org/abs/2210.13097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.13097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.12859">
    <id>http://arxiv.org/abs/2210.12859v2</id>
    <updated>2022-11-02T18:04:27Z</updated>
    <published>2022-10-23T21:23:59Z</published>
    <title>A Stack-Free Traversal Algorithm for Left-Balanced k-d Trees</title>
    <summary>  We present an algorithm that allows for find-closest-point and kNN-style
traversals of left-balanced k-d trees, without the need for either recursion or
software-managed stacks; instead using only current and last previously
traversed node to compute which node to traverse next.
</summary>
    <author>
      <name>Ingo Wald</name>
    </author>
    <link href="http://arxiv.org/abs/2210.12859v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.12859v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2210.11918">
    <id>http://arxiv.org/abs/2210.11918v2</id>
    <updated>2023-02-06T09:08:42Z</updated>
    <published>2022-10-21T12:41:17Z</published>
    <title>Splay Top Trees</title>
    <summary>  The top tree data structure is an important and fundamental tool in dynamic
graph algorithms. Top trees have existed for decades, and today serve as an
ingredient in many state-of-the-art algorithms for dynamic graphs. In this
work, we give a new direct proof of the existence of top trees, facilitating
simpler and more direct implementations of top trees, based on ideas from splay
trees. This result hinges on new insights into the structure of top trees, and
in particular the structure of each root path in a top tree.
</summary>
    <author>
      <name>Jacob Holm</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Copenhagen</arxiv:affiliation>
    </author>
    <author>
      <name>Eva Rotenberg</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technical University of Denmark</arxiv:affiliation>
    </author>
    <author>
      <name>Alice Ryhl</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technical University of Denmark</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/1.9781611977585.ch28</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/1.9781611977585.ch28" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 6 figures, published at SOSA'23, license information
  updated</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Symposium on Simplicity in Algorithms (SOSA), pp. 305-331.
  Society for Industrial and Applied Mathematics, 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2210.11918v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.11918v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.12496">
    <id>http://arxiv.org/abs/2211.12496v1</id>
    <updated>2022-11-22T18:59:24Z</updated>
    <published>2022-11-22T18:59:24Z</published>
    <title>An Algorithmic Bridge Between Hamming and Levenshtein Distances</title>
    <summary>  The edit distance between strings classically assigns unit cost to every
character insertion, deletion, and substitution, whereas the Hamming distance
only allows substitutions. In many real-life scenarios, insertions and
deletions (abbreviated indels) appear frequently but significantly less so than
substitutions. To model this, we consider substitutions being cheaper than
indels, with cost $1/a$ for a parameter $a\ge 1$. This basic variant, denoted
$ED_a$, bridges classical edit distance ($a=1$) with Hamming distance
($a\to\infty$), leading to interesting algorithmic challenges: Does the time
complexity of computing $ED_a$ interpolate between that of Hamming distance
(linear time) and edit distance (quadratic time)? What about approximating
$ED_a$?
  We first present a simple deterministic exact algorithm for $ED_a$ and
further prove that it is near-optimal assuming the Orthogonal Vectors
Conjecture. Our main result is a randomized algorithm computing a
$(1+\epsilon)$-approximation of $ED_a(X,Y)$, given strings $X,Y$ of total
length $n$ and a bound $k\ge ED_a(X,Y)$. For simplicity, let us focus on $k\ge
1$ and a constant $\epsilon > 0$; then, our algorithm takes $\tilde{O}(n/a +
ak^3)$ time. Unless $a=\tilde{O}(1)$ and for small enough $k$, this running
time is sublinear in $n$. We also consider a very natural version that asks to
find a $(k_I, k_S)$-alignment -- an alignment with at most $k_I$ indels and
$k_S$ substitutions. In this setting, we give an exact algorithm and, more
importantly, an $\tilde{O}(nk_I/k_S + k_S\cdot k_I^3)$-time
$(1,1+\epsilon)$-bicriteria approximation algorithm. The latter solution is
based on the techniques we develop for $ED_a$ for $a=\Theta(k_S / k_I)$. These
bounds are in stark contrast to unit-cost edit distance, where state-of-the-art
algorithms are far from achieving $(1+\epsilon)$-approximation in sublinear
time, even for a favorable choice of $k$.
</summary>
    <author>
      <name>Elazar Goldenberg</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Robert Krauthgamer</name>
    </author>
    <author>
      <name>Barna Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The full version of a paper accepted to ITCS 2023; abstract shortened
  to meet arXiv requirements</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.12496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.12496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.11856">
    <id>http://arxiv.org/abs/2211.11856v1</id>
    <updated>2022-11-21T21:21:30Z</updated>
    <published>2022-11-21T21:21:30Z</published>
    <title>String Covering: A Survey</title>
    <summary>  The study of strings is an important combinatorial field that precedes the
digital computer. Strings can be very long, trillions of letters, so it is
important to find compact representations. Here we first survey various forms
of one potential compaction methodology, the cover of a given string x,
initially proposed in a simple form in 1990, but increasingly of interest as
more sophisticated variants have been discovered. We then consider covering by
a seed; that is, a cover of a superstring of x. We conclude with many proposals
for research directions that could make significant contributions to string
processing in future.
</summary>
    <author>
      <name>Neerja Mhaskar</name>
    </author>
    <author>
      <name>W. F. Smyth</name>
    </author>
    <link href="http://arxiv.org/abs/2211.11856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.11856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.12225">
    <id>http://arxiv.org/abs/2211.12225v1</id>
    <updated>2022-11-22T12:32:38Z</updated>
    <published>2022-11-22T12:32:38Z</published>
    <title>Reversible Programming: A Case Study of Two String-Matching Algorithms</title>
    <summary>  String matching is a fundamental problem in algorithm. This study examines
the development and construction of two reversible string-matching algorithms:
a naive string-matching algorithm and the Rabin-Karp algorithm. The algorithms
are used to introduce reversible computing concepts, beginning from basic
reversible programming techniques to more advanced considerations about the
injectivization of the polynomial hash-update function employed by the
Rabin-Karp algorithm. The results are two clean input-preserving reversible
algorithms that require no additional space and have the same asymptotic time
complexity as their classic irreversible originals. This study aims to
contribute to the body of reversible algorithms and to the discipline of
reversible programming.
</summary>
    <author>
      <name>Robert Glück</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Copenhagen University</arxiv:affiliation>
    </author>
    <author>
      <name>Tetsuo Yokoyama</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Nanzan University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.373.1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.373.1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings HCVS/VPT 2022, arXiv:2211.10675</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 373, 2022, pp. 1-13</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2211.12225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.12225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3; I.1.2; F.2.2; D.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.11009">
    <id>http://arxiv.org/abs/2211.11009v1</id>
    <updated>2022-11-20T15:57:02Z</updated>
    <published>2022-11-20T15:57:02Z</published>
    <title>Optimal resizable arrays</title>
    <summary>  A \emph{resizable array} is an array that can \emph{grow} and \emph{shrink}
by the addition or removal of items from its end, or both its ends, while still
supporting constant-time \emph{access} to each item stored in the array given
its \emph{index}. Since the size of an array, i.e., the number of items in it,
varies over time, space-efficient maintenance of a resizable array requires
dynamic memory management. A standard doubling technique allows the maintenance
of an array of size~$N$ using only $O(N)$ space, with $O(1)$ amortized time, or
even $O(1)$ worst-case time, per operation. Sitarski and Brodnik et al.\
describe much better solutions that maintain a resizable array of size~$N$
using only $N+O(\sqrt{N})$ space, still with $O(1)$ time per operation. Brodnik
et al.\ give a simple proof that this is best possible.
  We distinguish between the space needed for \emph{storing} a resizable array,
and accessing its items, and the \emph{temporary} space that may be needed
while growing or shrinking the array. For every integer $r\ge 2$, we show that
$N+O(N^{1/r})$ space is sufficient for storing and accessing an array of
size~$N$, if $N+O(N^{1-1/r})$ space can be used briefly during grow and shrink
operations. Accessing an item by index takes $O(1)$ worst-case time while grow
and shrink operations take $O(r)$ amortized time. Using an exact analysis of a
\emph{growth game}, we show that for any data structure from a wide class of
data structures that uses only $N+O(N^{1/r})$ space to store the array, the
amortized cost of grow is $\Omega(r)$, even if only grow and access operations
are allowed. The time for grow and shrink operations cannot be made worst-case,
unless $r=2$.
</summary>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <author>
      <name>Uri Zwick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in SOSA 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.11009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.11009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68Q25" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; E.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.09417">
    <id>http://arxiv.org/abs/2211.09417v1</id>
    <updated>2022-11-17T08:56:02Z</updated>
    <published>2022-11-17T08:56:02Z</published>
    <title>Some Results on Digital Segments and Balanced Words</title>
    <summary>  We exhibit combinatorial results on Christoffel words and binary balanced
words that are motivated by their geometric interpretation as approximations of
digital segments. We show that for every pair $(a,b)$ of positive integers, all
the binary balanced words with $a$ zeroes and $b$ ones are good approximations
of the Euclidean segment from $(0,0)$ to $(a,b)$, in the sense that they encode
paths that are contained within the digital bar delimited by the lower and the
upper Christoffel words of slope $b/a$. We then give a closed formula for
counting the exact number of balanced words with $a$ zeroes and $b$ ones. We
also study minimal non-balanced words and prefixes of Christoffel words.
</summary>
    <author>
      <name>Alessandro De Luca</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.09417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.09417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.08157">
    <id>http://arxiv.org/abs/2211.08157v2</id>
    <updated>2023-01-18T17:32:16Z</updated>
    <published>2022-11-15T14:09:39Z</published>
    <title>Genome-on-Diet: Taming Large-Scale Genomic Analyses via Sparsified
  Genomics</title>
    <summary>  Searching for similar genomic sequences is an essential and fundamental step
in biomedical research and an overwhelming majority of genomic analyses.
State-of-the-art computational methods performing such comparisons fail to cope
with the exponential growth of genomic sequencing data. We introduce the
concept of sparsified genomics where we systematically exclude a large number
of bases from genomic sequences and enable much faster and more
memory-efficient processing of the sparsified, shorter genomic sequences, while
providing similar or even higher accuracy compared to processing non-sparsified
sequences. Sparsified genomics provides significant benefits to many genomic
analyses and has broad applicability. We show that sparsifying genomic
sequences greatly accelerates the state-of-the-art read mapper (minimap2) by
2.57-5.38x, 1.13-2.78x, and 3.52-6.28x using real Illumina, HiFi, and ONT
reads, respectively, while providing up to 2.1x smaller memory footprint, 2x
smaller index size, and more truly detected small and structural variations
compared to minimap2. Sparsifying genomic sequences makes containment search
through very large genomes and very large databases 72.7-75.88x faster and
723.3x more storage-efficient than searching through non-sparsified genomic
sequences (with CMash and KMC3). Sparsifying genomic sequences enables robust
microbiome discovery by providing 54.15-61.88x faster and 720x more
storage-efficient taxonomic profiling of metagenomic samples over the
state-of-art tool (Metalign). We design and open-source a framework called
Genome-on-Diet as an example tool for sparsified genomics, which can be freely
downloaded from https://github.com/CMU-SAFARI/Genome-on-Diet.
</summary>
    <author>
      <name>Mohammed Alser</name>
    </author>
    <author>
      <name>Julien Eudine</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <link href="http://arxiv.org/abs/2211.08157v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.08157v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.09251">
    <id>http://arxiv.org/abs/2211.09251v1</id>
    <updated>2022-11-16T22:50:40Z</updated>
    <published>2022-11-16T22:50:40Z</published>
    <title>On the Power of Learning-Augmented BSTs</title>
    <summary>  We present the first Learning-Augmented Binary Search Tree(BST) that attains
Static Optimality and Working-Set Bound given rough predictions. Following the
recent studies in algorithms with predictions and learned index structures,
Lin, Luo, and Woodruff (ICML 2022) introduced the concept of Learning-Augmented
BSTs, which aim to improve BSTs with learned advice. Unfortunately, their
construction gives only static optimality under strong assumptions on the
input.
  In this paper, we present a simple BST maintenance scheme that benefits from
learned advice. With proper predictions, the scheme achieves Static Optimality
and Working-Set Bound, respectively, which are important performance measures
for BSTs. Moreover, the scheme is robust to prediction errors and makes no
assumption on the input.
</summary>
    <author>
      <name>Jingbang Chen</name>
    </author>
    <author>
      <name>Li Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2211.09251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.09251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.07794">
    <id>http://arxiv.org/abs/2211.07794v1</id>
    <updated>2022-11-14T23:16:27Z</updated>
    <published>2022-11-14T23:16:27Z</published>
    <title>Augmented Thresholds for MONI</title>
    <summary>  MONI (Rossi et al., 2022) can store a pangenomic dataset T in small space and
later, given a pattern P, quickly find the maximal exact matches (MEMs) of P
with respect to T. In this paper we consider its one-pass version (Boucher et
al., 2021), whose query times are dominated in our experiments by longest
common extension (LCE) queries. We show how a small modification lets us avoid
most of these queries and thus significantly speeds up MONI in practice while
only slightly increasing its size.
</summary>
    <author>
      <name>César Martínez-Guardiola</name>
    </author>
    <author>
      <name>Nathaniel K. Brown</name>
    </author>
    <author>
      <name>Fernando Silva-Coira</name>
    </author>
    <author>
      <name>Dominik Köppl</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Susana Ladra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures, preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.07794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.07794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.07644">
    <id>http://arxiv.org/abs/2211.07644v1</id>
    <updated>2022-11-13T14:10:57Z</updated>
    <published>2022-11-13T14:10:57Z</published>
    <title>Bounds and Estimates on the Average Edit Distance</title>
    <summary>  The edit distance is a metric of dissimilarity between strings, widely
applied in computational biology, speech recognition, and machine learning. Let
$e_k(n)$ denote the average edit distance between random, independent strings
of $n$ characters from an alphabet of size $k$. For $k \geq 2$, it is an open
problem how to efficiently compute the exact value of $\alpha_{k}(n) =
e_k(n)/n$ as well as of $\alpha_{k} = \lim_{n \to \infty} \alpha_{k}(n)$, a
limit known to exist.
  This paper shows that $\alpha_k(n)-Q(n) \leq \alpha_k \leq \alpha_k(n)$, for
a specific $Q(n)=\Theta(\sqrt{\log n / n})$, a result which implies that
$\alpha_k$ is computable. The exact computation of $\alpha_k(n)$ is explored,
leading to an algorithm running in time $T=\mathcal{O}(n^2k\min(3^n,k^n))$, a
complexity that makes it of limited practical use.
  An analysis of statistical estimates is proposed, based on McDiarmid's
inequality, showing how $\alpha_k(n)$ can be evaluated with good accuracy, high
confidence level, and reasonable computation time, for values of $n$ say up to
a quarter million. Correspondingly, 99.9\% confidence intervals of width
approximately $10^{-2}$ are obtained for $\alpha_k$.
  Combinatorial arguments on edit scripts are exploited to analytically
characterize an efficiently computable lower bound $\beta_k^*$ to $\alpha_k$,
such that $ \lim_{k \to \infty} \beta_k^*=1$. In general, $\beta_k^* \leq
\alpha_k \leq 1-1/k$; for $k$ greater than a few dozens, computing $\beta_k^*$
is much faster than generating good statistical estimates with confidence
intervals of width $1-1/k-\beta_k^*$.
  The techniques developed in the paper yield improvements on most previously
published numerical values as well as results for alphabet sizes and string
lengths not reported before.
</summary>
    <author>
      <name>Gianfranco Bilardi</name>
    </author>
    <author>
      <name>Michele Schimd</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 1 figure, 9 tables, submitted for review</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.07644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.07644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R05 (Primary) 41A25, 68W32 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.06044">
    <id>http://arxiv.org/abs/2211.06044v1</id>
    <updated>2022-11-11T07:35:32Z</updated>
    <published>2022-11-11T07:35:32Z</published>
    <title>External-memory dictionaries with worst-case update cost</title>
    <summary>  The $B^{\epsilon}$-tree [Brodal and Fagerberg 2003] is a simple I/O-efficient
external-memory-model data structure that supports updates orders of magnitude
faster than B-tree with a query performance comparable to the B-tree: for any
positive constant $\epsilon&lt;1$ insertions and deletions take
$O(\frac{1}{B^{1-\epsilon}}\log_{B}N)$ time (rather than $O(\log_BN)$ time for
the classic B-tree), queries take $O(\log_BN)$ time and range queries returning
$k$ items take $O(\log_BN+\frac{k}{B})$ time. Although the $B^{\epsilon}$-tree
has an optimal update/query tradeoff, the runtimes are amortized. Another
structure, the write-optimized skip list, introduced by Bender et al. [PODS
2017], has the same performance as the $B^{\epsilon}$-tree but with runtimes
that are randomized rather than amortized. In this paper, we present a variant
of the $B^{\epsilon}$-tree with deterministic worst-case running times that are
identical to the original's amortized running times.
</summary>
    <author>
      <name>Rathish Das</name>
    </author>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/2211.06044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.06044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="0706.2893">
    <id>http://arxiv.org/abs/0706.2893v1</id>
    <updated>2007-06-20T14:42:45Z</updated>
    <published>2007-06-20T14:42:45Z</published>
    <title>Dualheap Sort Algorithm: An Inherently Parallel Generalization of
  Heapsort</title>
    <summary>  A generalization of the heapsort algorithm is proposed. At the expense of
about 50% more comparison and move operations for typical cases, the dualheap
sort algorithm offers several advantages over heapsort: improved cache
performance, better performance if the input happens to be already sorted, and
easier parallel implementations.
</summary>
    <author>
      <name>Greg Sepesi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0706.2893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.2893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1201.2969">
    <id>http://arxiv.org/abs/1201.2969v1</id>
    <updated>2012-01-13T23:26:27Z</updated>
    <published>2012-01-13T23:26:27Z</published>
    <title>SparseDTW: A Novel Approach to Speed up Dynamic Time Warping</title>
    <summary>  We present a new space-efficient approach, (SparseDTW), to compute the
Dynamic Time Warping (DTW) distance between two time series that always yields
the optimal result. This is in contrast to other known approaches which
typically sacrifice optimality to attain space efficiency. The main idea behind
our approach is to dynamically exploit the existence of similarity and/or
correlation between the time series. The more the similarity between the time
series the less space required to compute the DTW between them. To the best of
our knowledge, all other techniques to speedup DTW, impose apriori constraints
and do not exploit similarity characteristics that may be present in the data.
We conduct experiments and demonstrate that SparseDTW outperforms previous
approaches.
</summary>
    <author>
      <name>Ghazi Al-Naymat</name>
    </author>
    <author>
      <name>Sanjay Chawla</name>
    </author>
    <author>
      <name>Javid Taheri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Al-Naymat, G., S. Chawla, and J. Taheri, "SparseDTW: A Novel
  Approach to Speed up Dynamic Time Warping", The 2009 Australasian Data
  Mining, vol. 101, Melbourne, Australia, ACM Digital Library, pp. 117-127,
  12/2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.2969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.2969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1507.04220">
    <id>http://arxiv.org/abs/1507.04220v1</id>
    <updated>2015-07-15T13:52:08Z</updated>
    <published>2015-07-15T13:52:08Z</published>
    <title>A numerical analysis of Quicksort: How many cases are bad cases?</title>
    <summary>  We present numerical results for the probability of bad cases for Quicksort,
i.e. cases of input data for which the sorting cost considerably exceeds that
of the average. Dynamic programming was used to compute solutions of the
recurrence for the frequency distributions of comparisons. From these
solutions, probabilities of numbers of comparisons above certain thresholds
relative to the average were extracted. Computations were done for array sizes
up to n = 500 elements and for several methods to select the partitioning
element, from a simple random selection to what we call "recursive median of
three medians." We found that the probability strongly depends on the selection
method: for n = 500 and a theshold 25% above the average number of comparisons
it ranges from 2.2*10^(-3) to 3.0*10^(-23). A version of Quicksort based on the
recursive median of medians approach is proposed, for which our data suggest a
worst case time complexity of O(n^1.37).
</summary>
    <author>
      <name>Guido Hartmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.04220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1507.03823">
    <id>http://arxiv.org/abs/1507.03823v1</id>
    <updated>2015-07-14T12:36:15Z</updated>
    <published>2015-07-14T12:36:15Z</published>
    <title>A Lower Bound on Supporting Predecessor Search in $k$ sorted Arrays</title>
    <summary>  We seek to perform efficient queries for the predecessor among $n$ values
stored in $k$ sorted arrays. Evading the $\Omega(n \log k)$ lower bound from
merging $k$ arrays, we support predecessor queries in $O(\log n)$ time after
$O(n \log(\frac{k}{\log n}))$ construction time. By applying Ben-Or's
technique, we establish that this is optimal for strict predecessor queries,
i.e., every data structure supporting $O(\log n)$-time strict predecessor
queries requires $\Omega(n \log(\frac{k}{\log n}))$ construction time. Our
approach generalizes as a template for deriving similar lower bounds on the
construction time of data structures with some desired query time.
</summary>
    <author>
      <name>Carsten Grimm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work was presented at the Young Researcher Workshop on Automata,
  Languages and Programming (YR-ICALP 2015), July 5th, 2015 in Kyoto, Japan</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.03823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.03823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.16860">
    <id>http://arxiv.org/abs/2211.16860v1</id>
    <updated>2022-11-30T10:01:31Z</updated>
    <published>2022-11-30T10:01:31Z</published>
    <title>Gapped String Indexing in Subquadratic Space and Sublinear Query Time</title>
    <summary>  In Gapped String Indexing, the goal is to compactly represent a string $S$ of
length $n$ such that given queries consisting of two strings $P_1$ and $P_2$,
called patterns, and an integer interval $[\alpha, \beta]$, called gap range,
we can quickly find occurrences of $P_1$ and $P_2$ in $S$ with distance in
$[\alpha, \beta]$. Due to the many applications of this fundamental problem in
computational biology and elsewhere, there is a great body of work for
restricted or parameterised variants of the problem. However, for the general
problem statement, no improvements upon the trivial $\mathcal{O}(n)$-space
$\mathcal{O}(n)$-query time or $\Omega(n^2)$-space $\mathcal{\tilde{O}}(|P_1| +
|P_2| + \mathrm{occ})$-query time solutions were known so far. We break this
barrier obtaining interesting trade-offs with polynomially subquadratic space
and polynomially sublinear query time. In particular, we show that, for every
$0\leq \delta \leq 1$, there is a data structure for Gapped String Indexing
with either $\mathcal{\tilde{O}}(n^{2-\delta/3})$ or
$\mathcal{\tilde{O}}(n^{3-2\delta})$ space and $\mathcal{\tilde{O}}(|P_1| +
|P_2| + n^{\delta}\cdot (\mathrm{occ}+1))$ query time, where $\mathrm{occ}$ is
the number of reported occurrences. As a new fundamental tool towards obtaining
our main result, we introduce the Shifted Set Intersection problem: preprocess
a collection of sets $S_1, \ldots, S_k$ of integers such that given queries
consisting of three integers $i,j,s$, we can quickly output YES if and only if
there exist $a \in S_i$ and $b \in S_j$ with $a+s = b$. We start by showing
that the Shifted Set Intersection problem is equivalent to the indexing variant
of 3SUM (3SUM Indexing) [Golovnev et al., STOC 2020]. Via several steps of
reduction we then show that the Gapped String Indexing problem reduces to
polylogarithmically many instances of the Shifted Set Intersection problem.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Moshe Lewenstein</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Eva Rotenberg</name>
    </author>
    <author>
      <name>Teresa Anna Steiner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.16860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.16860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.16660">
    <id>http://arxiv.org/abs/2211.16660v2</id>
    <updated>2023-02-09T20:37:12Z</updated>
    <published>2022-11-30T01:11:52Z</published>
    <title>Approximating binary longest common subsequence in almost-linear time</title>
    <summary>  The Longest Common Subsequence (LCS) is a fundamental string similarity
measure, and computing the LCS of two strings is a classic algorithms question.
A textbook dynamic programming algorithm gives an exact algorithm in quadratic
time, and this is essentially best possible under plausible fine-grained
complexity assumptions, so a natural problem is to find faster approximation
algorithms. When the inputs are two binary strings, there is a simple
$\frac{1}{2}$-approximation in linear time: compute the longest common all-0s
or all-1s subsequence. It has been open whether a better approximation is
possible even in truly subquadratic time. Rubinstein and Song showed that the
answer is yes under the assumption that the two input strings have equal
lengths. We settle the question, generalizing their result to unequal length
strings, proving that, for any $\varepsilon>0$, there exists $\delta>0$ and a
$(\frac{1}{2}+\delta)$-approximation algorithm for binary LCS that runs in
$n^{1+\varepsilon}$ time. As a consequence of our result and a result of Akmal
and Vassilevska-Williams, for any $\varepsilon>0$, there exists a
$(\frac{1}{q}+\delta)$-approximation for LCS over $q$-ary strings in
$n^{1+\varepsilon}$ time.
  Our techniques build on the recent work of Guruswami, He, and Li who proved
new bounds for error-correcting codes tolerating deletion errors. They prove a
combinatorial "structure lemma" for strings which classifies them according to
their oscillation patterns. We prove and use an algorithmic generalization of
this structure lemma, which may be of independent interest.
</summary>
    <author>
      <name>Xiaoyu He</name>
    </author>
    <author>
      <name>Ray Li</name>
    </author>
    <link href="http://arxiv.org/abs/2211.16660v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.16660v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.15945">
    <id>http://arxiv.org/abs/2211.15945v1</id>
    <updated>2022-11-29T06:07:00Z</updated>
    <published>2022-11-29T06:07:00Z</published>
    <title>Quantum Speed-ups for String Synchronizing Sets, Longest Common
  Substring, and k-mismatch Matching</title>
    <summary>  Longest Common Substring (LCS) is an important text processing problem, which
has recently been investigated in the quantum query model. The decisional
version of this problem, LCS with threshold $d$, asks whether two length-$n$
input strings have a common substring of length $d$. The two extreme cases,
$d=1$ and $d=n$, correspond respectively to Element Distinctness and
Unstructured Search, two fundamental problems in quantum query complexity.
However, the intermediate case $1\ll d\ll n$ was not fully understood. We show
that the complexity of LCS with threshold $d$ smoothly interpolates between the
two extreme cases up to $n^{o(1)}$ factors: LCS with threshold $d$ has a
quantum algorithm in $n^{2/3+o(1)}/d^{1/6}$ query complexity and time
complexity, and requires at least $\Omega(n^{2/3}/d^{1/6})$ quantum query
complexity. Our result improves upon previous upper bounds $\tilde O(\min
\{n/d^{1/2}, n^{2/3}\})$ (Le Gall and Seddighin ITCS 2022, Akmal and Jin SODA
2022), and answers an open question of Akmal and Jin.
  Our main technical contribution is a quantum speed-up of the powerful String
Synchronizing Set technique introduced by Kempa and Kociumaka (STOC 2019). It
consistently samples $n/\tau^{1-o(1)}$ synchronizing positions in the string
depending on their length-$\Theta(\tau)$ contexts, and each synchronizing
position can be reported by a quantum algorithm in $\tilde O(\tau^{1/2+o(1)})$
time.
  As another application of our quantum string synchronizing set, we study the
$k$-mismatch Matching problem, which asks if the pattern has an occurrence in
the text with at most $k$ Hamming mismatches. Using a structural result of
Charalampopoulos, Kociumaka, and Wellnitz (FOCS 2020), we obtain a quantum
algorithm for $k$-mismatch matching with $k^{3/4} n^{1/2+o(1)}$ query
complexity and $\tilde O(kn^{1/2})$ time complexity.
</summary>
    <author>
      <name>Ce Jin</name>
    </author>
    <author>
      <name>Jakob Nogler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SODA 2023. Abstract shortened to meet arXiv requirements</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.15945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.14229">
    <id>http://arxiv.org/abs/2211.14229v1</id>
    <updated>2022-11-23T09:43:55Z</updated>
    <published>2022-11-23T09:43:55Z</published>
    <title>On fractal patterns in Ulam words</title>
    <summary>  We show that already a seemingly simple set of Ulam words $\unicode{x2013}$
those with two $1$'s $\unicode{x2013}$ possess an intricate intrinsic
structure. We create a logarithmic-time algorithm to determine whether any
given such word is Ulam, uncovering properties such as biperiodicity and
various parity conditions, as well as sharp bounds on the number of $0$'s
outside the two $1$'s. We also discover and prove that sets of Ulam words
indexed by the number $y$ of $0$'s between the two $1$'s have an inherent dual
hierarchical structure, determined by the arithmetic properties of $y.$ In
particular, this allows us to construct an infinite family of self-similar
fractals $\tilde{U}(y)$ indexed by the set of $2$-adic integers $y,$ containing
for example the outward Sierpinski gasket as $\tilde{U}(-1).$
</summary>
    <author>
      <name>Andrei Mandelshtam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.14229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.14229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.13434">
    <id>http://arxiv.org/abs/2211.13434v2</id>
    <updated>2022-12-03T21:42:52Z</updated>
    <published>2022-11-24T06:29:49Z</published>
    <title>A fast and simple $O (z \log n)$-space index for finding approximately
  longest common substrings</title>
    <summary>  We describe how, given a text $T [1..n]$ and a positive constant $\epsilon$,
we can build a simple $O (z \log n)$-space index, where $z$ is the number of
phrases in the LZ77 parse of $T$, such that later, given a pattern $P [1..m]$,
in $O (m \log \log z + \mathrm{polylog} (m + z))$ time and with high
probability we can find a substring of $P$ that occurs in $T$ and whose length
is at least a $(1 - \epsilon)$-fraction of the length of a longest common
substring of $P$ and $T$.
</summary>
    <author>
      <name>Nick Fagan</name>
    </author>
    <author>
      <name>Jorge Hermo González</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/2211.13434v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.13434v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2211.13254">
    <id>http://arxiv.org/abs/2211.13254v2</id>
    <updated>2022-12-03T21:24:34Z</updated>
    <published>2022-11-23T19:07:34Z</published>
    <title>Space-efficient RLZ-to-LZ77 conversion</title>
    <summary>  Consider a text $T [1..n]$ prefixed by a reference sequence $R = T
[1..\ell]$. We show how, given $R$ and the $z'$-phrase relative Lempel-Ziv
parse of $T [\ell + 1..n]$ with respect to $R$, we can build the LZ77 parse of
$T$ in $n\,\mathrm{polylog} (n)$ time and $O (\ell + z')$ total space.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <link href="http://arxiv.org/abs/2211.13254v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.13254v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2203.17139">
    <id>http://arxiv.org/abs/2203.17139v3</id>
    <updated>2022-10-25T17:49:28Z</updated>
    <published>2022-03-31T16:02:18Z</published>
    <title>Prefix Filter: Practically and Theoretically Better Than Bloom</title>
    <summary>  Many applications of approximate membership query data structures, or
filters, require only an incremental filter that supports insertions but not
deletions. However, the design space of incremental filters is missing a "sweet
spot" filter that combines space efficiency, fast queries, and fast insertions.
Incremental filters, such as the Bloom and blocked Bloom filter, are not space
efficient. Dynamic filters (i.e., supporting deletions), such as the cuckoo or
vector quotient filter, are space efficient but do not exhibit consistently
fast insertions and queries.
  In this paper, we propose the prefix filter, an incremental filter that
addresses the above challenge: (1) its space (in bits) is similar to
state-of-the-art dynamic filters; (2) query throughput is high and is
comparable to that of the cuckoo filter; and (3) insert throughput is high with
overall build times faster than those of the vector quotient filter and cuckoo
filter by $1.39\times$-$1.46\times$ and $3.2\times$-$3.5\times$, respectively.
We present a rigorous analysis of the prefix filter that holds also for
practical set sizes (i.e., $n=2^{25}$). The analysis deals with the probability
of failure, false positive rate, and probability that an operation requires
accessing more than a single cache line.
</summary>
    <author>
      <name>Tomer Even</name>
    </author>
    <author>
      <name>Guy Even</name>
    </author>
    <author>
      <name>Adam Morrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of VLDB'22 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.17139v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.17139v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2203.02763">
    <id>http://arxiv.org/abs/2203.02763v3</id>
    <updated>2022-09-12T16:32:32Z</updated>
    <published>2022-03-05T14:55:12Z</published>
    <title>Online List Labeling: Breaking the $\log^2n$ Barrier</title>
    <summary>  The online list labeling problem is an algorithmic primitive with a large
literature of upper bounds, lower bounds, and applications. The goal is to
store a dynamically-changing set of $n$ items in an array of $m$ slots, while
maintaining the invariant that the items appear in sorted order, and while
minimizing the relabeling cost, defined to be the number of items that are
moved per insertion/deletion.
  For the linear regime, where $m = (1 + \Theta(1)) n$, an upper bound of
$O(\log^2 n)$ on the relabeling cost has been known since 1981. A lower bound
of $\Omega(\log^2 n)$ is known for deterministic algorithms and for so-called
smooth algorithms, but the best general lower bound remains $\Omega(\log n)$.
The central open question in the field is whether $O(\log^2 n)$ is optimal for
all algorithms.
  In this paper, we give a randomized data structure that achieves an expected
relabeling cost of $O(\log^{3/2} n)$ per operation. More generally, if $m = (1
+ \varepsilon) n$ for $\varepsilon = O(1)$, the expected relabeling cost
becomes $O(\varepsilon^{-1} \log^{3/2} n)$.
  Our solution is history independent, meaning that the state of the data
structure is independent of the order in which items are inserted/deleted. For
history-independent data structures, we also prove a matching lower bound: for
all $\epsilon$ between $1 / n^{1/3}$ and some sufficiently small positive
constant, the optimal expected cost for history-independent list-labeling
solutions is $\Theta(\varepsilon^{-1}\log^{3/2} n)$.
</summary>
    <author>
      <name>Michael A. Bender</name>
    </author>
    <author>
      <name>Alex Conway</name>
    </author>
    <author>
      <name>Martín Farach-Colton</name>
    </author>
    <author>
      <name>Hanna Komlós</name>
    </author>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <author>
      <name>Nicole Wein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version for FOCS 2022 camera ready</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.02763v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.02763v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2203.00164">
    <id>http://arxiv.org/abs/2203.00164v1</id>
    <updated>2022-03-01T00:41:00Z</updated>
    <published>2022-03-01T00:41:00Z</published>
    <title>Quantum jumbled pattern matching</title>
    <summary>  Let $S_1, S_2 \in \Sigma^*$ strings, we say that $S_1$ {\em jumble match}
$S_2$ if they are permutations of each other. Given a text $T$ of size $N$ and
a string $S \in \Sigma^*$, the problem of \emph{Jumbled Pattern Matching} (JPM)
is to determine all substrings in $T$ jumbled matching $S$. In classical
computing, a widespread conjecture is that JPM requires
$\Omega(N^{2-\epsilon})$ preprocessing time and space for $O(1)$ query time, or
$\Omega(N^{1-\delta})$ query time in the online version, with $\epsilon, \delta
>0$. In this paper, we present a quantum algorithm for the online JPM in
$O(\sqrt{N})$ time.
</summary>
    <author>
      <name>Julio Juárez-Xochitemol</name>
    </author>
    <author>
      <name>Edgar Chávez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">six pages, three figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.00164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.00164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2202.08066">
    <id>http://arxiv.org/abs/2202.08066v1</id>
    <updated>2022-02-16T13:48:09Z</updated>
    <published>2022-02-16T13:48:09Z</published>
    <title>Almost-Optimal Sublinear-Time Edit Distance in the Low Distance Regime</title>
    <summary>  We revisit the task of computing the edit distance in sublinear time. In the
$(k,K)$-gap edit distance problem the task is to distinguish whether the edit
distance of two strings is at most $k$ or at least $K$. It has been established
by Goldenberg, Krauthgamer and Saha (FOCS '19), with improvements by Kociumaka
and Saha (FOCS '20), that the $(k,k^2)$-gap problem can be solved in time
$\widetilde O(n/k+\operatorname{poly}(k))$. One of the most natural questions
in this line of research is whether the $(k,k^2)$-gap is best-possible for the
running time $\widetilde O(n/k+\operatorname{poly}(k))$.
  In this work we answer this question by significantly improving the gap.
Specifically, we show that in time $O(n/k+\operatorname{poly}(k))$ we can even
solve the $(k,k^{1+o(1)})$-gap problem. This is the first algorithm that breaks
the $(k,k^2)$-gap in this running time. Our algorithm is almost optimal in the
following sense: In the low distance regime ($k\le n^{0.19}$) our running time
becomes $O(n/k)$, which matches a known $n/k^{1+o(1)}$ lower bound for the
$(k,k^{1+o(1)})$-gap problem up to lower order factors.
  Our result also reveals a surprising similarity of Hamming distance and edit
distance in the low distance regime: For both, the $(k,k^{1+o(1)})$-gap problem
has time complexity $n/k^{1\pm o(1)}$ for small $k$.
  In contrast to previous work, which employed a subsampled variant of the
Landau-Vishkin algorithm, we instead build upon the algorithm of Andoni,
Krauthgamer and Onak (FOCS '10). We first simplify their approach and then show
how to to effectively prune their computation tree in order to obtain a
sublinear-time algorithm in the given time bound. Towards that, we use a
variety of structural insights on the (local and global) patterns that can
emerge during this process and design appropriate property testers to
effectively detect these patterns.
</summary>
    <author>
      <name>Karl Bringmann</name>
    </author>
    <author>
      <name>Alejandro Cassis</name>
    </author>
    <author>
      <name>Nick Fischer</name>
    </author>
    <author>
      <name>Vasileios Nakos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at STOC'22. Abstract shortened to fit arXiv requirements</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.08066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.08066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2202.04185">
    <id>http://arxiv.org/abs/2202.04185v1</id>
    <updated>2022-02-08T22:58:10Z</updated>
    <published>2022-02-08T22:58:10Z</published>
    <title>OSM-tree: A Sortedness-Aware Index</title>
    <summary>  Indexes facilitate efficient querying when the selection predicate is on an
indexed key. As a result, when loading data, if we anticipate future selective
(point or range) queries, we typically maintain an index that is gradually
populated as new data is ingested. In that respect, indexing can be perceived
as the process of adding structure to an incoming, otherwise unsorted, data
collection. The process of adding structure comes at a cost, as instead of
simply appending incoming data, every new entry is inserted into the index. If
the data ingestion order matches the indexed attribute order, the ingestion
cost is entirely redundant and can be avoided (e.g., via bulk loading in a
B+-tree). However, state-of-the-art index designs do not benefit when data is
ingested in an order that is close to being sorted but not fully sorted. In
this paper, we study how indexes can benefit from partial data sortedness or
near-sortedness, and we propose an ensemble of techniques that combine bulk
loading, index appends, variable node fill/split factor, and buffering, to
optimize the ingestion cost of a tree index in presence of partial data
sortedness. We further augment the proposed design with necessary metadata
structures to ensure competitive read performance. We apply the proposed design
paradigm on a state-of-the-art B+-tree, and we propose the Ordered Sort-Merge
tree (OSM-tree). OSM-tree outperforms the state of the art by up to 8.8x in
ingestion performance in the presence of sortedness, while falling back to a
B+-tree's ingestion performance when data is scrambled. OSM-tree offers
competitive query performance, leading to performance benefits between 28% and
5x for mixed read/write workloads.
</summary>
    <author>
      <name>Aneesh Raman</name>
    </author>
    <author>
      <name>Subhadeep Sarkar</name>
    </author>
    <author>
      <name>Matthaios Olma</name>
    </author>
    <author>
      <name>Manos Athanassoulis</name>
    </author>
    <link href="http://arxiv.org/abs/2202.04185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.04185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.08820">
    <id>http://arxiv.org/abs/1602.08820v1</id>
    <updated>2016-02-29T04:38:50Z</updated>
    <published>2016-02-29T04:38:50Z</published>
    <title>Compressing Graphs and Indexes with Recursive Graph Bisection</title>
    <summary>  Graph reordering is a powerful technique to increase the locality of the
representations of graphs, which can be helpful in several applications. We
study how the technique can be used to improve compression of graphs and
inverted indexes.
  We extend the recent theoretical model of Chierichetti et al. (KDD 2009) for
graph compression, and show how it can be employed for compression-friendly
reordering of social networks and web graphs and for assigning document
identifiers in inverted indexes. We design and implement a novel theoretically
sound reordering algorithm that is based on recursive graph bisection.
  Our experiments show a significant improvement of the compression rate of
graph and indexes over existing heuristics. The new method is relatively simple
and allows efficient parallel and distributed implementations, which is
demonstrated on graphs with billions of vertices and hundreds of billions of
edges.
</summary>
    <author>
      <name>Laxman Dhulipala</name>
    </author>
    <author>
      <name>Igor Kabiljo</name>
    </author>
    <author>
      <name>Brian Karrer</name>
    </author>
    <author>
      <name>Giuseppe Ottaviano</name>
    </author>
    <author>
      <name>Sergey Pupyrev</name>
    </author>
    <author>
      <name>Alon Shalita</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2939672.2939862</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2939672.2939862" rel="related"/>
    <link href="http://arxiv.org/abs/1602.08820v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08820v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.08829">
    <id>http://arxiv.org/abs/1602.08829v1</id>
    <updated>2016-02-29T05:40:31Z</updated>
    <published>2016-02-29T05:40:31Z</published>
    <title>Access Time Tradeoffs in Archive Compression</title>
    <summary>  Web archives, query and proxy logs, and so on, can all be very large and
highly repetitive; and are accessed only sporadically and partially, rather
than continually and holistically. This type of data is ideal for
compression-based archiving, provided that random-access to small fragments of
the original data can be achieved without needing to decompress everything. The
recent RLZ (relative Lempel Ziv) compression approach uses a semi-static model
extracted from the text to be compressed, together with a greedy factorization
of the whole text encoded using static integer codes. Here we demonstrate more
precisely than before the scenarios in which RLZ excels. We contrast RLZ with
alternatives based on block-based adaptive methods, including approaches that
"prime" the encoding for each block, and measure a range of implementation
options using both hard-disk (HDD) and solid-state disk (SSD) drives. For HDD,
the dominant factor affecting access speed is the compression rate achieved,
even when this involves larger dictionaries and larger blocks. When the data is
on SSD the same effects are present, but not as markedly, and more complex
trade-offs apply.
</summary>
    <author>
      <name>Matthias Petri</name>
    </author>
    <author>
      <name>Alistair Moffat</name>
    </author>
    <author>
      <name>P. C. Nagesh</name>
    </author>
    <author>
      <name>Anthony Wirth</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-28940-3_2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-28940-3_2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Note that the final published version of this paper prepared by
  Springer/LNCS introduced errors in the publication process in Figures 1, 2,
  and 3 that are not present in this preprint. In all other regards the
  preprint and the published version are identical in their content</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Asia Information Retrieval Societies Conference (AIRS), LNCS vol.
  9460, pages 15-28, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.08829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.00023">
    <id>http://arxiv.org/abs/1602.00023v1</id>
    <updated>2016-01-29T21:58:42Z</updated>
    <published>2016-01-29T21:58:42Z</published>
    <title>Optimal Prefix Free Codes With Partial Sorting</title>
    <summary>  We describe an algorithm computing an optimal prefix free code for $n$
unsorted positive weights in time within $O(n(1+\lg \alpha))\subseteq O(n\lg
n)$, where the alternation $\alpha\in[1..n-1]$ measures the amount of sorting
required by the computation. This asymptotical complexity is within a constant
factor of the optimal in the algebraic decision tree computational model, in
the worst case over all instances of size $n$ and alternation $\alpha$. Such
results refine the state of the art complexity of $\Theta(n\lg n)$ in the worst
case over instances of size $n$ in the same computational model, a landmark in
compression and coding since 1952, by the mere combination of van Leeuwen's
algorithm to compute optimal prefix free codes from sorted weights (known since
1976), with Deferred Data Structures to partially sort a multiset depending on
the queries on it (known since 1988).
</summary>
    <author>
      <name>Jérémy Barbay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, no figures. arXiv admin note: text overlap with
  arXiv:1204.5801</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.00023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1601.08051">
    <id>http://arxiv.org/abs/1601.08051v1</id>
    <updated>2016-01-29T10:58:24Z</updated>
    <published>2016-01-29T10:58:24Z</published>
    <title>Minimal Suffix and Rotation of a Substring in Optimal Time</title>
    <summary>  For a text given in advance, the substring minimal suffix queries ask to
determine the lexicographically minimal non-empty suffix of a substring
specified by the location of its occurrence in the text. We develop a data
structure answering such queries optimally: in constant time after linear-time
preprocessing. This improves upon the results of Babenko et al. (CPM 2014),
whose trade-off solution is characterized by $\Theta(n\log n)$ product of these
time complexities. Next, we extend our queries to support concatenations of
$O(1)$ substrings, for which the construction and query time is preserved. We
apply these generalized queries to compute lexicographically minimal and
maximal rotations of a given substring in constant time after linear-time
preprocessing.
  Our data structures mainly rely on properties of Lyndon words and Lyndon
factorizations. We combine them with further algorithmic and combinatorial
tools, such as fusion trees and the notion of order isomorphism of strings.
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <link href="http://arxiv.org/abs/1601.08051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.08051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32 (Primary), 68P05, 68R15 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1801.00776">
    <id>http://arxiv.org/abs/1801.00776v4</id>
    <updated>2018-12-01T23:06:06Z</updated>
    <published>2017-12-29T21:23:58Z</published>
    <title>Sorting Real Numbers in $O(n\sqrt{\log n})$ Time and Linear Space</title>
    <summary>  We present an $O(n\sqrt{\log n})$ time and linear space algorithm for sorting
real numbers. This breaks the long time illusion that real numbers have to be
sorted by comparison sorting and take $\Omega (n\log n)$ time to be sorted.
</summary>
    <author>
      <name>Yijie Han</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed some issues in the early versions</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.00776v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00776v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W40" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2301.00754">
    <id>http://arxiv.org/abs/2301.00754v2</id>
    <updated>2023-01-17T15:48:04Z</updated>
    <published>2023-01-02T17:07:53Z</published>
    <title>Algorithms for Massive Data -- Lecture Notes</title>
    <summary>  These are the lecture notes for the course CM0622 - Algorithms for Massive
Data, Ca' Foscari University of Venice. The goal of this course is to introduce
algorithmic techniques for dealing with massive data: data so large that it
does not fit in the computer's memory. Broadly speaking, there are two main
solutions to deal with massive data: (lossless) compressed data structures and
(lossy) data sketches. These notes cover the latter topic: probabilistic
filters, sketching under various metrics, Locality Sensitive Hashing, nearest
neighbour search, algorithms on streams (pattern matching, counting).
</summary>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">refactored structure</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.00754v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.00754v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2212.09562">
    <id>http://arxiv.org/abs/2212.09562v1</id>
    <updated>2022-12-19T15:51:56Z</updated>
    <published>2022-12-19T15:51:56Z</published>
    <title>High Performance Construction of RecSplit Based Minimal Perfect Hash
  Functions</title>
    <summary>  A minimal perfect hash function (MPHF) is a bijection from a set of objects S
to the first |S| integers. It can be used as a building block in databases and
data compression. RecSplit [Esposito et al., ALENEX20] is currently the most
space efficient practical minimal perfect hash function. Its main building
blocks are splittings and bijections. Using a tree-like data structure,
RecSplit first splits the input set into small sets of constant size l and then
computes a bijection on each leaf. Both splittings and bijections heavily rely
on trying multiple hash functions in a brute-force way. We greatly improve the
construction time of RecSplit using two orthogonal approaches. On the one hand,
we explore the trade-off between (exponential time) brute force and more
informed (polynomial time) search heuristics. Rotation fitting hashes the
objects in each leaf to two sets and tries to combine them to a bijection by
cyclically shifting one set to fill the holes in the other. ShockHash
constructs a small cuckoo hash table in each leaf, which is overloaded to hold
more objects than the asymptotic maximum. On the other hand, we harness
parallelism on the level of bits, vectors, cores, and GPUs. In combination, the
resulting improvements yield speedups up to 241 on a CPU and up to 2072 using a
GPU. The original RecSplit implementation needs 19 minutes to construct an MPHF
for 1 Million objects with 1.56 bits per object. On the GPU, we achieve the
same space usage in 1.5 seconds. Given that the speedups are larger than the
increase in energy consumption, our implementation is more energy efficient
than the original implementation. As a result, our improved RecSplit
implementation is now the approach to perfect hashing with the fastest
construction time over a wide range of space budgets. Surprisingly, this even
holds for rather high space budgets where asymptotically faster methods are
available.
</summary>
    <author>
      <name>Dominik Bez</name>
    </author>
    <author>
      <name>Florian Kurpicz</name>
    </author>
    <author>
      <name>Hans-Peter Lehmann</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <link href="http://arxiv.org/abs/2212.09562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.09562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2212.07870">
    <id>http://arxiv.org/abs/2212.07870v2</id>
    <updated>2023-02-02T13:29:27Z</updated>
    <published>2022-12-15T14:39:53Z</published>
    <title>Parameterized Algorithms for String Matching to DAGs: Funnels and Beyond</title>
    <summary>  The problem of String Matching to Labeled Graphs (SMLG) asks to find all the
paths in a labeled graph $G = (V, E)$ whose spellings match that of an input
string $S \in \Sigma^m$. SMLG can be solved in quadratic $O(m|E|)$ time [Amir
et al., JALG], which was proven to be optimal by a recent lower bound
conditioned on SETH [Equi et al., ICALP 2019]. The lower bound states that no
strongly subquadratic time algorithm exists, even if restricted to directed
acyclic graphs (DAGs).
  In this work we present the first parameterized algorithms for SMLG in DAGs.
Our parameters capture the topological structure of $G$. All our results are
derived from a generalization of the Knuth-Morris-Pratt algorithm [Park and
Kim, CPM 1995] optimized to work in time proportional to the number of
prefix-incomparable matches.
  To obtain the parameterization in the topological structure of $G$, we first
study a special class of DAGs called funnels [Millani et al., JCO] and
generalize them to $k$-funnels and the class $ST_k$. We present several novel
characterizations and algorithmic contributions on both funnels and their
generalizations.
</summary>
    <author>
      <name>Manuel Caceres</name>
    </author>
    <link href="http://arxiv.org/abs/2212.07870v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.07870v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2212.05098">
    <id>http://arxiv.org/abs/2212.05098v2</id>
    <updated>2022-12-15T20:35:53Z</updated>
    <published>2022-12-09T19:55:19Z</published>
    <title>Transcoding Unicode Characters with AVX-512 Instructions</title>
    <summary>  Intel includes on its recent processors a powerful set of instructions
capable of processing 512-bit registers with a single instruction (AVX-512).
Some of these instructions have no equivalent in earlier instruction sets. We
leverage these instructions to efficiently transcode strings between the most
common formats: UTF-8 and UTF-16. With our novel algorithms, we are often twice
as fast as the previous best solutions. For example, we transcode Chinese text
from UTF-8 to UTF-16 at more than 5 GiB/s using fewer than 2 CPU instructions
per character. To ensure reproducibility, we make our software freely available
as an open source library.
</summary>
    <author>
      <name>Robert Clausecker</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <link href="http://arxiv.org/abs/2212.05098v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.05098v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2212.03067">
    <id>http://arxiv.org/abs/2212.03067v1</id>
    <updated>2022-12-06T15:46:07Z</updated>
    <published>2022-12-06T15:46:07Z</published>
    <title>Pareto Optimal Compression of Genomic Dictionaries, with or without
  Random Access in Main Memory</title>
    <summary>  Motivation: A Genomic Dictionary, i.e., the set of the k-mers appearing in a
genome, is a fundamental source of genomic information: its collection is the
first step in strategic computational methods ranging from assembly to sequence
comparison and phylogeny. Unfortunately, it is costly to store. This motivates
some recent studies regarding the compression of those k-mer sets. However,
such an area does not have the maturity of genomic compression, lacking an
homogeneous and methodologically sound experimental foundation that allows to
fairly compare the relative merits of the available solutions, and that takes
into account also the rich choices of compression methods that can be used.
  Results: We provide such a foundation here, supporting it with an extensive
set of experiments that use reference datasets and a carefully selected set of
representative data compressors. Our results highlight the spectrum of
compressor choices one has in terms of Pareto Optimality of compression vs.
post-processing, this latter being important when the Dictionary needs to be
decompressed many times. In addition to the useful indications, not available
elsewhere, that this study offers to the researchers interested in storing
k-mer dictionaries in compressed form, a software system that can be readily
used to explore the Pareto Optimal solutions available r a given Dictionary is
also provided.
  Availability: The software system is available at
https://github.com/GenGrim76/Pareto-Optimal-GDC, together with user manuals and
installation instructions.
  Contact: raffaele.giancarlo@unipa.it
  Supplementary information: Additional data are available in the Supplementary
Material.
</summary>
    <author>
      <name>Raffaele Giancarlo</name>
    </author>
    <author>
      <name>Gennaro Grimaudo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Main: 13 pages, 3 tables, 3 figures; Supplementary Material: 17
  pages, 20 tables, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.03067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.03067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2212.02317">
    <id>http://arxiv.org/abs/2212.02317v1</id>
    <updated>2022-12-05T14:42:15Z</updated>
    <published>2022-12-05T14:42:15Z</published>
    <title>Word Equations in Synergy with Regular Constraints (Technical Report)</title>
    <summary>  When eating spaghetti, one should have the sauce and noodles mixed instead of
eating them separately. We argue that also in string solving, word equations
and regular constraints are better mixed together than approached separately as
in most current string solvers. We propose a fast algorithm, complete for the
fragment of chain-free constraints, in which word equations and regular
constraints are tightly integrated and exchange information, efficiently
pruning the cases generated by each other and limiting possible combinatorial
explosion. The algorithm is based on a novel language-based characterisation of
satisfiability of word equations with regular constraints. We experimentally
show that our prototype implementation is competitive with the best string
solvers and even superior in that it is the fastest on difficult examples and
has the least number of timeouts.
</summary>
    <author>
      <name>František Blahoudek</name>
    </author>
    <author>
      <name>Yu-Fang Chen</name>
    </author>
    <author>
      <name>David Chocholatý</name>
    </author>
    <author>
      <name>Vojtěch Havlena</name>
    </author>
    <author>
      <name>Lukáš Holík</name>
    </author>
    <author>
      <name>Ondřej Lengál</name>
    </author>
    <author>
      <name>Juraj Síč</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proc. of FM'23</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.02317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.02317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2212.02327">
    <id>http://arxiv.org/abs/2212.02327v3</id>
    <updated>2022-12-21T17:48:16Z</updated>
    <published>2022-12-05T14:56:45Z</published>
    <title>Space-efficient conversions from SLPs</title>
    <summary>  Given a straight-line program with $g$ rules for a text $T [1..n]$, we can
build the $z$-phrase the LZ77 parse of $T$ in $O (g)$ space and either
$n\,\mathrm{polylog} (n)$ time with high probability or $O (g z \log^2 n)$ time
deterministically. We can also build a locally consistent grammar of optimal
size $g' = O(\delta\log\frac{n}{\delta})$ in $O(n\log n)$ expected time and
$O(g+g')$ space.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Artur Jeż</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/2212.02327v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.02327v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2212.00946">
    <id>http://arxiv.org/abs/2212.00946v1</id>
    <updated>2022-12-02T03:19:44Z</updated>
    <published>2022-12-02T03:19:44Z</published>
    <title>Trie-Compressed Intersectable Sets</title>
    <summary>  We introduce space- and time-efficient algorithms and data structures for the
offline set intersection problem. We show that a sorted integer set $S
\subseteq [0{..}u)$ of $n$ elements can be represented using compressed space
while supporting $k$-way intersections in adaptive
$O(k\delta\lg{\!(u/\delta)})$ time, $\delta$ being the alternation measure
introduced by Barbay and Kenyon. Our experimental results suggest that our
approaches are competitive in practice, outperforming the most efficient
alternatives (Partitioned Elias-Fano indexes, Roaring Bitmaps, and Recursive
Universe Partitioning (RUP)) in several scenarios, offering in general relevant
space-time trade-offs.
</summary>
    <author>
      <name>Diego Arroyuelo</name>
    </author>
    <author>
      <name>Juan Pablo Castillo</name>
    </author>
    <link href="http://arxiv.org/abs/2212.00946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.00946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2212.01156">
    <id>http://arxiv.org/abs/2212.01156v1</id>
    <updated>2022-12-02T13:20:36Z</updated>
    <published>2022-12-02T13:20:36Z</published>
    <title>Computing the optimal BWT of very large string collections</title>
    <summary>  It is known that the exact form of the Burrows-Wheeler-Transform (BWT) of a
string collection depends, in most implementations, on the input order of the
strings in the collection. Reordering strings of an input collection affects
the number of equal-letter runs $r$, arguably the most important parameter of
BWT-based data structures, such as the FM-index or the $r$-index. Bentley,
Gibney, and Thankachan [ESA 2020] introduced a linear-time algorithm for
computing the permutation of the input collection which yields the minimum
number of runs of the resulting BWT.
  In this paper, we present the first tool that guarantees a
Burrows-Wheeler-Transform with minimum number of runs (optBWT), by combining i)
an algorithm that builds the BWT from a string collection (either SAIS-based
[Cenzato et al., SPIRE 2021] or BCR [Bauer et al., CPM 2011]); ii) the SAP
array data structure introduced in [Cox et al., Bioinformatics, 2012]; and iii)
the algorithm by Bentley et al.
  We present results both on real-life and simulated data, showing that the
improvement achieved in terms of $r$ with respect to the input order is
significant and the overhead created by the computation of the optimal BWT
negligible, making our tool competitive with other tools for BWT-computation in
terms of running time and space usage. In particular, on real data the optBWT
obtains up to 31 times fewer runs with only a $1.39\times$ slowdown.
  Source code is available at https://github.com/davidecenzato/optimalBWT.git.
</summary>
    <author>
      <name>Davide Cenzato</name>
    </author>
    <author>
      <name>Veronica Guerrini</name>
    </author>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.01156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.01156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2206.01784">
    <id>http://arxiv.org/abs/2206.01784v1</id>
    <updated>2022-06-03T19:08:55Z</updated>
    <published>2022-06-03T19:08:55Z</published>
    <title>Onesweep: A Faster Least Significant Digit Radix Sort for GPUs</title>
    <summary>  We present Onesweep, a least-significant digit (LSD) radix sorting algorithm
for large GPU sorting problems residing in global memory. Our parallel
algorithm employs a method of single-pass prefix sum that only requires ~2n
global read/write operations for each digit-binning iteration. This exhibits a
significant reduction in last-level memory traffic versus contemporary GPU
radix sorting implementations, where each iteration of digit binning requires
two passes through the dataset totaling ~3n global memory operations.
  On the NVIDIA A100 GPU, our approach achieves 29.4 GKey/s when sorting 256M
random 32-bit keys. Compared to CUB, the current state-of-the-art GPU LSD radix
sort, our approach provides a speedup of ~1.5x. For 32-bit keys with varied
distributions, our approach provides more consistent performance compared to
HRS, the current state-of-the-art GPU MSD radix sort, and outperforms it in
almost all cases.
</summary>
    <author>
      <name>Andy Adinets</name>
    </author>
    <author>
      <name>Duane Merrill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 11 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.01784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.01784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.01373">
    <id>http://arxiv.org/abs/2302.01373v1</id>
    <updated>2023-02-02T19:17:03Z</updated>
    <published>2023-02-02T19:17:03Z</published>
    <title>Optimal Heaviest Induced Ancestors</title>
    <summary>  We revisit the Heaviest Induced Ancestors (HIA) problem that was introduced
by Gagie, Gawrychowski, and Nekrich [CCCG 2013] and has a number of
applications in string algorithms. Let $T_1$ and $T_2$ be two rooted trees
whose nodes have weights that are increasing in all root-to-leaf paths, and
labels on the leaves, such that no two leaves of a tree have the same label. A
pair of nodes $(u, v)\in T_1 \times T_2$ is \emph{induced} if and only if there
is a label shared by leaf-descendants of $u$ and $v$. In an HIA query, given
nodes $x \in T_1$ and $y \in T_2$, the goal is to find an induced pair of nodes
$(u, v)$ of the maximum total weight such that $u$ is an ancestor of~$x$ and
$v$ is an ancestor of $y$.
  Let $n$ be the upper bound on the sizes of the two trees. It is known that no
data structure of size $\tilde{\mathcal{O}}(n)$ can answer HIA queries in
$o(\log n / \log \log n)$ time [Charalampopoulos, Gawrychowski, Pokorski; ICALP
2020]. This (unconditional) lower bound is a $\operatorname{polyloglog} n$
factor away from the query time of the fastest $\tilde{\mathcal{O}}(n)$-size
data structure known to date for the HIA problem [Abedin, Hooshmand, Ganguly,
Thankachan; Algorithmica 2022]. In this work, we resolve the query-time
complexity of the HIA problem for the near-linear space regime by presenting a
data structure that can be built in $\tilde{\mathcal{O}}(n)$ time and answers
HIA queries in $\mathcal{O}(\log n/\log\log n)$ time. As a direct corollary, we
obtain an $\tilde{\mathcal{O}}(n)$-size data structure that maintains the LCS
of a static string and a dynamic string, both of length at most $n$, in time
optimal for this space regime.
  The main ingredients of our approach are fractional cascading and the
utilization of an $\mathcal{O}(\log n/ \log\log n)$-depth tree decomposition.
</summary>
    <author>
      <name>Panagiotis Charalampopoulos</name>
    </author>
    <author>
      <name>Bartłomiej Dudek</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Karol Pokorski</name>
    </author>
    <link href="http://arxiv.org/abs/2302.01373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.01373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.00724">
    <id>http://arxiv.org/abs/2302.00724v1</id>
    <updated>2023-02-01T19:38:20Z</updated>
    <published>2023-02-01T19:38:20Z</published>
    <title>Order-Preserving Squares in Strings</title>
    <summary>  An order-preserving square in a string is a fragment of the form $uv$ where
$u\neq v$ and $u$ is order-isomorphic to $v$. We show that a string $w$ of
length $n$ over an alphabet of size $\sigma$ contains $\mathcal{O}(\sigma n)$
order-preserving squares that are distinct as words. This improves the upper
bound of $\mathcal{O}(\sigma^{2}n)$ by Kociumaka, Radoszewski, Rytter, and
Wale\'n [TCS 2016]. Further, for every $\sigma$ and $n$ we exhibit a string
with $\Omega(\sigma n)$ order-preserving squares that are distinct as words,
thus establishing that our upper bound is asymptotically tight. Finally, we
design an $\mathcal{O}(\sigma n)$ time algorithm that outputs all
order-preserving squares that occur in a given string and are distinct as
words. By our lower bound, this is optimal in the worst case.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Samah Ghazawi</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <link href="http://arxiv.org/abs/2302.00724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.00724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2301.13563">
    <id>http://arxiv.org/abs/2301.13563v1</id>
    <updated>2023-01-31T11:22:01Z</updated>
    <published>2023-01-31T11:22:01Z</published>
    <title>The Thue-Morse sequence in base 3/2</title>
    <summary>  We discuss the base 3/2 representation of the natural numbers. We prove that
the sum of digits function of the representation is a fixed point of a 2-block
substitution on an infinite alphabet, and that this implies that sum of digits
function modulo 2 of the representation is a fixed point $x_{3/2}$ of a 2-block
substitution on $\{0,1\}$. We prove that $x_{3/2}$ is mirror invariant, and
present a list of conjectured properties of $x_{3/2}$, which we think will be
hard to prove. Finally, we make a comparison with a variant of the base 3/2
representation, and give a general result on $p$-$q$-block substitutions.
</summary>
    <author>
      <name>Michel Dekking</name>
    </author>
    <link href="http://arxiv.org/abs/2301.13563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.13563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 11B85, Secondary 68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2301.10564">
    <id>http://arxiv.org/abs/2301.10564v1</id>
    <updated>2023-01-25T13:05:38Z</updated>
    <published>2023-01-25T13:05:38Z</published>
    <title>Succinct Planar Encoding with Minor Operations</title>
    <summary>  Let $G$ be an unlabeled planar and simple $n$-vertex graph. {We present a
succinct encoding of $G$ that provides induced-minor operations, i.e., edge
contraction and vertex deletions. Any sequence of such operations is processed
in $O(n)$ time.} In addition, the encoding provides constant time per element
neighborhood access and degree queries. Using optional hash tables the encoding
additionally provides constant {expected} time adjacency queries as well as an
edge-deletion operation {(thus, all minor operations are supported)} such that
any number of such edge deletions are computed in $O(n)$ {expected} time.
  Constructing the encoding requires $O(n)$ bits and $O(n)$ time. The encoding
requires $\mathcal{H}(n) + o(n)$ bits of space with $\mathcal{H}(n)$ being the
entropy of encoding a planar graph with $n$ vertices. Our data structure is
based on the recent result of Holm et al.~[ESA 2017] who presented a linear
time contraction data structure that allows to maintain parallel edges and
works for labeled graphs, but uses $O(n \log n)$ bits of space. We combine the
techniques used by Holm et al. with novel ideas and the succinct encoding of
Blelloch and Farzan~[CPM 2010] for arbitrary separable graphs. Our result
partially answers the question raised by Blelloch and Farzan if their encoding
can be modified to allow modifications of the graph.
</summary>
    <author>
      <name>Frank Kammer</name>
    </author>
    <author>
      <name>Johannes Meintrup</name>
    </author>
    <link href="http://arxiv.org/abs/2301.10564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.10564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2301.09477">
    <id>http://arxiv.org/abs/2301.09477v1</id>
    <updated>2023-01-23T15:21:12Z</updated>
    <published>2023-01-23T15:21:12Z</published>
    <title>Sliding Window String Indexing in Streams</title>
    <summary>  Given a string $S$ over an alphabet $\Sigma$, the 'string indexing problem'
is to preprocess $S$ to subsequently support efficient pattern matching
queries, i.e., given a pattern string $P$ report all the occurrences of $P$ in
$S$. In this paper we study the 'streaming sliding window string indexing
problem'. Here the string $S$ arrives as a stream, one character at a time, and
the goal is to maintain an index of the last $w$ characters, called the
'window', for a specified parameter $w$. At any point in time a pattern
matching query for a pattern $P$ may arrive, also streamed one character at a
time, and all occurrences of $P$ within the current window must be returned.
The streaming sliding window string indexing problem naturally captures
scenarios where we want to index the most recent data (i.e. the window) of a
stream while supporting efficient pattern matching.
  Our main result is a simple $O(w)$ space data structure that uses $O(\log w)$
time with high probability to process each character from both the input string
$S$ and the pattern string $P$. Reporting each occurrence from $P$ uses
additional constant time per reported occurrence. Compared to previous work in
similar scenarios this result is the first to achieve an efficient worst-case
time per character from the input stream. We also consider a delayed variant of
the problem, where a query may be answered at any point within the next
$\delta$ characters that arrive from either stream. We present an $O(w +
\delta)$ space data structure for this problem that improves the above time
bounds to $O(\log(w/\delta))$. In particular, for a delay of $\delta = \epsilon
w$ we obtain an $O(w)$ space data structure with constant time processing per
character. The key idea to achieve our result is a novel and simple
hierarchical structure of suffix trees of independent interest, inspired by the
classic log-structured merge trees.
</summary>
    <author>
      <name>Philip Bille</name>
    </author>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <author>
      <name>Inge Li Gørtz</name>
    </author>
    <author>
      <name>Max Rishøj Pedersen</name>
    </author>
    <author>
      <name>Tord Joakim Stordalen</name>
    </author>
    <link href="http://arxiv.org/abs/2301.09477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.09477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2301.05338">
    <id>http://arxiv.org/abs/2301.05338v1</id>
    <updated>2023-01-13T00:18:45Z</updated>
    <published>2023-01-13T00:18:45Z</published>
    <title>Computing matching statistics on Wheeler DFAs</title>
    <summary>  Matching statistics were introduced to solve the approximate string matching
problem, which is a recurrent subroutine in bioinformatics applications. In
2010, Ohlebusch et al. [SPIRE 2010] proposed a time and space efficient
algorithm for computing matching statistics which relies on some components of
a compressed suffix tree - notably, the longest common prefix (LCP) array. In
this paper, we show how their algorithm can be generalized from strings to
Wheeler deterministic finite automata. Most importantly, we introduce a notion
of LCP array for Wheeler automata, thus establishing a first clear step towards
extending (compressed) suffix tree functionalities to labeled graphs.
</summary>
    <author>
      <name>Alessio Conte</name>
    </author>
    <author>
      <name>Nicola Cotumaccio</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Giovanni Manzini</name>
    </author>
    <author>
      <name>Nicola Prezza</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <link href="http://arxiv.org/abs/2301.05338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2301.04295">
    <id>http://arxiv.org/abs/2301.04295v2</id>
    <updated>2023-01-12T04:42:35Z</updated>
    <published>2023-01-11T04:03:49Z</published>
    <title>Linear Time Online Algorithms for Constructing Linear-size Suffix Trie</title>
    <summary>  The suffix trees are fundamental data structures for various kinds of string
processing. The suffix tree of a text string $T$ of length $n$ has $O(n)$ nodes
and edges, and the string label of each edge is encoded by a pair of positions
in $T$. Thus, even after the tree is built, the input string $T$ needs to be
kept stored and random access to $T$ is still needed. The \emph{linear-size
suffix tries} (\emph{LSTs}), proposed by Crochemore et al. [Linear-size suffix
tries, TCS 638:171-178, 2016], are a "stand-alone" alternative to the suffix
trees. Namely, the LST of an input text string $T$ of length $n$ occupies
$O(n)$ total space, and supports pattern matching and other tasks with the same
efficiency as the suffix tree without the need to store the input text string
$T$. Crochemore et al. proposed an \emph{offline} algorithm which transforms
the suffix tree of $T$ into the LST of $T$ in $O(n \log \sigma)$ time and
$O(n)$ space, where $\sigma$ is the alphabet size. In this paper, we present
two types of \emph{online} algorithms which "directly" construct the LST, from
right to left, and from left to right, without constructing the suffix tree as
an intermediate structure. Both algorithms construct the LST incrementally when
a new symbol is read, and do not access the previously read symbols. Both of
the right-to-left construction algorithm and the left-to-right construction
algorithm work in $O(n \log \sigma)$ time and $O(n)$ space. The main feature of
our algorithms is that the input text string does not need to be stored.
</summary>
    <author>
      <name>Diptarama Hendrian</name>
    </author>
    <author>
      <name>Takuya Takagi</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <author>
      <name>Keisuke Goto</name>
    </author>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 13 figures. arXiv admin note: text overlap with
  arXiv:1901.10045</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.04295v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.04295v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2301.03074">
    <id>http://arxiv.org/abs/2301.03074v1</id>
    <updated>2023-01-08T16:59:14Z</updated>
    <published>2023-01-08T16:59:14Z</published>
    <title>SeedTree: A Dynamically Optimal and Local Self-Adjusting Tree</title>
    <summary>  We consider the fundamental problem of designing a self-adjusting tree, which
efficiently and locally adapts itself towards the demand it serves (namely
accesses to the items stored by the tree nodes), striking a balance between the
benefits of such adjustments (enabling faster access) and their costs
(reconfigurations). This problem finds applications, among others, in the
context of emerging demand-aware and reconfigurable datacenter networks and
features connections to self-adjusting data structures. Our main contribution
is SeedTree, a dynamically optimal self-adjusting tree which supports local
(i.e., greedy) routing, which is particularly attractive under highly dynamic
demands. SeedTree relies on an innovative approach which defines a set of
unique paths based on randomized item addresses, and uses a small constant
number of items per node. We complement our analytical results by showing the
benefits of SeedTree empirically, evaluating it on various synthetic and
real-world communication traces.
</summary>
    <author>
      <name>Arash Pourdamghani</name>
    </author>
    <author>
      <name>Chen Avin</name>
    </author>
    <author>
      <name>Robert Sama</name>
    </author>
    <author>
      <name>Stefan Schmid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.03074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.03074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2301.01571">
    <id>http://arxiv.org/abs/2301.01571v1</id>
    <updated>2023-01-04T12:47:35Z</updated>
    <published>2023-01-04T12:47:35Z</published>
    <title>Reconstructing words using queries on subwords or factors</title>
    <summary>  We study word reconstruction problems. Improving a previous result by P.
Fleischmann, M. Lejeune, F. Manea, D. Nowotka and M. Rigo, we prove that, for
any unknown word $w$ of length $n$ over an alphabet of cardinality $k$, $w$ can
be reconstructed from the number of occurrences as subwords (or scattered
factors) of $O(k^2\sqrt{n\log_2(n)})$ words. Two previous upper bounds obtained
by S. S. Skiena and G. Sundaram are also slightly improved: one when
considering information on the existence of subwords instead of on the numbers
of their occurrences, and, the other when considering information on the
existence of factors.
</summary>
    <author>
      <name>Gwenaël Richomme</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UPVM</arxiv:affiliation>
    </author>
    <author>
      <name>Matthieu Rosenfeld</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UM</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2301.01571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.01571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2301.00797">
    <id>http://arxiv.org/abs/2301.00797v2</id>
    <updated>2023-01-06T18:17:13Z</updated>
    <published>2023-01-02T18:42:20Z</published>
    <title>Parameterized Lower Bounds for Problems in P via Fine-Grained
  Cross-Compositions</title>
    <summary>  We provide a general framework to exclude parameterized running times of the
form $O(\ell^\beta+ n^\gamma)$ for problems that have polynomial running time
lower bounds under hypotheses from fine-grained complexity. Our framework is
based on cross-compositions from parameterized complexity. We (conditionally)
exclude running times of the form $O(\ell^{{\gamma}/{(\gamma-1)} - \epsilon} +
n^\gamma)$ for any $1&lt;\gamma&lt;2$ and $\epsilon>0$ for the following problems:
  - Longest Common Subsequence: Given two length-$n$ strings and
$\ell\in\mathbb{N}$, is there a common subsequence of length $\ell$?
  - Discrete Fr\'echet Distance: Given two lists of $n$ points each and $k\in
\mathbb{N}$, is the Fr\'echet distance of the lists at most $k$? Here $\ell$ is
the maximum number of points which one list is ahead of the other list in an
optimum traversal.
  Moreover, we exclude running times $O(\ell^{{2\gamma}/{(\gamma -1)}-\epsilon}
+ n^\gamma)$ for any $1&lt;\gamma&lt;3$ and $\epsilon>0$ for:
  - Negative Triangle: Given an edge-weighted graph with $n$ vertices, is there
a triangle whose sum of edge-weights is negative? Here $\ell$ is the order of a
maximum connected component.
  - Triangle Collection: Given a vertex-colored graph with $n$ vertices, is
there for each triple of colors a triangle whose vertices have these three
colors? Here $\ell$ is the order of a maximum connected component.
  - 2nd Shortest Path: Given an $n$-vertex edge-weighted directed graph, two
vertices $s$ and $t$, and $k \in \mathbb{N}$, has the second longest
$s$-$t$-path length at most $k$? Here $\ell$ is the directed feedback vertex
set.
  Except for 2nd Shortest Path all these running time bounds are tight, that
is, algorithms with running time $O(\ell^{{\gamma}/{(\gamma-1)}} + n^\gamma )$
for any $1 &lt; \gamma &lt; 2$ and $O(\ell^{{2\gamma}/{(\gamma -1)}} + n^\gamma)$ for
any $1 &lt; \gamma &lt; 3$, respectively, are known.
</summary>
    <author>
      <name>Klaus Heeger</name>
    </author>
    <author>
      <name>André Nichterlein</name>
    </author>
    <author>
      <name>Rolf Niedermeier</name>
    </author>
    <link href="http://arxiv.org/abs/2301.00797v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.00797v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.09239">
    <id>http://arxiv.org/abs/2302.09239v1</id>
    <updated>2023-02-18T05:25:51Z</updated>
    <published>2023-02-18T05:25:51Z</published>
    <title>Faster Wavelet Trees with Quad Vectors</title>
    <summary>  Given a text, rank and select queries return the number of occurrences of a
character up to a position (rank) or the position of a character with a given
rank (select). These queries have applications in, e.g., compression,
computational geometry, and pattern matching in the form of the backwards
search -- the backbone of many compressed full-text indices. A wavelet tree is
a compact data structure that for a text of length $n$ over an alphabet of size
$\sigma$ requires only $n\lceil\log\sigma\rceil(1+o(1))$ bits of space and can
answer rank and select queries in $\Theta(\log \sigma)$ time. Wavelet trees are
used in the applications described above.
  In this paper, we show how to improve query performance of wavelet trees by
using a 4-ary tree instead of a binary tree as basis of the wavelet tree. To
this end, we present a space-efficient rank and select data structure for quad
vectors. The 4-ary tree layout of a wavelet tree helps to halve the number of
cache misses during queries and thus reduces the query latency. Our
experimental evaluation shows that our 4-ary wavelet tree can improve the
latency of rank and select queries by a factor of $\approx 2$ compared to the
wavelet tree implementations contained in the widely used Succinct Data
Structure Library (SDSL).
</summary>
    <author>
      <name>Matteo Ceregini</name>
    </author>
    <author>
      <name>Florian Kurpicz</name>
    </author>
    <author>
      <name>Rossano Venturini</name>
    </author>
    <link href="http://arxiv.org/abs/2302.09239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.09239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.07235">
    <id>http://arxiv.org/abs/2302.07235v1</id>
    <updated>2023-02-14T18:29:09Z</updated>
    <published>2023-02-14T18:29:09Z</published>
    <title>Compressibility-Aware Quantum Algorithms on Strings</title>
    <summary>  Sublinear time quantum algorithms have been established for many fundamental
problems on strings. This work demonstrates that new, faster quantum algorithms
can be designed when the string is highly compressible. We focus on two popular
and theoretically significant compression algorithms -- the Lempel-Ziv77
algorithm (LZ77) and the Run-length-encoded Burrows-Wheeler Transform (RL-BWT),
and obtain the results below.
  We first provide a quantum algorithm running in $\tilde{O}(\sqrt{zn})$ time
for finding the LZ77 factorization of an input string $T[1..n]$ with $z$
factors. Combined with multiple existing results, this yields an
$\tilde{O}(\sqrt{rn})$ time quantum algorithm for finding the RL-BWT encoding
with $r$ BWT runs. Note that $r = \tilde{\Theta}(z)$. We complement these
results with lower bounds proving that our algorithms are optimal (up to
polylog factors).
  Next, we study the problem of compressed indexing, where we provide a
$\tilde{O}(\sqrt{rn})$ time quantum algorithm for constructing a recently
designed $\tilde{O}(r)$ space structure with equivalent capabilities as the
suffix tree. This data structure is then applied to numerous problems to obtain
sublinear time quantum algorithms when the input is highly compressible. For
example, we show that the longest common substring of two strings of total
length $n$ can be computed in $\tilde{O}(\sqrt{zn})$ time, where $z$ is the
number of factors in the LZ77 factorization of their concatenation. This beats
the best known $\tilde{O}(n^\frac{2}{3})$ time quantum algorithm when $z$ is
sufficiently small.
</summary>
    <author>
      <name>Daniel Gibney</name>
    </author>
    <author>
      <name>Sharma V. Thankachan</name>
    </author>
    <link href="http://arxiv.org/abs/2302.07235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.07235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.05682">
    <id>http://arxiv.org/abs/2302.05682v1</id>
    <updated>2023-02-11T12:38:26Z</updated>
    <published>2023-02-11T12:38:26Z</published>
    <title>A Simple Data Structure for Maintaining a Discrete Probability
  Distribution</title>
    <summary>  We revisit the following problem: given a set of indices $S = \{1, \dots,
n\}$ and weights $w_1, \dots, w_n \in \mathbb{R}_{> 0}$, provide samples from
$S$ with distribution $p(i) = w_i / W$ where $W = \sum_j w_j$ gives the proper
normalization. In the static setting, there is a simple data structure due to
Walker called Alias Table that allows for samples to be drawn in constant time.
A more challenging task is to maintain the distribution in a dynamic setting,
where elements may be added or removed, or weights may change over time; here,
existing solutions restrict the permissible weights, require rebuilding of the
associated data structure after a number of updates, or are rather complex.
  In this paper, we describe, analyze, and engineer a simple data structure for
maintaining a discrete probability distribution in the dynamic setting.
Construction of the data structure for an arbitrary distribution takes time
$O(n)$, sampling takes expected time $O(1)$, and updates of size $\Delta = O(W
/ n)$ can be processed in time $O(1)$. To evaluate the efficiency of the data
structure we conduct an experimental study. The results suggest that the
dynamic sampling performance is comparable to the static Alias Table with a
minor slowdown.
</summary>
    <author>
      <name>Daniel Allendorf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to SEA 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.05682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.05682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.06252">
    <id>http://arxiv.org/abs/2302.06252v1</id>
    <updated>2023-02-13T10:45:23Z</updated>
    <published>2023-02-13T10:45:23Z</published>
    <title>Near-Optimal Dynamic Time Warping on Run-Length Encoded Strings</title>
    <summary>  We give an $\tilde O(n^2)$ time algorithm for computing the exact Dynamic
Time Warping distance between two strings whose run-length encoding is of size
at most $n$. This matches (up to log factors) the known (conditional) lower
bound, and should be compared with the previous fastest $O(n^3)$ time exact
algorithm and the $\tilde O(n^2)$ time approximation algorithm.
</summary>
    <author>
      <name>Itai Boneh</name>
    </author>
    <author>
      <name>Shay Golan</name>
    </author>
    <author>
      <name>Shay Mozes</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <link href="http://arxiv.org/abs/2302.06252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.06252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.04640">
    <id>http://arxiv.org/abs/2302.04640v2</id>
    <updated>2023-02-10T12:20:13Z</updated>
    <published>2023-02-09T13:52:47Z</published>
    <title>Prefixes of the Fibonacci word</title>
    <summary>  Mignosi, Restivo, and Salemi (1998) proved that for all $\epsilon > 0$ there
exists an integer $N$ such that all prefixes of the Fibonacci word of length
$\geq N$ contain a suffix of exponent $\alpha^2-\epsilon$, where $\alpha =
(1+\sqrt{5})/2$ is the golden ratio. In this note we show how to prove an
explicit version of this theorem with tools from automata theory and logic.
Along the way we gain a better understanding of the repetitive structure of the
Fibonacci word.
</summary>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2302.04640v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.04640v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.01748">
    <id>http://arxiv.org/abs/2302.01748v2</id>
    <updated>2023-02-15T18:35:38Z</updated>
    <published>2023-02-03T14:10:33Z</published>
    <title>Chaining of Maximal Exact Matches in Graphs</title>
    <summary>  We study the problem of finding maximal exact matches (MEMs) between a query
string $Q$ and a labeled directed acyclic graph (DAG) $G=(V,E,\ell)$ and
subsequently co-linearly chaining these matches. We show that it suffices to
compute MEMs between node labels and $Q$ (node MEMs) to encode full MEMs. Node
MEMs can be computed in linear time and we show how to co-linearly chain them
to solve the Longest Common Subsequence (LCS) problem between $Q$ and $G$. Our
chaining algorithm is the first to consider a symmetric formulation of the
chaining problem in graphs and runs in $O(k^2|V| + |E| + kN\log N)$ time, where
$k$ is the width (minimum number of paths covering the nodes) of $G$, and $N$
is the number of node MEMs. We then consider the problem of finding MEMs when
the input graph is an indexable elastic founder graph (subclass of labeled DAGs
studied by Equi et al., Algorithmica 2022). For arbitrary input graphs, the
problem cannot be solved in truly sub-quadratic time under SETH (Equi et al.,
ICALP 2019). We show that we can report all MEMs between $Q$ and an indexable
elastic founder graph in time $O(nH^2 + m + M_\kappa)$, where $n$ is the total
length of node labels, $H$ is the maximum number of nodes in a block of the
graph, $m = |Q|$, and $M_\kappa$ is the number of MEMs of length at least
$\kappa$. The results extend to the indexing problem, where the graph is
preprocessed and a set of queries is processed as a batch.
</summary>
    <author>
      <name>Nicola Rizzo</name>
    </author>
    <author>
      <name>Manuel Cáceres</name>
    </author>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.01748v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.01748v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.04475">
    <id>http://arxiv.org/abs/2302.04475v1</id>
    <updated>2023-02-09T07:37:45Z</updated>
    <published>2023-02-09T07:37:45Z</published>
    <title>Locally consistent decomposition of strings with applications to edit
  distance sketching</title>
    <summary>  In this paper we provide a new locally consistent decomposition of strings.
Each string $x$ is decomposed into blocks that can be described by grammars of
size $\widetilde{O}(k)$ (using some amount of randomness). If we take two
strings $x$ and $y$ of edit distance at most $k$ then their block decomposition
uses the same number of grammars and the $i$-th grammar of $x$ is the same as
the $i$-th grammar of $y$ except for at most $k$ indexes $i$. The edit distance
of $x$ and $y$ equals to the sum of edit distances of pairs of blocks where $x$
and $y$ differ. Our decomposition can be used to design a sketch of size
$\widetilde{O}(k^2)$ for edit distance, and also a rolling sketch for edit
distance of size $\widetilde{O}(k^2)$. The rolling sketch allows to update the
sketched string by appending a symbol or removing a symbol from the beginning
of the string.
</summary>
    <author>
      <name>Sudatta Bhattacharya</name>
    </author>
    <author>
      <name>Michal Koucký</name>
    </author>
    <link href="http://arxiv.org/abs/2302.04475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.04475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.03690">
    <id>http://arxiv.org/abs/2302.03690v2</id>
    <updated>2023-02-13T15:19:00Z</updated>
    <published>2023-02-06T20:38:56Z</published>
    <title>Storing a Trie with Compact and Predictable Space</title>
    <summary>  This paper proposed a storing approach for trie structures, called Coordinate
Hash Trie. For a trie with $n$ nodes and an alphabet with size $m$, the
execution time of finding, inserting and deleting a child node, is $O(1)$ for
the average case, $O(m)$ for the worst case. The space used by this approach is
$O(n)$, unrelated to $m$. The constant of space consumption is predictable,
with no need for reallocation or resizing.
</summary>
    <author>
      <name>Yuxuan Dong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.03690v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.03690v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.04229">
    <id>http://arxiv.org/abs/2302.04229v1</id>
    <updated>2023-02-08T17:59:03Z</updated>
    <published>2023-02-08T17:59:03Z</published>
    <title>Weighted Edit Distance Computation: Strings, Trees and Dyck</title>
    <summary>  Given two strings of length $n$ over alphabet $\Sigma$, and an upper bound
$k$ on their edit distance, the algorithm of Myers (Algorithmica'86) and Landau
and Vishkin (JCSS'88) computes the unweighted string edit distance in
$\mathcal{O}(n+k^2)$ time. Till date, it remains the fastest algorithm for
exact edit distance computation, and it is optimal under the Strong Exponential
Hypothesis (STOC'15). Over the years, this result has inspired many
developments, including fast approximation algorithms for string edit distance
as well as similar $\tilde{\mathcal{O}}(n+$poly$(k))$-time algorithms for
generalizations to tree and Dyck edit distances. Surprisingly, all these
results hold only for unweighted instances.
  While unweighted edit distance is theoretically fundamental, almost all
real-world applications require weighted edit distance, where different weights
are assigned to different edit operations and may vary with the characters
being edited. Given a weight function $w: \Sigma \cup \{\varepsilon \}\times
\Sigma \cup \{\varepsilon \} \rightarrow \mathbb{R}_{\ge 0}$ (such that
$w(a,a)=0$ and $w(a,b)\ge 1$ for all $a,b\in \Sigma \cup \{\varepsilon\}$ with
$a\ne b$), the goal is to find an alignment that minimizes the total weight of
edits. Except for the vanilla $\mathcal{O}(n^2)$-time dynamic-programming
algorithm and its almost trivial $\mathcal{O}(nk)$-time implementation, none of
the aforementioned developments on the unweighted edit distance apply to the
weighted variant. In this paper, we propose the first
$\mathcal{O}(n+$poly$(k))$-time algorithm that computes weighted string edit
distance exactly, thus bridging a fundamental gap between our understanding of
unweighted and weighted edit distance. We then generalize this result to
weighted tree and Dyck edit distances, which lead to a deterministic algorithm
that improves upon the previous work for unweighted tree edit distance.
</summary>
    <author>
      <name>Debarati Das</name>
    </author>
    <author>
      <name>Jacob Gilbert</name>
    </author>
    <author>
      <name>MohammadTaghi Hajiaghayi</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Barna Saha</name>
    </author>
    <link href="http://arxiv.org/abs/2302.04229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.04229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.02586">
    <id>http://arxiv.org/abs/2302.02586v1</id>
    <updated>2023-02-06T06:44:14Z</updated>
    <published>2023-02-06T06:44:14Z</published>
    <title>Optimal LZ-End Parsing is Hard</title>
    <summary>  LZ-End is a variant of the well-known Lempel-Ziv parsing family such that
each phrase of the parsing has a previous occurrence, with the additional
constraint that the previous occurrence must end at the end of a previous
phrase. LZ-End was initially proposed as a greedy parsing, where each phrase is
determined greedily from left to right, as the longest factor that satisfies
the above constraint~[Kreft &amp; Navarro, 2010]. In this work, we consider an
optimal LZ-End parsing that has the minimum number of phrases in such parsings.
We show that a decision version of computing the optimal LZ-End parsing is
NP-complete by showing a reduction from the vertex cover problem. Moreover, we
give a MAX-SAT formulation for the optimal LZ-End parsing adapting an approach
for computing various NP-hard repetitiveness measures recently presented by
[Bannai et al., 2022]. We also consider the approximation ratio of the size of
greedy LZ-End parsing to the size of the optimal LZ-End parsing, and give a
lower bound of the ratio which asymptotically approaches $2$.
</summary>
    <author>
      <name>Hideo Bannai</name>
    </author>
    <author>
      <name>Mitsuru Funakoshi</name>
    </author>
    <author>
      <name>Kazuhiro Kurita</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Kazuhisa Seto</name>
    </author>
    <author>
      <name>Takeaki Uno</name>
    </author>
    <link href="http://arxiv.org/abs/2302.02586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.02586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.11532">
    <id>http://arxiv.org/abs/2302.11532v2</id>
    <updated>2023-02-26T21:12:00Z</updated>
    <published>2023-02-22T18:17:12Z</published>
    <title>Runs of Ones in Binary Strings</title>
    <summary>  We give three different computations of the total number of runs of length
$i$ in binary $n$-strings, and we discuss the connection of this problem with
the compositions of $n$.
</summary>
    <author>
      <name>Félix Balado</name>
    </author>
    <author>
      <name>Guénolé C. M. Silvestre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.11532v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.11532v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.00792">
    <id>http://arxiv.org/abs/2104.00792v1</id>
    <updated>2021-04-01T22:34:48Z</updated>
    <published>2021-04-01T22:34:48Z</published>
    <title>Scalable Hash Table for NUMA Systems</title>
    <summary>  Hash tables are used in a plethora of applications, including database
operations, DNA sequencing, string searching, and many more. As such, there are
many parallelized hash tables targeting multicore, distributed, and
accelerator-based systems. We present in this work a multi-GPU hash table
implementation that can process keys at a throughput comparable to that of
distributed hash tables. Distributed CPU hash tables have received
significantly more attention than GPU-based hash tables. We show that a single
node with multiple GPUs offers roughly the same performance as a 500-1,000-core
CPU-based cluster. Our algorithm's key component is our use of multiple
sparse-graph data structures and binning techniques to build the hash table. As
has been shown individually, these components can be written with massive
parallelism that is amenable to GPU acceleration. Since we focus on an
individual node, we also leverage communication primitives that are typically
prohibitive in distributed environments. We show that our new multi-GPU
algorithm shares many of the same features of the single GPU algorithm -- thus
we have efficient collision management capabilities and can deal with a large
number of duplicates. We evaluate our algorithm on two multi-GPU compute nodes:
1) an NVIDIA DGX2 server with 16 GPUs and 2) an IBM Power 9 Processor with 6
NVIDIA GPUs. With 32-bit keys, our implementation processes 8B keys per second,
comparable to some 500-1,000-core CPU-based clusters and 4X faster than prior
single-GPU implementations.
</summary>
    <author>
      <name>Alok Tripathy</name>
    </author>
    <author>
      <name>Oded Green</name>
    </author>
    <link href="http://arxiv.org/abs/2104.00792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.00792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2102.11489">
    <id>http://arxiv.org/abs/2102.11489v2</id>
    <updated>2021-11-07T00:51:24Z</updated>
    <published>2021-02-23T04:50:48Z</published>
    <title>Optimal Sorting Circuits for Short Keys</title>
    <summary>  A long-standing open question in the algorithms and complexity literature is
whether there exist sorting circuits of size $o(n \log n)$. A recent work by
Asharov, Lin, and Shi (SODA'21) showed that if the elements to be sorted have
short keys whose length $k = o(\log n)$, then one can indeed overcome the
$n\log n$ barrier for sorting circuits, by leveraging non-comparison-based
techniques. More specifically, Asharov et al.~showed that there exist $O(n)
\cdot \min(k, \log n)$-sized sorting circuits for $k$-bit keys, ignoring
$poly\log^*$ factors. Interestingly, the recent works by Farhadi et al.
(STOC'19) and Asharov et al. (SODA'21) also showed that the above result is
essentially optimal for every key length $k$, assuming that the famous Li-Li
network coding conjecture holds. Note also that proving any {\it unconditional}
super-linear circuit lower bound for a wide class of problems is beyond the
reach of current techniques.
  Unfortunately, the approach taken by Asharov et al.~to achieve optimality in
size somewhat crucially relies on sacrificing the depth: specifically, their
circuit is super-{\it poly}logarithmic in depth even for 1-bit keys. Asharov et
al.~phrase it as an open question how to achieve optimality both in size and
depth. In this paper, we close this important gap in our understanding. We
construct a sorting circuit of size $O(n) \cdot \min(k, \log n)$ (ignoring
$poly\log^*$ terms) and depth $O(\log n)$. To achieve this, our approach
departs significantly from the prior works. Our result can be viewed as a
generalization of the landmark result by Ajtai, Koml\'os, and Szemer\'edi
(STOC'83), simultaneously in terms of size and depth. Specifically, for $k =
o(\log n)$, we achieve asymptotical improvements in size over the AKS sorting
circuit, while preserving optimality in depth.
</summary>
    <author>
      <name>Wei-Kai Lin</name>
    </author>
    <author>
      <name>Elaine Shi</name>
    </author>
    <link href="http://arxiv.org/abs/2102.11489v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.11489v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.06201">
    <id>http://arxiv.org/abs/2101.06201v1</id>
    <updated>2021-01-15T16:33:03Z</updated>
    <published>2021-01-15T16:33:03Z</published>
    <title>Solving one variable word equations in the free group in cubic time</title>
    <summary>  A word equation with one variable in a free group is given as $U = V$, where
both $U$ and $V$ are words over the alphabet of generators of the free group
and $X, X^{-1}$, for a fixed variable $X$. An element of the free group is a
solution when substituting it for $X$ yields a true equality (interpreted in
the free group) of left- and right-hand sides. It is known that the set of all
solutions of a given word equation with one variable is a finite union of sets
of the form $\{\alpha w^i \beta \: : \: i \in \mathbb Z \}$, where $\alpha, w,
\beta$ are reduced words over the alphabet of generators, and a polynomial-time
algorithm (of a high degree) computing this set is known. We provide a cubic
time algorithm for this problem, which also shows that the set of solutions
consists of at most a quadratic number of the above-mentioned sets. The
algorithm uses only simple tools of word combinatorics and group theory and is
simple to state. Its analysis is involved and focuses on the combinatorics of
occurrences of powers of a word within a larger word.
</summary>
    <author>
      <name>Robert Ferens</name>
    </author>
    <author>
      <name>Artur Jeż</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">52 pages, accepted to STACS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.06201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.06201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2101.02003">
    <id>http://arxiv.org/abs/2101.02003v1</id>
    <updated>2021-01-06T13:18:02Z</updated>
    <published>2021-01-06T13:18:02Z</published>
    <title>Algorithms and Hardness for Multidimensional Range Updates and Queries</title>
    <summary>  Traditional orthogonal range problems allow queries over a static set of
points, each with some value. Dynamic variants allow points to be added or
removed, one at a time. To support more powerful updates, we introduce the Grid
Range class of data structure problems over integer arrays in one or more
dimensions. These problems allow range updates (such as filling all cells in a
range with a constant) and queries (such as finding the sum or maximum of
values in a range). In this work, we consider these operations along with
updates that replace each cell in a range with the minimum, maximum, or sum of
its existing value, and a constant. In one dimension, it is known that segment
trees can be leveraged to facilitate any $n$ of these operations in
$\tilde{O}(n)$ time overall. Other than a few specific cases, until now, higher
dimensional variants have been largely unexplored.
  We show that no truly subquadratic time algorithm can support certain pairs
of these updates simultaneously without falsifying several popular conjectures.
On the positive side, we show that truly subquadratic algorithms can be
obtained for variants induced by other subsets. We provide two approaches to
designing such algorithms that can be generalised to online and higher
dimensional settings. First, we give almost-tight $\tilde{O}(n^{3/2})$ time
algorithms for single-update variants where the update operation distributes
over the query operation. Second, for other variants, we provide a general
framework for reducing to instances with a special geometry. Using this, we
show that $O(m^{3/2-\epsilon})$ time algorithms for counting paths and walks of
length 2 and 3 between vertex pairs in sparse graphs imply truly subquadratic
data structures for certain variants; to this end, we give an
$\tilde{O}(m^{(4\omega-1)/(2\omega+1)}) = O(m^{1.478})$ time algorithm for
counting simple 3-paths between vertex pairs.
</summary>
    <author>
      <name>Joshua Lau</name>
    </author>
    <author>
      <name>Angus Ritossa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.ITCS.2021.37</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.ITCS.2021.37" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 3 figures, 1 table. Full version of paper to appear in ITCS
  2021. Abstract abridged for arXiv limits</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.02003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2.2; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2209.06038">
    <id>http://arxiv.org/abs/2209.06038v2</id>
    <updated>2022-09-15T21:04:32Z</updated>
    <published>2022-09-13T14:40:35Z</published>
    <title>A Hash Table Without Hash Functions, and How to Get the Most Out of Your
  Random Bits</title>
    <summary>  This paper considers the basic question of how strong of a probabilistic
guarantee can a hash table, storing $n$ $(1 + \Theta(1)) \log n$-bit key/value
pairs, offer? Past work on this question has been bottlenecked by limitations
of the known families of hash functions: The only hash tables to achieve
failure probabilities less than $1 / 2^{\polylog n}$ require access to
fully-random hash functions -- if the same hash tables are implemented using
the known explicit families of hash functions, their failure probabilities
become $1 / \poly(n)$.
  To get around these obstacles, we show how to construct a randomized data
structure that has the same guarantees as a hash table, but that \emph{avoids
the direct use of hash functions}. Building on this, we are able to construct a
hash table using $O(n)$ random bits that achieves failure probability $1 /
n^{n^{1 - \epsilon}}$ for an arbitrary positive constant $\epsilon$.
  In fact, we show that this guarantee can even be achieved by a \emph{succinct
dictionary}, that is, by a dictionary that uses space within a $1 + o(1)$
factor of the information-theoretic optimum.
  Finally we also construct a succinct hash table whose probabilistic
guarantees fall on a different extreme, offering a failure probability of $1 /
\poly(n)$ while using only $\tilde{O}(\log n)$ random bits. This latter result
matches (up to low-order terms) a guarantee previously achieved by
Dietzfelbinger et al., but with increased space efficiency and with several
surprising technical components.
</summary>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <link href="http://arxiv.org/abs/2209.06038v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.06038v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2208.10578">
    <id>http://arxiv.org/abs/2208.10578v1</id>
    <updated>2022-08-22T20:20:43Z</updated>
    <published>2022-08-22T20:20:43Z</published>
    <title>Simpler and Better Cardinality Estimators for HyperLogLog and PCSA</title>
    <summary>  \emph{Cardinality Estimation} (aka \emph{Distinct Elements}) is a classic
problem in sketching with many industrial applications. Although sketching
\emph{algorithms} are fairly simple, analyzing the cardinality
\emph{estimators} is notoriously difficult, and even today the state-of-the-art
sketches such as HyperLogLog and (compressed) \PCSA{} are not covered in
graduate level Big Data courses.
  In this paper we define a class of \emph{generalized remaining area} (\tGRA)
estimators, and observe that HyperLogLog, LogLog, and some estimators for PCSA
are merely instantiations of \tGRA{} for various integral values of $\tau$. We
then analyze the limiting relative variance of \tGRA{} estimators. It turns out
that the standard estimators for HyperLogLog and PCSA can be improved by
choosing a \emph{fractional} value of $\tau$. The resulting estimators come
\emph{very} close to the Cram\'{e}r-Rao lower bounds for HyperLogLog{} and PCSA
derived from their Fisher information. Although the Cram\'{e}r-Rao lower bound
\emph{can} be achieved with the Maximum Likelihood Estimator (MLE), the MLE is
cumbersome to compute and dynamically update. In contrast, \tGRA{} estimators
are trivial to update in constant time.
  Our presentation assumes only basic calculus and probability, not any complex
analysis~\cite{FlajoletM85,DurandF03,FlajoletFGM07}.
</summary>
    <author>
      <name>Seth Pettie</name>
    </author>
    <author>
      <name>Dingyu Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to sosa23</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.10578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.10578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.12713">
    <id>http://arxiv.org/abs/2207.12713v1</id>
    <updated>2022-07-26T08:00:15Z</updated>
    <published>2022-07-26T08:00:15Z</published>
    <title>Implementing the Comparison-Based External Sort</title>
    <summary>  In the age of big data, sorting is an indispensable operation for DBMSes and
similar systems. Having data sorted can help produce query plans with
significantly lower run times. It also can provide other benefits like having
non-blocking operators which will produce data steadily (without bursts), or
operators with reduced memory footprint.
  Sorting may be required on any step of query processing, i.e., be it source
data or intermediate results. At the same time, the data to be sorted may not
fit into main memory. In this case, an external sort operator, which writes
intermediate results to disk, should be used.
  In this paper we consider an external sort operator of the comparison-based
sort type. We discuss its implementation and describe related design decisions.
Our aim is to study the impact on performance of a data structure used on the
merge step. For this, we have experimentally evaluated three data structures
implemented inside a DBMS.
  Results have shown that it is worthwhile to make an effort to implement an
efficient data structure for run merging, even on modern commodity computers
which are usually disk-bound. Moreover, we demonstrated that using a loser tree
is a more efficient approach than both the naive approach and the heap-based
one.
</summary>
    <author>
      <name>Michael Polyntsov</name>
    </author>
    <author>
      <name>Valentin Grigorev</name>
    </author>
    <author>
      <name>Kirill Smirnov</name>
    </author>
    <author>
      <name>George Chernishev</name>
    </author>
    <link href="http://arxiv.org/abs/2207.12713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.12713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; E.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.07800">
    <id>http://arxiv.org/abs/2302.07800v1</id>
    <updated>2023-02-15T17:37:38Z</updated>
    <published>2023-02-15T17:37:38Z</published>
    <title>An Efficient B-tree Implementation for Memory-Constrained Embedded
  Systems</title>
    <summary>  Embedded devices collect and process significant amounts of data in a variety
of applications including environmental monitoring, industrial automation and
control, and other Internet of Things (IoT) applications. Storing data
efficiently is critically important, especially when the device must perform
local processing on the data. The most widely used data structure for high
performance query and insert is the B-tree. However, existing implementations
consume too much memory for small embedded devices and often rely on operating
system support. This work presents an extremely memory efficient implementation
of B-trees for embedded devices that functions on the smallest devices and does
not require an operating system. Experimental results demonstrate that the
B-tree implementation can run on devices with as little as 4 KB of RAM while
efficiently processing thousands of records.
</summary>
    <author>
      <name>Nadir Ould-Khessal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of British Columbia</arxiv:affiliation>
    </author>
    <author>
      <name>Scott Fazackerley</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of British Columbia</arxiv:affiliation>
    </author>
    <author>
      <name>Ramon Lawrence</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of British Columbia</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the 19th International Conference on Embedded Systems,
  Cyber-physical Systems, and Applications (ESCS'21). Code is available at
  https://github.com/ubco-db</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.07800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.07800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2301.03084">
    <id>http://arxiv.org/abs/2301.03084v3</id>
    <updated>2023-03-05T20:52:51Z</updated>
    <published>2023-01-08T18:39:26Z</published>
    <title>Dynamic Binary Search Trees: Improved Lower Bounds for the Greedy-Future
  Algorithm</title>
    <summary>  Binary search trees (BSTs) are one of the most basic and widely used data
structures. The best static tree for serving a sequence of queries (searches)
can be computed by dynamic programming. In contrast, when the BSTs are allowed
to be dynamic (i.e. change by rotations between searches), we still do not know
how to compute the optimal algorithm (OPT) for a given sequence. One of the
candidate algorithms whose serving cost is suspected to be optimal up-to a
(multiplicative) constant factor is known by the name Greedy Future (GF). In an
equivalent geometric way of representing queries on BSTs, GF is in fact
equivalent to another algorithm called Geometric Greedy (GG). Most of the
results on GF are obtained using the geometric model and the study of GG.
Despite this intensive recent fruitful research, the best lower bound we have
on the competitive ratio of GF is $\frac{4}{3}$. Furthermore, it has been
conjectured that the additive gap between the cost of GF and OPT is only linear
in the number of queries. In this paper we prove a lower bound of $2$ on the
competitive ratio of GF, and we prove that the additive gap between the cost of
GF and OPT can be $\Omega(m \cdot \log\log n)$ where $n$ is the number of items
in the tree and $m$ is the number of queries.
</summary>
    <author>
      <name>Yaniv Sadeh</name>
    </author>
    <author>
      <name>Haim Kaplan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.STACS.2023.53</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.STACS.2023.53" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to STACS 2023; Revised appendix A.1: simplified construction
  and analysis; Fixed an issue in Theorems 6-7</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.03084v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.03084v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2207.05495">
    <id>http://arxiv.org/abs/2207.05495v1</id>
    <updated>2022-07-03T18:35:16Z</updated>
    <published>2022-07-03T18:35:16Z</published>
    <title>An Improved Algorithm for Finding the Shortest Synchronizing Words</title>
    <summary>  A synchronizing word of a deterministic finite complete automaton is a word
whose action maps every state to a single one. Finding a shortest or a short
synchronizing word is a central computational problem in the theory of
synchronizing automata and is applied in other areas such as model-based
testing and the theory of codes. Because the problem of finding a shortest
synchronizing word is computationally hard, among \emph{exact} algorithms only
exponential ones are known. We redesign the previously fastest known exact
algorithm based on the bidirectional breadth-first search and improve it with
respect to time and space in a practical sense. We develop new algorithmic
enhancements and adapt the algorithm to multithreaded and GPU computing. Our
experiments show that the new algorithm is multiple times faster than the
previously fastest one and its advantage quickly grows with the hardness of the
problem instance. Given a modest time limit, we compute the lengths of the
shortest synchronizing words for random binary automata up to 570 states,
significantly beating the previous record. We refine the experimental
estimation of the average reset threshold of these automata. Finally, we
develop a general computational package devoted to the problem, where an
efficient and practical implementation of our algorithm is included, together
with several well-known heuristics.
</summary>
    <author>
      <name>Marek Szykuła</name>
    </author>
    <author>
      <name>Adam Zyzik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of ESA 2022 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.05495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.05495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2109.14497">
    <id>http://arxiv.org/abs/2109.14497v2</id>
    <updated>2022-01-09T06:42:56Z</updated>
    <published>2021-09-29T15:31:27Z</published>
    <title>Ruler Wrapping</title>
    <summary>  In 1985 Hopcroft, Joseph and Whitesides showed it is NP-complete to decide
whether a carpenter's ruler with segments of given positive lengths can be
folded into a line of at most a given length, such that the folded hinges
alternate between 180 degrees clockwise and 180 degrees counter-clockwise. At
the open-problem session of 33rd Canadian Conference on Computational Geometry
(CCCG '21), O'Rourke proposed a natural variation of this problem called {\em
ruler wrapping}, in which all folded hinges must be folded the same way. In
this paper we show O'Rourke's variation has an linear-time solution. We also
show how, given a sequence of positive numbers, in linear time we can partition
it into the maximum number of substrings whose totals are non-decreasing.
</summary>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Mozhgan Saeidi</name>
    </author>
    <author>
      <name>Allan Sapucaia</name>
    </author>
    <link href="http://arxiv.org/abs/2109.14497v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.14497v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2109.01892">
    <id>http://arxiv.org/abs/2109.01892v2</id>
    <updated>2022-02-05T19:44:47Z</updated>
    <published>2021-09-04T15:43:26Z</published>
    <title>Fast Succinct Retrieval and Approximate Membership using Ribbon</title>
    <summary>  A retrieval data structure for a static function $f:S\rightarrow \{0,1\}^r$
supports queries that return $f(x)$ for any $x \in S$. Retrieval data
structures can be used to implement a static approximate membership query data
structure (AMQ), i.e., a Bloom filter alternative, with false positive rate
$2^{-r}$. The information-theoretic lower bound for both tasks is $r|S|$ bits.
While succinct theoretical constructions using $(1+o(1))r|S|$ bits were known,
these could not achieve very small overheads in practice because they have an
unfavorable space--time tradeoff hidden in the asymptotic costs or because
small overheads would only be reached for physically impossible input sizes.
With bumped ribbon retrieval (BuRR), we present the first practical succinct
retrieval data structure. In an extensive experimental evaluation BuRR achieves
space overheads well below 1\,\% while being faster than most previously used
retrieval data structures (typically with space overheads at least an order of
magnitude larger) and faster than classical Bloom filters (with space overhead
$\geq 44\,\%$). This efficiency, including favorable constants, stems from a
combination of simplicity, word parallelism, and high locality. We additionally
describe homogeneous ribbon filter AMQs, which are even simpler and faster at
the price of slightly larger space overhead.
</summary>
    <author>
      <name>Peter C. Dillinger</name>
    </author>
    <author>
      <name>Lorenz Hübschle-Schneider</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <author>
      <name>Stefan Walzer</name>
    </author>
    <link href="http://arxiv.org/abs/2109.01892v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.01892v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2108.13735">
    <id>http://arxiv.org/abs/2108.13735v1</id>
    <updated>2021-08-31T10:36:04Z</updated>
    <published>2021-08-31T10:36:04Z</published>
    <title>Hierarchical Bitmap Indexing for Range and Membership Queries on
  Multidimensional Arrays</title>
    <summary>  Traditional indexing techniques commonly employed in da\-ta\-ba\-se systems
perform poorly on multidimensional array scientific data. Bitmap indices are
widely used in commercial databases for processing complex queries, due to
their effective use of bit-wise operations and space-efficiency. However,
bitmap indices apply natively to relational or linearized datasets, which is
especially notable in binned or compressed indices.
  We propose a new method for multidimensional array indexing that overcomes
the dimensionality-induced inefficiencies. The hierarchical indexing method is
based on $n$-di\-men\-sional sparse trees for dimension partitioning, with
bound number of individual, adaptively binned indices for attribute
partitioning. This indexing performs well on range involving both dimensions
and attributes, as it prunes the search space early, avoids reading entire
index data, and does at most a single index traversal. Moreover, the indexing
is easily extensible to membership queries.
  The indexing method was implemented on top of a state of the art bitmap
indexing library Fastbit. We show that the hierarchical bitmap index
outperforms conventional bitmap indexing built on auxiliary attribute for each
dimension. Furthermore, the adaptive binning significantly reduces the amount
of bins and therefore memory requirements.
</summary>
    <author>
      <name>Luboš Krčál</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Czech Technical University in Prague, Czech Republic</arxiv:affiliation>
    </author>
    <author>
      <name>Shen-Shyang Ho</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Rowan University, Glassboro, NJ, USA</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Holub</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Czech Technical University in Prague, Czech Republic</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.13735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.13735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2108.10865">
    <id>http://arxiv.org/abs/2108.10865v1</id>
    <updated>2021-08-23T17:44:32Z</updated>
    <published>2021-08-23T17:44:32Z</published>
    <title>On Specialization of a Program Model of Naive Pattern Matching in
  Strings (Extended Abstract)</title>
    <summary>  We have proved that for any pattern p the tail recursive program model of
naive pattern matching may be automatically specialized w.r.t. the pattern p to
a specialized version of the so-called KMP-algorithm, using the Higman-Kruskal
relation that controls the unfolding/folding. Given an input string, the
corresponding residual program finds the first occurrence of p in the string in
linear time on the string length. The current state of the automated program
specialization art based on unfolding/folding is too weak in order to be able
to reproduce the proof, done by hands, of the uniform property above, while it
known before that program specialization is sometimes able to produce the
KMP-algorithm for a few concrete static patterns.
</summary>
    <author>
      <name>Andrei P. Nemytykh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Program Systems Institute of Russian Academy of Sciences</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-proceedings paper presented at the 31st International Symposium
  on Logic-Based Program Synthesis and Transformation (LOPSTR 2021), Tallinn,
  Estonia, and Virtual, September 7-8, 2021 (arXiv:2107.10160)</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.10865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.10865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.4; I.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2107.14577">
    <id>http://arxiv.org/abs/2107.14577v1</id>
    <updated>2021-07-30T12:25:43Z</updated>
    <published>2021-07-30T12:25:43Z</published>
    <title>Fast direct access to variable length codes</title>
    <summary>  We consider the issue of direct access to any letter of a sequence encoded
with a variable length code and stored in the computer's memory, which is a
special case of the random access problem to compressed memory. The
characteristics according to which methods are evaluated are the access time to
one letter and the memory used. The proposed methods, with various trade-offs
between the characteristics, outperform the known ones.
</summary>
    <author>
      <name>Boris Ryabko</name>
    </author>
    <link href="http://arxiv.org/abs/2107.14577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.14577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.2; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.06525">
    <id>http://arxiv.org/abs/2106.06525v2</id>
    <updated>2021-06-17T08:28:32Z</updated>
    <published>2021-06-11T17:50:02Z</published>
    <title>ExtendedHyperLogLog: Analysis of a new Cardinality Estimator</title>
    <summary>  We discuss the problem of counting distinct elements in a stream. A stream is
usually considered as a sequence of elements that come one at a time. An exact
solution to the problem requires memory space of the size of the stream. For
many applications this solution is infeasible due to very large streams. The
solution in that case, is to use a probabilistic data structure (also called
sketch), from which we can estimate with high accuracy the cardinality of the
stream. We present a new algorithm, ExtendedHyperLogLog (EHLL), which is based
on the state-of-the-art algorithm, HyperLogLog (HLL). In order to achieve the
same accuracy as HLL, EHLL uses 16% less memory. In recent years, a martingale
approach has bean developed. In the martingale setting we receive better
accuracy at the price of not being able to merge sketches. EHLL also works in
the martingale setting. Martingale EHLL achieves the same accuracy as
Martingale HLL using 12% less memory.
</summary>
    <author>
      <name>Tal Ohayon</name>
    </author>
    <link href="http://arxiv.org/abs/2106.06525v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.06525v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.06950">
    <id>http://arxiv.org/abs/2106.06950v3</id>
    <updated>2021-10-14T18:38:38Z</updated>
    <published>2021-06-13T09:30:41Z</published>
    <title>An efficient way to manage ranges of data with Wise Red-Black Trees</title>
    <summary>  This paper describes the most efficient way to manage operations on ranges of
elements within an ordered set. The goal is to improve existing solutions, by
optimizing the average-case time complexity and getting rid of heavy
multiplicative constants in the worst-case, without sacrificing space
complexity. This is a high-impact operation in practical applications,
performed by introducing a new data structure called Wise Red-Black Tree, an
augmented version of the Red-Black Tree.
</summary>
    <author>
      <name>Alberto Boffi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added references to order-statistic trees. Corrected some terms and
  form. Results unchanged</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.06950v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.06950v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.07017">
    <id>http://arxiv.org/abs/2106.07017v1</id>
    <updated>2021-06-13T14:58:53Z</updated>
    <published>2021-06-13T14:58:53Z</published>
    <title>The k-mappability problem revisited</title>
    <summary>  The $k$-mappability problem has two integers parameters $m$ and $k$. For
every subword of size $m$ in a text $S$, we wish to report the number of
indices in $S$ in which the word occurs with at most $k$ mismatches.
  The problem was lately tackled by Alzamel et al. For a text with constant
alphabet $\Sigma$ and $k \in O(1)$, they present an algorithm with linear space
and $O(n\log^{k+1}n)$ time. For the case in which $k = 1$ and a constant size
alphabet, a faster algorithm with linear space and $O(n\log(n)\log\log(n))$
time was presented in a 2020 paper by Alzamel et al.
  In this work, we enhance the techniques of Alzamel et al.'s 2020 paper to
obtain an algorithm with linear space and $O(n \log(n))$ time for $k = 1$. Our
algorithm removes the constraint of the alphabet being of constant size. We
also present linear algorithms for the case of $k=1$, $|\Sigma|\in O(1)$ and
$m=\Omega(\sqrt{n})$.
</summary>
    <author>
      <name>Amihood Amir</name>
    </author>
    <author>
      <name>Itai Boneh</name>
    </author>
    <author>
      <name>Eitan Kondratovsky</name>
    </author>
    <link href="http://arxiv.org/abs/2106.07017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.07017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2106.00323">
    <id>http://arxiv.org/abs/2106.00323v1</id>
    <updated>2021-06-01T08:53:08Z</updated>
    <published>2021-06-01T08:53:08Z</published>
    <title>Boosting the Search Performance of B+-tree for Non-volatile Memory with
  Sentinels</title>
    <summary>  The next-generation non-volatile memory (NVM) is striding into computer
systems as a new tier as it incorporates both DRAM's byte-addressability and
disk's persistency. Researchers and practitioners have considered building
persistent memory by placing NVM on the memory bus for CPU to directly load and
store data. As a result, cache-friendly data structures have been developed for
NVM. One of them is the prevalent B+-tree. State-of-the-art in-NVM B+-trees
mainly focus on the optimization of write operations (insertion and deletion).
However, search is of vital importance for B+-tree. Not only search-intensive
workloads benefit from an optimized search, but insertion and deletion also
rely on a preceding search operation to proceed. In this paper, we attentively
study a sorted B+-tree node that spans over contiguous cache lines. Such cache
lines exhibit a monotonically increasing trend and searching a target key
across them can be accelerated by estimating a range the key falls into. To do
so, we construct a probing Sentinel Array in which a sentinel stands for each
cache line of B+-tree node. Checking the Sentinel Array avoids scanning
unnecessary cache lines and hence significantly reduces cache misses for a
search. A quantitative evaluation shows that using Sentinel Arrays boosts the
search performance of state-of-the-art in-NVM B+-trees by up to 48.4% while the
cost of maintaining of Sentinel Array is low.
</summary>
    <author>
      <name>Chongnan Ye</name>
    </author>
    <author>
      <name>Chundong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted and Presented at MSC 2020 (@ESWeek 2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.00323v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.00323v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2104.15003">
    <id>http://arxiv.org/abs/2104.15003v2</id>
    <updated>2023-02-23T16:18:08Z</updated>
    <published>2021-04-30T13:51:48Z</published>
    <title>Memory-Optimal Non-Blocking Queues</title>
    <summary>  In addition to the memory used for storing data elements, a concurrent
implementation of a dynamic data structure exhibits a metadata overhead related
to resolving synchronization issues caused by concurrent data accesses. An
implementation can be more or less memory-friendly, depending on how much
memory it requires for the metadata. A memory-optimal implementation enjoys the
minimal possible memory overhead, which, in practice, reduces cache misses and
unnecessary memory reclamation.
  In this paper, we discuss the memory-optimality question in the context of
non-blocking bounded queues. We observe first that, in special cases when the
ABA problem is precluded, e.g., by assuming that the hardware supports LL/SC
instructions or that the queue is only used for storing distinct elements,
there exist lock-free queues with constant memory overhead (in the number of
concurrent processes $O(n)$). For the general and more realistic case, we
present a CAS-based lock-free bounded queue implementation with $O(n)$ memory
overhead. We show that this overhead is asymptotically optimal: it is
impossible to construct a non-blocking concurrent bounded queue with the
constant memory overhead, e.g., a bounded queue that maintains a fixed-size
array for the data and uses counters for enqueue/dequeue operations. This work
opens a new research avenue devoted to the memory-optimality phenomenon in
concurrent data structures.
</summary>
    <author>
      <name>Vitaly Aksenov</name>
    </author>
    <author>
      <name>Nikita Koval</name>
    </author>
    <author>
      <name>Petr Kuznetsov</name>
    </author>
    <author>
      <name>Anton Paramonov</name>
    </author>
    <link href="http://arxiv.org/abs/2104.15003v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.15003v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.13005">
    <id>http://arxiv.org/abs/2112.13005v3</id>
    <updated>2023-02-06T14:13:19Z</updated>
    <published>2021-12-24T09:26:55Z</published>
    <title>Quantum Linear Algorithm for Edit Distance Using the Word QRAM Model</title>
    <summary>  Many problems that can be solved in quadratic time have bit-parallel
speed-ups with factor $w$, where $w$ is the computer word size. For example,
edit distance of two strings of length $n$ can be solved in $O(n^2/w)$ time. In
a reasonable classical model of computation, one can assume $w=\Theta(\log n)$.
There are conditional lower bounds for such problems stating that speed-ups
with factor $n^\epsilon$ for any $\epsilon>0$ would lead to breakthroughs in
complexity theory. However, these conditional lower bounds do not cover quantum
models of computing. Indeed, Boroujeni et al. (J. ACM, 2021) showed that edit
distance can be approximated within a factor $3$ in sub-quadratic time
$O(n^{1.81})$ using quantum computing. They also showed that, in their chosen
model of quantum computing, the approximation factor cannot be improved using
sub-quadractic time.
  To break through the aforementioned classical conditional lower bounds and
this latest quantum lower bound, we enrich the model of computation with a
quantum random access memory (QRAM), obtaining what we call the word QRAM
model. Under this model, we show how to convert the bit-parallelism of
quadratic time solvable problems into quantum algorithms that attain speed-ups
with factor $n$. The technique we use is simple and general enough to apply to
many bit-parallel algorithms that use Boolean logics and bit-shifts. To apply
it to edit distance, we first show that the famous $O(n^2/w)$ time bit-parallel
algorithm of Myers (J. ACM, 1999) can be adjusted to work without arithmetic +
operations. As a direct consequence of applying our technique to this variant,
we obtain linear time edit distance algorithm under the word QRAM model for
constant alphabet. We give further results on a restricted variant of the word
QRAM model to give more insights to the limits of the model.
</summary>
    <author>
      <name>Massimo Equi</name>
    </author>
    <author>
      <name>Arianne Meijer-van de Griend</name>
    </author>
    <author>
      <name>Veli Mäkinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An incorrect assumption invalidates the results</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.13005v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.13005v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="81P68" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.4; F.1.3; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.11127">
    <id>http://arxiv.org/abs/2112.11127v1</id>
    <updated>2021-12-21T11:59:06Z</updated>
    <published>2021-12-21T11:59:06Z</published>
    <title>Optimal Gap Sequences in Shellsort for $n\leq16$ Elements</title>
    <summary>  Optimal gap sequences in Shellsort, defined as gap sequences having the
minimised maximum number of comparisons for a fixed number of pairwise distinct
elements, are found by minimax search in reduced permutational spaces, namely
Bad $(s,1)$-sorted permutations. Exact optimal gap sequences in Shellsort for
$n\leq16$ pairwise distinct elements are established, and the best known gap
sequences for $17\leq n\leq 30$ are listed with conjectures made. It notably
discovers some optimal gap sequences consist of increments larger than the half
of the total number of the elements to sort.
</summary>
    <author>
      <name>Ying Wai Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2112.11127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.11112">
    <id>http://arxiv.org/abs/2112.11112v1</id>
    <updated>2021-12-21T11:31:00Z</updated>
    <published>2021-12-21T11:31:00Z</published>
    <title>Empirically Improved Tokuda Gap Sequence in Shellsort</title>
    <summary>  Experiments are conducted to improve Tokuda (1992) gap sequence in Shellsort
into $\gamma$-sequences, and the best result is the gap sequence in which the
$k$-th increment $h_k$ is given by \begin{align} h_k=\left\lceil
\frac{\gamma^k-1}{\gamma-1} \right\rceil \end{align} , where
$\gamma=2.243609061420001...$ and $k\in\mathbb{N}_1$. The first few increments
of the gap sequence are \begin{align} 1,\, 4,\, 9,\, 20,\, 45,\, 102,\, 230,\,
516,\,1158,\,2599,\,5831,\,13082,\,29351,\,65853,\, 147748,\,331490,\,743735,\,
...\end{align}It empirically yields less numbers of comparison on average than
Tokuda (1992) gap sequence. In the procedure of search, it reveals the
potential existence of a new type of fractal.
</summary>
    <author>
      <name>Ying Wai Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2112.11112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.06188">
    <id>http://arxiv.org/abs/2112.06188v1</id>
    <updated>2021-12-12T09:08:41Z</updated>
    <published>2021-12-12T09:08:41Z</published>
    <title>Parallel Batch-Dynamic $k$d-Trees</title>
    <summary>  $k$d-trees are widely used in parallel databases to support efficient
neighborhood/similarity queries. Supporting parallel updates to $k$d-trees is
therefore an important operation. In this paper, we present BDL-tree, a
parallel, batch-dynamic implementation of a $k$d-tree that allows for efficient
parallel $k$-NN queries over dynamically changing point sets. BDL-trees consist
of a log-structured set of $k$d-trees which can be used to efficiently insert
or delete batches of points in parallel with polylogarithmic depth.
Specifically, given a BDL-tree with $n$ points, each batch of $B$ updates takes
$O(B\log^2{(n+B)})$ amortized work and $O(\log(n+B)\log\log{(n+B)})$ depth
(parallel time). We provide an optimized multicore implementation of BDL-trees.
Our optimizations include parallel cache-oblivious $k$d-tree construction and
parallel bloom filter construction.
  Our experiments on a 36-core machine with two-way hyper-threading using a
variety of synthetic and real-world datasets show that our implementation of
BDL-tree achieves a self-relative speedup of up to $34.8\times$ ($28.4\times$
on average) for batch insertions, up to $35.5\times$ ($27.2\times$ on average)
for batch deletions, and up to $46.1\times$ ($40.0\times$ on average) for
$k$-nearest neighbor queries. In addition, it achieves throughputs of up to
14.5 million updates/second for batch-parallel updates and 6.7 million
queries/second for $k$-NN queries. We compare to two baseline $k$d-tree
implementations and demonstrate that BDL-trees achieve a good tradeoff between
the two baseline options for implementing batch updates.
</summary>
    <author>
      <name>Rahul Yesantharao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIT CSAIL</arxiv:affiliation>
    </author>
    <author>
      <name>Yiqiu Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIT CSAIL</arxiv:affiliation>
    </author>
    <author>
      <name>Laxman Dhulipala</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIT CSAIL</arxiv:affiliation>
    </author>
    <author>
      <name>Julian Shun</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIT CSAIL</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2112.06188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.06188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.03229">
    <id>http://arxiv.org/abs/2112.03229v1</id>
    <updated>2021-12-06T18:38:26Z</updated>
    <published>2021-12-06T18:38:26Z</published>
    <title>SIMD-Optimized Search Over Sorted Data</title>
    <summary>  Applications often require a fast, single-threaded search algorithm over
sorted data, typical in table-lookup operations. We explore various search
algorithms for a large number of search candidates over a relatively small
array of logarithmically-distributed sorted data. These include an innovative
hash-based search that takes advantage of floating point representation to bin
data by the exponent. Algorithms that can be optimized to take advantage of
SIMD vector instructions are of particular interest. We then conduct a case
study applying our results and analyzing algorithmic performance with the
EOSPAC package. EOSPAC is a table look-up library for manipulation and
interpolation of SESAME equation-of-state data. Our investigation results in a
couple of algorithms with better performance with a best case 8x speedup over
the original EOSPAC Hunt-and-Locate implementation. Our techniques are
generalizable to other instances of search algorithms seeking to get a
performance boost from vectorization.
</summary>
    <author>
      <name>Benjamin Mastripolito</name>
    </author>
    <author>
      <name>Nicholas Koskelo</name>
    </author>
    <author>
      <name>Dylan Weatherred</name>
    </author>
    <author>
      <name>David A. Pimentel</name>
    </author>
    <author>
      <name>Daniel Sheppard</name>
    </author>
    <author>
      <name>Anna Pietarila Graham</name>
    </author>
    <author>
      <name>Laura Monroe</name>
    </author>
    <author>
      <name>Robert Robey</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1115/1.4052728</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1115/1.4052728" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Comput. Inf. Sci. Eng. Apr 2022, 22(2)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.03229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.03229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2112.00408">
    <id>http://arxiv.org/abs/2112.00408v2</id>
    <updated>2022-05-02T10:00:33Z</updated>
    <published>2021-12-01T10:49:20Z</published>
    <title>Approximating Length-Restricted Means under Dynamic Time Warping</title>
    <summary>  We study variants of the mean problem under the $p$-Dynamic Time Warping
($p$-DTW) distance, a popular and robust distance measure for sequential data.
In our setting we are given a set of finite point sequences over an arbitrary
metric space and we want to compute a mean point sequence of given length that
minimizes the sum of $p$-DTW distances, each raised to the
$q$\textsuperscript{th} power, between the input sequences and the mean
sequence. In general, the problem is $\mathrm{NP}$-hard and known not to be
fixed-parameter tractable in the number of sequences. On the positive side, we
show that restricting the length of the mean sequence significantly reduces the
hardness of the problem. We give an exact algorithm running in polynomial time
for constant-length means. We explore various approximation algorithms that
provide a trade-off between the approximation factor and the running time. Our
approximation algorithms have a running time with only linear dependency on the
number of input sequences. In addition, we use our mean algorithms to obtain
clustering algorithms with theoretical guarantees.
</summary>
    <author>
      <name>Maike Buchin</name>
    </author>
    <author>
      <name>Anne Driemel</name>
    </author>
    <author>
      <name>Koen van Greevenbroek</name>
    </author>
    <author>
      <name>Ioannis Psarros</name>
    </author>
    <author>
      <name>Dennis Rohde</name>
    </author>
    <link href="http://arxiv.org/abs/2112.00408v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.00408v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.15478">
    <id>http://arxiv.org/abs/2111.15478v3</id>
    <updated>2022-12-21T18:09:44Z</updated>
    <published>2021-11-30T15:14:07Z</published>
    <title>A new near-linear time algorithm for k-nearest neighbor search using a
  compressed cover tree</title>
    <summary>  Given a reference set $R$ of $n$ points and a query set $Q$ of $m$ points in
a metric space, this paper studies an important problem of finding $k$-nearest
neighbors of every point $q \in Q$ in the set $R$ in a near-linear time. In the
paper at ICML 2006, Beygelzimer, Kakade, and Langford introduced a cover tree
and attempted to prove that that its construction requires $O(n\log n)$ time
while the nearest neighbor search needs $O(n\log m)$ time with a hidden
dimensionality factor.
  In 2015, section~5.3 of Curtin's PhD thesis pointed out that the proof of the
latter claim might contain a serious gap in time estimation. In the paper at
TopoInVis 2022, the authors built explicit counterexamples for a key step in
the proofs of both claims. The past obstacles will be overcome by a simpler
compressed cover tree on the reference set $R$. The first new algorithm
constructs a compressed cover tree in $O(n \log n)$ time. The second new
algorithm finds all $k$-nearest neighbors of all points from $Q$ using a
compressed cover tree in time $O(m(k+\log n)\log k)$ with a hidden
dimensionality factor depending on point distributions of the sets $R,Q$ but
not on their sizes.
</summary>
    <author>
      <name>Yury Elkin</name>
    </author>
    <author>
      <name>Vitaliy Kurlin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be submitted as a publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.15478v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.15478v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.07222">
    <id>http://arxiv.org/abs/2111.07222v1</id>
    <updated>2021-11-14T01:57:23Z</updated>
    <published>2021-11-14T01:57:23Z</published>
    <title>Stochastic and Worst-Case Generalized Sorting Revisited</title>
    <summary>  The \emph{generalized sorting problem} is a restricted version of standard
comparison sorting where we wish to sort $n$ elements but only a subset of
pairs are allowed to be compared. Formally, there is some known graph $G = (V,
E)$ on the $n$ elements $v_1, \dots, v_n$, and the goal is to determine the
true order of the elements using as few comparisons as possible, where all
comparisons $(v_i, v_j)$ must be edges in $E$. We are promised that if the true
ordering is $x_1 &lt; x_2 &lt; \cdots &lt; x_n$ for $\{x_i\}$ an unknown permutation of
the vertices $\{v_i\}$, then $(x_i, x_{i+1}) \in E$ for all $i$: this
Hamiltonian path ensures that sorting is actually possible.
  In this work, we improve the bounds for generalized sorting on both random
graphs and worst-case graphs. For Erd\H{o}s-Renyi random graphs $G(n, p)$ (with
the promised Hamiltonian path added to ensure sorting is possible), we provide
an algorithm for generalized sorting with an expected $O(n \log (np))$
comparisons, which we prove to be optimal for query complexity. This strongly
improves over the best known algorithm of Huang, Kannan, and Khanna (FOCS
2011), which uses $\tilde{O}(\min(n \sqrt{np}, n/p^2))$ comparisons. For
arbitrary graphs $G$ with $n$ vertices and $m$ edges (again with the promised
Hamiltonian path), we provide an algorithm for generalized sorting with
$\tilde{O}(\sqrt{mn})$ comparisons. This improves over the best known algorithm
of Huang et al., which uses $\min(m, \tilde{O}(n^{3/2}))$ comparisons.
</summary>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <author>
      <name>Shyam Narayanan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">FOCS 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2111.07222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.07222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2111.06856">
    <id>http://arxiv.org/abs/2111.06856v1</id>
    <updated>2021-11-12T18:17:18Z</updated>
    <published>2021-11-12T18:17:18Z</published>
    <title>Approximate Membership Query Filters with a False Positive Free Set</title>
    <summary>  In the last decade, significant efforts have been made to reduce the false
positive rate of approximate membership checking structures. This has led to
the development of new structures such as cuckoo filters and xor filters.
Adaptive filters that can react to false positives as they occur to avoid them
for future queries to the same elements have also been recently developed. In
this paper, we propose a new type of static filters that completely avoid false
positives for a given set of negative elements and show how they can be
efficiently implemented using xor probing filters. Several constructions of
these filters with a false positive free set are proposed that minimize the
memory and speed overheads introduced by avoiding false positives. The proposed
filters have been extensively evaluated to validate their functionality and
show that in many cases both the memory and speed overheads are negligible. We
also discuss several use cases to illustrate the potential benefits of the
proposed filters in practical applications.
</summary>
    <author>
      <name>Pedro Reviriego</name>
    </author>
    <author>
      <name>Alfonso Sánchez-Macián</name>
    </author>
    <author>
      <name>Stefan Walzer</name>
    </author>
    <author>
      <name>Peter C. Dillinger</name>
    </author>
    <link href="http://arxiv.org/abs/2111.06856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.06856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2110.05540">
    <id>http://arxiv.org/abs/2110.05540v1</id>
    <updated>2021-10-11T18:22:46Z</updated>
    <published>2021-10-11T18:22:46Z</published>
    <title>Parallel Batched Interpolation Search Tree</title>
    <summary>  Ordered set (and map) is one of the most used data type. In addition to
standard set operations, like insert, delete and contains, it can provide
set-set operations such as union, intersection, and difference. Each of these
set-set operations is equivalent to batched operations: the data structure
should process a set of operations insert, delete, and contains. It is obvious
that we want these "large" operations to be parallelized. Typically, these sets
are implemented with the trees of logarithmic height, such as 2-3 tree, Treap,
AVL tree, Red-Black tree, etc. Until now, little attention was devoted to data
structures that work better but under several restrictions on the data. In this
work, we parallelize Interpolation Search Tree which serves each request from a
smooth distribution in doubly-logarithmic time. Our data structure of size $n$
performs a batch of $m$ operations in $O(m \log\log n)$ work and poly-log span.
</summary>
    <author>
      <name>Vitaly Aksenov</name>
    </author>
    <author>
      <name>Ilya Kokorin</name>
    </author>
    <author>
      <name>Alena Martsenyuk</name>
    </author>
    <link href="http://arxiv.org/abs/2110.05540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.05540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.03402">
    <id>http://arxiv.org/abs/2007.03402v2</id>
    <updated>2020-07-09T10:37:53Z</updated>
    <published>2020-07-06T09:51:41Z</published>
    <title>Quantum Lower and Upper Bounds for 2D-Grid and Dyck Language</title>
    <summary>  We study the quantum query complexity of two problems.
  First, we consider the problem of determining if a sequence of parentheses is
a properly balanced one (a Dyck word), with a depth of at most $k$. We call
this the $Dyck_{k,n}$ problem. We prove a lower bound of $\Omega(c^k
\sqrt{n})$, showing that the complexity of this problem increases exponentially
in $k$. Here $n$ is the length of the word. When $k$ is a constant, this is
interesting as a representative example of star-free languages for which a
surprising $\tilde{O}(\sqrt{n})$ query quantum algorithm was recently
constructed by Aaronson et al. Their proof does not give rise to a general
algorithm. When $k$ is not a constant, $Dyck_{k,n}$ is not context-free. We
give an algorithm with $O\left(\sqrt{n}(\log{n})^{0.5k}\right)$ quantum queries
for $Dyck_{k,n}$ for all $k$. This is better than the trival upper bound $n$
for $k=o\left(\frac{\log(n)}{\log\log n}\right)$.
  Second, we consider connectivity problems on grid graphs in 2 dimensions, if
some of the edges of the grid may be missing. By embedding the "balanced
parentheses" problem into the grid, we show a lower bound of
$\Omega(n^{1.5-\epsilon})$ for the directed 2D grid and
$\Omega(n^{2-\epsilon})$ for the undirected 2D grid. The directed problem is
interesting as a black-box model for a class of classical dynamic programming
strategies including the one that is usually used for the well-known edit
distance problem. We also show a generalization of this result to more than 2
dimensions.
</summary>
    <author>
      <name>Andris Ambainis</name>
    </author>
    <author>
      <name>Kaspars Balodis</name>
    </author>
    <author>
      <name>Jānis Iraids</name>
    </author>
    <author>
      <name>Kamil Khadiev</name>
    </author>
    <author>
      <name>Vladislavs Kļevickis</name>
    </author>
    <author>
      <name>Krišjānis Prūsis</name>
    </author>
    <author>
      <name>Yixin Shen</name>
    </author>
    <author>
      <name>Juris Smotrovs</name>
    </author>
    <author>
      <name>Jevgēnijs Vihrovs</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1911.12638</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.03402v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.03402v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.01098">
    <id>http://arxiv.org/abs/2005.01098v2</id>
    <updated>2020-06-22T13:21:42Z</updated>
    <published>2020-05-03T14:29:35Z</published>
    <title>A Dynamic Space-Efficient Filter with Constant Time Operations</title>
    <summary>  A dynamic dictionary is a data structure that maintains sets of cardinality
at most $n$ from a given universe and supports insertions, deletions, and
membership queries. A filter approximates membership queries with a one-sided
error that occurs with probability at most $\epsilon$. The goal is to obtain
dynamic filters that are space-efficient (the space is $1+o(1)$ times the
information-theoretic lower bound) and support all operations in constant time
with high probability. One approach to designing filters is to reduce to the
retrieval problem. When the size of the universe is polynomial in $n$, this
approach yields a space-efficient dynamic filter as long as the error parameter
$\epsilon$ satisfies $\log(1/\epsilon) = \omega(\log\log n)$.
  For the case that $\log(1/\epsilon) = O(\log\log n)$, we present the first
space-efficient dynamic filter with constant time operations in the worst case
(whp). In contrast, the space-efficient dynamic filter of Pagh, Pagh, Rao (SODA
2005) supports insertions and deletions in amortized expected constant time.
Our approach employs the classic reduction of Carter et al. (STOC 1978) on a
new type of dictionary construction that supports random multisets.
</summary>
    <author>
      <name>Ioana Oriana Bercea</name>
    </author>
    <author>
      <name>Guy Even</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in SWAT 2020. Earlier version of the paper available
  at arXiv:1911.05060</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.01098v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.01098v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2004.02335">
    <id>http://arxiv.org/abs/2004.02335v1</id>
    <updated>2020-04-05T22:26:05Z</updated>
    <published>2020-04-05T22:26:05Z</published>
    <title>The n-dimensional k-vector and its application to orthogonal range
  searching</title>
    <summary>  This work focuses on the definition and study of the n-dimensional k-vector,
an algorithm devised to perform orthogonal range searching in static databases
with multiple dimensions. The methodology first finds the order in which to
search the dimensions, and then, performs the search using a modified
projection method. In order to determine the dimension order, the algorithm
uses the k-vector, a range searching technique for one dimension that
identifies the number of elements contained in the searching range. Then, using
this information, the algorithm predicts and selects the best approach to deal
with each dimension. The algorithm has a worst case complexity of
$\mathcal{O}(nd(k/n)^{2/d})$, where $k$ is the number of elements retrieved,
$n$ is the number of elements in the database, and $d$ is the number of
dimensions of the database. This work includes a detailed description of the
methodology as well as a study of the algorithm performance.
</summary>
    <author>
      <name>David Arnas</name>
    </author>
    <author>
      <name>Carl Leake</name>
    </author>
    <author>
      <name>Daniele Mortari</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.amc.2019.125010</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.amc.2019.125010" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Applied Mathematics and Computation, Vol. 372, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.02335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.02335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.09584">
    <id>http://arxiv.org/abs/2003.09584v1</id>
    <updated>2020-03-21T06:07:17Z</updated>
    <published>2020-03-21T06:07:17Z</published>
    <title>Hidden Words Statistics for Large Patterns</title>
    <summary>  We study here the so called subsequence pattern matching also known as hidden
pattern matching in which one searches for a given pattern $w$ of length $m$ as
a subsequence in a random text of length $n$. The quantity of interest is the
number of occurrences of $w$ as a subsequence (i.e., occurring in not
necessarily consecutive text locations). This problem finds many applications
from intrusion detection, to trace reconstruction, to deletion channel, and to
DNA-based storage systems. In all of these applications, the pattern $w$ is of
variable length. To the best of our knowledge this problem was only tackled for
a fixed length $m=O(1)$ [Flajolet, Szpankowski and Vall\'ee, 2006]. In our main
result we prove that for $m=o(n^{1/3})$ the number of subsequence occurrences
is normally distributed. In addition, we show that under some constraints on
the structure of $w$ the asymptotic normality can be extended to
$m=o(\sqrt{n})$. For a special pattern $w$ consisting of the same symbol, we
indicate that for $m=o(n)$ the distribution of number of subsequences is either
asymptotically normal or asymptotically log normal. We conjecture that this
dichotomy is true for all patterns. We use Hoeffding's projection method for
$U$-statistics to prove our findings.
</summary>
    <author>
      <name>Svante Janson</name>
    </author>
    <author>
      <name>Wojciech Szpankowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.09584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.09584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2003.01902">
    <id>http://arxiv.org/abs/2003.01902v1</id>
    <updated>2020-03-04T05:41:34Z</updated>
    <published>2020-03-04T05:41:34Z</published>
    <title>Notes on Randomized Algorithms</title>
    <summary>  Lecture notes for the Yale Computer Science course CPSC 469/569 Randomized
Algorithms. Suitable for use as a supplementary text for an introductory
graduate or advanced undergraduate course on randomized algorithms. Discusses
tools from probability theory, including random variables and expectations,
union bound arguments, concentration bounds, applications of martingales and
Markov chains, and the Lov\'asz Local Lemma. Algorithmic topics include
analysis of classic randomized algorithms such as Quicksort and Hoare's FIND,
randomized tree data structures, hashing, Markov chain Monte Carlo sampling,
randomized approximate counting, derandomization, quantum computing, and some
examples of randomized distributed algorithms.
</summary>
    <author>
      <name>James Aspnes</name>
    </author>
    <link href="http://arxiv.org/abs/2003.01902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68-01" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.11171">
    <id>http://arxiv.org/abs/2002.11171v2</id>
    <updated>2020-04-17T10:29:56Z</updated>
    <published>2020-02-25T20:54:36Z</published>
    <title>Dynamic Set Cover: Improved Amortized and Worst-Case Update Time</title>
    <summary>  In the dynamic minimum set cover problem, a challenge is to minimize the
update time while guaranteeing close to the optimal $\min(O(\log n), f)$
approximation factor. (Throughout, $m$, $n$, $f$, and $C$ are parameters
denoting the maximum number of sets, number of elements, frequency, and the
cost range.) In the high-frequency range, when $f=\Omega(\log n)$, this was
achieved by a deterministic $O(\log n)$-approximation algorithm with $O(f \log
n)$ amortized update time [Gupta et al. STOC'17]. In the low-frequency range,
the line of work by Gupta et al. [STOC'17], Abboud et al. [STOC'19], and
Bhattacharya et al. [ICALP'15, IPCO'17, FOCS'19] led to a deterministic
$(1+\epsilon)f$-approximation algorithm with $O(f \log (Cn)/\epsilon^2)$
amortized update time. In this paper we improve the latter update time and
provide the first bounds that subsume (and sometimes improve) the
state-of-the-art dynamic vertex cover algorithms. We obtain:
  1. $(1+\epsilon)f$-approximation ratio in $O(f\log^2 (Cn)/\epsilon^3)$
worst-case update time: No non-trivial worst-case update time was previously
known for dynamic set cover. Our bound subsumes and improves by a logarithmic
factor the $O(\log^3 n/\text{poly}(\epsilon))$ worst-case update time for
unweighted dynamic vertex cover (i.e., when $f=2$ and $C=1$) by Bhattacharya et
al. [SODA'17].
  2. $(1+\epsilon)f$-approximation ratio in
$O\left((f^2/\epsilon^3)+(f/\epsilon^2) \log C\right)$ amortized update time:
This result improves the previous $O(f \log (Cn)/\epsilon^2)$ update time bound
for most values of $f$ in the low-frequency range, i.e. whenever $f=o(\log n)$.
It is the first that is independent of $m$ and $n$. It subsumes the constant
amortized update time of Bhattacharya and Kulkarni [SODA'19] for unweighted
dynamic vertex cover (i.e., when $f = 2$ and $C = 1$).
</summary>
    <author>
      <name>Sayan Bhattacharya</name>
    </author>
    <author>
      <name>Monika Henzinger</name>
    </author>
    <author>
      <name>Danupon Nanongkai</name>
    </author>
    <author>
      <name>Xiaowei Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This new version contains an additional result on worst-case update
  time and a revised extended abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.11171v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11171v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2002.01178">
    <id>http://arxiv.org/abs/2002.01178v1</id>
    <updated>2020-02-04T09:18:26Z</updated>
    <published>2020-02-04T09:18:26Z</published>
    <title>Faster Binary Mean Computation Under Dynamic Time Warping</title>
    <summary>  Many consensus string problems are based on Hamming distance. We replace
Hamming distance by the more flexible (e.g., easily coping with different input
string lengths) dynamic time warping distance, best known from applications in
time series mining. Doing so, we study the problem of finding a mean string
that minimizes the sum of (squared) dynamic time warping distances to a given
set of input strings. While this problem is known to be NP-hard (even for
strings over a three-element alphabet), we address the binary alphabet case
which is known to be polynomial-time solvable. We significantly improve on a
previously known algorithm in terms of worst-case running time. Moreover, we
also show the practical usefulness of one of our algorithms in experiments with
real-world and synthetic data. Finally, we identify special cases solvable in
linear time (e.g., finding a mean of only two binary input strings) and report
some empirical findings concerning combinatorial properties of optimal means.
</summary>
    <author>
      <name>Nathan Schaar</name>
    </author>
    <author>
      <name>Vincent Froese</name>
    </author>
    <author>
      <name>Rolf Niedermeier</name>
    </author>
    <link href="http://arxiv.org/abs/2002.01178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.01178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.11607">
    <id>http://arxiv.org/abs/2001.11607v2</id>
    <updated>2020-10-05T19:57:04Z</updated>
    <published>2020-01-30T23:38:04Z</published>
    <title>Optimally selecting the top $k$ values from $X+Y$ with layer-ordered
  heaps</title>
    <summary>  Selection and sorting the Cartesian sum, $X+Y$, are classic and important
problems. Here, a new algorithm is presented, which generates the top $k$
values of the form $X_i+Y_j$. The algorithm relies only on median-of-medians
and is simple to implement. Furthermore, it uses data structures contiguous in
memory, and is fast in practice. The presented algorithm is demonstrated to be
theoretically optimal.
</summary>
    <author>
      <name>Oliver Serang</name>
    </author>
    <link href="http://arxiv.org/abs/2001.11607v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11607v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.00562">
    <id>http://arxiv.org/abs/2001.00562v4</id>
    <updated>2022-05-04T03:12:28Z</updated>
    <published>2020-01-02T18:55:14Z</published>
    <title>Optimal Entropy Compression and Purification in Quantum Bits</title>
    <summary>  Global unitary transformations (OPTSWAPS) that optimally increase the bias of
any mixed computation qubit in a quantum system -- represented by a diagonal
density matrix -- towards a particular state of the computational basis which,
in effect, increases its purity are presented. Quantum circuits that achieve
this by implementing the above data compression technique -- a generalization
of the 3B-Comp used before -- are described. These circuits enable purity
increment in the computation qubit by maximally transferring part of its von
Neumann or Shannon entropy to any number of surrounding qubits and are valid
for the complete range of initial biases. Using the optswaps, a practicable new
method that algorithmically achieves hierarchy-dependent cooling of qubits to
their respective limits in an engineered quantum register opened to the
heat-bath is delineated. In addition to multi-qubit purification and satisfying
two of DiVincenzo's criteria for quantum computation in some architectures, the
implications of this work for quantum data compression and quantum
thermodynamics are discussed.
</summary>
    <author>
      <name>Varad R. Pande</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 11 + 1 (external) figures; v4: revised manuscript</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.00562v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00562v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2001.00413">
    <id>http://arxiv.org/abs/2001.00413v1</id>
    <updated>2020-01-02T12:36:41Z</updated>
    <published>2020-01-02T12:36:41Z</published>
    <title>Analysis and Evaluation of Non-Blocking Interpolation Search Trees</title>
    <summary>  We start by summarizing the recently proposed implementation of the first
non-blocking concurrent interpolation search tree (C-IST) data structure. We
then analyze the individual operations of the C-IST, and show that they are
correct and linearizable. We furthermore show that lookup (and several other
non-destructive operations) are wait-free, and that the insert and delete
operations are lock-free. We continue by showing that the C-IST has the
following properties. For arbitrary key distributions, this data structure
ensures worst-case $O(\log n + p)$ amortized time for search, insertion and
deletion traversals. When the input key distributions are smooth, lookups run
in expected $O(\log \log n + p)$ time, and insertion and deletion run in
expected amortized $O(\log \log n + p)$ time, where $p$ is a bound on the
number of threads. Finally, we present an extended experimental evaluation of
the non-blocking IST performance.
</summary>
    <author>
      <name>Aleksandar Prokopec</name>
    </author>
    <author>
      <name>Trevor Brown</name>
    </author>
    <author>
      <name>Dan Alistarh</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.00931">
    <id>http://arxiv.org/abs/2010.00931v2</id>
    <updated>2020-10-28T11:19:31Z</updated>
    <published>2020-10-02T11:51:03Z</published>
    <title>Splay trees on trees</title>
    <summary>  Search trees on trees (STTs) are a far-reaching generalization of binary
search trees (BSTs), allowing the efficient exploration of tree-structured
domains. (BSTs are the special case in which the underlying domain is a path.)
Trees on trees have been extensively studied under various guises in computer
science and discrete mathematics.
  Recently Bose, Cardinal, Iacono, Koumoutsos, and Langerman (SODA 2020)
considered adaptive STTs and observed that, apart from notable exceptions, the
machinery developed for BSTs in the past decades does not readily transfer to
STTs. In particular, they asked whether the optimal STT can be efficiently
computed or approximated (by analogy to Knuth's algorithm for optimal BSTs),
and whether natural self-adjusting BSTs such as Splay trees (Sleator, Tarjan,
1983) can be extended to this more general setting.
  We answer both questions affirmatively. First, we show that a $(1 +
\frac{1}{t})$-approximation of an optimal size-$n$ STT for a given search
distribution can be computed in time $O(n^{2t + 1})$ for all integers $t \geq
1$. Second, we identify a broad family of STTs with linear rotation-distance,
allowing the generalization of Splay trees to the STT setting. We show that our
generalized Splay satisfies a static optimality theorem, asymptotically
matching the cost of the optimal STT in an online fashion, i.e. without
knowledge of the search distribution. Our results suggest an extension of the
dynamic optimality conjecture for Splay trees to the broader setting of trees
on trees.
</summary>
    <author>
      <name>Benjamin Aram Berendsohn</name>
    </author>
    <author>
      <name>László Kozma</name>
    </author>
    <link href="http://arxiv.org/abs/2010.00931v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.00931v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2008.10577">
    <id>http://arxiv.org/abs/2008.10577v3</id>
    <updated>2020-10-30T10:53:56Z</updated>
    <published>2020-08-24T17:29:44Z</published>
    <title>Fast and Simple Modular Subset Sum</title>
    <summary>  We revisit the Subset Sum problem over the finite cyclic group $\mathbb{Z}_m$
for some given integer $m$. A series of recent works has provided near-optimal
algorithms for this problem under the Strong Exponential Time Hypothesis.
Koiliaris and Xu (SODA'17, TALG'19) gave a deterministic algorithm running in
time $\tilde{O}(m^{5/4})$, which was later improved to $O(m \log^7 m)$
randomized time by Axiotis et al. (SODA'19).
  In this work, we present two simple algorithms for the Modular Subset Sum
problem running in near-linear time in $m$, both efficiently implementing
Bellman's iteration over $\mathbb{Z}_m$. The first one is a randomized
algorithm running in time $O(m \log^2 m)$, that is based solely on rolling hash
and an elementary data-structure for prefix sums; to illustrate its simplicity
we provide a short and efficient implementation of the algorithm in Python. Our
second solution is a deterministic algorithm running in time $O(m\
\mathrm{polylog}\ m)$, that uses dynamic data structures for string
manipulation.
  We further show that the techniques developed in this work can also lead to
simple algorithms for the All Pairs Non-Decreasing Paths Problem (APNP) on
undirected graphs, matching the near-optimal running time of $\tilde{O}(n^2)$
provided in the recent work of Duan et al. (ICALP'19).
</summary>
    <author>
      <name>Kyriakos Axiotis</name>
    </author>
    <author>
      <name>Arturs Backurs</name>
    </author>
    <author>
      <name>Karl Bringmann</name>
    </author>
    <author>
      <name>Ce Jin</name>
    </author>
    <author>
      <name>Vasileios Nakos</name>
    </author>
    <author>
      <name>Christos Tzamos</name>
    </author>
    <author>
      <name>Hongxun Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at SOSA'21</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.10577v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.10577v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2008.05594">
    <id>http://arxiv.org/abs/2008.05594v1</id>
    <updated>2020-08-12T22:51:47Z</updated>
    <published>2020-08-12T22:51:47Z</published>
    <title>Cadences in Grammar-Compressed Strings</title>
    <summary>  Cadences are structurally maximal arithmetic progressions of indices
corresponding to equal characters in an underlying string.
  This paper provides a polynomial time detection algorithm for 3-cadences in
grammar-compressed binary strings. This algorithm also translates to a linear
time detection algorithm for 3-cadences in uncompressed binary strings.
  Furthermore, this paper proves that several variants of the cadence detection
problem are NP-complete for grammar-compressed strings. As a consequence, the
equidistant subsequence matching problem with patterns of length three is
NP-complete for grammar-compressed ternary strings.
</summary>
    <author>
      <name>Julian Pape-Lange</name>
    </author>
    <link href="http://arxiv.org/abs/2008.05594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.05594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2008.05398">
    <id>http://arxiv.org/abs/2008.05398v1</id>
    <updated>2020-08-12T15:46:04Z</updated>
    <published>2020-08-12T15:46:04Z</published>
    <title>Soft Sequence Heaps</title>
    <summary>  Chazelle [JACM00] introduced the soft heap as a building block for efficient
minimum spanning tree algorithms, and recently Kaplan et al. [SOSA2019] showed
how soft heaps can be applied to achieve simpler algorithms for various
selection problems. A soft heap trades-off accuracy for efficiency, by allowing
$\epsilon N$ of the items in a heap to be corrupted after a total of $N$
insertions, where a corrupted item is an item with artificially increased key
and $0 &lt; \epsilon \leq 1/2$ is a fixed error parameter. Chazelle's soft heaps
are based on binomial trees and support insertions in amortized
$O(\lg(1/\epsilon))$ time and extract-min operations in amortized $O(1)$ time.
  In this paper we explore the design space of soft heaps. The main
contribution of this paper is an alternative soft heap implementation based on
merging sorted sequences, with time bounds matching those of Chazelle's soft
heaps. We also discuss a variation of the soft heap by Kaplan et al.
[SICOMP2013], where we avoid performing insertions lazily. It is based on
ternary trees instead of binary trees and matches the time bounds of Kaplan et
al., i.e. amortized $O(1)$ insertions and amortized $O(\lg(1/\epsilon))$
extract-min. Both our data structures only introduce corruptions after
extract-min operations which return the set of items corrupted by the
operation.
</summary>
    <author>
      <name>Gerth Stølting Brodal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.05398v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.05398v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2008.02769">
    <id>http://arxiv.org/abs/2008.02769v2</id>
    <updated>2020-09-21T15:02:20Z</updated>
    <published>2020-08-06T17:13:58Z</published>
    <title>Fine-Grained Complexity of Regular Expression Pattern Matching and
  Membership</title>
    <summary>  The currently fastest algorithm for regular expression pattern matching and
membership improves the classical O(nm) time algorithm by a factor of about
log^{3/2}n. Instead of focussing on general patterns we analyse homogeneous
patterns of bounded depth in this work. For them a classification splitting the
types in easy (strongly sub-quadratic) and hard (essentially quadratic time
under SETH) is known. We take a very fine-grained look at the hard pattern
types from this classification and show a dichotomy: few types allow
super-poly-logarithmic improvements while the algorithms for the other pattern
types can only be improved by a constant number of log-factors, assuming the
Formula-SAT Hypothesis.
</summary>
    <author>
      <name>Philipp Schepper</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.ESA.2020.80</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.ESA.2020.80" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of the paper accepted at ESA 2020; v2: typos and
  reference to conference version corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.02769v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.02769v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2008.01768">
    <id>http://arxiv.org/abs/2008.01768v3</id>
    <updated>2021-01-12T07:02:34Z</updated>
    <published>2020-08-04T19:03:40Z</published>
    <title>A Data-Structure for Approximate Longest Common Subsequence of A Set of
  Strings</title>
    <summary>  Given a set of $k$ strings $I$, their longest common subsequence (LCS) is the
string with the maximum length that is a subset of all the strings in $I$. A
data-structure for this problem preprocesses $I$ into a data-structure such
that the LCS of a set of query strings $Q$ with the strings of $I$ can be
computed faster. Since the problem is NP-hard for arbitrary $k$, we allow an
error that allows some characters to be replaced by other characters. We define
the approximation version of the problem with an extra input $m$, which is the
length of the regular expression (regex) that describes the input, and the
approximation factor is the logarithm of the number of possibilities in the
regex returned by the algorithm, divided by the logarithm regex with the
minimum number of possibilities. Then, we use a tree data-structure to achieve
sublinear-time LCS queries. We also explain how the idea can be extended to the
longest increasing subsequence (LIS) problem.
</summary>
    <author>
      <name>Sepideh Aghamolaei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An optimal exact sketch for the LCS of two strings was already known:
  arXiv:1810.01238 as well as an approximation algorithm with weights:
  https://doi.org/10.1016/j.ic.2010.12.006 The edit distance of regular
  languages was also known: https://doi.org/10.3390/a11110165 Using these
  subroutines in any algorithm for the LCS of k strings gives a better result</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.01768v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.01768v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2008.01765">
    <id>http://arxiv.org/abs/2008.01765v3</id>
    <updated>2021-04-29T09:06:48Z</updated>
    <published>2020-08-04T18:45:40Z</published>
    <title>Bucket Oblivious Sort: An Extremely Simple Oblivious Sort</title>
    <summary>  We propose a conceptually simple oblivious sort and oblivious random
permutation algorithms called bucket oblivious sort and bucket oblivious random
permutation. Bucket oblivious sort uses $6n\log n$ time (measured by the number
of memory accesses) and $2Z$ client storage with an error probability
exponentially small in $Z$. The above runtime is only $3\times$ slower than a
non-oblivious merge sort baseline; for $2^{30}$ elements, it is $5\times$
faster than bitonic sort, the de facto oblivious sorting algorithm in practical
implementations.
</summary>
    <author>
      <name>Gilad Asharov</name>
    </author>
    <author>
      <name>T-H. Hubert Chan</name>
    </author>
    <author>
      <name>Kartik Nayak</name>
    </author>
    <author>
      <name>Rafael Pass</name>
    </author>
    <author>
      <name>Ling Ren</name>
    </author>
    <author>
      <name>Elaine Shi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in SOSA@SODA 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.01765v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.01765v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.14368">
    <id>http://arxiv.org/abs/2007.14368v1</id>
    <updated>2020-07-28T17:19:03Z</updated>
    <published>2020-07-28T17:19:03Z</published>
    <title>A Simple Sublinear Algorithm for Gap Edit Distance</title>
    <summary>  We study the problem of estimating the edit distance between two
$n$-character strings. While exact computation in the worst case is believed to
require near-quadratic time, previous work showed that in certain regimes it is
possible to solve the following {\em gap edit distance} problem in sub-linear
time: distinguish between inputs of distance $\le k$ and $>k^2$. Our main
result is a very simple algorithm for this benchmark that runs in time $\tilde
O(n/\sqrt{k})$, and in particular settles the open problem of obtaining a truly
sublinear time for the entire range of relevant $k$.
  Building on the same framework, we also obtain a $k$-vs-$k^2$ algorithm for
the one-sided preprocessing model with $\tilde O(n)$ preprocessing time and
$\tilde O(n/k)$ query time (improving over a recent $\tilde O(n/k+k^2)$-query
time algorithm for the same problem [GRS'20].
</summary>
    <author>
      <name>Joshua Brakensiek</name>
    </author>
    <author>
      <name>Moses Charikar</name>
    </author>
    <author>
      <name>Aviad Rubinstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.14368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.14368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.13471">
    <id>http://arxiv.org/abs/2007.13471v1</id>
    <updated>2020-07-27T12:18:52Z</updated>
    <published>2020-07-27T12:18:52Z</published>
    <title>Internal Quasiperiod Queries</title>
    <summary>  Internal pattern matching requires one to answer queries about factors of a
given string. Many results are known on answering internal period queries,
asking for the periods of a given factor. In this paper we investigate (for the
first time) internal queries asking for covers (also known as quasiperiods) of
a given factor. We propose a data structure that answers such queries in
$O(\log n \log \log n)$ time for the shortest cover and in $O(\log n (\log \log
n)^2)$ time for a representation of all the covers, after $O(n \log n)$ time
and space preprocessing.
</summary>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Costas Iliopoulos</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Juliusz Straszyński</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <author>
      <name>Wiktor Zuba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the SPIRE 2020 proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.13471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.13471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2007.07575">
    <id>http://arxiv.org/abs/2007.07575v3</id>
    <updated>2021-06-24T14:18:51Z</updated>
    <published>2020-07-15T09:51:23Z</published>
    <title>A linear-time parameterized algorithm for computing the width of a DAG</title>
    <summary>  The width $k$ of a directed acyclic graph (DAG) $G = (V, E)$ equals the
largest number of pairwise non-reachable vertices. Computing the width dates
back to Dilworth's and Fulkerson's results in the 1950s, and is doable in
quadratic time in the worst case. Since $k$ can be small in practical
applications, research has also studied algorithms whose complexity is
parameterized on $k$. Despite these efforts, it is still open whether there
exists a linear-time $O(f(k)(|V| + |E|))$ parameterized algorithm computing the
width. We answer this question affirmatively by presenting an $O(k^24^k|V| +
k2^k|E|)$ time algorithm, based on a new notion of frontier antichains. As we
process the vertices in a topological order, all frontier antichains can be
maintained with the help of several combinatorial properties, paying only
$f(k)$ along the way. The fact that the width can be computed by a single
$f(k)$-sweep of the DAG is a new surprising insight into this classical
problem. Our algorithm also allows deciding whether the DAG has width at most
$w$ in time $O(f(\min(w,k))(|V|+|E|))$.
</summary>
    <author>
      <name>Manuel Cáceres</name>
    </author>
    <author>
      <name>Massimo Cairo</name>
    </author>
    <author>
      <name>Brendan Mumey</name>
    </author>
    <author>
      <name>Romeo Rizzi</name>
    </author>
    <author>
      <name>Alexandru I. Tomescu</name>
    </author>
    <link href="http://arxiv.org/abs/2007.07575v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.07575v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.07902">
    <id>http://arxiv.org/abs/1904.07902v1</id>
    <updated>2019-04-16T18:06:25Z</updated>
    <published>2019-04-16T18:06:25Z</published>
    <title>Heuristic algorithms for the Longest Filled Common Subsequence Problem</title>
    <summary>  At CPM 2017, Castelli et al. define and study a new variant of the Longest
Common Subsequence Problem, termed the Longest Filled Common Subsequence
Problem (LFCS). For the LFCS problem, the input consists of two strings $A$ and
$B$ and a multiset of characters $\mathcal{M}$. The goal is to insert the
characters from $\mathcal{M}$ into the string $B$, thus obtaining a new string
$B^*$, such that the Longest Common Subsequence (LCS) between $A$ and $B^*$ is
maximized. Casteli et al. show that the problem is NP-hard and provide a
3/5-approximation algorithm for the problem.
  In this paper we study the problem from the experimental point of view. We
introduce, implement and test new heuristic algorithms and compare them with
the approximation algorithm of Casteli et al. Moreover, we introduce an Integer
Linear Program (ILP) model for the problem and we use the state of the art ILP
solver, Gurobi, to obtain exact solution for moderate sized instances.
</summary>
    <author>
      <name>Radu Stefan Mincu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Bucharest, Romania</arxiv:affiliation>
    </author>
    <author>
      <name>Alexandru Popa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Bucharest, Romania</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Institute for Research and Development in Informatics, Bucharest, Romania</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SYNASC.2018.00075</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SYNASC.2018.00075" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted and presented as a proceedings paper at SYNASC 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.07902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C59" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.05451">
    <id>http://arxiv.org/abs/1904.05451v1</id>
    <updated>2019-04-10T21:36:47Z</updated>
    <published>2019-04-10T21:36:47Z</published>
    <title>Reducing approximate Longest Common Subsequence to approximate Edit
  Distance</title>
    <summary>  Given a pair of strings, the problems of computing their Longest Common
Subsequence and Edit Distance have been extensively studied for decades. For
exact algorithms, LCS and Edit Distance (with character insertions and
deletions) are equivalent; the state of the art running time is (almost)
quadratic and this is tight under plausible fine-grained complexity
assumptions. But for approximation algorithms the picture is different: there
is a long line of works with improved approximation factors for Edit Distance,
but for LCS (with binary strings) only a trivial $1/2$-approximation was known.
In this work we give a reduction from approximate LCS to approximate Edit
Distance, yielding the first efficient $(1/2+\epsilon)$-approximation algorithm
for LCS for some constant $\epsilon>0$.
</summary>
    <author>
      <name>Aviad Rubinstein</name>
    </author>
    <author>
      <name>Zhao Song</name>
    </author>
    <link href="http://arxiv.org/abs/1904.05451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1903.03520">
    <id>http://arxiv.org/abs/1903.03520v1</id>
    <updated>2019-03-08T15:47:51Z</updated>
    <published>2019-03-08T15:47:51Z</published>
    <title>The One-Way Communication Complexity of Dynamic Time Warping Distance</title>
    <summary>  We resolve the randomized one-way communication complexity of Dynamic Time
Warping (DTW) distance. We show that there is an efficient one-way
communication protocol using $\widetilde{O}(n/\alpha)$ bits for the problem of
computing an $\alpha$-approximation for DTW between strings $x$ and $y$ of
length $n$, and we prove a lower bound of $\Omega(n / \alpha)$ bits for the
same problem. Our communication protocol works for strings over an arbitrary
metric of polynomial size and aspect ratio, and we optimize the logarithmic
factors depending on properties of the underlying metric, such as when the
points are low-dimensional integer vectors equipped with various metrics or
have bounded doubling dimension. We also consider linear sketches of DTW,
showing that such sketches must have size $\Omega(n)$.
</summary>
    <author>
      <name>Vladimir Braverman</name>
    </author>
    <author>
      <name>Moses Charikar</name>
    </author>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <author>
      <name>David P. Woodruff</name>
    </author>
    <author>
      <name>Lin F. Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1903.03520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.03520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2012.10985">
    <id>http://arxiv.org/abs/2012.10985v1</id>
    <updated>2020-12-20T18:02:47Z</updated>
    <published>2020-12-20T18:02:47Z</published>
    <title>Learning Halfspaces With Membership Queries</title>
    <summary>  Active learning is a subfield of machine learning, in which the learning
algorithm is allowed to choose the data from which it learns. In some cases, it
has been shown that active learning can yield an exponential gain in the number
of samples the algorithm needs to see, in order to reach generalization error
$\leq \epsilon$. In this work we study the problem of learning halfspaces with
membership queries. In the membership query scenario, we allow the learning
algorithm to ask for the label of every sample in the input space. We suggest a
new algorithm for this problem, and prove it achieves a near optimal label
complexity in some cases. We also show that the algorithm works well in
practice, and significantly outperforms uncertainty sampling.
</summary>
    <author>
      <name>Ori Kelner</name>
    </author>
    <link href="http://arxiv.org/abs/2012.10985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.10985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.11144">
    <id>http://arxiv.org/abs/2011.11144v1</id>
    <updated>2020-11-23T00:14:23Z</updated>
    <published>2020-11-23T00:14:23Z</published>
    <title>Searching and Sorting with O(n^2) processors in O(1) time</title>
    <summary>  The proliferation of number of processing elements (PEs) in parallel computer
systems, along with the use of more extensive parallelization of algorithms
causes the interprocessor communications dominate VLSI chip space. This paper
proposes a new architecture to overcome this issue by using simple crosspoint
switches to pair PEs instead of a complex interconnection network. Based on the
cyclic permutation wiring idea described in \cite{oruc2016self}, this pairing
leads to a linear crosspoint array of $n(n-1)/2$ processing elements and as
many crosspoints. We demonstrate the versatility of this new parallel
architecture by designing fast searching and sorting algorithms for it. In
particular, we show that finding a minimum, maximum, and searching a list of
$n$ elements can all be performed in $O(1)$ time with elementary logic gates
with $O(n)$ fan-in, and in $O(\lg n)$ time with $O(1)$ fan-in. We further show
that sorting a list of $n$ elements can also be carried out in $O(1)$ time
using elementary logic gates with $O(n)$ fan-in and threshold logic gates. The
sorting time increases to $O(\lg n\lg\lg n)$ if only elementary logic gates
with $O(1)$ fan-in are used. The algorithm can find the maximum among $n$
elements in $O(1)$ time, and sort $n$ elements in $O(\lg n (\lg\lg n))$ time.
In addition, we show how other fundamental queries can be handled within the
same order of time complexities.
</summary>
    <author>
      <name>Taeyoung An</name>
    </author>
    <author>
      <name>A. Yavuz Oruc</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.11144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.11144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.02615">
    <id>http://arxiv.org/abs/2011.02615v3</id>
    <updated>2021-12-09T21:41:58Z</updated>
    <published>2020-11-05T02:09:04Z</published>
    <title>Competitive Data-Structure Dynamization</title>
    <summary>  Data-structure dynamization is a general approach for making static data
structures dynamic. It is used extensively in geometric settings and in the
guise of so-called merge (or compaction) policies in big-data databases such as
Google Bigtable and LevelDB (our focus). Previous theoretical work is based on
worst-case analyses for uniform inputs -- insertions of one item at a time and
constant read rate. In practice, merge policies must not only handle batch
insertions and varying read/write ratios, they can take advantage of such
non-uniformity to reduce cost on a per-input basis.
  To model this, we initiate the study of data-structure dynamization through
the lens of competitive analysis, via two new online set-cover problems. For
each, the input is a sequence of disjoint sets of weighted items. The sets are
revealed one at a time. The algorithm must respond to each with a set cover
that covers all items revealed so far. It obtains the cover incrementally from
the previous cover by adding one or more sets and optionally removing existing
sets. For each new set the algorithm incurs build cost equal to the weight of
the items in the set. In the first problem the objective is to minimize total
build cost plus total query cost, where the algorithm incurs a query cost at
each time $t$ equal to the current cover size. In the second problem, the
objective is to minimize the build cost while keeping the query cost from
exceeding $k$ (a given parameter) at any time. We give deterministic online
algorithms for both variants, with competitive ratios of $\Theta(\log^* n)$ and
$k$, respectively. The latter ratio is optimal for the second variant.
</summary>
    <author>
      <name>Claire Mathieu</name>
    </author>
    <author>
      <name>Rajmohan Rajaraman</name>
    </author>
    <author>
      <name>Neal E. Young</name>
    </author>
    <author>
      <name>Arman Yousefi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SODA 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.02615v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.02615v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W27, 68P15, 68R05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.2; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2011.00172">
    <id>http://arxiv.org/abs/2011.00172v1</id>
    <updated>2020-10-31T02:39:11Z</updated>
    <published>2020-10-31T02:39:11Z</published>
    <title>Generalized Sorting with Predictions</title>
    <summary>  Generalized sorting problem, also known as sorting with forbidden
comparisons, was first introduced by Huang et al. together with a randomized
algorithm which requires $\tilde O(n^{3/2})$ probes. We study this problem with
additional predictions for all pairs of allowed comparisons as input. We
propose a randomized algorithm which uses $O(n \log n+w)$ probes with high
probability and a deterministic algorithm which uses $O(nw)$ probes, where $w$
is the number of mistakes made by prediction.
</summary>
    <author>
      <name>Pinyan Lu</name>
    </author>
    <author>
      <name>Xuandi Ren</name>
    </author>
    <author>
      <name>Enze Sun</name>
    </author>
    <author>
      <name>Yubo Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in SOSA 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.00172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.00172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.14464">
    <id>http://arxiv.org/abs/2010.14464v1</id>
    <updated>2020-10-27T17:23:18Z</updated>
    <published>2020-10-27T17:23:18Z</published>
    <title>Dynamic Boundary Time Warping for Sub-sequence Matching with Few
  Examples</title>
    <summary>  The paper presents a novel method of finding a fragment in a long temporal
sequence similar to the set of shorter sequences. We are the first to propose
an algorithm for such a search that does not rely on computing the average
sequence from query examples. Instead, we use query examples as is, utilizing
all of them simultaneously. The introduced method based on the Dynamic Time
Warping (DTW) technique is suited explicitly for few-shot query-by-example
retrieval tasks. We evaluate it on two different few-shot problems from the
field of Natural Language Processing. The results show it either outperforms
baselines and previous approaches or achieves comparable results when a low
number of examples is available.
</summary>
    <author>
      <name>Łukasz Borchmann</name>
    </author>
    <author>
      <name>Dawid Jurkiewicz</name>
    </author>
    <author>
      <name>Filip Graliński</name>
    </author>
    <author>
      <name>Tomasz Górecki</name>
    </author>
    <link href="http://arxiv.org/abs/2010.14464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.14464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.13170">
    <id>http://arxiv.org/abs/2010.13170v3</id>
    <updated>2021-05-02T09:00:34Z</updated>
    <published>2020-10-25T17:35:05Z</published>
    <title>An Improved Sketching Algorithm for Edit Distance</title>
    <summary>  We provide improved upper bounds for the simultaneous sketching complexity of
edit distance. Consider two parties, Alice with input $x\in\Sigma^n$ and Bob
with input $y\in\Sigma^n$, that share public randomness and are given a promise
that the edit distance $\mathsf{ed}(x,y)$ between their two strings is at most
some given value $k$. Alice must send a message $sx$ and Bob must send $sy$ to
a third party Charlie, who does not know the inputs but shares the same public
randomness and also knows $k$. Charlie must output $\mathsf{ed}(x,y)$ precisely
as well as a sequence of $\mathsf{ed}(x,y)$ edits required to transform $x$
into $y$. The goal is to minimize the lengths $|sx|, |sy|$ of the messages
sent.
  The protocol of Belazzougui and Zhang (FOCS 2016), building upon the random
walk method of Chakraborty, Goldenberg, and Kouck\'y (STOC 2016), achieves a
maximum message length of $\tilde O(k^8)$ bits, where $\tilde O(\cdot)$ hides
$\mathrm{poly}(\log n)$ factors. In this work we build upon Belazzougui and
Zhang's protocol and provide an improved analysis demonstrating that a slight
modification of their construction achieves a bound of $\tilde O(k^3)$.
</summary>
    <author>
      <name>Ce Jin</name>
    </author>
    <author>
      <name>Jelani Nelson</name>
    </author>
    <author>
      <name>Kewen Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.STACS.2021.45</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.STACS.2021.45" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in STACS 2021. Fixed the title to match the conference
  version</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.13170v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.13170v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.09884">
    <id>http://arxiv.org/abs/2010.09884v2</id>
    <updated>2020-10-26T20:31:54Z</updated>
    <published>2020-10-15T13:17:44Z</published>
    <title>Sorting Short Keys in Circuits of Size o(n log n)</title>
    <summary>  We consider the classical problem of sorting an input array containing $n$
elements, where each element is described with a $k$-bit comparison-key and a
$w$-bit payload. A long-standing open problem is whether there exist $(k + w)
\cdot o(n \log n)$-sized boolean circuits for sorting. We show that one can
overcome the $n\log n$ barrier when the keys to be sorted are short.
Specifically, we prove that there is a circuit with $(k + w) \cdot O(n k) \cdot
\poly(\log^*n - \log^* (w + k))$ boolean gates capable of sorting any input
array containing $n$ elements, each described with a $k$-bit key and a $w$-bit
payload. Therefore, if the keys to be sorted are short, say, $k &lt; o(\log n)$,
our result is asymptotically better than the classical AKS sorting network
(ignoring $\poly\log^*$ terms); and we also overcome the $n \log n$ barrier in
such cases. Such a result might be surprising initially because it is long
known that comparator-based techniques must incur $\Omega(n \log n)$ comparator
gates even when the keys to be sorted are only $1$-bit long (e.g., see Knuth's
"Art of Programming" textbook). To the best of our knowledge, we are the first
to achieve non-trivial results for sorting circuits using non-comparison-based
techniques. We also show that if the Li-Li network coding conjecture is true,
our upper bound is optimal, barring $\poly\log^*$ terms, for every $k$ as long
as $k = O(\log n)$.
</summary>
    <author>
      <name>Gilad Asharov</name>
    </author>
    <author>
      <name>Wei-Kai Lin</name>
    </author>
    <author>
      <name>Elaine Shi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SODA'21</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.09884v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09884v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.00901">
    <id>http://arxiv.org/abs/1910.00901v1</id>
    <updated>2019-10-02T12:18:47Z</updated>
    <published>2019-10-02T12:18:47Z</published>
    <title>Sublinear Algorithms for Gap Edit Distance</title>
    <summary>  The edit distance is a way of quantifying how similar two strings are to one
another by counting the minimum number of character insertions, deletions, and
substitutions required to transform one string into the other. A simple dynamic
programming computes the edit distance between two strings of length $n$ in
$O(n^2)$ time, and a more sophisticated algorithm runs in time $O(n+t^2)$ when
the edit distance is $t$ [Landau, Myers and Schmidt, SICOMP 1998]. In pursuit
of obtaining faster running time, the last couple of decades have seen a flurry
of research on approximating edit distance, including polylogarithmic
approximation in near-linear time [Andoni, Krauthgamer and Onak, FOCS 2010],
and a constant-factor approximation in subquadratic time [Chakrabarty, Das,
Goldenberg, Kouck\'y and Saks, FOCS 2018].
  We study sublinear-time algorithms for small edit distance, which was
investigated extensively because of its numerous applications. Our main result
is an algorithm for distinguishing whether the edit distance is at most $t$ or
at least $t^2$ (the quadratic gap problem) in time
$\tilde{O}(\frac{n}{t}+t^3)$. This time bound is sublinear roughly for all $t$
in $[\omega(1), o(n^{1/3})]$, which was not known before. The best previous
algorithms solve this problem in sublinear time only for $t=\omega(n^{1/3})$
[Andoni and Onak, STOC 2009].
  Our algorithm is based on a new approach that adaptively switches between
uniform sampling and reading contiguous blocks of the input strings. In
contrast, all previous algorithms choose which coordinates to query
non-adaptively. Moreover, it can be extended to solve the $t$ vs
$t^{2-\epsilon}$ gap problem in time $\tilde{O}(\frac{n}{t^{1-\epsilon}}+t^3)$.
</summary>
    <author>
      <name>Elazar Goldenberg</name>
    </author>
    <author>
      <name>Robert Krauthgamer</name>
    </author>
    <author>
      <name>Barna Saha</name>
    </author>
    <link href="http://arxiv.org/abs/1910.00901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.00773">
    <id>http://arxiv.org/abs/1910.00773v2</id>
    <updated>2020-09-09T17:45:31Z</updated>
    <published>2019-10-02T04:34:11Z</published>
    <title>Approximating the Geometric Edit Distance</title>
    <summary>  Edit distance is a measurement of similarity between two sequences such as
strings, point sequences, or polygonal curves. Many matching problems from a
variety of areas, such as signal analysis, bioinformatics, etc., need to be
solved in a geometric space. Therefore, the geometric edit distance (GED) has
been studied. In this paper, we describe the first strictly sublinear
approximate near-linear time algorithm for computing the GED of two point
sequences in constant dimensional Euclidean space. Specifically, we present a
randomized (O(n\log^2n)) time (O(\sqrt n))-approximation algorithm. Then, we
generalize our result to give a randomized $\alpha$-approximation algorithm for
any $\alpha\in [\sqrt{\log n}, \sqrt{n / \log n}]$, running in time
$O(n^2/\alpha^2 \log n)$. Both algorithms are Monte Carlo and return
approximately optimal solutions with high probability.
</summary>
    <author>
      <name>Kyle Fox</name>
    </author>
    <author>
      <name>Xinyi Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, ISAAC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.00773v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00773v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.11600">
    <id>http://arxiv.org/abs/1909.11600v1</id>
    <updated>2019-09-25T16:35:43Z</updated>
    <published>2019-09-25T16:35:43Z</published>
    <title>A New Deterministic Algorithm for Dynamic Set Cover</title>
    <summary>  We present a deterministic dynamic algorithm for maintaining a
$(1+\epsilon)f$-approximate minimum cost set cover with
$O(f\log(Cn)/\epsilon^2)$ amortized update time, when the input set system is
undergoing element insertions and deletions. Here, $n$ denotes the number of
elements, each element appears in at most $f$ sets, and the cost of each set
lies in the range $[1/C, 1]$. Our result, together with that of Gupta et al.
[STOC`17], implies that there is a deterministic algorithm for this problem
with $O(f\log(Cn))$ amortized update time and $O(\min(\log n,
f))$-approximation ratio, which nearly matches the polynomial-time hardness of
approximation for minimum set cover in the static setting. Our update time is
only $O(\log (Cn))$ away from a trivial lower bound.
  Prior to our work, the previous best approximation ratio guaranteed by
deterministic algorithms was $O(f^2)$, which was due to Bhattacharya et al.
[ICALP`15]. In contrast, the only result that guaranteed $O(f)$-approximation
was obtained very recently by Abboud et al. [STOC`19], who designed a dynamic
algorithm with $(1+\epsilon)f$-approximation ratio and $O(f^2 \log n/\epsilon)$
amortized update time. Besides the extra $O(f)$ factor in the update time
compared to our and Gupta et al.'s results, the Abboud et al. algorithm is
randomized, and works only when the adversary is oblivious and the sets are
unweighted (each set has the same cost).
  We achieve our result via the primal-dual approach, by maintaining a
fractional packing solution as a dual certificate. Unlike previous primal-dual
algorithms that try to satisfy some local constraints for individual sets at
all time, our algorithm basically waits until the dual solution changes
significantly globally, and fixes the solution only where the fix is needed.
</summary>
    <author>
      <name>Sayan Bhattacharya</name>
    </author>
    <author>
      <name>Monika Henzinger</name>
    </author>
    <author>
      <name>Danupon Nanongkai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in FOCS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.11600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.11600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.02364">
    <id>http://arxiv.org/abs/1909.02364v5</id>
    <updated>2019-11-28T11:29:13Z</updated>
    <published>2019-09-05T12:47:54Z</published>
    <title>A Simple Reduction for Full-Permuted Pattern Matching Problems on
  Multi-Track Strings</title>
    <summary>  In this paper we study a variant of string pattern matching which deals with
tuples of strings known as \textit{multi-track strings}. Multi-track strings
are a generalisation of strings (or \textit{single-track strings}) that have
primarily found uses in problems related to searching multiple genomes and
music information retrieval. A multi-track string $\mathcal{T} = (t_1, t_2,
t_3, \ldots , t_N)$ of length $n$ and track count $N$ is a multi-set of $N$
strings of length $n$ with characters drawn from a common alphabet of size
$\sigma_U$. Given two multi-track strings $\mathcal{T} = (t_1, t_2, t_3, \ldots
, t_N)$ and $ \mathcal{P} = (p_1, p_2, p_3, \ldots , p_N)$ of length $n$ and
track count $N$, there is a \textit{full-permuted-match} between $\mathcal{P}$
and $\mathcal{T}$ if $t_{r_i} = p_i$ for all $i \in \{1,2,3,\ldots N \}$ and
some permutation $(r_1, r_2, r_3\ldots,r_N)$ of $(1, 2, 3,\ldots,N)$, we denote
this $\mathcal{P}\asymp\mathcal{T}$.
  Efficient algorithms for some full-permuted-match problems on multi-track
strings have recently been presented. In this paper we show a reduction from a
multi-track string of length $n$ and track count $N$ with alphabet size
$\sigma_U$, to a single-track string of length $2n-1$ with alphabet size
$\sigma_U^N$. Through this reduction we allow any string algorithm to be used
on multi-track string problems using $\asymp$ as the match relation. For
polynomial time algorithms on single-track strings of length $n$ there is a
multiplicative penalty of not more than $\mathcal{O}(N)$-time for the same
algorithm on mt-strings of length $n$ and track count $N$.
</summary>
    <author>
      <name>Carl Barton</name>
    </author>
    <author>
      <name>Ewan Birney</name>
    </author>
    <author>
      <name>Tomas Fitzgerald</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Basic error made in lemma on sorting suffixes</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.02364v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02364v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.02929">
    <id>http://arxiv.org/abs/1907.02929v2</id>
    <updated>2019-11-26T11:55:25Z</updated>
    <published>2019-07-05T16:52:40Z</published>
    <title>Improved local search for graph edit distance</title>
    <summary>  The graph edit distance (GED) measures the dissimilarity between two graphs
as the minimal cost of a sequence of elementary operations transforming one
graph into another. This measure is fundamental in many areas such as
structural pattern recognition or classification. However, exactly computing
GED is NP-hard. Among different classes of heuristic algorithms that were
proposed to compute approximate solutions, local search based algorithms
provide the tightest upper bounds for GED. In this paper, we present K-REFINE
and RANDPOST. K-REFINE generalizes and improves an existing local search
algorithm and performs particularly well on small graphs. RANDPOST is a general
warm start framework that stochastically generates promising initial solutions
to be used by any local search based GED algorithm. It is particularly
efficient on large graphs. An extensive empirical evaluation demonstrates that
both K-REFINE and RANDPOST perform excellently in practice.
</summary>
    <author>
      <name>Nicolas Boria</name>
    </author>
    <author>
      <name>David B. Blumenthal</name>
    </author>
    <author>
      <name>Sébastien Bougleux</name>
    </author>
    <author>
      <name>Luc Brun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patrec.2019.10.028</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patrec.2019.10.028" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition Letters 129, pages 19-25, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1907.02929v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02929v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05C60, 68W25, 65D15, 68T20" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1907.01032">
    <id>http://arxiv.org/abs/1907.01032v2</id>
    <updated>2019-07-20T07:35:46Z</updated>
    <published>2019-07-01T19:33:29Z</published>
    <title>On Slicing Sorted Integer Sequences</title>
    <summary>  Representing sorted integer sequences in small space is a central problem for
large-scale retrieval systems such as Web search engines. Efficient query
resolution, e.g., intersection or random access, is achieved by carefully
partitioning the sequences. In this work we describe and compare two different
partitioning paradigms: partitioning by cardinality and partitioning by
universe. Although the ideas behind such paradigms have been known in the
coding and algorithmic community since many years, inverted index compression
has extensively adopted the former paradigm, whereas the latter has received
only little attention. As a result, an experimental comparison between these
two is missing for the setting of inverted index compression. We also propose
and implement a solution that recursively slices the universe of representation
of a sequence to achieve compact storage and attain to fast query execution.
Albeit larger than some state-of-the-art representations, this slicing approach
substantially improves the performance of list intersections and unions while
operating in compressed space, thus offering an excellent space/time trade-off
for the problem.
</summary>
    <author>
      <name>Giulio Ermanno Pibiri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.01032v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01032v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.13011">
    <id>http://arxiv.org/abs/1905.13011v1</id>
    <updated>2019-05-29T17:56:55Z</updated>
    <published>2019-05-29T17:56:55Z</published>
    <title>Don't Persist All : Efficient Persistent Data Structures</title>
    <summary>  Data structures used in software development have inbuilt redundancy to
improve software reliability and to speed up performance. Examples include a
Doubly Linked List which allows a faster deletion due to the presence of the
previous pointer. With the introduction of Persistent Memory, storing the
redundant data fields into persistent memory adds a significant write overhead,
and reduces performance. In this work, we focus on three data structures -
Doubly Linked List, B+Tree and Hashmap, and showcase alternate partly
persistent implementations where we only store a limited set of data fields to
persistent memory. After a crash/restart, we use the persistent data fields to
recreate the data structures along with the redundant data fields. We compare
our implementation with the base implementation and show that we achieve
speedups around 5-20% for some data structures, and up to 165% for a
flush-dominated data structure.
</summary>
    <author>
      <name>Pratyush Mahapatra</name>
    </author>
    <author>
      <name>Mark D. Hill</name>
    </author>
    <author>
      <name>Michael M. Swift</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.13011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.13011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1905.03424">
    <id>http://arxiv.org/abs/1905.03424v7</id>
    <updated>2021-04-12T08:43:38Z</updated>
    <published>2019-05-09T03:08:32Z</published>
    <title>An invertible transform for efficient string matching in labeled
  digraphs</title>
    <summary>  Let $G = (V, E)$ be a digraph where each vertex is unlabeled, each edge is
labeled by a character in some alphabet $\Omega$, and any two edges with both
the same head and the same tail have different labels. The powerset
construction gives a transform of $G$ into a weakly connected digraph $G' =
(V', E')$ that enables solving the decision problem of whether there is a walk
in $G$ matching an arbitrarily long query string $q$ in time linear in $|q|$
and independent of $|E|$ and $|V|$. We show $G$ is uniquely determined by $G'$
when for every $v_\ell \in V$, there is some distinct string $s_\ell$ on
$\Omega$ such that $v_\ell$ is the origin of a closed walk in $G$ matching
$s_\ell$, and no other walk in $G$ matches $s_\ell$ unless it starts and ends
at $v_\ell$. We then exploit this invertibility condition to strategically
alter any $G$ so its transform $G'$ enables retrieval of all $t$ terminal
vertices of walks in the unaltered $G$ matching $q$ in $O(|q| + t \log |V|)$
time. We conclude by proposing two defining properties of a class of transforms
that includes the Burrows-Wheeler transform and the transform presented here.
</summary>
    <author>
      <name>Abhinav Nellore</name>
    </author>
    <author>
      <name>Austin Nguyen</name>
    </author>
    <author>
      <name>Reid F. Thompson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.CPM.2021.20</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.CPM.2021.20" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures; v7 is the content of the camera-ready copy for
  CPM 2021 incorporating reviewer feedback</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CPM 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.03424v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.03424v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1909.13541">
    <id>http://arxiv.org/abs/1909.13541v2</id>
    <updated>2020-02-25T14:23:54Z</updated>
    <published>2019-09-30T09:10:13Z</published>
    <title>An Average-Compress Algorithm for the Sample Mean Problem under Dynamic
  Time Warping</title>
    <summary>  Computing a sample mean of time series under dynamic time warping (DTW) is
NP-hard. Consequently, there is an ongoing research effort to devise efficient
heuristics. The majority of heuristics have been developed for the constrained
sample mean problem that assumes a solution of predefined length. In contrast,
research on the unconstrained sample mean problem is underdeveloped. In this
article, we propose a generic average-compress (AC) algorithm for solving the
unconstrained problem. The algorithm alternates between averaging (A-step) and
compression (C-step). The A-step takes an initial guess as input and returns an
approximation of a sample mean. Then the C-step reduces the length of the
approximate solution. The compressed approximation serves as initial guess of
the A-step in the next iteration. The purpose of the C-step is to direct the
algorithm to more promising solutions of shorter length. The proposed algorithm
is generic in the sense that any averaging and any compression method can be
used. Experimental results show that the AC algorithm substantially outperforms
current state-of-the-art algorithms for time series averaging.
</summary>
    <author>
      <name>Brijnesh Jain</name>
    </author>
    <author>
      <name>Vincent Froese</name>
    </author>
    <author>
      <name>David Schultz</name>
    </author>
    <link href="http://arxiv.org/abs/1909.13541v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.13541v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1904.08934">
    <id>http://arxiv.org/abs/1904.08934v1</id>
    <updated>2019-04-18T01:23:13Z</updated>
    <published>2019-04-18T01:23:13Z</published>
    <title>Convex Graph Invariant Relaxations For Graph Edit Distance</title>
    <summary>  The edit distance between two graphs is a widely used measure of similarity
that evaluates the smallest number of vertex and edge deletions/insertions
required to transform one graph to another. It is NP-hard to compute in
general, and a large number of heuristics have been proposed for approximating
this quantity. With few exceptions, these methods generally provide upper
bounds on the edit distance between two graphs. In this paper, we propose a new
family of computationally tractable convex relaxations for obtaining lower
bounds on graph edit distance. These relaxations can be tailored to the
structural properties of the particular graphs via convex graph invariants.
Specific examples that we highlight in this paper include constraints on the
graph spectrum as well as (tractable approximations of) the stability number
and the maximum-cut values of graphs. We prove under suitable conditions that
our relaxations are tight (i.e., exactly compute the graph edit distance) when
one of the graphs consists of few eigenvalues. We also validate the utility of
our framework on synthetic problems as well as real applications involving
molecular structure comparison problems in chemistry.
</summary>
    <author>
      <name>Utkan Onur Candogan</name>
    </author>
    <author>
      <name>Venkat Chandrasekaran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.08934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C25, 90C22, 90C90, 90C35" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1802.07375">
    <id>http://arxiv.org/abs/1802.07375v3</id>
    <updated>2018-03-04T02:55:57Z</updated>
    <published>2018-02-20T23:33:23Z</published>
    <title>Periodicity in Data Streams with Wildcards</title>
    <summary>  We investigate the problem of detecting periodic trends within a string $S$
of length $n$, arriving in the streaming model, containing at most $k$ wildcard
characters, where $k=o(n)$. A wildcard character is a special character that
can be assigned any other character. We say $S$ has wildcard-period $p$ if
there exists an assignment to each of the wildcard characters so that in the
resulting stream the length $n-p$ prefix equals the length $n-p$ suffix. We
present a two-pass streaming algorithm that computes wildcard-periods of $S$
using $\mathcal{O}(k^3\,\mathsf{polylog}\,n)$ bits of space, while we also show
that this problem cannot be solved in sublinear space in one pass. We then give
a one-pass randomized streaming algorithm that computes all wildcard-periods
$p$ of $S$ with $p&lt;\frac{n}{2}$ and no wildcard characters appearing in the
last $p$ symbols of $S$, using $\mathcal{O}(k^3\mathsf{polylog}\, n)$ space.
</summary>
    <author>
      <name>Funda Ergün</name>
    </author>
    <author>
      <name>Elena Grigorescu</name>
    </author>
    <author>
      <name>Erfan Sadeqi Azer</name>
    </author>
    <author>
      <name>Samson Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at CSR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07375v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07375v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1802.05471">
    <id>http://arxiv.org/abs/1802.05471v2</id>
    <updated>2018-12-29T22:03:58Z</updated>
    <published>2018-02-15T10:45:34Z</published>
    <title>Smooth heaps and a dual view of self-adjusting data structures</title>
    <summary>  We present a new connection between self-adjusting binary search trees (BSTs)
and heaps, two fundamental, extensively studied, and practically relevant
families of data structures. Roughly speaking, we map an arbitrary heap
algorithm within a natural model, to a corresponding BST algorithm with the
same cost on a dual sequence of operations (i.e. the same sequence with the
roles of time and key-space switched). This is the first general transformation
between the two families of data structures.
  There is a rich theory of dynamic optimality for BSTs (i.e. the theory of
competitiveness between BST algorithms). The lack of an analogous theory for
heaps has been noted in the literature. Through our connection, we transfer all
instance-specific lower bounds known for BSTs to a general model of heaps,
initiating a theory of dynamic optimality for heaps.
  On the algorithmic side, we obtain a new, simple and efficient heap
algorithm, which we call the smooth heap. We show the smooth heap to be the
heap-counterpart of Greedy, the BST algorithm with the strongest proven and
conjectured properties from the literature, widely believed to be
instance-optimal. Assuming the optimality of Greedy, the smooth heap is also
optimal within our model of heap algorithms. As corollaries of results known
for Greedy, we obtain instance-specific upper bounds for the smooth heap, with
applications in adaptive sorting.
  Intriguingly, the smooth heap, although derived from a non-practical BST
algorithm, is simple and easy to implement (e.g. it stores no auxiliary data
besides the keys and tree pointers). It can be seen as a variation on the
popular pairing heap data structure, extending it with a "power-of-two-choices"
type of heuristic.
</summary>
    <author>
      <name>László Kozma</name>
    </author>
    <author>
      <name>Thatchaphol Saranurak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at STOC 2018, light revision, additional figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05471v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05471v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1802.05490">
    <id>http://arxiv.org/abs/1802.05490v1</id>
    <updated>2018-02-15T11:34:51Z</updated>
    <published>2018-02-15T11:34:51Z</published>
    <title>Grammar-based Compression of Unranked Trees</title>
    <summary>  We introduce forest straight-line programs (FSLPs) as a compressed
representation of unranked ordered node-labelled trees. FSLPs are based on the
operations of forest algebra and generalize tree straight-line programs. We
compare the succinctness of FSLPs with two other compression schemes for
unranked trees: top dags and tree straight-line programs of first-child/next
sibling encodings. Efficient translations between these formalisms are
provided. Finally, we show that equality of unranked trees in the setting where
certain symbols are associative or commutative can be tested in polynomial
time. This generalizes previous results for testing isomorphism of compressed
unordered ranked trees.
</summary>
    <author>
      <name>Adrià Gascón</name>
    </author>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Sebastian Maneth</name>
    </author>
    <author>
      <name>Carl Philipp Reh</name>
    </author>
    <author>
      <name>Kurt Sieber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper at CSR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P30, 68Q42" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1801.09431">
    <id>http://arxiv.org/abs/1801.09431v1</id>
    <updated>2018-01-29T10:22:20Z</updated>
    <published>2018-01-29T10:22:20Z</published>
    <title>Generalized Leapfrogging Samplesort: A Class of $O(n \log^2 n)$
  Worst-Case Complexity and $O(n \log n)$ Average-Case Complexity Sorting
  Algorithms</title>
    <summary>  The original Leapfrogging Samplesort operates on a sorted sample of size $s$
and an unsorted part of size $s+1$. We generalize this to a sorted sample of
size $s$ and an unsorted part of size $(2^k-1)(s+1)$, where $k = O(1)$. We
present a practical implementation of this class of algorithms and we show that
the worst-case complexity is $O(n \log^2 n)$ and the average-case complexity is
$O(n \log n)$.
  Keywords: Samplesort, Quicksort, Leapfrogging Samplesort, sorting, analysis
of algorithms.
</summary>
    <author>
      <name>Eliezer A. Albacea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09431v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09431v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1801.09159">
    <id>http://arxiv.org/abs/1801.09159v2</id>
    <updated>2018-05-02T22:08:09Z</updated>
    <published>2018-01-28T00:43:49Z</published>
    <title>Faster Approximate(d) Text-to-Pattern L1 Distance</title>
    <summary>  The problem of finding \emph{distance} between \emph{pattern} of length $m$
and \emph{text} of length $n$ is a typical way of generalizing pattern matching
to incorporate dissimilarity score. For both Hamming and $L_1$ distances only a
super linear upper bound $\widetilde{O}(n\sqrt{m})$ are known, which prompts
the question of relaxing the problem: either by asking for $(1 \pm
\varepsilon)$ approximate distance (every distance is reported up to a
multiplicative factor), or $k$-approximated distance (distances exceeding $k$
are reported as $\infty$). We focus on $L_1$ distance, for which we show new
algorithms achieving complexities respectively $\widetilde{O}(\varepsilon^{-1}
n)$ and $\widetilde{O}((m+k\sqrt{m}) \cdot n/m)$. This is a significant
improvement upon previous algorithms with runtime
$\widetilde{O}(\varepsilon^{-2} n)$ of Lipsky and Porat [Algorithmica 2011] and
$\widetilde{O}(n\sqrt{k})$ of Amir, Lipsky, Porat and Umanski [CPM 2005].
</summary>
    <author>
      <name>Przemysław Uznański</name>
    </author>
    <link href="http://arxiv.org/abs/1801.09159v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09159v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1801.04641">
    <id>http://arxiv.org/abs/1801.04641v4</id>
    <updated>2019-02-09T19:47:26Z</updated>
    <published>2018-01-15T02:16:39Z</published>
    <title>Strategies for Stable Merge Sorting</title>
    <summary>  We introduce new stable natural merge sort algorithms, called $2$-merge sort
and $\alpha$-merge sort. We prove upper and lower bounds for several merge sort
algorithms, including Timsort, Shivers' sort, $\alpha$-stack sorts, and our new
$2$-merge and $\alpha$-merge sorts. The upper and lower bounds have the forms
$c \cdot n \log m$ and $c \cdot n \log n$ for inputs of length~$n$ comprising
$m$~monotone runs. For Timsort, we prove a lower bound of $(1.5 - o(1)) n \log
n$. For $2$-merge sort, we prove optimal upper and lower bounds of
approximately $(1.089 \pm o(1))n \log m$. We prove similar asymptotically
matching upper and lower bounds for $\alpha$-merge sort, when $\varphi &lt; \alpha
&lt; 2$, where $\varphi$ is the golden ratio.
  Our bounds are in terms of merge cost; this upper bounds the number of
comparisons and accurately models runtime. The merge strategies can be used for
any stable merge sort, not just natural merge sorts. The new $2$-merge and
$\alpha$-merge sorts have better worst-case merge cost upper bounds and are
slightly simpler to implement than the widely-used Timsort; they also perform
better in experiments. We report also experimental comparisons with algorithms
developed by Munro-Wild and Jug\'e subsequently to the results of the present
paper.
</summary>
    <author>
      <name>Sam Buss</name>
    </author>
    <author>
      <name>Alexander Knop</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.04641v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04641v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P10 (Primary) 68W40 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1912.01668">
    <id>http://arxiv.org/abs/1912.01668v1</id>
    <updated>2019-12-03T20:10:31Z</updated>
    <published>2019-12-03T20:10:31Z</published>
    <title>Learning Multi-dimensional Indexes</title>
    <summary>  Scanning and filtering over multi-dimensional tables are key operations in
modern analytical database engines. To optimize the performance of these
operations, databases often create clustered indexes over a single dimension or
multi-dimensional indexes such as R-trees, or use complex sort orders (e.g.,
Z-ordering). However, these schemes are often hard to tune and their
performance is inconsistent across different datasets and queries. In this
paper, we introduce Flood, a multi-dimensional in-memory index that
automatically adapts itself to a particular dataset and workload by jointly
optimizing the index structure and data storage. Flood achieves up to three
orders of magnitude faster performance for range scans with predicates than
state-of-the-art multi-dimensional indexes or sort orders on real-world
datasets and workloads. Our work serves as a building block towards an
end-to-end learned database system.
</summary>
    <author>
      <name>Vikram Nathan</name>
    </author>
    <author>
      <name>Jialin Ding</name>
    </author>
    <author>
      <name>Mohammad Alizadeh</name>
    </author>
    <author>
      <name>Tim Kraska</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3318464.3380579</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3318464.3380579" rel="related"/>
    <link href="http://arxiv.org/abs/1912.01668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.01668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1911.13014">
    <id>http://arxiv.org/abs/1911.13014v1</id>
    <updated>2019-11-29T09:35:04Z</updated>
    <published>2019-11-29T09:35:04Z</published>
    <title>SOSD: A Benchmark for Learned Indexes</title>
    <summary>  A groundswell of recent work has focused on improving data management systems
with learned components. Specifically, work on learned index structures has
proposed replacing traditional index structures, such as B-trees, with learned
models. Given the decades of research committed to improving index structures,
there is significant skepticism about whether learned indexes actually
outperform state-of-the-art implementations of traditional structures on
real-world data. To answer this question, we propose a new benchmarking
framework that comes with a variety of real-world datasets and baseline
implementations to compare against. We also show preliminary results for
selected index structures, and find that learned models indeed often outperform
state-of-the-art implementations, and are therefore a promising direction for
future research.
</summary>
    <author>
      <name>Andreas Kipf</name>
    </author>
    <author>
      <name>Ryan Marcus</name>
    </author>
    <author>
      <name>Alexander van Renen</name>
    </author>
    <author>
      <name>Mihail Stoian</name>
    </author>
    <author>
      <name>Alfons Kemper</name>
    </author>
    <author>
      <name>Tim Kraska</name>
    </author>
    <author>
      <name>Thomas Neumann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2019 Workshop on Machine Learning for Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.13014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.13014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.13971">
    <id>http://arxiv.org/abs/1910.13971v3</id>
    <updated>2021-05-10T20:54:17Z</updated>
    <published>2019-10-30T16:38:33Z</published>
    <title>Superset Technique for Approximate Recovery in One-Bit Compressed
  Sensing</title>
    <summary>  One-bit compressed sensing (1bCS) is a method of signal acquisition under
extreme measurement quantization that gives important insights on the limits of
signal compression and analog-to-digital conversion. The setting is also
equivalent to the problem of learning a sparse hyperplane-classifier. In this
paper, we propose a novel approach for signal recovery in nonadaptive 1bCS that
matches the sample complexity of the current best methods. We construct 1bCS
matrices that are universal - i.e. work for all signals under a model - and at
the same time recover very general random sparse signals with high probability.
In our approach, we divide the set of samples (measurements) into two parts,
and use the first part to recover the superset of the support of a sparse
vector. The second set of measurements is then used to approximate the signal
within the superset. While support recovery in 1bCS is well-studied, recovery
of superset of the support requires fewer samples, and to our knowledge has not
been previously considered for the purpose of approximate recovery of signals.
</summary>
    <author>
      <name>Larkin Flodin</name>
    </author>
    <author>
      <name>Venkata Gandikota</name>
    </author>
    <author>
      <name>Arya Mazumdar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 2 figures. Number of measurements needed in Theorem 4 and
  Corollaries 5-6 were incorrect in previous versions, and are now corrected.
  More details are discussed in Remarks 1 and 2</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Advances in Neural Information Processing Systems, pp.
  10387-10396. 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.13971v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.13971v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1910.01749">
    <id>http://arxiv.org/abs/1910.01749v1</id>
    <updated>2019-10-03T22:27:56Z</updated>
    <published>2019-10-03T22:27:56Z</published>
    <title>Finding monotone patterns in sublinear time</title>
    <summary>  We study the problem of finding monotone subsequences in an array from the
viewpoint of sublinear algorithms. For fixed $k \in \mathbb{N}$ and
$\varepsilon > 0$, we show that the non-adaptive query complexity of finding a
length-$k$ monotone subsequence of $f \colon [n] \to \mathbb{R}$, assuming that
$f$ is $\varepsilon$-far from free of such subsequences, is $\Theta((\log
n)^{\lfloor \log_2 k \rfloor})$. Prior to our work, the best algorithm for this
problem, due to Newman, Rabinovich, Rajendraprasad, and Sohler (2017), made
$(\log n)^{O(k^2)}$ non-adaptive queries; and the only lower bound known, of
$\Omega(\log n)$ queries for the case $k = 2$, followed from that on testing
monotonicity due to Erg\"un, Kannan, Kumar, Rubinfeld, and Viswanathan (2000)
and Fischer (2004).
</summary>
    <author>
      <name>Omri Ben-Eliezer</name>
    </author>
    <author>
      <name>Clément L. Canonne</name>
    </author>
    <author>
      <name>Shoham Letzter</name>
    </author>
    <author>
      <name>Erik Waingarten</name>
    </author>
    <link href="http://arxiv.org/abs/1910.01749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.01749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1702.03657">
    <id>http://arxiv.org/abs/1702.03657v1</id>
    <updated>2017-02-13T07:18:13Z</updated>
    <published>2017-02-13T07:18:13Z</published>
    <title>Trie Compression for GPU Accelerated Multi-Pattern Matching</title>
    <summary>  Graphics Processing Units allow for running massively parallel applications
offloading the CPU from computationally intensive resources, however GPUs have
a limited amount of memory. In this paper a trie compression algorithm for
massively parallel pattern matching is presented demonstrating 85% less space
requirements than the original highly efficient parallel failure-less
aho-corasick, whilst demonstrating over 22 Gbps throughput. The algorithm
presented takes advantage of compressed row storage matrices as well as shared
and texture memory on the GPU.
</summary>
    <author>
      <name>Xavier Bellekens</name>
    </author>
    <author>
      <name>Amar Seeam</name>
    </author>
    <author>
      <name>Christos Tachtatzis</name>
    </author>
    <author>
      <name>Robert Atkinson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures. Accepted and Published in The Ninth International
  Conferences on Pervasive Patterns and Applications PATTERNS 2017 (19 - 23/02,
  2017 - Athens, Greece)</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.03657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1702.02405">
    <id>http://arxiv.org/abs/1702.02405v2</id>
    <updated>2017-05-30T04:04:25Z</updated>
    <published>2017-02-08T12:46:12Z</published>
    <title>A Family of Approximation Algorithms for the Maximum Duo-Preservation
  String Mapping Problem</title>
    <summary>  In the Maximum Duo-Preservation String Mapping problem we are given two
strings and wish to map the letters of the former to the letters of the latter
so as to maximise the number of duos. A duo is a pair of consecutive letters
that is mapped to a pair of consecutive letters in the same order. This is
complementary to the well-studied Minimum Common String Partition problem,
where the goal is to partition the former string into blocks that can be
permuted and concatenated to obtain the latter string.
  Maximum Duo-Preservation String Mapping is APX-hard. After a series of
improvements, Brubach [WABI 2016] showed a polynomial-time $3.25$-approximation
algorithm. Our main contribution is that for any $\epsilon>0$ there exists a
polynomial-time $(2+\epsilon)$-approximation algorithm. Similarly to a previous
solution by Boria et al. [CPM 2016], our algorithm uses the local search
technique. However, this is used only after a certain preliminary greedy
procedure, which gives us more structure and makes a more general local search
possible. We complement this with a specialised version of the algorithm that
achieves $2.67$-approximation in quadratic time.
</summary>
    <author>
      <name>Bartłomiej Dudek</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Piotr Ostropolski-Nalewaja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in CPM 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.02405v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02405v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1702.02321">
    <id>http://arxiv.org/abs/1702.02321v2</id>
    <updated>2017-04-18T01:39:24Z</updated>
    <published>2017-02-08T08:28:46Z</published>
    <title>Position Heaps for Parameterized Strings</title>
    <summary>  We propose a new indexing structure for parameterized strings, called
parameterized position heap. Parameterized position heap is applicable for
parameterized pattern matching problem, where the pattern matches a substring
of the text if there exists a bijective mapping from the symbols of the pattern
to the symbols of the substring. We propose an online construction algorithm of
parameterized position heap of a text and show that our algorithm runs in
linear time with respect to the text size. We also show that by using
parameterized position heap, we can find all occurrences of a pattern in the
text in linear time with respect to the product of the pattern size and the
alphabet size.
</summary>
    <author>
      <name> Diptarama</name>
    </author>
    <author>
      <name>Takashi Katsura</name>
    </author>
    <author>
      <name>Yuhei Otomo</name>
    </author>
    <author>
      <name>Kazuyuki Narisawa</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures, accepted to CPM 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.02321v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02321v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1702.01877">
    <id>http://arxiv.org/abs/1702.01877v1</id>
    <updated>2017-02-07T04:40:50Z</updated>
    <published>2017-02-07T04:40:50Z</published>
    <title>A local search 2.917-approximation algorithm for duo-preservation string
  mapping</title>
    <summary>  We study the {\em maximum duo-preservation string mapping} ({\sc Max-Duo})
problem, which is the complement of the well studied {\em minimum common string
partition} ({\sc MCSP}) problem. Both problems have applications in many fields
including text compression and bioinformatics. Motivated by an earlier local
search algorithm, we present an improved approximation and show that its
performance ratio is no greater than ${35}/{12} &lt; 2.917$. This beats the
current best $3.25$-approximation for {\sc Max-Duo}. The performance analysis
of our algorithm is done through a complex yet interesting amortization. Two
lower bounds on the locality gap of our algorithm are also provided.
</summary>
    <author>
      <name>Yao Xu</name>
    </author>
    <author>
      <name>Yong Chen</name>
    </author>
    <author>
      <name>Taibo Luo</name>
    </author>
    <author>
      <name>Guohui Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 33 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.01877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1702.01284">
    <id>http://arxiv.org/abs/1702.01284v2</id>
    <updated>2017-02-23T20:30:22Z</updated>
    <published>2017-02-04T13:25:05Z</published>
    <title>New cardinality estimation algorithms for HyperLogLog sketches</title>
    <summary>  This paper presents new methods to estimate the cardinalities of data sets
recorded by HyperLogLog sketches. A theoretically motivated extension to the
original estimator is presented that eliminates the bias for small and large
cardinalities. Based on the maximum likelihood principle a second unbiased
method is derived together with a robust and efficient numerical algorithm to
calculate the estimate. The maximum likelihood approach can also be applied to
more than a single HyperLogLog sketch. In particular, it is shown that it gives
more precise cardinality estimates for union, intersection, or relative
complements of two sets that are both represented by HyperLogLog sketches
compared to the conventional technique using the inclusion-exclusion principle.
All the new methods are demonstrated and verified by extensive simulations.
</summary>
    <author>
      <name>Otmar Ertl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected typos</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.01284v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01284v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W15, 68W25, 62-07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; I.1.2; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1811.06127">
    <id>http://arxiv.org/abs/1811.06127v2</id>
    <updated>2019-09-03T00:48:06Z</updated>
    <published>2018-11-15T00:40:26Z</published>
    <title>Vectorized Character Counting for Faster Pattern Matching</title>
    <summary>  Many modern sequence alignment tools implement fast string matching using the
space efficient data structure called FM-index. The succinct nature of this
data structure presents unique challenges for the algorithm designers. In this
paper, we explore the opportunities for parallelization of the exact and
inexact matches and present an efficient SIMD solution for the Occ portion of
the algorithm. Our implementation computes all eight Occ values required for
the inexact match algorithm step in a single pass. We showcase the algorithm
performance in a multi-core genome aligner and discuss effects of the memory
prefetch.
</summary>
    <author>
      <name>Roman Snytsar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5220/0007258201490154</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5220/0007258201490154" rel="related"/>
    <link href="http://arxiv.org/abs/1811.06127v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06127v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1808.00643">
    <id>http://arxiv.org/abs/1808.00643v1</id>
    <updated>2018-08-02T02:47:14Z</updated>
    <published>2018-08-02T02:47:14Z</published>
    <title>On the tails of the limiting QuickSort density</title>
    <summary>  We give upper and lower asymptotic bounds for the left tail and for the right
tail of the continuous limiting QuickSort density f that are nearly matching in
each tail. The bounds strengthen results from a paper of Svante Janson (2015)
concerning the corresponding distribution function F. Furthermore, we obtain
similar bounds on absolute values of derivatives of f of each order.
</summary>
    <author>
      <name>James Allen Fill</name>
    </author>
    <author>
      <name>Wei-Chun Hung</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary: 68P10, Secondary: 60E05, 60C05" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.02807">
    <id>http://arxiv.org/abs/1803.02807v1</id>
    <updated>2018-03-07T18:34:11Z</updated>
    <published>2018-03-07T18:34:11Z</published>
    <title>Flexible and Efficient Algorithms for Abelian Matching in Strings</title>
    <summary>  The abelian pattern matching problem consists in finding all substrings of a
text which are permutations of a given pattern. This problem finds application
in many areas and can be solved in linear time by a naive sliding window
approach. In this short communication we present a new class of algorithms
based on a new efficient fingerprint computation approach, called
Heap-Counting, which turns out to be fast, flexible and easy to be implemented.
It can be proved that our solutions have a linear worst case time complexity
and, in addition, we present an extensive experimental evaluation which shows
that our newly presented algorithms are among the most efficient and flexible
solutions in practice for the abelian matching problem in strings.
</summary>
    <author>
      <name>Simone Faro</name>
    </author>
    <author>
      <name>Arianna Pavone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a short preliminary version of a full paper submitted to an
  international journal. Most examples, details, lemmas and theorems have been
  omitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.02807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.02807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1803.00938">
    <id>http://arxiv.org/abs/1803.00938v1</id>
    <updated>2018-03-02T16:26:13Z</updated>
    <published>2018-03-02T16:26:13Z</published>
    <title>Multivariate Fine-Grained Complexity of Longest Common Subsequence</title>
    <summary>  We revisit the classic combinatorial pattern matching problem of finding a
longest common subsequence (LCS). For strings $x$ and $y$ of length $n$, a
textbook algorithm solves LCS in time $O(n^2)$, but although much effort has
been spent, no $O(n^{2-\varepsilon})$-time algorithm is known. Recent work
indeed shows that such an algorithm would refute the Strong Exponential Time
Hypothesis (SETH) [Abboud, Backurs, Vassilevska Williams + Bringmann,
K\"unnemann FOCS'15].
  Despite the quadratic-time barrier, for over 40 years an enduring scientific
interest continued to produce fast algorithms for LCS and its variations.
Particular attention was put into identifying and exploiting input parameters
that yield strongly subquadratic time algorithms for special cases of interest,
e.g., differential file comparison. This line of research was successfully
pursued until 1990, at which time significant improvements came to a halt. In
this paper, using the lens of fine-grained complexity, our goal is to (1)
justify the lack of further improvements and (2) determine whether some special
cases of LCS admit faster algorithms than currently known.
  To this end, we provide a systematic study of the multivariate complexity of
LCS, taking into account all parameters previously discussed in the literature:
the input size $n:=\max\{|x|,|y|\}$, the length of the shorter string
$m:=\min\{|x|,|y|\}$, the length $L$ of an LCS of $x$ and $y$, the numbers of
deletions $\delta := m-L$ and $\Delta := n-L$, the alphabet size, as well as
the numbers of matching pairs $M$ and dominant pairs $d$. For any class of
instances defined by fixing each parameter individually to a polynomial in
terms of the input size, we prove a SETH-based lower bound matching one of
three known algorithms. Specifically, we determine the optimal running time for
LCS under SETH as $(n+\min\{d, \delta \Delta, \delta m\})^{1\pm o(1)}$.
  [...]
</summary>
    <author>
      <name>Karl Bringmann</name>
    </author>
    <author>
      <name>Marvin Künnemann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/1.9781611975031.79</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/1.9781611975031.79" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at SODA'18. Full Version. 66 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.00938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1802.08663">
    <id>http://arxiv.org/abs/1802.08663v1</id>
    <updated>2018-02-23T18:09:14Z</updated>
    <published>2018-02-23T18:09:14Z</published>
    <title>Synchronization Strings: List Decoding for Insertions and Deletions</title>
    <summary>  We study codes that are list-decodable under insertions and deletions.
Specifically, we consider the setting where a codeword over some finite
alphabet of size $q$ may suffer from $\delta$ fraction of adversarial deletions
and $\gamma$ fraction of adversarial insertions. A code is said to be
$L$-list-decodable if there is an (efficient) algorithm that, given a received
word, reports a list of $L$ codewords that include the original codeword.
  Using the concept of synchronization strings, introduced by the first two
authors [STOC 2017], we show some surprising results. We show that for every
$0\leq\delta&lt;1$, every $0\leq\gamma&lt;\infty$ and every $\epsilon>0$ there exist
efficient codes of rate $1-\delta-\epsilon$ and constant alphabet (so
$q=O_{\delta,\gamma,\epsilon}(1)$) and sub-logarithmic list sizes. We stress
that the fraction of insertions can be arbitrarily large and the rate is
independent of this parameter. Our result sheds light on the remarkable
asymmetry between the impact of insertions and deletions from the point of view
of error-correction: Whereas deletions cost in the rate of the code, insertion
costs are borne by the adversary and not the code!
  We also prove several tight bounds on the parameters of list-decodable insdel
codes. In particular, we show that the alphabet size of insdel codes needs to
be exponentially large in $\epsilon^{-1}$, where $\epsilon$ is the gap to
capacity above. Our result even applies to settings where the unique-decoding
capacity equals the list-decoding capacity and when it does so, it shows that
the alphabet size needs to be exponentially large in the gap to capacity. This
is sharp contrast to the Hamming error model where alphabet size polynomial in
$\epsilon^{-1}$ suffices for unique decoding and also shows that the
exponential dependence on the alphabet size in previous works that constructed
insdel codes is actually necessary!
</summary>
    <author>
      <name>Bernhard Haeupler</name>
    </author>
    <author>
      <name>Amirbehshad Shahrasbi</name>
    </author>
    <author>
      <name>Madhu Sudan</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1704.05254">
    <id>http://arxiv.org/abs/1704.05254v1</id>
    <updated>2017-04-18T09:49:45Z</updated>
    <published>2017-04-18T09:49:45Z</published>
    <title>Grammar-Based Graph Compression</title>
    <summary>  We present a new graph compressor that works by recursively detecting
repeated substructures and representing them through grammar rules. We show
that for a large number of graphs the compressor obtains smaller
representations than other approaches. Specific queries such as reachability
between two nodes or regular path queries can be evaluated in linear time (or
quadratic times, respectively), over the grammar, thus allowing speed-ups
proportional to the compression ratio.
</summary>
    <author>
      <name>Sebastian Maneth</name>
    </author>
    <author>
      <name>Fabian Peternek</name>
    </author>
    <link href="http://arxiv.org/abs/1704.05254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1704.04615">
    <id>http://arxiv.org/abs/1704.04615v2</id>
    <updated>2017-04-18T12:45:50Z</updated>
    <published>2017-04-15T09:58:02Z</published>
    <title>FMtree: A fast locating algorithm of FM-indexes for genomic data</title>
    <summary>  Motivation: As a fundamental task in bioinformatics, searching for massive
short patterns over a long text is widely accelerated by various compressed
full-text indexes. These indexes are able to provide similar searching
functionalities to classical indexes, e.g., suffix trees and suffix arrays,
while requiring less space. For genomic data, a well-known family of compressed
full-text index, called FM-indexes, presents unmatched performance in practice.
One major drawback of FM-indexes is that their locating operations, which
report all occurrence positions of patterns in a given text, are particularly
slow, especially for the patterns with many occurrences.
  Results: In this paper, we introduce a novel locating algorithm, FMtree, to
fast retrieve all occurrence positions of any pattern via FM-indexes. When
searching for a pattern over a given text, FMtree organizes the search space of
the locating operation into a conceptual quadtree. As a result, multiple
occurrence positions of this pattern can be retrieved simultaneously by
traversing the quadtree. Compared with the existing locating algorithms, our
tree-based algorithm reduces large numbers of redundant operations and presents
better data locality. Experimental results show that FMtree is usually one
order of magnitude faster than the state-of-the-art algorithms, and still
memory-efficient.
</summary>
    <author>
      <name>Haoyu Cheng</name>
    </author>
    <author>
      <name>Ming Wu</name>
    </author>
    <author>
      <name>Yun Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1704.04615v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04615v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1704.04472">
    <id>http://arxiv.org/abs/1704.04472v2</id>
    <updated>2018-12-17T12:13:57Z</updated>
    <published>2017-04-14T16:34:08Z</published>
    <title>Maximal Unbordered Factors of Random Strings</title>
    <summary>  A border of a string is a non-empty prefix of the string that is also a
suffix of the string, and a string is unbordered if it has no border other than
itself. Loptev, Kucherov, and Starikovskaya [CPM 2015] conjectured the
following: If we pick a string of length $n$ from a fixed non-unary alphabet
uniformly at random, then the expected maximum length of its unbordered factors
is $n - O(1)$. We confirm this conjecture by proving that the expected value
is, in fact, ${n - \Theta(\sigma^{-1})}$, where $\sigma$ is the size of the
alphabet. This immediately implies that we can find such a maximal unbordered
factor in linear time on average. However, we go further and show that the
optimum average-case running time is in $\Omega (\sqrt{n}) \cap O (\sqrt{n
\log_\sigma n})$ due to analogous bounds by Czumaj and G\k{a}sieniec [CPM 2000]
for the problem of computing the shortest period of a uniformly random string.
</summary>
    <author>
      <name>Patrick Hagge Cording</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Mathias Bæk Tejs Knudsen</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-46049-9_9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-46049-9_9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary version with weaker results was presented at the 23rd
  Symposium on String Processing and Information Retrieval (SPIRE '16)</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.04472v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04472v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1704.04205">
    <id>http://arxiv.org/abs/1704.04205v1</id>
    <updated>2017-04-13T16:36:44Z</updated>
    <published>2017-04-13T16:36:44Z</published>
    <title>Hybridizing Non-dominated Sorting Algorithms: Divide-and-Conquer Meets
  Best Order Sort</title>
    <summary>  Many production-grade algorithms benefit from combining an asymptotically
efficient algorithm for solving big problem instances, by splitting them into
smaller ones, and an asymptotically inefficient algorithm with a very small
implementation constant for solving small subproblems. A well-known example is
stable sorting, where mergesort is often combined with insertion sort to
achieve a constant but noticeable speed-up.
  We apply this idea to non-dominated sorting. Namely, we combine the
divide-and-conquer algorithm, which has the currently best known asymptotic
runtime of $O(N (\log N)^{M - 1})$, with the Best Order Sort algorithm, which
has the runtime of $O(N^2 M)$ but demonstrates the best practical performance
out of quadratic algorithms.
  Empirical evaluation shows that the hybrid's running time is typically not
worse than of both original algorithms, while for large numbers of points it
outperforms them by at least 20%. For smaller numbers of objectives, the
speedup can be as large as four times.
</summary>
    <author>
      <name>Margarita Markina</name>
    </author>
    <author>
      <name>Maxim Buzdalov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3067695.3076074</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3067695.3076074" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A two-page abstract of this paper will appear in the proceedings
  companion of the 2017 Genetic and Evolutionary Computation Conference (GECCO
  2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.04205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1704.01646">
    <id>http://arxiv.org/abs/1704.01646v1</id>
    <updated>2017-04-05T20:30:27Z</updated>
    <published>2017-04-05T20:30:27Z</published>
    <title>Streaming Pattern Matching with d Wildcards</title>
    <summary>  In the pattern matching with $d$ wildcards problem one is given a text $T$ of
length $n$ and a pattern $P$ of length $m$ that contains $d$ wildcard
characters, each denoted by a special symbol $'?'$. A wildcard character
matches any other character. The goal is to establish for each $m$-length
substring of $T$ whether it matches $P$. In the streaming model variant of the
pattern matching with $d$ wildcards problem the text $T$ arrives one character
at a time and the goal is to report, before the next character arrives, if the
last $m$ characters match $P$ while using only $o(m)$ words of space.
  In this paper we introduce two new algorithms for the $d$ wildcard pattern
matching problem in the streaming model. The first is a randomized Monte Carlo
algorithm that is parameterized by a constant $0\leq \delta \leq 1$. This
algorithm uses $\tilde{O}(d^{1-\delta})$ amortized time per character and
$\tilde{O}(d^{1+\delta})$ words of space. The second algorithm, which is used
as a black box in the first algorithm, is a randomized Monte Carlo algorithm
which uses $O(d+\log m)$ worst-case time per character and $O(d\log m)$ words
of space.
</summary>
    <author>
      <name>Shay Golan</name>
    </author>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00453-018-0521-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00453-018-0521-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract appeared in ESA 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.01646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1703.06644">
    <id>http://arxiv.org/abs/1703.06644v1</id>
    <updated>2017-03-20T09:34:51Z</updated>
    <published>2017-03-20T09:34:51Z</published>
    <title>Reoptimization of the Closest Substring Problem under Pattern Length
  Modification</title>
    <summary>  This study investigates whether reoptimization can help in solving the
closest substring problem. We are dealing with the following reoptimization
scenario. Suppose, we have an optimal l-length closest substring of a given set
of sequences S. How can this information be beneficial in obtaining an
(l+k)-length closest substring for S? In this study, we show that the problem
is still computationally hard even with k=1. We present greedy approximation
algorithms that make use of the given information and prove that it has an
additive error that grows as the parameter k increases. Furthermore, we present
hard instances for each algorithm to show that the computed approximation ratio
is tight. We also show that we can slightly improve the running-time of the
existing polynomial-time approximation scheme (PTAS) for the original problem
through reoptimization.
</summary>
    <author>
      <name>Jhoirene B. Clemente</name>
    </author>
    <author>
      <name>Henry N. Adorna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 17th Philippine Computing Society Congress</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.06644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W25" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1703.06061">
    <id>http://arxiv.org/abs/1703.06061v1</id>
    <updated>2017-03-17T15:56:50Z</updated>
    <published>2017-03-17T15:56:50Z</published>
    <title>Approximation ratio of RePair</title>
    <summary>  In a seminal paper of Charikar et al.~on the smallest grammar problem, the
authors derive upper and lower bounds on the approximation ratios for several
grammar-based compressors. Here we improve the lower bound for the famous {\sf
RePair} algorithm from $\Omega(\sqrt{\log n})$ to $\Omega(\log n/\log\log n)$.
The family of words used in our proof is defined over a binary alphabet, while
the lower bound from Charikar et al. needs an alphabet of logarithmic size in
the length of the provided words.
</summary>
    <author>
      <name>Danny Hucke</name>
    </author>
    <author>
      <name>Artur Jez</name>
    </author>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <link href="http://arxiv.org/abs/1703.06061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2, E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1703.02224">
    <id>http://arxiv.org/abs/1703.02224v1</id>
    <updated>2017-03-07T05:43:56Z</updated>
    <published>2017-03-07T05:43:56Z</published>
    <title>Space-efficient K-MER algorithm for generalized suffix tree</title>
    <summary>  Suffix trees have emerged to be very fast for pattern searching yielding O
(m) time, where m is the pattern size. Unfortunately their high memory
requirements make it impractical to work with huge amounts of data. We present
a memory efficient algorithm of a generalized suffix tree which reduces the
space size by a factor of 10 when the size of the pattern is known beforehand.
Experiments on the chromosomes and Pizza&amp;Chili corpus show significant
advantages of our algorithm over standard linear time suffix tree construction
in terms of memory usage for pattern searching.
</summary>
    <author>
      <name>Freeson Kaniwa</name>
    </author>
    <author>
      <name>Venu Madhav Kuthadi</name>
    </author>
    <author>
      <name>Otlhapile Dinakenyane</name>
    </author>
    <author>
      <name>Heiko Schroeder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Information Technology Convergence and
  Services (IJITCS) Vol.7, No.1, February 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.02224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.02224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1703.00640">
    <id>http://arxiv.org/abs/1703.00640v1</id>
    <updated>2017-03-02T07:12:12Z</updated>
    <published>2017-03-02T07:12:12Z</published>
    <title>Faster truncated integer multiplication</title>
    <summary>  We present new algorithms for computing the low n bits or the high n bits of
the product of two n-bit integers. We show that these problems may be solved in
asymptotically 75% of the time required to compute the full 2n-bit product,
assuming that the underlying integer multiplication algorithm relies on
computing cyclic convolutions of real sequences.
</summary>
    <author>
      <name>David Harvey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W30 (Primary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.0; F.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1703.00687">
    <id>http://arxiv.org/abs/1703.00687v1</id>
    <updated>2017-03-02T09:54:11Z</updated>
    <published>2017-03-02T09:54:11Z</published>
    <title>Even faster sorting of (not only) integers</title>
    <summary>  In this paper we introduce RADULS2, the fastest parallel sorter based on
radix algorithm. It is optimized to process huge amounts of data making use of
modern multicore CPUs. The main novelties include: extremely optimized
algorithm for handling tiny arrays (up to about a hundred of records) that
could appear even billions times as subproblems to handle and improved
processing of larger subarrays with better use of non-temporal memory stores.
</summary>
    <author>
      <name>Marek Kokot</name>
    </author>
    <author>
      <name>Sebastian Deorowicz</name>
    </author>
    <author>
      <name>Maciej Dlugosz</name>
    </author>
    <link href="http://arxiv.org/abs/1703.00687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1706.04928">
    <id>http://arxiv.org/abs/1706.04928v2</id>
    <updated>2017-06-30T09:52:24Z</updated>
    <published>2017-06-15T15:28:42Z</published>
    <title>The complexity of the Multiple Pattern Matching Problem for random
  strings</title>
    <summary>  We generalise a multiple string pattern matching algorithm, recently proposed
by Fredriksson and Grabowski [J. Discr. Alg. 7, 2009], to deal with arbitrary
dictionaries on an alphabet of size $s$. If $r_m$ is the number of words of
length $m$ in the dictionary, and $\phi(r) = \max_m \ln(s\, m\, r_m)/m$, the
complexity rate for the string characters to be read by this algorithm is at
most $\kappa_{{}_\textrm{UB}}\, \phi(r)$ for some constant
$\kappa_{{}_\textrm{UB}}$. On the other side, we generalise the classical lower
bound of Yao [SIAM J. Comput. 8, 1979], for the problem with a single pattern,
to deal with arbitrary dictionaries, and determine it to be at least
$\kappa_{{}_\textrm{LB}}\, \phi(r)$. This proves the optimality of the
algorithm, improving and correcting previous claims.
</summary>
    <author>
      <name>Frédérique Bassino</name>
    </author>
    <author>
      <name>Tsinjo Rakotoarimalala</name>
    </author>
    <author>
      <name>Andrea Sportiello</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.04928v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04928v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1706.02828">
    <id>http://arxiv.org/abs/1706.02828v1</id>
    <updated>2017-06-09T04:04:18Z</updated>
    <published>2017-06-09T04:04:18Z</published>
    <title>On-line Assembling Mitochondrial DNA from de novo transcriptome</title>
    <summary>  This paper is focused in designing an efficient on-line algorithm to
reconstruct a DNA sequence and search the genes in it, we assume that the
segment have no mutation or reading error, the algorithm is based on de Bruijn
Graph for reconstructing the DNA from the segments taking k-mers large enough
no to generate cycles, once the sequence is ready a Boyer-Moore's algorithm
implementation is used to search the genes inside de sequence using starts and
stop codons, this solution give a high performance when all genes can be found,
and there is no need to read all the segments to reach maximum number of genes,
but due to the online nature one cannot be sure about the finals genes given
</summary>
    <author>
      <name>Juan David Arcila Moreno</name>
    </author>
    <author>
      <name>Santiago Passos</name>
    </author>
    <author>
      <name>Mauricio Toro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.10277">
    <id>http://arxiv.org/abs/1705.10277v7</id>
    <updated>2017-12-17T22:53:02Z</updated>
    <published>2017-05-29T16:24:11Z</published>
    <title>Inverse Lyndon words and Inverse Lyndon factorizations of words</title>
    <summary>  Motivated by applications to string processing, we introduce variants of the
Lyndon factorization called inverse Lyndon factorizations. Their factors, named
inverse Lyndon words, are in a class that strictly contains anti-Lyndon words,
that is Lyndon words with respect to the inverse lexicographic order. The
Lyndon factorization of a nonempty word w is unique but w may have several
inverse Lyndon factorizations. We prove that any nonempty word w admits a
canonical inverse Lyndon factorization, named ICFL(w), that maintains the main
properties of the Lyndon factorization of w: it can be computed in linear time,
it is uniquely determined, it preserves a compatibility property for sorting
suffixes. In particular, the compatibility property of ICFL(w) is a consequence
of another result: any factor in ICFL(w) is a concatenation of consecutive
factors of the Lyndon factorization of w with respect to the inverse
lexicographic order.
</summary>
    <author>
      <name>Paola Bonizzoni</name>
    </author>
    <author>
      <name>Clelia De Felice</name>
    </author>
    <author>
      <name>Rocco Zaccagnino</name>
    </author>
    <author>
      <name>Rosalba Zizza</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.aam.2018.08.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.aam.2018.08.005" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Applied Mathematics, Vol. 101, pp. 281-319, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1705.10277v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10277v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1; F.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.09642">
    <id>http://arxiv.org/abs/1705.09642v6</id>
    <updated>2017-08-10T17:49:07Z</updated>
    <published>2017-05-26T16:31:59Z</published>
    <title>Multiresolution Priority Queues</title>
    <summary>  Priority queues are container data structures essential to many high
performance computing (HPC) applications. In this paper, we introduce
multiresolution priority queues, a data structure that improves the performance
of the standard heap based implementations by trading off a controllable amount
of resolution in the space of priorities. The new data structure can reduce the
worst case performance of inserting an element from O(log(n)) to O(log(r)),
where n is the number of elements in the queue and r is the number of
resolution groups in the priority space. The worst case cost of removing the
top element is O(1). When the number of elements in the table is high, the
amortized cost to insert an element becomes O(1).
</summary>
    <author>
      <name>Jordi Ros-Giralt</name>
    </author>
    <author>
      <name>Alan Commike</name>
    </author>
    <author>
      <name>Peter Cullen</name>
    </author>
    <author>
      <name>Jeff Lucovsky</name>
    </author>
    <author>
      <name>Dilip Madathil</name>
    </author>
    <author>
      <name>Richard Lethin</name>
    </author>
    <link href="http://arxiv.org/abs/1705.09642v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09642v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.09504">
    <id>http://arxiv.org/abs/1705.09504v1</id>
    <updated>2017-05-26T09:55:38Z</updated>
    <published>2017-05-26T09:55:38Z</published>
    <title>New Variants of Pattern Matching with Constants and Variables</title>
    <summary>  Given a text and a pattern over two types of symbols called constants and
variables, the parameterized pattern matching problem is to find all
occurrences of substrings of the text that the pattern matches by substituting
a variable in the text for each variable in the pattern, where the substitution
should be injective. The function matching problem is a variant of it that
lifts the injection constraint. In this paper, we discuss variants of those
problems, where one can substitute a constant or a variable for each variable
of the pattern. We give two kinds of algorithms for both problems, a
convolution-based method and an extended KMP-based method, and analyze their
complexity.
</summary>
    <author>
      <name>Yuki Igarashi</name>
    </author>
    <author>
      <name> Diptarama</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.09504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.09438">
    <id>http://arxiv.org/abs/1705.09438v1</id>
    <updated>2017-05-26T05:40:54Z</updated>
    <published>2017-05-26T05:40:54Z</published>
    <title>Duel and sweep algorithm for order-preserving pattern matching</title>
    <summary>  Given a text $T$ and a pattern $P$ over alphabet $\Sigma$, the classic exact
matching problem searches for all occurrences of pattern $P$ in text $T$.
Unlike exact matching problem, order-preserving pattern matching (OPPM)
considers the relative order of elements, rather than their real values. In
this paper, we propose an efficient algorithm for OPPM problem using the
"duel-and-sweep" paradigm. Our algorithm runs in $O(n + m\log m)$ time in
general and $O(n + m)$ time under an assumption that the characters in a string
can be sorted in linear time with respect to the string size. We also perform
experiments and show that our algorithm is faster that KMP-based algorithm.
Last, we introduce the two-dimensional order preserved pattern matching and
give a duel and sweep algorithm that runs in $O(n^2)$ time for duel stage and
$O(n^2 m)$ time for sweeping time with $O(m^3)$ preprocessing time.
</summary>
    <author>
      <name>Davaajav Jargalsaikhan</name>
    </author>
    <author>
      <name> Diptarama</name>
    </author>
    <author>
      <name>Ryo Yoshinaka</name>
    </author>
    <author>
      <name>Ayumi Shinohara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.09438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.05105">
    <id>http://arxiv.org/abs/1705.05105v3</id>
    <updated>2017-05-24T19:52:38Z</updated>
    <published>2017-05-15T07:54:06Z</published>
    <title>Assembling sequences of DNA using an on-line algorithm based on DeBruijn
  graphs</title>
    <summary>  The problem of assembling DNA fragments starting from imperfect strings given
by a sequencer, classified as NP hard when trying to get perfect answers, has a
huge importance in several fields, because of its relation with the possibility
of detecting similarities between animals, dangerous pests in crops, and so on.
Some of the algorithms and data structures that have been created to solve this
problem are Needleman Wunsch algorithm, DeBruijn graphs and greedy algorithms
working on overlaps graphs; these try to work out the problem from different
approaches that give place to certain advantages and disadvantages to be
discussed.
  In this article we first expose a summary of the research done on already
created solutions for the DNA assembly problem, to present later an on-line
solution to the same matter, which, despite not considering mutations, would
have the capacity of using only the necessary amount of readings to assemble an
user specified amount of genes.
</summary>
    <author>
      <name>Juan Manuel Ciro Restrepo</name>
    </author>
    <author>
      <name>Andrés Felipe Zapata Palacio</name>
    </author>
    <author>
      <name>Mauricio Toro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.05105v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05105v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3; E.1; F.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1705.00849">
    <id>http://arxiv.org/abs/1705.00849v1</id>
    <updated>2017-05-02T08:14:42Z</updated>
    <published>2017-05-02T08:14:42Z</published>
    <title>Improved Average Complexity for Comparison-Based Sorting</title>
    <summary>  This paper studies the average complexity on the number of comparisons for
sorting algorithms. Its information-theoretic lower bound is $n \lg n - 1.4427n
+ O(\log n)$. For many efficient algorithms, the first $n\lg n$ term is easy to
achieve and our focus is on the (negative) constant factor of the linear term.
The current best value is $-1.3999$ for the MergeInsertion sort. Our new value
is $-1.4106$, narrowing the gap by some $25\%$. An important building block of
our algorithm is "two-element insertion," which inserts two numbers $A$ and
$B$, $A&lt;B$, into a sorted sequence $T$. This insertion algorithm is still
sufficiently simple for rigorous mathematical analysis and works well for a
certain range of the length of $T$ for which the simple binary insertion does
not, thus allowing us to take a complementary approach with the binary
insertion.
</summary>
    <author>
      <name>Kazuo Iwama</name>
    </author>
    <author>
      <name>Junichi Teruyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.00849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1704.07710">
    <id>http://arxiv.org/abs/1704.07710v1</id>
    <updated>2017-04-25T14:21:35Z</updated>
    <published>2017-04-25T14:21:35Z</published>
    <title>Succinct Approximate Rank Queries</title>
    <summary>  We consider the problem of summarizing a multi set of elements in $\{1, 2,
\ldots , n\}$ under the constraint that no element appears more than $\ell$
times. The goal is then to answer \emph{rank} queries --- given $i\in\{1, 2,
\ldots , n\}$, how many elements in the multi set are smaller than $i$? ---
with an additive error of at most $\Delta$ and in constant time. For this
problem, we prove a lower bound of $\mathcal B_{\ell,n,\Delta}\triangleq$
$\left\lfloor{\frac{n}{\left\lceil{\Delta / \ell}\right\rceil}}\right\rfloor $
$\log\big({\max\{\left\lfloor{\ell / \Delta}\right\rfloor,1\} + 1}\big)$ bits
and provide a \emph{succinct} construction that uses $\mathcal
B_{\ell,n,\Delta}(1+o(1))$ bits. Next, we generalize our data structure to
support processing of a stream of integers in $\{0,1,\ldots,\ell\}$, where upon
a query for some $i\le n$ we provide a $\Delta$-additive approximation for the
sum of the \emph{last} $i$ elements. We show that this too can be done using
$\mathcal B_{\ell,n,\Delta}(1+o(1))$ bits and in constant time. This yields the
first sub linear space algorithm that computes approximate sliding window sums
in $O(1)$ time, where the window size is given at the query time; additionally,
it requires only $(1+o(1))$ more space than is needed for a fixed window size.
</summary>
    <author>
      <name>Ran Ben Basat</name>
    </author>
    <link href="http://arxiv.org/abs/1704.07710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1704.05660">
    <id>http://arxiv.org/abs/1704.05660v1</id>
    <updated>2017-04-19T09:08:55Z</updated>
    <published>2017-04-19T09:08:55Z</published>
    <title>Alphabet-dependent Parallel Algorithm for Suffix Tree Construction for
  Pattern Searching</title>
    <summary>  Suffix trees have recently become very successful data structures in handling
large data sequences such as DNA or Protein sequences. Consequently parallel
architectures have become ubiquitous. We present a novel alphabet-dependent
parallel algorithm which attempts to take advantage of the perverseness of the
multicore architecture. Microsatellites are important for their biological
relevance hence our algorithm is based on time efficient construction for
identification of such. We experimentally achieved up to 15x speedup over the
sequential algorithm on different input sizes of biological sequences.
</summary>
    <author>
      <name>Freeson Kaniwa</name>
    </author>
    <author>
      <name>Venu Madhav Kuthadi</name>
    </author>
    <author>
      <name>Otlhapile Dinakenyane</name>
    </author>
    <author>
      <name>Heiko Schroeder</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Grid and Distributed Computing, Vol. 10,
  No. 1 (2017), pp.9-20</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.05660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1708.07271">
    <id>http://arxiv.org/abs/1708.07271v3</id>
    <updated>2018-02-18T23:36:00Z</updated>
    <published>2017-08-24T03:47:52Z</published>
    <title>Exploiting Computation-Friendly Graph Compression Methods</title>
    <summary>  Computing the product of the (binary) adjacency matrix of a large graph with
a real-valued vector is an important operation that lies at the heart of
various graph analysis tasks, such as computing PageRank. In this paper we show
that some well-known Web and social graph compression formats are
computation-friendly, in the sense that they allow boosting the computation. In
particular, we show that the format of Boldi and Vigna allows computing the
product in time proportional to the compressed graph size. Our experimental
results show speedups of at least 2 on graphs that were compressed at least 5
times with respect to the original. We show that other successful graph
compression formats enjoy this property as well.
</summary>
    <author>
      <name>Alexandre P. Francisco</name>
    </author>
    <author>
      <name>Travis Gagie</name>
    </author>
    <author>
      <name>Susana Ladra</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. Accepted to 2018 Data
  Compression Conference (DCC)</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07271v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07271v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1708.04381">
    <id>http://arxiv.org/abs/1708.04381v1</id>
    <updated>2017-08-15T02:12:48Z</updated>
    <published>2017-08-15T02:12:48Z</published>
    <title>Streaming Periodicity with Mismatches</title>
    <summary>  We study the problem of finding all $k$-periods of a length-$n$ string $S$,
presented as a data stream. $S$ is said to have $k$-period $p$ if its prefix of
length $n-p$ differs from its suffix of length $n-p$ in at most $k$ locations.
  We give a one-pass streaming algorithm that computes the $k$-periods of a
string $S$ using $\text{poly}(k, \log n)$ bits of space, for $k$-periods of
length at most $\frac{n}{2}$. We also present a two-pass streaming algorithm
that computes $k$-periods of $S$ using $\text{poly}(k, \log n)$ bits of space,
regardless of period length. We complement these results with comparable lower
bounds.
</summary>
    <author>
      <name>Funda Ergün</name>
    </author>
    <author>
      <name>Elena Grigorescu</name>
    </author>
    <author>
      <name>Erfan Sadeqi Azer</name>
    </author>
    <author>
      <name>Samson Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1708.04381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1708.01130">
    <id>http://arxiv.org/abs/1708.01130v1</id>
    <updated>2017-08-03T13:30:41Z</updated>
    <published>2017-08-03T13:30:41Z</published>
    <title>Efficient pattern matching in degenerate strings with the
  Burrows-Wheeler transform</title>
    <summary>  A degenerate or indeterminate string on an alphabet $\Sigma$ is a sequence of
non-empty subsets of $\Sigma$. Given a degenerate string $t$ of length $n$, we
present a new method based on the Burrows--Wheeler transform for searching for
a degenerate pattern of length $m$ in $t$ running in $O(mn)$ time on a constant
size alphabet $\Sigma$. Furthermore, it is a hybrid pattern-matching technique
that works on both regular and degenerate strings. A degenerate string is said
to be conservative if its number of non-solid letters is upper-bounded by a
fixed positive constant $q$; in this case we show that the search complexity
time is $O(qm^2)$. Experimental results show that our method performs well in
practice.
</summary>
    <author>
      <name>Jacqueline W. Daykin</name>
    </author>
    <author>
      <name>Richard Groult</name>
    </author>
    <author>
      <name>Yannick Guesnet</name>
    </author>
    <author>
      <name>Thierry Lecroq</name>
    </author>
    <author>
      <name>Arnaud Lefebvre</name>
    </author>
    <author>
      <name>Martine Léonard</name>
    </author>
    <author>
      <name>Laurent Mouchard</name>
    </author>
    <author>
      <name>Élise Prieur-Gaston</name>
    </author>
    <author>
      <name>Bruce Watson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.01130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.08186">
    <id>http://arxiv.org/abs/1707.08186v1</id>
    <updated>2017-07-25T19:43:06Z</updated>
    <published>2017-07-25T19:43:06Z</published>
    <title>Persistent Cache-oblivious Streaming Indexes</title>
    <summary>  In [SPAA2007], Bender et al. define a streaming B-tree (or index) as one that
supports updates in amortized $o(1)$ IOs, and present a structure achieving
amortized $O((\log N)/B)$ IOs and queries in $O(\log N)$ IOs. We extend their
result to the partially-persistent case. For a version $v$, let $N_v$ be the
number of keys accessible at $v$ and $N$ be the total number of updates. We
give a data structure using space $O(N)$, supporting updates to a leaf version
$v$ with $O((\log N_{v})/B)$ amortized IOs and answering range queries
returning $Z$ elements with $O(\log N_{v} + Z/B)$ IOs on average (where the
average is over all queries covering disjoint key ranges at a given version).
This is the first persistent `streaming' index we are aware of, i.e. that
supports updates in $o(1)$ IOs and supports efficient range queries.
</summary>
    <author>
      <name>Andrew Twigg</name>
    </author>
    <link href="http://arxiv.org/abs/1707.08186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.07727">
    <id>http://arxiv.org/abs/1707.07727v2</id>
    <updated>2019-12-03T14:15:29Z</updated>
    <published>2017-07-24T19:47:22Z</published>
    <title>Greedy Shortest Common Superstring Approximation in Compact Space</title>
    <summary>  Given a set of strings, the shortest common superstring problem is to find
the shortest possible string that contains all the input strings. The problem
is NP-hard, but a lot of work has gone into designing approximation algorithms
for solving the problem. We present the first time and space efficient
implementation of the classic greedy heuristic which merges strings in
decreasing order of overlap length. Our implementation works in $O(n \log
\sigma)$ time and bits of space, where $n$ is the total length of the input
strings in characters, and $\sigma$ is the size of the alphabet. After index
construction, a practical implementation of our algorithm uses roughly $5 n
\log \sigma$ bits of space and reasonable time for a real dataset that consists
of DNA fragments.
</summary>
    <author>
      <name>Jarno Alanko</name>
    </author>
    <author>
      <name>Tuukka Norri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 Pages, 3 figures, accepted to the 24th International Symposium on
  String Processing and Information Retrieval (SPIRE 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07727v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07727v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.05613">
    <id>http://arxiv.org/abs/1707.05613v1</id>
    <updated>2017-07-18T13:52:49Z</updated>
    <published>2017-07-18T13:52:49Z</published>
    <title>The Compressed Overlap Index</title>
    <summary>  For analysing text algorithms, for computing superstrings, or for testing
random number generators, one needs to compute all overlaps between any pairs
of words in a given set. The positions of overlaps of a word onto itself, or of
two words, are needed to compute the absence probability of a word in a random
text, or the numbers of common words shared by two random texts. In all these
contexts, one needs to compute or to query overlaps between pairs of words in a
given set. For this sake, we designed COvI, a compressed overlap index that
supports multiple queries on overlaps: like computing the correlation of two
words, or listing pairs of words whose longest overlap is maximal among all
possible pairs. COvI stores overlaps in a hierarchical and non-redundant
manner. We propose an implementation that can handle datasets of millions of
words and still answer queries efficiently. Comparison with a baseline solution
- called FullAC - relying on the Aho-Corasick automaton shows that COvI
provides significant advantages. For similar construction times, COvI requires
half the memory FullAC, and still solves complex queries much faster.
</summary>
    <author>
      <name>Rodrigo Canovas</name>
    </author>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Eric Rivals</name>
    </author>
    <link href="http://arxiv.org/abs/1707.05613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.02769">
    <id>http://arxiv.org/abs/1707.02769v1</id>
    <updated>2017-07-10T09:26:15Z</updated>
    <published>2017-07-10T09:26:15Z</published>
    <title>Compressed Representation of Dynamic Binary Relations with Applications</title>
    <summary>  We introduce a dynamic data structure for the compact representation of
binary relations $\mathcal{R} \subseteq A \times B$. The data structure is a
dynamic variant of the k$^2$-tree, a static compact representation that takes
advantage of clustering in the binary relation to achieve compression. Our
structure can efficiently check whether two objects $(a,b) \in A \times B$ are
related, and list the objects of $B$ related to some $a \in A$ and vice versa.
Additionally, our structure allows inserting and deleting pairs $(a,b)$ in the
relation, as well as modifying the base sets $A$ and $B$. We test our dynamic
data structure in different contexts, including the representation of Web
graphs and RDF databases. Our experiments show that our dynamic data structure
achieves good compression ratios and fast query times, close to those of a
static representation, while also providing efficient support for updates in
the represented binary relation.
</summary>
    <author>
      <name>Nieves R. Brisaboa</name>
    </author>
    <author>
      <name>Ana Cerdeira-Pena</name>
    </author>
    <author>
      <name>Guillermo de Bernardo</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941, Information Systems (2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.01182">
    <id>http://arxiv.org/abs/1707.01182v1</id>
    <updated>2017-07-04T23:23:34Z</updated>
    <published>2017-07-04T23:23:34Z</published>
    <title>Biased Predecessor Search</title>
    <summary>  We consider the problem of performing predecessor searches in a bounded
universe while achieving query times that depend on the distribution of
queries. We obtain several data structures with various properties: in
particular, we give data structures that achieve expected query times
logarithmic in the entropy of the distribution of queries but with space
bounded in terms of universe size, as well as data structures that use only
linear space but with query times that are higher (but still sublinear)
functions of the entropy. For these structures, the distribution is assumed to
be known. We also consider individual query times on universe elements with
general weights, as well as the case when the distribution is not known in
advance.
</summary>
    <author>
      <name>Prosenjit Bose</name>
    </author>
    <author>
      <name>Rolf Fagerberg</name>
    </author>
    <author>
      <name>John Howat</name>
    </author>
    <author>
      <name>Pat Morin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00453-016-0146-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00453-016-0146-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Also appeared at LATIN'14</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Algorithmica 76(4): 1097-1105 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.01182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1706.07290">
    <id>http://arxiv.org/abs/1706.07290v1</id>
    <updated>2017-06-20T19:14:12Z</updated>
    <published>2017-06-20T19:14:12Z</published>
    <title>New Cardinality Estimation Methods for HyperLogLog Sketches</title>
    <summary>  This work presents new cardinality estimation methods for data sets recorded
by HyperLogLog sketches. A simple derivation of the original estimator was
found, that also gives insight how to correct its deficiencies. The result is
an improved estimator that is unbiased over the full cardinality range, is easy
computable, and does not rely on empirically determined data as previous
approaches. Based on the maximum likelihood principle a second unbiased
estimation method is presented which can also be extended to estimate
cardinalities of union, intersection, or relative complements of two sets that
are both represented as HyperLogLog sketches. Experimental results show that
this approach is more precise than the conventional technique using the
inclusion-exclusion principle.
</summary>
    <author>
      <name>Otmar Ertl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 10 figures, 1 table. arXiv admin note: substantial text
  overlap with arXiv:1702.01284</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1706.06940">
    <id>http://arxiv.org/abs/1706.06940v2</id>
    <updated>2017-07-10T18:24:04Z</updated>
    <published>2017-06-21T14:50:32Z</published>
    <title>Faster batched range minimum queries</title>
    <summary>  Range Minimum Query (RMQ) is an important building brick of many compressed
data structures and string matching algorithms. Although this problem is
essentially solved in theory, with sophisticated data structures allowing for
constant time queries, there are scenarios in which the number of queries, $q$,
is rather small and given beforehand, which encourages to use a simpler
approach. A recent work by Alzamel et al. starts with contracting the input
array to a much shorter one, with its size proportional to $q$. In this work,
we build upon their solution, speeding up handling small batches of queries by
a factor of 3.8--7.8 (the gap grows with $q$). The key idea that helped us
achieve this advantage is adapting the well-known Sparse Table technique to
work on blocks, with speculative block minima comparisons. We also propose an
even much faster (but possibly using more space) variant without the array
contraction.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Tomasz Kowalski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Prague Stringology Conference 2017. Compared to v1, bugs
  in Table 2 were fixed</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.06940v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06940v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1710.10660">
    <id>http://arxiv.org/abs/1710.10660v1</id>
    <updated>2017-10-29T18:06:27Z</updated>
    <published>2017-10-29T18:06:27Z</published>
    <title>Improved Bounds for Testing Forbidden Order Patterns</title>
    <summary>  A sequence $f\colon\{1,\dots,n\}\to\mathbb{R}$ contains a permutation $\pi$
of length $k$ if there exist $i_1&lt;\dots&lt;i_k$ such that, for all $x,y$,
$f(i_x)&lt;f(i_y)$ if and only if $\pi(x)&lt;\pi(y)$; otherwise, $f$ is said to be
$\pi$-free. In this work, we consider the problem of testing for $\pi$-freeness
with one-sided error, continuing the investigation of [Newman et al., SODA'17].
  We demonstrate a surprising behavior for non-adaptive tests with one-sided
error: While a trivial sampling-based approach yields an $\varepsilon$-test for
$\pi$-freeness making $\Theta(\varepsilon^{-1/k} n^{1-1/k})$ queries, our lower
bounds imply that this is almost optimal for most permutations! Specifically,
for most permutations $\pi$ of length $k$, any non-adaptive one-sided
$\varepsilon$-test requires
$\varepsilon^{-1/(k-\Theta(1))}n^{1-1/(k-\Theta(1))}$ queries; furthermore, the
permutations that are hardest to test require
$\Theta(\varepsilon^{-1/(k-1)}n^{1-1/(k-1)})$ queries, which is tight in $n$
and $\varepsilon$.
  Additionally, we show two hierarchical behaviors here. First, for any $k$ and
$l\leq k-1$, there exists some $\pi$ of length $k$ that requires
$\tilde{\Theta}_{\varepsilon}(n^{1-1/l})$ non-adaptive queries. Second, we show
an adaptivity hierarchy for $\pi=(1,3,2)$ by proving upper and lower bounds for
(one- and two-sided) testing of $\pi$-freeness with $r$ rounds of adaptivity.
The results answer open questions of Newman et al. and [Canonne and Gur,
CCC'17].
</summary>
    <author>
      <name>Omri Ben-Eliezer</name>
    </author>
    <author>
      <name>Clément L. Canonne</name>
    </author>
    <link href="http://arxiv.org/abs/1710.10660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1710.08937">
    <id>http://arxiv.org/abs/1710.08937v3</id>
    <updated>2018-05-31T12:16:00Z</updated>
    <published>2017-10-24T18:12:35Z</published>
    <title>Exact Mean Computation in Dynamic Time Warping Spaces</title>
    <summary>  Dynamic time warping constitutes a major tool for analyzing time series. In
particular, computing a mean series of a given sample of series in dynamic time
warping spaces (by minimizing the Fr\'echet function) is a challenging
computational problem, so far solved by several heuristic and inexact
strategies. We spot some inaccuracies in the literature on exact mean
computation in dynamic time warping spaces. Our contributions comprise an exact
dynamic program computing a mean (useful for benchmarking and evaluating known
heuristics). Based on this dynamic program, we empirically study properties
like uniqueness and length of a mean. Moreover, experimental evaluations reveal
substantial deficits of state-of-the-art heuristics in terms of their output
quality. We also give an exact polynomial-time algorithm for the special case
of binary time series.
</summary>
    <author>
      <name>Markus Brill</name>
    </author>
    <author>
      <name>Till Fluschnik</name>
    </author>
    <author>
      <name>Vincent Froese</name>
    </author>
    <author>
      <name>Brijnesh Jain</name>
    </author>
    <author>
      <name>Rolf Niedermeier</name>
    </author>
    <author>
      <name>David Schultz</name>
    </author>
    <link href="http://arxiv.org/abs/1710.08937v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08937v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10, 37M10" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1710.08436">
    <id>http://arxiv.org/abs/1710.08436v5</id>
    <updated>2019-07-13T15:29:47Z</updated>
    <published>2017-10-23T18:02:16Z</published>
    <title>HyperMinHash: MinHash in LogLog space</title>
    <summary>  In this extended abstract, we describe and analyze a lossy compression of
MinHash from buckets of size $O(\log n)$ to buckets of size $O(\log\log n)$ by
encoding using floating-point notation. This new compressed sketch, which we
call HyperMinHash, as we build off a HyperLogLog scaffold, can be used as a
drop-in replacement of MinHash. Unlike comparable Jaccard index fingerprinting
algorithms in sub-logarithmic space (such as b-bit MinHash), HyperMinHash
retains MinHash's features of streaming updates, unions, and cardinality
estimation. For a multiplicative approximation error $1+ \epsilon$ on a Jaccard
index $ t $, given a random oracle, HyperMinHash needs $O\left(\epsilon^{-2}
\left( \log\log n + \log \frac{1}{ t \epsilon} \right)\right)$ space.
HyperMinHash allows estimating Jaccard indices of 0.01 for set cardinalities on
the order of $10^{19}$ with relative error of around 10\% using 64KiB of
memory; MinHash can only estimate Jaccard indices for cardinalities of
$10^{10}$ with the same memory consumption.
</summary>
    <author>
      <name>Yun William Yu</name>
    </author>
    <author>
      <name>Griffin M. Weber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.08436v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08436v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1710.07992">
    <id>http://arxiv.org/abs/1710.07992v1</id>
    <updated>2017-10-22T18:16:41Z</updated>
    <published>2017-10-22T18:16:41Z</published>
    <title>Twin Sort Technique</title>
    <summary>  The objective behind the Twin Sort technique is to sort the list of unordered
data elements efficiently and to allow efficient and simple arrangement of data
elements within the data structure with optimization of comparisons and
iterations in the sorting method. This sorting technique effectively terminates
the iterations when there is no need of comparison if the elements are all
sorted in between the iterations. Unlike Quick sort, Merge sorting technique,
this new sorting technique is based on the iterative method of sorting elements
within the data structure. So it will be advantageous for optimization of
iterations when there is no need for sorting elements. Finally, the Twin Sort
technique is more efficient and simple method of arranging elements within a
data structure and it is easy to implement when comparing to the other sorting
technique. By the introduction of optimization of comparison and iterations, it
will never allow the arranging task on the ordered elements.
</summary>
    <author>
      <name>Veeresh D</name>
    </author>
    <author>
      <name>Thimmaraju S. N</name>
    </author>
    <author>
      <name>Ravish G. K</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">core computer algorithm, 3 pages, conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Combined Research &amp; Development October
  2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.07992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.07992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1710.07505">
    <id>http://arxiv.org/abs/1710.07505v1</id>
    <updated>2017-10-20T12:32:37Z</updated>
    <published>2017-10-20T12:32:37Z</published>
    <title>Probabilistic Analysis of the Dual-Pivot Quicksort "Count"</title>
    <summary>  Recently, Aum\"uller and Dietzfelbinger proposed a version of a dual-pivot
quicksort, called "Count", which is optimal among dual-pivot versions with
respect to the average number of key comparisons required. In this note we
provide further probabilistic analysis of "Count". We derive an exact formula
for the average number of swaps needed by "Count" as well as an asymptotic
formula for the variance of the number of swaps and a limit law. Also for the
number of key comparisons the asymptotic variance and a limit law are
identified. We also consider both complexity measures jointly and find their
asymptotic correlation.
</summary>
    <author>
      <name>Ralph Neininger</name>
    </author>
    <author>
      <name>Jasmin Straub</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the proceedings of Analytic Algorithmics and
  Combinatorics (ANALCO18)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.07505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.07505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1710.00944">
    <id>http://arxiv.org/abs/1710.00944v1</id>
    <updated>2017-10-03T00:16:54Z</updated>
    <published>2017-10-03T00:16:54Z</published>
    <title>Ordered Dags: HypercubeSort</title>
    <summary>  We generalise the insertion into a binary heap to any directed acyclic graph
(DAG) with one source vertex. This lets us formulate a general method for
converting any such DAG into a data structure with priority queue interface. We
apply our method to a hypercube DAG to obtain a sorting algorithm of complexity
$\mathcal{O}(n\log^2 (n))$. As another curious application, we derive a
relationship between length of longest path and maximum degree of a vertex in a
DAG.
</summary>
    <author>
      <name>Mikhail Gudim</name>
    </author>
    <link href="http://arxiv.org/abs/1710.00944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1710.00586">
    <id>http://arxiv.org/abs/1710.00586v2</id>
    <updated>2017-10-03T05:13:06Z</updated>
    <published>2017-10-02T11:21:43Z</published>
    <title>Orthogonal Vectors Indexing</title>
    <summary>  In the recent years, intensive research work has been dedicated to prove
conditional lower bounds in order to reveal the inner structure of the class P.
These conditional lower bounds are based on many popular conjectures on
well-studied problems. One of the most heavily used conjectures is the
celebrated Strong Exponential Time Hypothesis (SETH). It turns out that
conditional hardness proved based on SETH goes, in many cases, through an
intermediate problem - the Orthogonal Vectors (OV) problem.
  Almost all research work regarding conditional lower bound was concentrated
on time complexity. Very little attention was directed toward space complexity.
In a recent work, Goldstein et al.[WADS 2017] set the stage for proving
conditional lower bounds regarding space and its interplay with time. In this
spirit, it is tempting to investigate the space complexity of a data structure
variant of OV which is called \emph{OV indexing}. In this problem $n$ boolean
vectors of size $c\log{n}$ are given for preprocessing. As a query, a vector
$v$ is given and we are required to verify if there is an input vector that is
orthogonal to it or not.
  This OV indexing problem is interesting in its own, but it also likely to
have strong implications on problems known to be conditionally hard, in terms
of time complexity, based on OV. Having this in mind, we study OV indexing in
this paper from many aspects. We give some space-efficient algorithms for the
problem, show a tradeoff between space and query time, describe how to solve
its reporting variant, shed light on an interesting connection between this
problem and the well-studied SetDisjointness problem and demonstrate how it can
be solved more efficiently on random input.
</summary>
    <author>
      <name>Isaac Goldstein</name>
    </author>
    <author>
      <name>Moshe Lewenstein</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <link href="http://arxiv.org/abs/1710.00586v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00586v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1709.10477">
    <id>http://arxiv.org/abs/1709.10477v1</id>
    <updated>2017-09-29T16:13:52Z</updated>
    <published>2017-09-29T16:13:52Z</published>
    <title>On-the-Fly Array Initialization in Less Space</title>
    <summary>  We show that for all given $n,t,w \in \{1,2,...\}$ with $n&lt;2^w$, an array of
$n$ entries of $w$ bits each can be represented on a word RAM with a word
length of $w$ bits in at most $nw+\lceil n(t/(2 w))^t\rceil$ bits of
uninitialized memory to support constant-time initialization of the whole array
and $O(t)$-time reading and writing of individual array entries. At one end of
this tradeoff, we achieve initialization and access (i.e., reading and writing)
in constant time with $nw+\lceil n/w^t\rceil$ bits for arbitrary fixed $t$, to
be compared with $nw+\Theta(n)$ bits for the best previous solution, and at the
opposite end, still with constant-time initialization, we support $O(\log
n)$-time access with just $nw+1$ bits, which is optimal for arbitrary access
times if the initialization executes fewer than $n$ steps.
</summary>
    <author>
      <name>Torben Hagerup</name>
    </author>
    <author>
      <name>Frank Kammer</name>
    </author>
    <link href="http://arxiv.org/abs/1709.10477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.10477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1709.10305">
    <id>http://arxiv.org/abs/1709.10305v1</id>
    <updated>2017-09-29T09:51:33Z</updated>
    <published>2017-09-29T09:51:33Z</published>
    <title>Fast Computation of Graph Edit Distance</title>
    <summary>  The graph edit distance (GED) is a well-established distance measure widely
used in many applications. However, existing methods for the GED computation
suffer from several drawbacks including oversized search space, huge memory
consumption, and lots of expensive backtracking. In this paper, we present
BSS_GED, a novel vertex-based mapping method for the GED computation. First, we
create a small search space by reducing the number of invalid and redundant
mappings involved in the GED computation. Then, we utilize beam-stack search
combined with two heuristics to efficiently compute GED, achieving a flexible
trade-off between available memory and expensive backtracking. Extensive
experiments demonstrate that BSS GED is highly efficient for the GED
computation on sparse as well as dense graphs and outperforms the
state-of-the-art GED methods. In addition, we also apply BSS_GED to the graph
similarity search problem and the practical results confirm its efficiency.
</summary>
    <author>
      <name>Xiaoyang Chen</name>
    </author>
    <author>
      <name>Hongwei Huo</name>
    </author>
    <author>
      <name>Jun Huan</name>
    </author>
    <author>
      <name>Jeffrey Scott Vitter</name>
    </author>
    <link href="http://arxiv.org/abs/1709.10305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.10305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1709.00164">
    <id>http://arxiv.org/abs/1709.00164v2</id>
    <updated>2017-09-23T02:14:24Z</updated>
    <published>2017-09-01T05:38:32Z</published>
    <title>Whole Genome Phylogenetic Tree Reconstruction Using Colored de Bruijn
  Graphs</title>
    <summary>  We present kleuren, a novel assembly-free method to reconstruct phylogenetic
trees using the Colored de Bruijn Graph. kleuren works by constructing the
Colored de Bruijn Graph and then traversing it, finding bubble structures in
the graph that provide phylogenetic signal. The bubbles are then aligned and
concatenated to form a supermatrix, from which a phylogenetic tree is inferred.
We introduce the algorithms that kleuren uses to accomplish this task, and show
its performance on reconstructing the phylogenetic tree of 12 Drosophila
species. kleuren reconstructed the established phylogenetic tree accurately,
and is a viable tool for phylogenetic tree reconstruction using whole genome
sequences. Software package available at: https://github.com/Colelyman/kleuren
</summary>
    <author>
      <name>Cole A. Lyman</name>
    </author>
    <author>
      <name>M. Stanley Fujimoto</name>
    </author>
    <author>
      <name>Anton Suvorov</name>
    </author>
    <author>
      <name>Paul M. Bodily</name>
    </author>
    <author>
      <name>Quinn Snell</name>
    </author>
    <author>
      <name>Keith A. Crandall</name>
    </author>
    <author>
      <name>Seth M. Bybee</name>
    </author>
    <author>
      <name>Mark J. Clement</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/BIBE.2017.00-44</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/BIBE.2017.00-44" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, accepted at BIBE 2017. Minor modifications to the
  text due to reviewer feedback and fixed typos</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.00164v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00164v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1712.08749">
    <id>http://arxiv.org/abs/1712.08749v1</id>
    <updated>2017-12-23T10:01:09Z</updated>
    <published>2017-12-23T10:01:09Z</published>
    <title>Cartesian trees and Lyndon trees</title>
    <summary>  The article describes the structural and algorithmic relations between
Cartesian trees and Lyndon Trees. This leads to a uniform presentation of the
Lyndon table of a word corresponding to the Next Nearest Smaller table of a
sequence of numbers. It shows how to efficiently compute runs, that is, maximal
periodicities occurring in a word.
</summary>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Luis M. S. Russo</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1712.05876">
    <id>http://arxiv.org/abs/1712.05876v3</id>
    <updated>2018-07-26T18:19:39Z</updated>
    <published>2017-12-15T23:39:45Z</published>
    <title>Bubble-Flip---A New Generation Algorithm for Prefix Normal Words</title>
    <summary>  We present a new recursive generation algorithm for prefix normal words.
These are binary strings with the property that no substring has more 1s than
the prefix of the same length. The new algorithm uses two operations on binary
strings, which exploit certain properties of prefix normal words in a smart
way. We introduce infinite prefix normal words and show that one of the
operations used by the algorithm, if applied repeatedly to extend the string,
produces an ultimately periodic infinite word, which is prefix normal.
Moreover, based on the original finite word, we can predict both the length and
the density of an ultimate period of this infinite word.
</summary>
    <author>
      <name>Ferdinando Cicalese</name>
    </author>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <author>
      <name>Massimiliano Rossi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2018.06.021</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2018.06.021" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 3 figures, accepted in Theoret. Comp. Sc.. This is the
  journal version of the paper with the same title at LATA 2018 (12th
  International Conference on Language and Automata Theory and Applications,
  Tel Aviv, April 9-11, 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05876v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05876v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1712.05822">
    <id>http://arxiv.org/abs/1712.05822v1</id>
    <updated>2017-12-15T19:46:47Z</updated>
    <published>2017-12-15T19:46:47Z</published>
    <title>Optimal top dag compression</title>
    <summary>  It is shown that for a given ordered node-labelled tree of size $n$ and with
$s$ many different node labels, one can construct in linear time a top dag of
height $O(\log n)$ and size $O(n / \log_\sigma n) \cap O(d \cdot \log n)$,
where $\sigma = \max\{ 2, s\}$ and $d$ is the size of the minimal dag. The size
bound $O(n / \log_\sigma n)$ is optimal and improves on previous bounds.
</summary>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Carl Philipp Reh</name>
    </author>
    <author>
      <name>Kurt Sieber</name>
    </author>
    <link href="http://arxiv.org/abs/1712.05822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P30, 68P05" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1712.05020">
    <id>http://arxiv.org/abs/1712.05020v2</id>
    <updated>2017-12-15T16:07:02Z</updated>
    <published>2017-12-13T21:51:02Z</published>
    <title>B-slack trees: Highly Space Efficient B-trees</title>
    <summary>  B-slack trees, a subclass of B-trees that have substantially better
worst-case space complexity, are introduced. They store $n$ keys in height
$O(\log_b n)$, where $b$ is the maximum node degree. Updates can be performed
in $O(\log_{\frac b 2} n)$ amortized time. A relaxed balance version, which is
well suited for concurrent implementation, is also presented.
</summary>
    <author>
      <name>Trevor Brown</name>
    </author>
    <link href="http://arxiv.org/abs/1712.05020v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05020v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1712.01208">
    <id>http://arxiv.org/abs/1712.01208v3</id>
    <updated>2018-04-30T07:54:41Z</updated>
    <published>2017-12-04T17:18:41Z</published>
    <title>The Case for Learned Index Structures</title>
    <summary>  Indexes are models: a B-Tree-Index can be seen as a model to map a key to the
position of a record within a sorted array, a Hash-Index as a model to map a
key to a position of a record within an unsorted array, and a BitMap-Index as a
model to indicate if a data record exists or not. In this exploratory research
paper, we start from this premise and posit that all existing index structures
can be replaced with other types of models, including deep-learning models,
which we term learned indexes. The key idea is that a model can learn the sort
order or structure of lookup keys and use this signal to effectively predict
the position or existence of records. We theoretically analyze under which
conditions learned indexes outperform traditional index structures and describe
the main challenges in designing learned index structures. Our initial results
show, that by using neural nets we are able to outperform cache-optimized
B-Trees by up to 70% in speed while saving an order-of-magnitude in memory over
several real-world data sets. More importantly though, we believe that the idea
of replacing core components of a data management system through learned models
has far reaching implications for future systems designs and that this work
just provides a glimpse of what might be possible.
</summary>
    <author>
      <name>Tim Kraska</name>
    </author>
    <author>
      <name>Alex Beutel</name>
    </author>
    <author>
      <name>Ed H. Chi</name>
    </author>
    <author>
      <name>Jeffrey Dean</name>
    </author>
    <author>
      <name>Neoklis Polyzotis</name>
    </author>
    <link href="http://arxiv.org/abs/1712.01208v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.01208v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1711.10385">
    <id>http://arxiv.org/abs/1711.10385v1</id>
    <updated>2017-11-28T16:34:46Z</updated>
    <published>2017-11-28T16:34:46Z</published>
    <title>Faster range minimum queries</title>
    <summary>  Range Minimum Query (RMQ) is an important building brick of many compressed
data structures and string matching algorithms. Although this problem is
essentially solved in theory, with sophisticated data structures allowing for
constant time queries, practical performance and construction time also matter.
Additionally, there are offline scenarios in which the number of queries, $q$,
is rather small and given beforehand, which encourages to use a simpler
approach. In this work, we present a simple data structure, with very fast
construction, which allows to handle queries in constant time on average. This
algorithm, however, requires access to the input data during queries (which is
not the case of sophisticated RMQ solutions). We subsequently refine our
technique, combining it with one of the existing succinct solutions with $O(1)$
worst-case time queries and no access to the input array. The resulting hybrid
is still a memory frugal data structure, spending usually up to about $3n$
bits, and providing competitive query times, especially for wide ranges. We
also show how to make our baseline data structure more compact. Experimental
results demonstrate that the proposed BbST (Block-based Sparse Table) variants
are competitive to existing solutions, also in the offline scenario.
</summary>
    <author>
      <name>Tomasz Kowalski</name>
    </author>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A (very preliminary) version of the manuscript was presented in
  Prague Stringology Conference 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.10385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1711.07746">
    <id>http://arxiv.org/abs/1711.07746v3</id>
    <updated>2018-04-03T12:17:47Z</updated>
    <published>2017-11-21T12:32:50Z</published>
    <title>The Hidden Binary Search Tree:A Balanced Rotation-Free Search Tree in
  the AVL RAM Model</title>
    <summary>  In this paper we generalize the definition of "Search Trees" (ST) to enable
reference values other than the key of prior inserted nodes. The idea builds on
the assumption an $n$-node AVL (or Red-Black) requires to assure $O(\log_2n)$
worst-case search time, namely, a single comparison between two keys takes
constant time. This means the size of each key in bits is fixed to $B=c\log_2
n$ ($c\geq1$) once $n$ is determined, otherwise the $O(1)$-time comparison
assumption does not hold. Based on this we calculate \emph{ideal} reference
values from the mid-point of the interval $0..2^B$. This idea follows
`recursively' to assure each node along the search path is provided a reference
value that guarantees an overall logarithmic time. Because the search tree
property works only when keys are compared to reference values and these values
are calculated only during searches, we term the data structure as the Hidden
Binary Search Tree (HBST). We show elementary functions to maintain the HSBT
height $O(B)=O(\log_2n)$. This result requires no special order on the input --
as does BST -- nor self-balancing procedures, as do AVL and Red-Black.
</summary>
    <author>
      <name>Saulo Queiroz</name>
    </author>
    <link href="http://arxiv.org/abs/1711.07746v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07746v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1711.03205">
    <id>http://arxiv.org/abs/1711.03205v1</id>
    <updated>2017-11-08T23:17:48Z</updated>
    <published>2017-11-08T23:17:48Z</published>
    <title>A Grammar Compression Algorithm based on Induced Suffix Sorting</title>
    <summary>  We introduce GCIS, a grammar compression algorithm based on the induced
suffix sorting algorithm SAIS, introduced by Nong et al. in 2009. Our solution
builds on the factorization performed by SAIS during suffix sorting. We
construct a context-free grammar on the input string which can be further
reduced into a shorter string by substituting each substring by its
correspondent factor. The resulting grammar is encoded by exploring some
redundancies, such as common prefixes between suffix rules, which are sorted
according to SAIS framework. When compared to well-known compression tools such
as Re-Pair and 7-zip, our algorithm is competitive and very effective at
handling repetitive string regarding compression ratio, compression and
decompression running time.
</summary>
    <author>
      <name>Daniel Saad Nogueira Nunes</name>
    </author>
    <author>
      <name>Felipe A. Louza</name>
    </author>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Mauricio Ayala-Rincón</name>
    </author>
    <author>
      <name>Gonzalo Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1711.03205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1711.01616">
    <id>http://arxiv.org/abs/1711.01616v3</id>
    <updated>2018-08-27T00:33:14Z</updated>
    <published>2017-11-05T17:10:40Z</published>
    <title>Bloom Filters, Adaptivity, and the Dictionary Problem</title>
    <summary>  The Bloom filter---or, more generally, an approximate membership query data
structure (AMQ)---maintains a compact, probabilistic representation of a set S
of keys from a universe U. An AMQ supports lookups, inserts, and (for some
AMQs) deletes. A query for an x in S is guaranteed to return "present." A query
for x not in S returns "absent" with probability at least 1-epsilon, where
epsilon is a tunable false positive probability. If a query returns "present,"
but x is not in S, then x is a false positive of the AMQ. Because AMQs have a
nonzero probability of false-positives, they require far less space than
explicit set representations.
  AMQs are widely used to speed up dictionaries that are stored remotely (e.g.,
on disk/across a network). Most AMQs offer weak guarantees on the number of
false positives they will return on a sequence of queries. The false-positive
probability of epsilon holds only for a single query. It is easy for an
adversary to drive an AMQ's false-positive rate towards 1 by simply repeating
false positives.
  This paper shows what it takes to get strong guarantees on the number of
false positives. We say that an AMQs is adaptive if it guarantees a
false-positive probability of epsilon for every query, regardless of answers to
previous queries. First, we prove that it is impossible to build a small
adaptive AMQ, even when the AMQ is immediately told whenever it returns a false
positive. We then show how to build an adaptive AMQ that partitions its state
into a small local component and a larger remote component. In addition to
being adaptive, the local component of our AMQ dominates existing AMQs in all
regards. It uses optimal space up to lower-order terms and supports queries and
updates in worst-case constant time, with high probability. Thus, we show that
adaptivity has no cost.
</summary>
    <author>
      <name>Michael A. Bender</name>
    </author>
    <author>
      <name>Martin Farach-Colton</name>
    </author>
    <author>
      <name>Mayank Goswami</name>
    </author>
    <author>
      <name>Rob Johnson</name>
    </author>
    <author>
      <name>Samuel McCauley</name>
    </author>
    <author>
      <name>Shikha Singh</name>
    </author>
    <link href="http://arxiv.org/abs/1711.01616v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01616v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1711.02035">
    <id>http://arxiv.org/abs/1711.02035v2</id>
    <updated>2018-03-05T18:39:29Z</updated>
    <published>2017-11-06T17:41:38Z</published>
    <title>Optimum Search Schemes for Approximate String Matching Using
  Bidirectional FM-Index</title>
    <summary>  Finding approximate occurrences of a pattern in a text using a full-text
index is a central problem in bioinformatics and has been extensively
researched. Bidirectional indices have opened new possibilities in this regard
allowing the search to start from anywhere within the pattern and extend in
both directions. In particular, use of search schemes (partitioning the pattern
and searching the pieces in certain orders with given bounds on errors) can
yield significant speed-ups. However, finding optimal search schemes is a
difficult combinatorial optimization problem.
  Here for the first time, we propose a mixed integer program (MIP) capable to
solve this optimization problem for Hamming distance with given number of
pieces. Our experiments show that the optimal search schemes found by our MIP
significantly improve the performance of search in bidirectional FM-index upon
previous ad-hoc solutions. For example, approximate matching of 101-bp Illumina
reads (with two errors) becomes 35 times faster than standard backtracking.
Moreover, despite being performed purely in the index, the running time of
search using our optimal schemes (for up to two errors) is comparable to the
best state-of-the-art aligners, which benefit from combining search in index
with in-text verification using dynamic programming. As a result, we anticipate
a full-fledged aligner that employs an intelligent combination of search in the
bidirectional FM-index using our optimal search schemes and in-text
verification using dynamic programming outperforms today's best aligners. The
development of such an aligner, called FAMOUS (Fast Approximate string Matching
using OptimUm search Schemes), is ongoing as our future work.
</summary>
    <author>
      <name>Kiavash Kianfar</name>
    </author>
    <author>
      <name>Christopher Pockrandt</name>
    </author>
    <author>
      <name>Bahman Torkamandi</name>
    </author>
    <author>
      <name>Haochen Luo</name>
    </author>
    <author>
      <name>Knut Reinert</name>
    </author>
    <link href="http://arxiv.org/abs/1711.02035v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02035v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2303.07708">
    <id>http://arxiv.org/abs/2303.07708v1</id>
    <updated>2023-03-14T08:50:48Z</updated>
    <published>2023-03-14T08:50:48Z</published>
    <title>Enumerating all minimal hitting sets in polynomial total time</title>
    <summary>  Consider a hypergraph (=set system) $\mathbb{H}$ whose $h$ hyperedges are
subsets of a set with w elements. We show that the $R$ minimal hitting sets of
$\mathbb{H}$ can be enumerated in polynomial total time $O(Rh^2 w^2)$.
</summary>
    <author>
      <name>Marcel Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.07708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.07708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2303.07229">
    <id>http://arxiv.org/abs/2303.07229v1</id>
    <updated>2023-03-13T16:02:44Z</updated>
    <published>2023-03-13T16:02:44Z</published>
    <title>Optimal Square Detection Over General Alphabets</title>
    <summary>  Squares (fragments of the form $xx$, for some string $x$) are arguably the
most natural type of repetition in strings. The basic algorithmic question
concerning squares is to check if a given string of length $n$ is square-free,
that is, does not contain a fragment of such form. Main and Lorentz [J.
Algorithms 1984] designed an $\mathcal{O}(n\log n)$ time algorithm for this
problem, and proved a matching lower bound assuming the so-called general
alphabet, meaning that the algorithm is only allowed to check if two characters
are equal. However, their lower bound also assumes that there are $\Omega(n)$
distinct symbols in the string. As an open question, they asked if there is a
faster algorithm if one restricts the size of the alphabet. Crochemore [Theor.
Comput. Sci. 1986] designed a linear-time algorithm for constant-size
alphabets, and combined with more recent results his approach in fact implies
such an algorithm for linearly-sortable alphabets. Very recently, Ellert and
Fischer [ICALP 2021] significantly relaxed this assumption by designing a
linear-time algorithm for general ordered alphabets, that is, assuming a linear
order on the characters that permits constant time order comparisons. However,
the open question of Main and Lorentz from 1984 remained unresolved for general
(unordered) alphabets. In this paper, we show that testing square-freeness of a
length-$n$ string over general alphabet of size $\sigma$ can be done with
$\mathcal{O}(n\log \sigma)$ comparisons, and cannot be done with $o(n\log
\sigma)$ comparisons. We complement this result with an $\mathcal{O}(n\log
\sigma)$ time algorithm in the Word RAM model. Finally, we extend the algorithm
to reporting all the runs (maximal repetitions) in the same complexity.
</summary>
    <author>
      <name>Jonas Ellert</name>
    </author>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Garance Gourdel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of a paper published in SODA 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.07229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.07229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2303.05799">
    <id>http://arxiv.org/abs/2303.05799v1</id>
    <updated>2023-03-10T09:09:13Z</updated>
    <published>2023-03-10T09:09:13Z</published>
    <title>Optimal-Hash Exact String Matching Algorithms</title>
    <summary>  String matching is the problem of finding all the occurrences of a pattern in
a text. We propose improved versions of the fast family of string matching
algorithms based on hashing $q$-grams. The improvement consists of considering
minimal values $q$ such that each $q$-grams of the pattern has a unique hash
value. The new algorithms are fastest than algorithm of the HASH family for
short patterns on large size alphabets.
</summary>
    <author>
      <name>Thierry Lecroq</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.05799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.05799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2303.04722">
    <id>http://arxiv.org/abs/2303.04722v1</id>
    <updated>2023-03-08T17:07:52Z</updated>
    <published>2023-03-08T17:07:52Z</published>
    <title>B-Treaps Revised: Write Efficient Randomized Block Search Trees with
  High Load</title>
    <summary>  Uniquely represented data structures represent each logical state with a
unique storage state. We study the problem of maintaining a dynamic set of $n$
keys from a totally ordered universe in this context.
  We introduce a two-layer data structure called
$(\alpha,\varepsilon)$-Randomized Block Search Tree (RBST) that is uniquely
represented and suitable for external memory. Though RBSTs naturally generalize
the well-known binary Treaps, several new ideas are needed to analyze the {\em
expected} search, update, and storage, efficiency in terms of block-reads,
block-writes, and blocks stored. We prove that searches have
$O(\varepsilon^{-1} + \log_\alpha n)$ block-reads, that $(\alpha,
\varepsilon)$-RBSTs have an asymptotic load-factor of at least
$(1-\varepsilon)$ for every $\varepsilon \in (0,1/2]$, and that dynamic updates
perform $O(\varepsilon^{-1} + \log_\alpha(n)/\alpha)$ block-writes, i.e.
$O(1/\varepsilon)$ writes if $\alpha=\Omega(\frac{\log n}{\log \log n} )$. Thus
$(\alpha, \varepsilon)$-RBSTs provide improved search, storage-, and
write-efficiency bounds in regard to the known, uniquely represented B-Treap
[Golovin; ICALP'09].
</summary>
    <author>
      <name>Roodabeh Safavi</name>
    </author>
    <author>
      <name>Martin P. Seybold</name>
    </author>
    <link href="http://arxiv.org/abs/2303.04722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.04722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2303.04478">
    <id>http://arxiv.org/abs/2303.04478v1</id>
    <updated>2023-03-08T09:59:00Z</updated>
    <published>2023-03-08T09:59:00Z</published>
    <title>Change a Bit to save Bytes: Compression for Floating Point Time-Series
  Data</title>
    <summary>  The number of IoT devices is expected to continue its dramatic growth in the
coming years and, with it, a growth in the amount of data to be transmitted,
processed and stored. Compression techniques that support analytics directly on
the compressed data could pave the way for systems to scale efficiently to
these growing demands. This paper proposes two novel methods for preprocessing
a stream of floating point data to improve the compression capabilities of
various IoT data compressors. In particular, these techniques are shown to be
helpful with recent compressors that allow for random access and analytics
while maintaining good compression. Our techniques improve compression with
reductions up to 80% when allowing for at most 1% of recovery error.
</summary>
    <author>
      <name>Francesco Taurone</name>
    </author>
    <author>
      <name>Daniel E. Lucani</name>
    </author>
    <author>
      <name>Marcell Fehér</name>
    </author>
    <author>
      <name>Qi Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2303.04478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.04478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2303.01726">
    <id>http://arxiv.org/abs/2303.01726v1</id>
    <updated>2023-03-03T06:11:37Z</updated>
    <published>2023-03-03T06:11:37Z</published>
    <title>On Sensitivity of Compact Directed Acyclic Word Graphs</title>
    <summary>  Compact directed acyclic word graphs (CDAWGs) [Blumer et al. 1987] are a
fundamental data structure on strings with applications in text pattern
searching, data compression, and pattern discovery. Intuitively, the CDAWG of a
string $T$ is obtained by merging isomorphic subtrees of the suffix tree
[Weiner 1973] of the same string $T$, thus CDAWGs are a compact indexing
structure. In this paper, we investigate the sensitivity of CDAWGs when a
single character edit operation (insertion, deletion, or substitution) is
performed at the left-end of the input string $T$, namely, we are interested in
the worst-case increase in the size of the CDAWG after an left-end edit
operation. We prove that if $e$ is the number of edges of the CDAWG for string
$T$, then the number of new edges added to the CDAWG after an left-edit
operation on $T$ is less than $e$. Further, we present almost matching lower
bounds on the sensitivity of CDAWGs for all cases of insertion, deletion, and
substitution.
</summary>
    <author>
      <name>Hiroto Fujimaru</name>
    </author>
    <author>
      <name>Yuto Nakashima</name>
    </author>
    <author>
      <name>Shunsuke Inenaga</name>
    </author>
    <link href="http://arxiv.org/abs/2303.01726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.01726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.13147">
    <id>http://arxiv.org/abs/2302.13147v1</id>
    <updated>2023-02-25T20:05:16Z</updated>
    <published>2023-02-25T20:05:16Z</published>
    <title>Smallest and Largest Block Palindrome Factorizations</title>
    <summary>  A \emph{palindrome} is a word that reads the same forwards and backwards. A
\emph{block palindrome factorization} (or \emph{BP-factorization}) is a
factorization of a word into blocks that becomes palindrome if each identical
block is replaced by a distinct symbol. We call the number of blocks in a
BP-factorization the \emph{width} of the BP-factorization. The \emph{largest
BP-factorization} of a word $w$ is the BP-factorization of $w$ with the maximum
width. We study words with certain BP-factorizations. First, we give a
recurrence for the number of length-$n$ words with largest BP-factorization of
width $t$. Second, we show that the expected width of the largest
BP-factorization of a word tends to a constant. Third, we give some results on
another extremal variation of BP-factorization, the \emph{smallest
BP-factorization}. A \emph{border} of a word $w$ is a non-empty word that is
both a proper prefix and suffix of $w$. Finally, we conclude by showing a
connection between words with a unique border and words whose smallest and
largest BP-factorizations coincide.
</summary>
    <author>
      <name>Daniel Gabric</name>
    </author>
    <author>
      <name>Jeffrey Shallit</name>
    </author>
    <link href="http://arxiv.org/abs/2302.13147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2302.13647">
    <id>http://arxiv.org/abs/2302.13647v1</id>
    <updated>2023-02-27T10:27:39Z</updated>
    <published>2023-02-27T10:27:39Z</published>
    <title>String attractors of fixed points of k-bonacci-like morphisms</title>
    <summary>  Firstly studied by Kempa and Prezza in 2018 as the cement of text compression
algorithms, string attractors have become a compelling object of theoretical
research within the community of combinatorics on words. In this context, they
have been studied for several families of finite and infinite words. In this
paper, we obtain string attractors of prefixes of particular infinite words
generalizing k-bonacci words (including the famous Fibonacci word) and obtained
as fixed points of k-bonacci-like morphisms. In fact, our description involves
the numeration systems classically derived from the considered morphisms
</summary>
    <author>
      <name>France Gheeraert</name>
    </author>
    <author>
      <name>Giuseppe Romana</name>
    </author>
    <author>
      <name>Manon Stipulanti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of a paper submitted to WORDS 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.13647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15, 05A05, 11A67, 68P05, 68Q45" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1712.09738">
    <id>http://arxiv.org/abs/1712.09738v2</id>
    <updated>2018-06-28T11:33:31Z</updated>
    <published>2017-12-28T02:17:49Z</published>
    <title>On the Decision Tree Complexity of String Matching</title>
    <summary>  String matching is one of the most fundamental problems in computer science.
A natural problem is to determine the number of characters that need to be
queried (i.e. the decision tree complexity) in a string in order to decide
whether this string contains a certain pattern. Rivest showed that for every
pattern $p$, in the worst case any deterministic algorithm needs to query at
least $n-|p|+1$ characters, where $n$ is the length of the string and $|p|$ is
the length of the pattern. He further conjectured that this bound is tight. By
using the adversary method, Tuza disproved this conjecture and showed that more
than one half of binary patterns are {\em evasive}, i.e. any algorithm needs to
query all the characters (see Section 1.1 for more details).
  In this paper, we give a query algorithm which settles the decision tree
complexity of string matching except for a negligible fraction of patterns. Our
algorithm shows that Tuza's criteria of evasive patterns are almost complete.
Using the algebraic approach of Rivest and Vuillemin, we also give a new
sufficient condition for the evasiveness of patterns, which is beyond Tuza's
criteria. In addition, our result reveals an interesting connection to
\emph{Skolem's Problem} in mathematics.
</summary>
    <author>
      <name>Xiaoyu He</name>
    </author>
    <author>
      <name>Neng Huang</name>
    </author>
    <author>
      <name>Xiaoming Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.09738v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09738v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1712.09230">
    <id>http://arxiv.org/abs/1712.09230v1</id>
    <updated>2017-12-26T10:55:12Z</updated>
    <published>2017-12-26T10:55:12Z</published>
    <title>Space-Efficient Algorithms for Longest Increasing Subsequence</title>
    <summary>  Given a sequence of integers, we want to find a longest increasing
subsequence of the sequence. It is known that this problem can be solved in
$O(n \log n)$ time and space. Our goal in this paper is to reduce the space
consumption while keeping the time complexity small. For $\sqrt{n} \le s \le
n$, we present algorithms that use $O(s \log n)$ bits and $O(\frac{1}{s} \cdot
n^{2} \cdot \log n)$ time for computing the length of a longest increasing
subsequence, and $O(\frac{1}{s} \cdot n^{2} \cdot \log^{2} n)$ time for finding
an actual subsequence. We also show that the time complexity of our algorithms
is optimal up to polylogarithmic factors in the framework of sequential access
algorithms with the prescribed amount of space.
</summary>
    <author>
      <name>Masashi Kiyomi</name>
    </author>
    <author>
      <name>Hirotaka Ono</name>
    </author>
    <author>
      <name>Yota Otachi</name>
    </author>
    <author>
      <name>Pascal Schweitzer</name>
    </author>
    <author>
      <name>Jun Tarui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, STACS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.09230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2303.14539">
    <id>http://arxiv.org/abs/2303.14539v1</id>
    <updated>2023-03-25T19:07:42Z</updated>
    <published>2023-03-25T19:07:42Z</published>
    <title>The analogue of overlap-freeness for the period-doubling sequence</title>
    <summary>  Good words are binary words avoiding factors 11 and 1001, and patterns 0000
and 00010100. We show that good words bear the same relationship to the
period-doubling sequence that overlap-free words bear to the Thue-Morse
sequence. We prove an analogue of Fife's Theorem for good words, exhibit the
lexicographically least and greatest infinite good words, and determine the
patterns avoided by the period doubling word.
</summary>
    <author>
      <name>James D. Currie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.14539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.14539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R15" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2303.10926">
    <id>http://arxiv.org/abs/2303.10926v1</id>
    <updated>2023-03-20T08:00:13Z</updated>
    <published>2023-03-20T08:00:13Z</published>
    <title>On the Maximal Independent Sets of $k$-mers with the Edit Distance</title>
    <summary>  In computational biology, $k$-mers and edit distance are fundamental
concepts. However, little is known about the metric space of all $k$-mers
equipped with the edit distance. In this work, we explore the structure of the
$k$-mer space by studying its maximal independent sets (MISs). An MIS is a
sparse sketch of all $k$-mers with nice theoretical properties, and therefore
admits critical applications in clustering, indexing, hashing, and sketching
large-scale sequencing data, particularly those with high error-rates. Finding
an MIS is a challenging problem, as the size of a $k$-mer space grows
geometrically with respect to $k$. We propose three algorithms for this
problem. The first and the most intuitive one uses a greedy strategy. The
second method implements two techniques to avoid redundant comparisons by
taking advantage of the locality-property of the $k$-mer space and the
estimated bounds on the edit distance. The last algorithm avoids expensive
calculations of the edit distance by translating the edit distance into the
shortest path in a specifically designed graph. These algorithms are
implemented and the calculated MISs of $k$-mer spaces and their statistical
properties are reported and analyzed for $k$ up to 15. Source code is freely
available at https://github.com/Shao-Group/kmerspace .
</summary>
    <author>
      <name>Leran Ma</name>
    </author>
    <author>
      <name>Ke Chen</name>
    </author>
    <author>
      <name>Mingfu Shao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.10926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.10926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1603.07457">
    <id>http://arxiv.org/abs/1603.07457v2</id>
    <updated>2016-04-05T20:15:57Z</updated>
    <published>2016-03-24T07:38:18Z</published>
    <title>Parameterized Pattern Matching -- Succinctly</title>
    <summary>  We consider the $Parameterized$ $Pattern$ $Matching$ problem, where a pattern
$P$ matches some location in a text $\mathsf{T}$ iff there is a one-to-one
correspondence between the alphabet symbols of the pattern to those of the
text. More specifically, assume that the text $\mathsf{T}$ contains $n$
characters from a static alphabet $\Sigma_s$ and a parameterized alphabet
$\Sigma_p$, where $\Sigma_s \cap \Sigma_p = \varnothing$ and $|\Sigma_s \cup
\Sigma_p|=\sigma$. A pattern $P$ matches a substring $S$ of $\mathsf{T}$ iff
the static characters match exactly, and there exists a one-to-one function
that renames the parameterized characters in $S$ to that in $P$. Previous
indexing solution [Baker, STOC 1993], known as $Parameterized$ $Suffix$ $Tree$,
requires $\Theta(n\log n)$ bits of space, and can find all $occ$ occurrences of
$P$ in $\mathcal{O}(|P|\log \sigma+ occ)$ time. In this paper, we present the
first succinct index that occupies $n \log \sigma + \mathcal{O}(n)$ bits and
answers queries in $\mathcal{O}((|P|+ occ\cdot \log n) \log\sigma\log \log
\sigma)$ time. We also present a compact index that occupies
$\mathcal{O}(n\log\sigma)$ bits and answers queries in $\mathcal{O}(|P|\log
\sigma+ occ\cdot \log n)$ time. Furthermore, the techniques are extended to
obtain the first succinct representation of the index of Shibuya for
$Structural$ $Matching$ [SWAT, 2000], and of Idury and Sch\"{a}ffer for
$Parameterized$ $Dictionary$ $Matching$ [CPM, 1994].
</summary>
    <author>
      <name>Arnab Ganguly</name>
    </author>
    <author>
      <name>Rahul Shah</name>
    </author>
    <author>
      <name>Sharma V. Thankachan</name>
    </author>
    <link href="http://arxiv.org/abs/1603.07457v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07457v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1612.08555">
    <id>http://arxiv.org/abs/1612.08555v1</id>
    <updated>2016-12-27T09:54:07Z</updated>
    <published>2016-12-27T09:54:07Z</published>
    <title>Monte Carlo Sort for unreliable human comparisons</title>
    <summary>  Algorithms which sort lists of real numbers into ascending order have been
studied for decades. They are typically based on a series of pairwise
comparisons and run entirely on chip. However people routinely sort lists which
depend on subjective or complex judgements that cannot be automated. Examples
include marketing research; where surveys are used to learn about customer
preferences for products, the recruiting process; where interviewers attempt to
rank potential employees, and sporting tournaments; where we infer team
rankings from a series of one on one matches. We develop a novel sorting
algorithm, where each pairwise comparison reflects a subjective human judgement
about which element is bigger or better. We introduce a finite and large error
rate to each judgement, and we take the cost of each comparison to
significantly exceed the cost of other computational steps. The algorithm must
request the most informative sequence of comparisons from the user; in order to
identify the correct sorted list with minimum human input. Our Discrete
Adiabatic Monte Carlo approach exploits the gradual acquisition of information
by tracking a set of plausible hypotheses which are updated after each
additional comparison.
</summary>
    <author>
      <name>Samuel L Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.08555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.08555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1603.04892">
    <id>http://arxiv.org/abs/1603.04892v1</id>
    <updated>2016-03-15T21:38:57Z</updated>
    <published>2016-03-15T21:38:57Z</published>
    <title>The landscape of bounds for binary search trees</title>
    <summary>  Binary search trees (BSTs) with rotations can adapt to various kinds of
structure in search sequences, achieving amortized access times substantially
better than the Theta(log n) worst-case guarantee. Classical examples of
structural properties include static optimality, sequential access, working
set, key-independent optimality, and dynamic finger, all of which are now known
to be achieved by the two famous online BST algorithms (Splay and Greedy).
(...)
  In this paper, we introduce novel properties that explain the efficiency of
sequences not captured by any of the previously known properties, and which
provide new barriers to the dynamic optimality conjecture. We also establish
connections between various properties, old and new. For instance, we show the
following.
  (i) A tight bound of O(n log d) on the cost of Greedy for d-decomposable
sequences. The result builds on the recent lazy finger result of Iacono and
Langerman (SODA 2016). On the other hand, we show that lazy finger alone cannot
explain the efficiency of pattern avoiding sequences even in some of the
simplest cases. (ii) A hierarchy of bounds using multiple lazy fingers,
addressing a recent question of Iacono and Langerman. (iii) The optimality of
the Move-to-root heuristic in the key-independent setting introduced by Iacono
(Algorithmica 2005). (iv) A new tool that allows combining any finite number of
sound structural properties. As an application, we show an upper bound on the
cost of a class of sequences that all known properties fail to capture. (v) The
equivalence between two families of BST properties. The observation on which
this connection is based was known before - we make it explicit, and apply it
to classical BST properties. (...)
</summary>
    <author>
      <name>Parinya Chalermsook</name>
    </author>
    <author>
      <name>Mayank Goswami</name>
    </author>
    <author>
      <name>László Kozma</name>
    </author>
    <author>
      <name>Kurt Mehlhorn</name>
    </author>
    <author>
      <name>Thatchaphol Saranurak</name>
    </author>
    <link href="http://arxiv.org/abs/1603.04892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.06426">
    <id>http://arxiv.org/abs/1602.06426v2</id>
    <updated>2018-03-19T20:54:58Z</updated>
    <published>2016-02-20T16:24:30Z</published>
    <title>A loopless and branchless $O(1)$ algorithm to generate the next Dyck
  word</title>
    <summary>  Let integer be any C/C++ unsigned integer type up to 64-bits long. Given a
Dyck word the following code returns the next Dyck word of the same size,
provided it exists.
  integer next_dyck_word(integer w) {
  integer const a = w &amp; -w;
  integer const b = w + a;
  integer c = w ^ b;
  c = (c / a >> 2) + 1;
  c = ((c * c - 1) &amp; 0xaaaaaaaaaaaaaaaa) | b;
  return c;
  }
</summary>
    <author>
      <name>Cassio Neri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First published on 19 July 2014 at https://github.com/cassioneri/Dyck</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.06426v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06426v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.05889">
    <id>http://arxiv.org/abs/1602.05889v1</id>
    <updated>2016-02-18T17:42:54Z</updated>
    <published>2016-02-18T17:42:54Z</published>
    <title>Distortion-Resistant Hashing for rapid search of similar DNA subsequence</title>
    <summary>  One of the basic tasks in bioinformatics is localizing a short subsequence
$S$, read while sequencing, in a long reference sequence $R$, like the human
geneome. A natural rapid approach would be finding a hash value for $S$ and
compare it with a prepared database of hash values for each of length $|S|$
subsequences of $R$. The problem with such approach is that it would only spot
a perfect match, while in reality there are lots of small changes:
substitutions, deletions and insertions.
  This issue could be repaired if having a hash function designed to tolerate
some small distortion accordingly to an alignment metric (like
Needleman-Wunch): designed to make that two similar sequences should most
likely give the same hash value. This paper discusses construction of
Distortion-Resistant Hashing (DRH) to generate such fingerprints for rapid
search of similar subsequences. The proposed approach is based on the rate
distortion theory: in a nearly uniform subset of length $|S|$ sequences, the
hash value represents the closest sequence to $S$. This gives some control of
the distance of collisions: sequences having the same hash value.
</summary>
    <author>
      <name>Jarek Duda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.05889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.05856">
    <id>http://arxiv.org/abs/1602.05856v1</id>
    <updated>2016-02-18T16:11:50Z</updated>
    <published>2016-02-18T16:11:50Z</published>
    <title>TwoPaCo: An efficient algorithm to build the compacted de Bruijn graph
  from many complete genomes</title>
    <summary>  Motivation: De Bruijn graphs have been proposed as a data structure to
facilitate the analysis of related whole genome sequences, in both a population
and comparative genomic settings. However, current approaches do not scale well
to many genomes of large size (such as mammalian genomes). Results: In this
paper, we present TwoPaCo, a simple and scalable low memory algorithm for the
direct construction of the compacted de Bruijn graph from a set of complete
genomes. We demonstrate that it can construct the graph for 100 simulated human
genomes in less then a day and eight real primates in less than two hours, on a
typical shared-memory machine. We believe that this progress will enable novel
biological analyses of hundreds of mammalian-sized genomes. Availability: Our
code and data is available for download from github.com/medvedevgroup/TwoPaCo
Contact: ium125@psu.edu
</summary>
    <author>
      <name>Ilia Minkin</name>
    </author>
    <author>
      <name>Son Pham</name>
    </author>
    <author>
      <name>Paul Medvedev</name>
    </author>
    <link href="http://arxiv.org/abs/1602.05856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.02362">
    <id>http://arxiv.org/abs/1602.02362v1</id>
    <updated>2016-02-07T11:54:08Z</updated>
    <published>2016-02-07T11:54:08Z</published>
    <title>On the circuit complexity of the standard and the Karatsuba methods of
  multiplying integers</title>
    <summary>  We provide accurate upper bounds on the Boolean circuit complexity of the
standard and the Karatsuba methods of integer multiplication
</summary>
    <author>
      <name>Igor S. Sergeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, published in Russian in Proc. XXII Conf. "Information means
  and technology" (Moscow, November 18--20, 2014). Vol. 3. Moscow, MPEI, 2014,
  180--187</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.02362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1602.00621">
    <id>http://arxiv.org/abs/1602.00621v2</id>
    <updated>2016-10-28T18:12:13Z</updated>
    <published>2016-02-01T18:08:47Z</published>
    <title>On pattern matching with k mismatches and few don't cares</title>
    <summary>  We consider the problem of pattern matching with $k$ mismatches, where there
can be don't care or wild card characters in the pattern. Specifically, given a
pattern $P$ of length $m$ and a text $T$ of length $n$, we want to find all
occurrences of $P$ in $T$ that have no more than $k$ mismatches. The pattern
can have don't care characters, which match any character. Without don't cares,
the best known algorithm for pattern matching with $k$ mismatches has a runtime
of $O(n\sqrt{k \log k})$. With don't cares in the pattern, the best
deterministic algorithm has a runtime of $O(nk polylog m)$. Therefore, there is
an important gap between the versions with and without don't cares.
  In this paper we give an algorithm whose runtime increases with the number of
don't cares. We define an {\em island} to be a maximal length substring of $P$
that does not contain don't cares. Let $q$ be the number of islands in $P$. We
present an algorithm that runs in $O(n\sqrt{k\log m}+n\min\{\sqrt[3]{qk\log^2
m},\sqrt{q\log m}\})$ time. If the number of islands $q$ is $O(k)$ this runtime
becomes $O(n\sqrt{k\log m})$, which essentially matches the best known runtime
for pattern matching with $k$ mismatches without don't cares. If the number of
islands $q$ is $O(k^2)$, this algorithm is asymptotically faster than the
previous best algorithm for pattern matching with $k$ mismatches with don't
cares in the pattern.
</summary>
    <author>
      <name>Marius Nicolae</name>
    </author>
    <author>
      <name>Sanguthevar Rajasekaran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ipl.2016.10.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ipl.2016.10.003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Information Processing Letters, Available online 27 October 2016,
  ISSN 0020-0190</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.00621v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00621v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1601.05020">
    <id>http://arxiv.org/abs/1601.05020v3</id>
    <updated>2016-03-08T15:43:48Z</updated>
    <published>2016-01-19T17:54:14Z</published>
    <title>Low Space External Memory Construction of the Succinct Permuted Longest
  Common Prefix Array</title>
    <summary>  The longest common prefix (LCP) array is a versatile auxiliary data structure
in indexed string matching. It can be used to speed up searching using the
suffix array (SA) and provides an implicit representation of the topology of an
underlying suffix tree. The LCP array of a string of length $n$ can be
represented as an array of length $n$ words, or, in the presence of the SA, as
a bit vector of $2n$ bits plus asymptotically negligible support data
structures. External memory construction algorithms for the LCP array have been
proposed, but those proposed so far have a space requirement of $O(n)$ words
(i.e. $O(n \log n)$ bits) in external memory. This space requirement is in some
practical cases prohibitively expensive. We present an external memory
algorithm for constructing the $2n$ bit version of the LCP array which uses
$O(n \log \sigma)$ bits of additional space in external memory when given a
(compressed) BWT with alphabet size $\sigma$ and a sampled inverse suffix array
at sampling rate $O(\log n)$. This is often a significant space gain in
practice where $\sigma$ is usually much smaller than $n$ or even constant. We
also consider the case of computing succinct LCP arrays for circular strings.
</summary>
    <author>
      <name>German Tischler</name>
    </author>
    <link href="http://arxiv.org/abs/1601.05020v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.05020v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1601.03428">
    <id>http://arxiv.org/abs/1601.03428v1</id>
    <updated>2016-01-13T22:19:20Z</updated>
    <published>2016-01-13T22:19:20Z</published>
    <title>The complexity of bit retrieval</title>
    <summary>  Bit retrieval is the problem of reconstructing a binary sequence from its
periodic autocorrelation, with applications in cryptography and x-ray
crystallography. After defining the problem, with and without noise, we
describe and compare various algorithms for solving it. A geometrical
constraint satisfaction algorithm, relaxed-reflect-reflect, is currently the
best algorithm for noisy bit retrieval.
</summary>
    <author>
      <name>Veit Elser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.03428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.01539">
    <id>http://arxiv.org/abs/1605.01539v2</id>
    <updated>2016-05-12T16:01:30Z</updated>
    <published>2016-05-05T09:39:59Z</published>
    <title>Rank and select: Another lesson learned</title>
    <summary>  Rank and select queries on bitmaps are essential building bricks of many
compressed data structures, including text indexes, membership and range
supporting spatial data structures, compressed graphs, and more. Theoretically
considered yet in 1980s, these primitives have also been a subject of vivid
research concerning their practical incarnations in the last decade. We present
a few novel rank/select variants, focusing mostly on speed, obtaining
competitive space-time results in the compressed setting. Our findings can be
summarized as follows: $(i)$ no single rank/select solution works best on any
kind of data (ours are optimized for concatenated bit arrays obtained from
wavelet trees for real text datasets), $(ii)$ it pays to efficiently handle
blocks consisting of all 0 or all 1 bits, $(iii)$ compressed select does not
have to be significantly slower than compressed rank at a comparable memory
use.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Marcin Raniszewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Compared to v1: slightly optimized rank implementations</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01539v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01539v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1604.08860">
    <id>http://arxiv.org/abs/1604.08860v4</id>
    <updated>2016-11-25T22:25:36Z</updated>
    <published>2016-04-28T14:31:49Z</published>
    <title>Designing optimal- and fast-on-average pattern matching algorithms</title>
    <summary>  Given a pattern $w$ and a text $t$, the speed of a pattern matching algorithm
over $t$ with regard to $w$, is the ratio of the length of $t$ to the number of
text accesses performed to search $w$ into $t$. We first propose a general
method for computing the limit of the expected speed of pattern matching
algorithms, with regard to $w$, over iid texts. Next, we show how to determine
the greatest speed which can be achieved among a large class of algorithms,
altogether with an algorithm running this speed. Since the complexity of this
determination make it impossible to deal with patterns of length greater than
4, we propose a polynomial heuristic. Finally, our approaches are compared with
9 pre-existing pattern matching algorithms from both a theoretical and a
practical point of view, i.e. both in terms of limit expected speed on iid
texts, and in terms of observed average speed on real data. In all cases, the
pre-existing algorithms are outperformed.
</summary>
    <author>
      <name>Gilles Didier</name>
    </author>
    <author>
      <name>Laurent Tichit</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08860v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08860v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1604.06264">
    <id>http://arxiv.org/abs/1604.06264v1</id>
    <updated>2016-04-21T11:42:41Z</updated>
    <published>2016-04-21T11:42:41Z</published>
    <title>Data Structure Lower Bounds for Document Indexing Problems</title>
    <summary>  We study data structure problems related to document indexing and pattern
matching queries and our main contribution is to show that the pointer machine
model of computation can be extremely useful in proving high and unconditional
lower bounds that cannot be obtained in any other known model of computation
with the current techniques. Often our lower bounds match the known space-query
time trade-off curve and in fact for all the problems considered, there is a
very good and reasonable match between the our lower bounds and the known upper
bounds, at least for some choice of input parameters. The problems that we
consider are set intersection queries (both the reporting variant and the
semi-group counting variant), indexing a set of documents for two-pattern
queries, or forbidden- pattern queries, or queries with wild-cards, and
indexing an input set of gapped-patterns (or two-patterns) to find those
matching a document given at the query time.
</summary>
    <author>
      <name>Peyman Afshani</name>
    </author>
    <author>
      <name>Jesper Sindahl Nielsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of the conference version that appeared at ICALP 2016,
  25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1604.06058">
    <id>http://arxiv.org/abs/1604.06058v3</id>
    <updated>2017-03-16T17:27:05Z</updated>
    <published>2016-04-20T18:52:00Z</published>
    <title>Succinct Choice Dictionaries</title>
    <summary>  The choice dictionary is introduced as a data structure that can be
initialized with a parameter $n\in\mathbb{N}=\{1,2,\ldots\}$ and subsequently
maintains an initially empty subset $S$ of $\{1,\ldots,n\}$ under insertion,
deletion, membership queries and an operation choice that returns an arbitrary
element of $S$. The choice dictionary appears to be fundamental in
space-efficient computing. We show that there is a choice dictionary that can
be initialized with $n$ and an additional parameter $t\in\mathbb{N}$ and
subsequently occupies $n+O(n(t/w)^t+\log n)$ bits of memory and executes each
of the four operations insert, delete, contains (i.e., a membership query) and
choice in $O(t)$ time on a word RAM with a word length of $w=\Omega(\log n)$
bits. In particular, with $w=\Theta(\log n)$, we can support insert, delete,
contains and choice in constant time using $n+O(n/(\log n)^t)$ bits for
arbitrary fixed $t$. We extend our results to maintaining several pairwise
disjoint subsets of $\{1,\ldots,n\}$.
  We study additional space-efficient data structures for subsets $S$ of
$\{1,\ldots,n\}$, including one that supports only insertion and an operation
extract-choice that returns and deletes an arbitrary element of $S$. All our
main data structures can be initialized in constant time and support efficient
iteration over the set $S$, and we can allow changes to $S$ while an iteration
over $S$ is in progress. We use these abilities crucially in designing the most
space-efficient algorithms known for solving a number of graph and other
combinatorial problems in linear time. In particular, given an undirected graph
$G$ with $n$ vertices and $m$ edges, we can output a spanning forest of $G$ in
$O(n+m)$ time with at most $(1+\epsilon)n$ bits of working memory for arbitrary
fixed $\epsilon>0$.
</summary>
    <author>
      <name>Torben Hagerup</name>
    </author>
    <author>
      <name>Frank Kammer</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06058v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06058v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1604.03132">
    <id>http://arxiv.org/abs/1604.03132v1</id>
    <updated>2016-04-11T20:10:48Z</updated>
    <published>2016-04-11T20:10:48Z</published>
    <title>Efficient Index Maintenance Under Dynamic Genome Modification</title>
    <summary>  Efficient text indexing data structures have enabled large-scale genomic
sequence analysis and are used to help solve problems ranging from assembly to
read mapping. However, these data structures typically assume that the
underlying reference text is static and will not change over the course of the
queries being made. Some progress has been made in exploring how certain text
indices, like the suffix array, may be updated, rather than rebuilt from
scratch, when the underlying reference changes. Yet, these update operations
can be complex in practice, difficult to implement, and give fairly pessimistic
worst-case bounds. We present a novel data structure, SkipPatch, for
maintaining a k-mer-based index over a dynamically changing genome. SkipPatch
pairs a hash-based k-mer index with an indexable skip list that is used to
efficiently maintain the set of edits that have been applied to the original
genome. SkipPatch is practically fast, significantly outperforming the dynamic
extended suffix array in terms of update and query speed.
</summary>
    <author>
      <name>Nitish Gupta</name>
    </author>
    <author>
      <name>Komal Sanjeev</name>
    </author>
    <author>
      <name>Tim Wall</name>
    </author>
    <author>
      <name>Carl Kingsford</name>
    </author>
    <author>
      <name>Rob Patro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">paper accepted at the RECOMB-Seq 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1604.02552">
    <id>http://arxiv.org/abs/1604.02552v4</id>
    <updated>2016-10-11T14:40:46Z</updated>
    <published>2016-04-09T11:48:47Z</published>
    <title>Computing Longest Increasing Subsequence Over Sequential Data Streams</title>
    <summary>  In this paper, we propose a data structure, a quadruple neighbor list
(QN-list, for short), to support real time queries of all longest increasing
subsequence (LIS) and LIS with constraints over sequential data streams. The
QN-List built by our algorithm requires $O(w)$ space, where $w$ is the time
window size. The running time for building the initial QN-List takes $O(w\log
w)$ time. Applying the QN-List, insertion of the new item takes $O(\log w)$
time and deletion of the first item takes $O(w)$ time. To the best of our
knowledge, this is the first work to support both LIS enumeration and LIS with
constraints computation by using a single uniform data structure for real time
sequential data streams. Our method outperforms the state-of-the-art methods in
both time and space cost, not only theoretically, but also empirically.
</summary>
    <author>
      <name>Youhuan Li</name>
    </author>
    <author>
      <name>Lei Zou</name>
    </author>
    <author>
      <name>Huaming Zhang</name>
    </author>
    <author>
      <name>Dongyan Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages (12+8)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02552v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02552v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1604.01789">
    <id>http://arxiv.org/abs/1604.01789v3</id>
    <updated>2020-09-27T00:31:25Z</updated>
    <published>2016-04-06T20:04:56Z</published>
    <title>GateKeeper: A New Hardware Architecture for Accelerating Pre-Alignment
  in DNA Short Read Mapping</title>
    <summary>  Motivation: High throughput DNA sequencing (HTS) technologies generate an
excessive number of small DNA segments -- called short reads -- that cause
significant computational burden. To analyze the entire genome, each of the
billions of short reads must be mapped to a reference genome based on the
similarity between a read and "candidate" locations in that reference genome.
The similarity measurement, called alignment, formulated as an approximate
string matching problem, is the computational bottleneck because: (1) it is
implemented using quadratic-time dynamic programming algorithms, and (2) the
majority of candidate locations in the reference genome do not align with a
given read due to high dissimilarity. Calculating the alignment of such
incorrect candidate locations consumes an overwhelming majority of a modern
read mapper's execution time. Therefore, it is crucial to develop a fast and
effective filter that can detect incorrect candidate locations and eliminate
them before invoking computationally costly alignment operations. Results: We
propose GateKeeper, a new hardware accelerator that functions as a
pre-alignment step that quickly filters out most incorrect candidate locations.
GateKeeper is the first design to accelerate pre-alignment using
Field-Programmable Gate Arrays (FPGAs), which can perform pre-alignment much
faster than software. GateKeeper can be integrated with any mapper that
performs sequence alignment for verification. When implemented on a single FPGA
chip, GateKeeper maintains high accuracy (on average >96%) while providing up
to 90-fold and 130-fold speedup over the state-of-the-art software
pre-alignment techniques, Adjacency Filter and Shifted Hamming Distance (SHD),
respectively. The addition of GateKeeper as a pre-alignment step can reduce the
verification time of the mrFAST mapper by a factor of 10. Availability:
https://github.com/BilkentCompGen/GateKeeper
</summary>
    <author>
      <name>Mohammed Alser</name>
    </author>
    <author>
      <name>Hasan Hassan</name>
    </author>
    <author>
      <name>Hongyi Xin</name>
    </author>
    <author>
      <name>Oğuz Ergin</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <author>
      <name>Can Alkan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/bioinformatics/btx342</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/bioinformatics/btx342" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bioinformatics. Nov 1;33(21):3355-3363, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.01789v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01789v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1604.01317">
    <id>http://arxiv.org/abs/1604.01317v1</id>
    <updated>2016-04-05T16:18:33Z</updated>
    <published>2016-04-05T16:18:33Z</published>
    <title>New Error Tolerant Method to Search Long Repeats in Symbol Sequences</title>
    <summary>  A new method to identify all sufficiently long repeating substrings in one or
several symbol sequences is proposed. The method is based on a specific gauge
applied to symbol sequences that guarantees identification of the repeating
substrings. It allows the matching of substrings to contain a given level of
errors. The gauge is based on the development of a heavily sparse dictionary of
repeats, thus drastically accelerating the search procedure. Some genomic
applications illustrate the method.
  This paper is the extended and detailed version of the presentation at the
third International Conference on Algorithms for Computational Biology to be
held at Trujillo, Spain, June 21-22, 2016.
</summary>
    <author>
      <name>Sergey Tsarev</name>
    </author>
    <author>
      <name>Michael Sadovsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11K31, 60-08, 62-07, 68Q25, 68U15, 68W99, 92-08, 92D25" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1604.01168">
    <id>http://arxiv.org/abs/1604.01168v2</id>
    <updated>2016-11-12T15:26:52Z</updated>
    <published>2016-04-05T08:21:53Z</published>
    <title>An Estimation of the Size of Non-Compact Suffix Trees</title>
    <summary>  A suffix tree is a data structure used mainly for pattern matching. It is
known that the space complexity of simple suffix trees is quadratic in the
length of the string. By a slight modification of the simple suffix trees one
gets the compact suffix trees, which have linear space complexity. The
motivation of this paper is the question whether the space complexity of simple
suffix trees is quadratic not only in the worst case, but also in expectation.
</summary>
    <author>
      <name>Bálint Vásárhelyi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01168v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01168v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1603.08777">
    <id>http://arxiv.org/abs/1603.08777v2</id>
    <updated>2017-05-15T13:36:55Z</updated>
    <published>2016-03-29T14:10:43Z</published>
    <title>Encoding Arguments</title>
    <summary>  Many proofs in discrete mathematics and theoretical computer science are
based on the probabilistic method. To prove the existence of a good object, we
pick a random object and show that it is bad with low probability. This method
is effective, but the underlying probabilistic machinery can be daunting.
"Encoding arguments" provide an alternative presentation in which probabilistic
reasoning is encapsulated in a "uniform encoding lemma". This lemma provides an
upper bound on the probability of an event using the fact that a uniformly
random choice from a set of size $n$ cannot be encoded with fewer than $\log_2
n$ bits on average. With the lemma, the argument reduces to devising an
encoding where bad objects have short codewords.
  In this expository article, we describe the basic method and provide a simple
tutorial on how to use it. After that, we survey many applications to classic
problems from discrete mathematics and computer science. We also give a
generalization for the case of non-uniform distributions, as well as a rigorous
justification for the use of non-integer codeword lengths in encoding
arguments. These latter two results allow encoding arguments to be applied more
widely and to produce tighter results.
</summary>
    <author>
      <name>Pat Morin</name>
    </author>
    <author>
      <name>Wolfgang Mulzer</name>
    </author>
    <author>
      <name>Tommy Reddad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3084288</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3084288" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">50 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Computing Surveys (CSUR), 50(3), July 2017, Article 46</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.08777v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08777v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1606.04718">
    <id>http://arxiv.org/abs/1606.04718v3</id>
    <updated>2017-07-27T07:02:49Z</updated>
    <published>2016-06-15T10:51:14Z</published>
    <title>Improved Space efficient linear time algorithms for BFS, DFS and
  applications</title>
    <summary>  Recent work by Elmasry et al. (STACS 2015) and Asano et al. (ISAAC 2014),
reconsidered classical fundamental graph algorithms focusing on improving the
space complexity. We continue this line of work focusing on space. Our first
result is a simple data structure that can maintain any subset $S$ of a
universe of $n$ elements using $n+o(n)$ bits and support in constant time,
apart from the standard insert, delete and membership queries, the operation
{\it findany} that finds and returns any element of the set (or outputs that
the set is empty). Using this we give a BFS implementation that takes $O(m+n)$
time using at most $2n+o(n)$ bits. Later, we further improve the space
requirement of BFS to at most $1.585n + o(n)$ bits. We demonstrate the use of
our data structure by developing another data structure using it that can
represent a sequence of $n$ non-negative integers $x_1, x_2, \ldots x_n$ using
at most $\sum_{i=1}^n x_i + 2n + o(\sum_{i=1}^n x_i+n)$ bits and, in constant
time, determine whether the $i$-th element is $0$ or decrement it otherwise. We
also discuss an algorithm for finding a minimum weight spanning tree of a
weighted undirected graph using at most $n+o(n)$ bits. We also provide an
implementation for DFS that takes $O(m+n)$ time and $O(n \lg(m/n))$ bits. Using
this DFS algorithm and other careful implementations, we can test
biconnectivity, 2-edge connectivity, and determine cut vertices, bridges etc
among others, essentially within the same time and space bounds required for
DFS. These improve the space required for earlier implementations from $\Omega
(n\lg n)$ bits.
</summary>
    <author>
      <name>Niranka Banerjee</name>
    </author>
    <author>
      <name>Sankardeep Chakraborty</name>
    </author>
    <author>
      <name>Venkatesh Raman</name>
    </author>
    <author>
      <name>Srinivasa Rao Satti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary version of this paper appears in the proceedings of
  COCOON 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04718v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04718v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1606.04042">
    <id>http://arxiv.org/abs/1606.04042v20</id>
    <updated>2017-01-07T17:31:32Z</updated>
    <published>2016-06-13T17:26:52Z</published>
    <title>Randomized Ternary Search Tries</title>
    <summary>  This paper presents a new kind of self-balancing ternary search trie that
uses a randomized balancing strategy adapted from Aragon and Seidel's
randomized binary search trees ("treaps"). After any sequence of insertions and
deletions of strings, the tree looks like a ternary search trie built by
inserting strings in random order. As a result, the time cost of searching,
inserting, or deleting a string of length k in a tree with n strings is at most
O(k + log n) with high probability.
</summary>
    <author>
      <name>Nicolai Diethelm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; v20: minor clarification in the "Analysis" section</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04042v20" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04042v20" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.08935">
    <id>http://arxiv.org/abs/1605.08935v1</id>
    <updated>2016-05-28T21:07:22Z</updated>
    <published>2016-05-28T21:07:22Z</published>
    <title>Algorithms to Compute the Lyndon Array</title>
    <summary>  We first describe three algorithms for computing the Lyndon array that have
been suggested in the literature, but for which no structured exposition has
been given. Two of these algorithms execute in quadratic time in the worst
case, the third achieves linear time, but at the expense of prior computation
of both the suffix array and the inverse suffix array of x. We then go on to
describe two variants of a new algorithm that avoids prior computation of
global data structures and executes in worst-case n log n time. Experimental
evidence suggests that all but one of these five algorithms require only linear
execution time in practice, with the two new algorithms faster by a small
factor. We conjecture that there exists a fast and worst-case linear-time
algorithm to compute the Lyndon array that is also elementary (making no use of
global data structures such as the suffix array).
</summary>
    <author>
      <name>Frantisek Franek</name>
    </author>
    <author>
      <name>A. S. M. Sohidull Islam</name>
    </author>
    <author>
      <name>M. Sohel Rahman</name>
    </author>
    <author>
      <name>W. F. Smyth</name>
    </author>
    <link href="http://arxiv.org/abs/1605.08935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.08448">
    <id>http://arxiv.org/abs/1605.08448v1</id>
    <updated>2016-05-26T20:32:50Z</updated>
    <published>2016-05-26T20:32:50Z</published>
    <title>Energy-Efficient Algorithms</title>
    <summary>  We initiate the systematic study of the energy complexity of algorithms (in
addition to time and space complexity) based on Landauer's Principle in
physics, which gives a lower bound on the amount of energy a system must
dissipate if it destroys information. We propose energy-aware variations of
three standard models of computation: circuit RAM, word RAM, and
transdichotomous RAM. On top of these models, we build familiar high-level
primitives such as control logic, memory allocation, and garbage collection
with zero energy complexity and only constant-factor overheads in space and
time complexity, enabling simple expression of energy-efficient algorithms. We
analyze several classic algorithms in our models and develop low-energy
variations: comparison sort, insertion sort, counting sort, breadth-first
search, Bellman-Ford, Floyd-Warshall, matrix all-pairs shortest paths, AVL
trees, binary heaps, and dynamic arrays. We explore the time/space/energy
trade-off and develop several general techniques for analyzing algorithms and
reducing their energy complexity. These results lay a theoretical foundation
for a new field of semi-reversible computing and provide a new framework for
the investigation of algorithms.
</summary>
    <author>
      <name>Erik D. Demaine</name>
    </author>
    <author>
      <name>Jayson Lynch</name>
    </author>
    <author>
      <name>Geronimo J. Mirano</name>
    </author>
    <author>
      <name>Nirvan Tyagi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 8 pdf figures, full version of work published in ITCS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.05404">
    <id>http://arxiv.org/abs/1605.05404v1</id>
    <updated>2016-05-18T00:20:00Z</updated>
    <published>2016-05-18T00:20:00Z</published>
    <title>CSA++: Fast Pattern Search for Large Alphabets</title>
    <summary>  Indexed pattern search in text has been studied for many decades. For small
alphabets, the FM-Index provides unmatched performance, in terms of both space
required and search speed. For large alphabets -- for example, when the tokens
are words -- the situation is more complex, and FM-Index representations are
compact, but potentially slow. In this paper we apply recent innovations from
the field of inverted indexing and document retrieval to compressed pattern
search, including for alphabets into the millions. Commencing with the
practical compressed suffix array structure developed by Sadakane, we show that
the Elias-Fano code-based approach to document indexing can be adapted to
provide new tradeoff options in indexed pattern search, and offers
significantly faster pattern processing compared to previous implementations,
as well as reduced space requirements. We report a detailed experimental
evaluation that demonstrates the relative advantages of the new approach, using
the standard Pizza&amp;Chili methodology and files, as well as applied use-cases
derived from large-scale data compression, and from natural language
processing. For large alphabets, the new structure gives rise to space
requirements that are close to those of the most highly-compressed FM-Index
variants, in conjunction with unparalleled search throughput rates.
</summary>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Alistair Moffat</name>
    </author>
    <author>
      <name>Matthias Petri</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.06327">
    <id>http://arxiv.org/abs/1605.06327v1</id>
    <updated>2016-05-20T12:41:13Z</updated>
    <published>2016-05-20T12:41:13Z</published>
    <title>Games from Basic Data Structures</title>
    <summary>  In this paper, we consider combinatorial game rulesets based on data
structures normally covered in an undergraduate Computer Science Data
Structures course: arrays, stacks, queues, priority queues, sets, linked lists,
and binary trees. We describe many rulesets as well as computational and
mathematical properties about them. Two of the rulesets, Tower Nim and Myopic
Col, are new. We show polynomial-time solutions to Tower Nim and to Myopic Col
on paths.
</summary>
    <author>
      <name>Mara Bovee</name>
    </author>
    <author>
      <name>Kyle Burke</name>
    </author>
    <author>
      <name>Craig Tennenhouse</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91A46" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.1.3; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.03390">
    <id>http://arxiv.org/abs/1605.03390v2</id>
    <updated>2016-05-12T22:54:52Z</updated>
    <published>2016-05-11T11:47:59Z</published>
    <title>Variance of the Internal Profile in Suffix Trees</title>
    <summary>  The precise analysis of the variance of the profile of a suffix tree has been
a longstanding open problem. We analyze three regimes of the asymptotic growth
of the variance of the profile of a suffix tree built from a randomly generated
binary string, in the nonuniform case. We utilize combinatorics on words,
singularity analysis, and the Mellin transform.
</summary>
    <author>
      <name>Jeffrey Gaither</name>
    </author>
    <author>
      <name>Mark Daniel Ward</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.03390v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03390v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.04098">
    <id>http://arxiv.org/abs/1605.04098v1</id>
    <updated>2016-05-13T09:42:10Z</updated>
    <published>2016-05-13T09:42:10Z</published>
    <title>Lightweight LCP Construction for Very Large Collections of Strings</title>
    <summary>  The longest common prefix array is a very advantageous data structure that,
combined with the suffix array and the Burrows-Wheeler transform, allows to
efficiently compute some combinatorial properties of a string useful in several
applications, especially in biological contexts. Nowadays, the input data for
many problems are big collections of strings, for instance the data coming from
"next-generation" DNA sequencing (NGS) technologies. In this paper we present
the first lightweight algorithm (called extLCP) for the simultaneous
computation of the longest common prefix array and the Burrows-Wheeler
transform of a very large collection of strings having any length. The
computation is realized by performing disk data accesses only via sequential
scans, and the total disk space usage never needs more than twice the output
size, excluding the disk space required for the input. Moreover, extLCP allows
to compute also the suffix array of the strings of the collection, without any
other further data structure is needed. Finally, we test our algorithm on real
data and compare our results with another tool capable to work in external
memory on large collections of strings.
</summary>
    <author>
      <name>Anthony J. Cox</name>
    </author>
    <author>
      <name>Fabio Garofalo</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jda.2016.03.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jda.2016.03.003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript version is made available under the CC-BY-NC-ND 4.0
  license http://creativecommons.org/licenses/by-nc-nd/4.0/ The final version
  of this manuscript is in press in Journal of Discrete Algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.02123">
    <id>http://arxiv.org/abs/1605.02123v1</id>
    <updated>2016-05-07T00:53:54Z</updated>
    <published>2016-05-07T00:53:54Z</published>
    <title>Average Size of a Suffix Tree for Markov Sources</title>
    <summary>  We study a suffix tree built from a sequence generated by a Markovian source.
Such sources are more realistic probabilistic models for text generation, data
compression, molecular applications, and so forth. We prove that the average
size of such a suffix tree is asymptotically equivalent to the average size of
a trie built over $n$ independent sequences from the same Markovian source.
This equivalence is only known for memoryless sources. We then derive a formula
for the size of a trie under Markovian model to complete the analysis for
suffix trees. We accomplish our goal by applying some novel techniques of
analytic combinatorics on words also known as analytic pattern matching.
</summary>
    <author>
      <name>Philippe Jacquet</name>
    </author>
    <author>
      <name>Wojciech Szpankowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AofA 2016 Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1605.01814">
    <id>http://arxiv.org/abs/1605.01814v1</id>
    <updated>2016-05-06T04:10:34Z</updated>
    <published>2016-05-06T04:10:34Z</published>
    <title>Asymmetric Rényi Problem and PATRICIA Tries</title>
    <summary>  In 1960, R\'enyi asked for the number of random queries necessary to recover
a hidden bijective labeling of n distinct objects. In each query one selects a
random subset of labels and asks, what is the set of objects that have these
labels? We consider here an asymmetric version of the problem in which in every
query an object is chosen with probability p > 1/2 and we ignore "inconclusive"
queries. We study the number of queries needed to recover the labeling in its
entirety (the height), to recover at least one single element (the fillup
level), and to recover a randomly chosen element (the typical depth). This
problem exhibits several remarkable behaviors: the depth D_n converges in
probability but not almost surely and while it satisfies the central limit
theorem its local limit theorem doesn't hold; the height H_n and the fillup
level F_n exhibit phase transitions with respect to p in the second term. To
obtain these results, we take a unified approach via the analysis of the
external profile, defined at level k as the number of elements recovered by the
kth query. We first establish new precise asymptotic results for the average
and variance, and a central limit law, for the external profile in the regime
where it grows polynomially with n. We then extend the external profile results
to the boundaries of the central region, leading to the solution of our problem
for the height and fillup level. As a bonus, our analysis implies novel results
for analogous parameters of random PATRICIA tries.
</summary>
    <author>
      <name>Michael Drmota</name>
    </author>
    <author>
      <name>Abram Magner</name>
    </author>
    <author>
      <name>Wojciech Szpankowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 5 figures, to appear in Proceedings of the 27th
  International Conference on Probabilistic, Combinatorial and Asymptotic
  Methods for the Analysis of Algorithms, Krakow, Poland, 4-8 July 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1608.00529">
    <id>http://arxiv.org/abs/1608.00529v1</id>
    <updated>2016-08-01T19:11:30Z</updated>
    <published>2016-08-01T19:11:30Z</published>
    <title>Hardness of Permutation Pattern Matching</title>
    <summary>  Permutation Pattern Matching (or PPM) is a decision problem whose input is a
pair of permutations $\pi$ and $\tau$, represented as sequences of integers,
and the task is to determine whether $\tau$ contains a subsequence
order-isomorphic to $\pi$. Bose, Buss and Lubiw proved that PPM is NP-complete
on general inputs.
  We show that PPM is NP-complete even when $\pi$ has no decreasing subsequence
of length 3 and $\tau$ has no decreasing subsequence of length 4. This provides
the first known example of PPM being hard when one or both of $\pi$ and
$\sigma$ are restricted to a proper hereditary class of permutations.
  This hardness result is tight in the sense that PPM is known to be polynomial
when both $\pi$ and $\tau$ avoid a decreasing subsequence of length 3, as well
as when $\pi$ avoids a decreasing subsequence of length 2. The result is also
tight in another sense: we will show that for any hereditary proper subclass C
of the class of permutations avoiding a decreasing sequence of length 3, there
is a polynomial algorithm solving PPM instances where $\pi$ is from C and
$\tau$ is arbitrary.
  We also obtain analogous hardness and tractability results for the class of
so-called skew-merged patterns.
  From these results, we deduce a complexity dichotomy for the PPM problem
restricted to $\pi$ belonging to $Av(\rho)$, where $Av(\rho)$ denotes the class
of permutations avoiding a permutation $\rho$. Specifically, we show that the
problem is polynomial when $\rho$ is in the set {1, 12, 21, 132, 213, 231,
312}, and it is NP-complete for any other $\rho$.
</summary>
    <author>
      <name>Vít Jelínek</name>
    </author>
    <author>
      <name>Jan Kynčl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.08342">
    <id>http://arxiv.org/abs/1607.08342v1</id>
    <updated>2016-07-28T08:02:23Z</updated>
    <published>2016-07-28T08:02:23Z</published>
    <title>A New Lightweight Algorithm to compute the BWT and the LCP array of a
  Set of Strings</title>
    <summary>  Indexing of very large collections of strings such as those produced by the
widespread sequencing technologies, heavily relies on multi-string
generalizations of the Burrows-Wheeler Transform (BWT), and for this problem
various in-memory algorithms have been proposed. The rapid growing of data that
are processed routinely, such as in bioinformatics, requires a large amount of
main memory, and this fact has motivated the development of algorithms, to
compute the BWT, that work almost entirely in external memory. On the other
hand, the related problem of computing the Longest Common Prefix (LCP) array is
often instrumental in several algorithms on collection of strings, such as
those that compute the suffix-prefix overlap among strings, which is an
essential step for many genome assembly algorithms. The best current
lightweight approach to compute BWT and LCP array on a set of $m$ strings, each
one $k$ characters long, has I/O complexity that is $O(mk^2 \log |\Sigma|)$
(where $|\Sigma|$ is the size of the alphabet), thus it is not optimal. In this
paper we propose a novel approach to build BWT and LCP array (simultaneously)
with $O(kmL(\log k +\log \sigma))$ I/O complexity, where $L$ is the length of
longest substring that appears at least twice in the input strings.
</summary>
    <author>
      <name>Paola Bonizzoni</name>
    </author>
    <author>
      <name>Gianluca Della Vedova</name>
    </author>
    <author>
      <name>Serena Nicosia</name>
    </author>
    <author>
      <name>Marco Previtali</name>
    </author>
    <author>
      <name>Raffaella Rizzi</name>
    </author>
    <link href="http://arxiv.org/abs/1607.08342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.08176">
    <id>http://arxiv.org/abs/1607.08176v1</id>
    <updated>2016-07-27T16:50:05Z</updated>
    <published>2016-07-27T16:50:05Z</published>
    <title>Suffix arrays with a twist</title>
    <summary>  The suffix array is a classic full-text index, combining effectiveness with
simplicity. We discuss three approaches aiming to improve its efficiency even
more: changes to the navigation, data layout and adding extra data. In short,
we show that $(i)$ how we search for the right interval boundary impacts
significantly the overall search speed, $(ii)$ a B-tree data layout easily wins
over the standard one, $(iii)$ the well-known idea of a lookup table for the
prefixes of the suffixes can be refined with using compression, $(iv)$ caching
prefixes of the suffixes in a helper array can pose a(nother) practical
space-time tradeoff.
</summary>
    <author>
      <name>Tomasz Kowalski</name>
    </author>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Kimmo Fredriksson</name>
    </author>
    <author>
      <name>Marcin Raniszewski</name>
    </author>
    <link href="http://arxiv.org/abs/1607.08176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.05994">
    <id>http://arxiv.org/abs/1607.05994v4</id>
    <updated>2020-01-28T10:45:55Z</updated>
    <published>2016-07-20T15:15:44Z</published>
    <title>Dynamic Time Warping and Geometric Edit Distance: Breaking the Quadratic
  Barrier</title>
    <summary>  Dynamic Time Warping (DTW) and Geometric Edit Distance (GED) are basic
similarity measures between curves or general temporal sequences (e.g., time
series) that are represented as sequences of points in some metric space $(X,
\mathrm{dist})$. The DTW and GED measures are massively used in various fields
of computer science, computational biology, and engineering. Consequently, the
tasks of computing these measures are among the core problems in P. Despite
extensive efforts to find more efficient algorithms, the best-known algorithms
for computing the DTW or GED between two sequences of points in $X =
\mathbb{R}^d$ are long-standing dynamic programming algorithms that require
quadratic runtime, even for the one-dimensional case $d = 1$, which is perhaps
one of the most used in practice.
  In this paper, we break the nearly 50 years old quadratic time bound for
computing DTW or GED between two sequences of $n$ points in $\mathbb{R}$, by
presenting deterministic algorithms that run in $O\left( n^2 / \log\log n
\right)$ time. Our algorithms can be extended to work also for higher
dimensional spaces $\mathbb{R}^d$, for any constant $d$, when the underlying
distance-metric $\mathrm{dist}$ is polyhedral (e.g., $L_1, L_\infty$).
</summary>
    <author>
      <name>Omer Gold</name>
    </author>
    <author>
      <name>Micha Sharir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Removing the $\log\log\log n$ factor from the runtime bound that
  appeared in previous versions</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05994v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05994v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.05157">
    <id>http://arxiv.org/abs/1607.05157v1</id>
    <updated>2016-07-18T16:14:30Z</updated>
    <published>2016-07-18T16:14:30Z</published>
    <title>Multi-view pattern matching</title>
    <summary>  We introduce the \textit{multi-view pattern matching} problem, where a text
can have multiple views. Each view is a string of the same size and drawn from
disjoint alphabets. The pattern is drawn from the union of all alphabets.
  The algorithm we present is an extension of the Horspool algorithm, and in
our experiments on synthetic data it shows an $3 \times$ improvement over the
naive baseline.
</summary>
    <author>
      <name>Matthias Galle</name>
    </author>
    <link href="http://arxiv.org/abs/1607.05157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.03718">
    <id>http://arxiv.org/abs/1607.03718v1</id>
    <updated>2016-07-13T13:16:17Z</updated>
    <published>2016-07-13T13:16:17Z</published>
    <title>Streaming Algorithms For Computing Edit Distance Without Exploiting
  Suffix Trees</title>
    <summary>  The edit distance is a way of quantifying how similar two strings are to one
another by counting the minimum number of character insertions, deletions, and
substitutions required to transform one string into the other.
  In this paper we study the computational problem of computing the edit
distance between a pair of strings where their distance is bounded by a
parameter $k\ll n$. We present two streaming algorithms for computing edit
distance: One runs in time $O(n+k^2)$ and the other $n+O(k^3)$. By writing
$n+O(k^3)$ we want to emphasize that the number of operations per an input
symbol is a small constant. In particular, the running time does not depend on
the alphabet size, and the algorithm should be easy to implement.
  Previously a streaming algorithm with running time $O(n+k^4)$ was given in
the paper by the current authors (STOC'16). The best off-line algorithm runs in
time $O(n+k^2)$ (Landau et al., 1998) which is known to be optimal under the
Strong Exponential Time Hypothesis.
</summary>
    <author>
      <name>Diptarka Chakraborty</name>
    </author>
    <author>
      <name>Elazar Goldenberg</name>
    </author>
    <author>
      <name>Michal Koucký</name>
    </author>
    <link href="http://arxiv.org/abs/1607.03718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.00208">
    <id>http://arxiv.org/abs/1607.00208v1</id>
    <updated>2016-07-01T11:36:17Z</updated>
    <published>2016-07-01T11:36:17Z</published>
    <title>An Optimal Algorithm for Range Search on Multidimensional Points</title>
    <summary>  This paper proposes an efficient and novel method to address range search on
multidimensional points in $\theta(t)$ time, where $t$ is the number of points
reported in $\Re^k$ space. This is accomplished by introducing a new data
structure, called BITS $k$d-tree. This structure also supports fast updation
that takes $\theta(1)$ time for insertion and $O(\log n)$ time for deletion.
The earlier best known algorithm for this problem is $O(\log^k n+t)$ time in
the pointer machine model.
</summary>
    <author>
      <name>T. Hema</name>
    </author>
    <author>
      <name>K. S. Easwarakumar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3923/ajit.2016.1723.1730</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3923/ajit.2016.1723.1730" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Asian Journal of Information Technology, 15:11,1723-1730, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.00208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1607.00138">
    <id>http://arxiv.org/abs/1607.00138v1</id>
    <updated>2016-07-01T07:44:07Z</updated>
    <published>2016-07-01T07:44:07Z</published>
    <title>Representing Pattern Matching Algorithms by Polynomial-Size Automata</title>
    <summary>  Pattern matching algorithms to find exact occurrences of a pattern
$S\in\Sigma^m$ in a text $T\in\Sigma^n$ have been analyzed extensively with
respect to asymptotic best, worst, and average case runtime. For more detailed
analyses, the number of text character accesses $X^{\mathcal{A},S}_n$ performed
by an algorithm $\mathcal{A}$ when searching a random text of length $n$ for a
fixed pattern $S$ has been considered. Constructing a state space and
corresponding transition rules (e.g. in a Markov chain) that reflect the
behavior of a pattern matching algorithm is a key step in existing analyses of
$X^{\mathcal{A},S}_n$ in both the asymptotic ($n\to\infty$) and the
non-asymptotic regime. The size of this state space is hence a crucial
parameter for such analyses. In this paper, we introduce a general methodology
to construct corresponding state spaces and demonstrate that it applies to a
wide range of algorithms, including Boyer-Moore (BM), Boyer-Moore-Horspool
(BMH), Backward Oracle Matching (BOM), and Backward (Non-Deterministic) DAWG
Matching (B(N)DM). In all cases except BOM, our method leads to state spaces of
size $O(m^3)$ for pattern length $m$, a result that has previously only been
obtained for BMH. In all other cases, only state spaces with size exponential
in $m$ had been reported. Our results immediately imply an algorithm to compute
the distribution of $X^{\mathcal{A},S}_n$ for fixed $S$, fixed $n$, and
$\mathcal{A}\in\{\text{BM},\text{BMH},\text{B(N)DM}\}$ in polynomial time for a
very general class of random text models.
</summary>
    <author>
      <name>Tobias Marschall</name>
    </author>
    <author>
      <name>Noemi E. Passing</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1606.06730">
    <id>http://arxiv.org/abs/1606.06730v1</id>
    <updated>2016-06-21T04:01:39Z</updated>
    <published>2016-06-21T04:01:39Z</published>
    <title>Two-stage algorithms for covering array construction</title>
    <summary>  Modern software systems often consist of many different components, each with
a number of options. Although unit tests may reveal faulty options for
individual components, functionally correct components may interact in
unforeseen ways to cause a fault. Covering arrays are used to test for
interactions among components systematically. A two-stage framework, providing
a number of concrete algorithms, is developed for the efficient construction of
covering arrays. %Our framework divides the construction in two stages. In the
first stage, a time and memory efficient randomized algorithm covers most of
the interactions. In the second stage, a more sophisticated search covers the
remainder in relatively few tests. In this way, the storage limitations of the
sophisticated search algorithms are avoided; hence the range of the number of
components for which the algorithm can be applied is extended, without
increasing the number of tests. Many of the framework instantiations can be
tuned to optimize a memory-quality trade-off, so that fewer tests can be
achieved using more memory. The algorithms developed outperform the currently
best known methods when the number of components ranges from 20 to 60, the
number of options for each ranges from 3 to 6, and $t$-way interactions are
covered for $t\in \{5,6\}$. In some cases a reduction in the number of tests by
more than $50\%$ is achieved.
</summary>
    <author>
      <name>Kaushik Sarkar</name>
    </author>
    <author>
      <name>Charles J. Colbourn</name>
    </author>
    <link href="http://arxiv.org/abs/1606.06730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05B30 (primary), 05B40, 05D40, 68W20 (secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; D.2.5; G.2.1; G.2.3; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1606.04763">
    <id>http://arxiv.org/abs/1606.04763v3</id>
    <updated>2018-09-25T11:52:39Z</updated>
    <published>2016-06-15T13:57:26Z</published>
    <title>A Simple Streaming Bit-parallel Algorithm for Swap Pattern Matching</title>
    <summary>  The pattern matching problem with swaps is to find all occurrences of a
pattern in a text while allowing the pattern to swap adjacent symbols. The goal
is to design fast matching algorithm that takes advantage of the bit
parallelism of bitwise machine instructions and has only streaming access to
the input. We introduce a new approach to solve this problem based on the graph
theoretic model and compare its performance to previously known algorithms. We
also show that an approach using deterministic finite automata cannot achieve
similarly efficient algorithms. Furthermore, we describe a fatal flaw in some
of the previously published algorithms based on the same model. Finally, we
provide experimental evaluation of our algorithm on real-world data.
</summary>
    <author>
      <name>Václav Blažej</name>
    </author>
    <author>
      <name>Ondřej Suchý</name>
    </author>
    <author>
      <name>Tomáš Valla</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04763v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04763v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1610.09223">
    <id>http://arxiv.org/abs/1610.09223v1</id>
    <updated>2016-10-28T14:03:11Z</updated>
    <published>2016-10-28T14:03:11Z</published>
    <title>Sort well with energy-constrained comparisons</title>
    <summary>  We study very simple sorting algorithms based on a probabilistic comparator
model. In our model, errors in comparing two elements are due to (1) the energy
or effort put in the comparison and (2) the difference between the compared
elements. Such algorithms keep comparing pairs of randomly chosen elements, and
they correspond to Markovian processes. The study of these Markov chains
reveals an interesting phenomenon. Namely, in several cases, the algorithm
which repeatedly compares only adjacent elements is better than the one making
arbitrary comparisons: on the long-run, the former algorithm produces sequences
that are "better sorted". The analysis of the underlying Markov chain poses new
interesting questions as the latter algorithm yields a non-reversible chain and
therefore its stationary distribution seems difficult to calculate explicitly.
</summary>
    <author>
      <name>Barbara Geissmann</name>
    </author>
    <author>
      <name>Paolo Penna</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.97.052108</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.97.052108" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 97, 052108 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.09223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1610.08305">
    <id>http://arxiv.org/abs/1610.08305v6</id>
    <updated>2018-11-09T17:11:08Z</updated>
    <published>2016-10-26T12:45:59Z</published>
    <title>Optimal In-Place Suffix Sorting</title>
    <summary>  The suffix array is a fundamental data structure for many applications that
involve string searching and data compression. Designing time/space-efficient
suffix array construction algorithms has attracted significant attention and
considerable advances have been made for the past 20 years. We obtain the
\emph{first} in-place suffix array construction algorithms that are optimal
both in time and space for (read-only) integer alphabets. Concretely, we make
the following contributions:
  1. For integer alphabets, we obtain the first suffix sorting algorithm which
takes linear time and uses only $O(1)$ workspace (the workspace is the total
space needed beyond the input string and the output suffix array). The input
string may be modified during the execution of the algorithm, but should be
restored upon termination of the algorithm.
  2. We strengthen the first result by providing the first in-place linear time
algorithm for read-only integer alphabets with $|\Sigma|=O(n)$ (i.e., the input
string cannot be modified). This algorithm settles the open problem posed by
Franceschini and Muthukrishnan in ICALP 2007. The open problem asked to design
in-place algorithms in $o(n\log n)$ time and ultimately, in $O(n)$ time for
(read-only) integer alphabets with $|\Sigma| \leq n$. Our result is in fact
slightly stronger since we allow $|\Sigma|=O(n)$.
  3. Besides, for the read-only general alphabets (i.e., only comparisons are
allowed), we present an optimal in-place $O(n\log n)$ time suffix sorting
algorithm, recovering the result obtained by Franceschini and Muthukrishnan
which was an open problem posed by Manzini and Ferragina in ESA 2002.
</summary>
    <author>
      <name>Zhize Li</name>
    </author>
    <author>
      <name>Jian Li</name>
    </author>
    <author>
      <name>Hongwei Huo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages. A disclaimer: https://suffixsorting.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.08305v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.08305v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1610.03007">
    <id>http://arxiv.org/abs/1610.03007v1</id>
    <updated>2016-10-10T17:40:26Z</updated>
    <published>2016-10-10T17:40:26Z</published>
    <title>Scalable Construction of Text Indexes</title>
    <summary>  The suffix array is the key to efficient solutions for myriads of string
processing problems in different applications domains, like data compression,
data mining, or Bioinformatics. With the rapid growth of available data, suffix
array construction algorithms had to be adapted to advanced computational
models such as external memory and distributed computing. In this article, we
present five suffix array construction algorithms utilizing the new algorithmic
big data batch processing framework Thrill, which allows us to process input
sizes in orders of magnitude that have not been considered before.
</summary>
    <author>
      <name>Timo Bingmann</name>
    </author>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Florian Kurpicz</name>
    </author>
    <link href="http://arxiv.org/abs/1610.03007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1610.02953">
    <id>http://arxiv.org/abs/1610.02953v1</id>
    <updated>2016-10-10T15:13:10Z</updated>
    <published>2016-10-10T15:13:10Z</published>
    <title>Comments on Dumitrescu's "A Selectable Sloppy Heap"</title>
    <summary>  Dumitrescu [arXiv:1607.07673] describes a data structure referred to as a
Selectable Sloppy Heap. We present a simplified approach, and also point out
aspects of Dumitrescu's exposition that require scrutiny.
</summary>
    <author>
      <name>Michael L. Fredman</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1609.04471">
    <id>http://arxiv.org/abs/1609.04471v1</id>
    <updated>2016-09-14T22:54:31Z</updated>
    <published>2016-09-14T22:54:31Z</published>
    <title>Sort Race</title>
    <summary>  Sorting is one of the oldest computing problems and is still very important
in the age of big data. Various algorithms and implementation techniques have
been proposed. In this study, we focus on comparison based, internal sorting
algorithms. We created 12 data types of various sizes for experiments and
tested extensively various implementations in a single setting. Using some
effective techniques, we discovered that quicksort is adaptive to nearly sorted
inputs and is still the best overall sorting algorithm. We also identified
which techniques are effective in timsort, one of the most popular and
efficient sorting method based on natural mergesort, and created our version of
mergesort, which runs faster than timsort on nearly sorted instances. Our
implementations of quicksort and mergesort are different from other
implementations reported in all textbooks or research articles, faster than any
version of the C library qsort functions, not only for randomly generated data,
but also for various types of nearly sorted data. This experiment can help the
user to choose the best sorting algorithm for the hard sorting job at hand.
This work provides a platform for anyone to test their own sorting algorithm
against the best in the field.
</summary>
    <author>
      <name>Hantao Zhang</name>
    </author>
    <author>
      <name>Baoluo Meng</name>
    </author>
    <author>
      <name>Yiwen Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1609.04471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1609.01400">
    <id>http://arxiv.org/abs/1609.01400v3</id>
    <updated>2017-02-18T10:05:01Z</updated>
    <published>2016-09-06T05:57:44Z</published>
    <title>Succinct data-structure for nearest colored node in a tree</title>
    <summary>  We give a succinct data-structure that stores a tree with colors on the
nodes. Given a node x and a color alpha, the structure finds the nearest node
to x with color alpha. This results improves the $O(n\log n)$-bits structure of
Gawrychowski et al.~[CPM 2016].
</summary>
    <author>
      <name>Dekel Tsur</name>
    </author>
    <link href="http://arxiv.org/abs/1609.01400v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01400v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1608.08927">
    <id>http://arxiv.org/abs/1608.08927v1</id>
    <updated>2016-08-31T16:23:07Z</updated>
    <published>2016-08-31T16:23:07Z</published>
    <title>The Generalized Smallest Grammar Problem</title>
    <summary>  The Smallest Grammar Problem -- the problem of finding the smallest
context-free grammar that generates exactly one given sequence -- has never
been successfully applied to grammatical inference. We investigate the reasons
and propose an extended formulation that seeks to minimize non-recursive
grammars, instead of straight-line programs. In addition, we provide very
efficient algorithms that approximate the minimization problem of this class of
grammars. Our empirical evaluation shows that we are able to find smaller
models than the current best approximations to the Smallest Grammar Problem on
standard benchmarks, and that the inferred rules capture much better the
syntactic structure of natural language.
</summary>
    <author>
      <name>Payam Siyari</name>
    </author>
    <author>
      <name>Matthias Gallé</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1608.08346">
    <id>http://arxiv.org/abs/1608.08346v1</id>
    <updated>2016-08-30T06:52:52Z</updated>
    <published>2016-08-30T06:52:52Z</published>
    <title>A family of fast exact pattern matching algorithms</title>
    <summary>  A family of comparison-based exact pattern matching algorithms is described.
They utilize multi-dimensional arrays in order to process more than one
adjacent text window in each iteration of the search cycle. This approach leads
to a lower average time complexity by the cost of space. The algorithms of this
family perform well for short patterns and middle size alphabets. In such case
the shift of the window by several pattern lengths at once is quite probable,
which is the main factor of algorithm success. Our algorithms outperform the
Boyer-Moore-Horspool algorithm, either in the original version or with Sunday's
Quick search modification, in a wide area of pattern length - alphabet size
plane. In some subareas the proposed algorithms are the fastest among all known
exact pattern matching algorithms. Namely, they perform best when alphabet size
is about 30-40 and pattern length is about 4-10. Such parameters are typical
for search in natural language text databases.
</summary>
    <author>
      <name>Igor O. Zavadskyi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Taras Shevchenko National University of Kyiv</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, submitted to 'Bulletin of Taras Shevchenko
  NationalUniversity of Kyiv'</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1608.04906">
    <id>http://arxiv.org/abs/1608.04906v4</id>
    <updated>2017-11-01T17:00:19Z</updated>
    <published>2016-08-17T09:41:53Z</published>
    <title>Quicksort Is Optimal For Many Equal Keys</title>
    <summary>  I prove that the average number of comparisons for median-of-$k$ Quicksort
(with fat-pivot a.k.a. three-way partitioning) is asymptotically only a
constant $\alpha_k$ times worse than the lower bound for sorting random
multisets with $\Omega(n^\varepsilon)$ duplicates of each value (for any
$\varepsilon>0$). The constant is $\alpha_k = \ln(2) /
\bigl(H_{k+1}-H_{(k+1)/2} \bigr)$, which converges to 1 as $k\to\infty$, so
Quicksort is asymptotically optimal for inputs with many duplicates. This
resolves a conjecture by Sedgewick and Bentley (1999, 2002) and constitutes the
first progress on the analysis of Quicksort with equal elements since
Sedgewick's 1977 article.
</summary>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/1.9781611975062.2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/1.9781611975062.2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v4 is a major reorganization of sections; a shortened version appears
  in the proceedings of ANALCO 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04906v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04906v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1608.02615">
    <id>http://arxiv.org/abs/1608.02615v2</id>
    <updated>2016-08-12T22:55:47Z</updated>
    <published>2016-08-08T20:36:53Z</published>
    <title>Bidirectional Conditional Insertion Sort algorithm; An efficient
  progress on the classical insertion sort</title>
    <summary>  In this paper, we proposed a new efficient sorting algorithm based on
insertion sort concept. The proposed algorithm called Bidirectional Conditional
Insertion Sort (BCIS). It is in-place sorting algorithm and it has remarkably
efficient average case time complexity when compared with classical insertion
sort (IS). By comparing our new proposed algorithm with the Quicksort
algorithm, BCIS indicated faster average case time for relatively small size
arrays up to 1500 elements. Furthermore, BCIS was observed to be faster than
Quicksort within high rate of duplicated elements even for large size array.
</summary>
    <author>
      <name>Adnan Saher Mohammed</name>
    </author>
    <author>
      <name>Şahin Emrah Amrahov</name>
    </author>
    <author>
      <name>Fatih V. Çelebi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.future.2017.01.034</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.future.2017.01.034" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">references not appeared cause arxiv system does not uploaded .bib
  files</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02615v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02615v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1501.06259">
    <id>http://arxiv.org/abs/1501.06259v1</id>
    <updated>2015-01-26T05:53:03Z</updated>
    <published>2015-01-26T05:53:03Z</published>
    <title>On Longest Repeat Queries</title>
    <summary>  Repeat finding in strings has important applications in subfields such as
computational biology. Surprisingly, all prior work on repeat finding did not
consider the constraint on the locality of repeats. In this paper, we propose
and study the problem of finding longest repetitive substrings covering
particular string positions. We propose an $O(n)$ time and space algorithm for
finding the longest repeat covering every position of a string of size $n$. Our
work is optimal since the reading and the storage of an input string of size
$n$ takes $O(n)$ time and space. Because any substring of a repeat is also a
repeat, our solution to longest repeat queries effectively provides a
"stabbing" tool for practitioners for finding most of the repeats that cover
particular string positions.
</summary>
    <author>
      <name>Atalay Mert İleri</name>
    </author>
    <author>
      <name>M. Oğuzhan Külekci</name>
    </author>
    <author>
      <name>Bojian Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.06259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.06259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1501.05542">
    <id>http://arxiv.org/abs/1501.05542v1</id>
    <updated>2015-01-22T15:51:32Z</updated>
    <published>2015-01-22T15:51:32Z</published>
    <title>Mespotine-RLE-basic v0.9 - An overhead-reduced and improved
  Run-Length-Encoding Method</title>
    <summary>  Run Length Encoding(RLE) is one of the oldest algorithms for data-compression
available, a method used for compression of large data into smaller and
therefore more compact data. It compresses by looking at the data for
repetitions of the same character in a row and storing the amount(called run)
and the respective character(called run_value) as target-data. Unfortunately it
only compresses within strict and special cases. Outside of these cases, it
increases the data-size, even doubles the size in worst cases compared to the
original, unprocessed data. In this paper, we will discuss modifications to
RLE, with which we will only store the run for characters, that are actually
compressible, getting rid of a lot of useless data like the runs of the
characters, that are uncompressible in the first place. This will be achieved
by storing the character first and the run second. Additionally we create a
bit-list of 256 positions(one for every possible ASCII-character), in which we
will store, if a specific (ASCII-)character is compressible(1) or not(0). Using
this list, we can now say, if a character is compressible (store [the
character]+[it's run]) or if it is not compressible (store [the character] only
and the next character is NOT a run, but the following character instead).
Using this list, we can also successfully decode the data(if the character is
compressible, the next character is a run, if not compressible, the next
character is a normal character). With that, we store runs only for characters,
that are compressible in the first place. In fact, in the worst case scenario,
the encoded data will create always just an overhead of the size of the
bit-list itself. With an alphabet of 256 different characters(i.e. ASCII) it
would be only a maximum of 32 bytes, no matter how big the original data was.
[...]
</summary>
    <author>
      <name>Meo Mespotine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages and algorithm-flowcharts</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.05542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.05542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A08, 94A24, 68P30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; I.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1501.04001">
    <id>http://arxiv.org/abs/1501.04001v1</id>
    <updated>2015-01-16T14:55:47Z</updated>
    <published>2015-01-16T14:55:47Z</published>
    <title>Efficient Algorithms for the Order Preserving Pattern Matching Problem</title>
    <summary>  Given a pattern x of length m and a text y of length n, both over an ordered
alphabet, the order-preserving pattern matching problem consists in finding all
substrings of the text with the same relative order as the pattern. It is an
approximate variant of the well known exact pattern matching problem which has
gained attention in recent years. This interesting problem finds applications
in a lot of fields as time series analysis, like share prices on stock markets,
weather data analysis or to musical melody matching. In this paper we present
two new filtering approaches which turn out to be much more effective in
practice than the previously presented methods. From our experimental results
it turns out that our proposed solutions are up to 2 times faster than the
previous solutions reducing the number of false positives up to 99%
</summary>
    <author>
      <name>Simone Faro</name>
    </author>
    <author>
      <name>Oğuzhan Külekci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures, submitted to SEA 2015 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.04001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.04001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1501.01429">
    <id>http://arxiv.org/abs/1501.01429v1</id>
    <updated>2015-01-07T10:22:12Z</updated>
    <published>2015-01-07T10:22:12Z</published>
    <title>Online Computation of Abelian Runs</title>
    <summary>  Given a word $w$ and a Parikh vector $\mathcal{P}$, an abelian run of period
$\mathcal{P}$ in $w$ is a maximal occurrence of a substring of $w$ having
abelian period $\mathcal{P}$. We give an algorithm that finds all the abelian
runs of period $\mathcal{P}$ in a word of length $n$ in time $O(n\times
|\mathcal{P}|)$ and space $O(\sigma+|\mathcal{P}|)$.
</summary>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Thierry Lecroq</name>
    </author>
    <author>
      <name>Arnaud Lefebvre</name>
    </author>
    <author>
      <name>Élise Prieur-Gaston</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proceedings of LATA 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.01429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.01429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1501.00611">
    <id>http://arxiv.org/abs/1501.00611v6</id>
    <updated>2015-02-09T02:12:18Z</updated>
    <published>2015-01-03T22:31:32Z</published>
    <title>A Review on the Tree Edit Distance Problem and Related
  Path-Decomposition Algorithms</title>
    <summary>  An ordered labeled tree is a tree in which the nodes are labeled and the
left-to-right order among siblings is relevant. The edit distance between two
ordered labeled trees is the minimum cost of changing one tree into the other
through a sequence of edit steps. In the literature, there are a class of
algorithms based on different yet closely related path-decomposition schemes.
This article reviews the principles of these algorithms, and studies the
concepts related to the algorithmic complexities as a consequence of the
decomposition schemes.
</summary>
    <author>
      <name>Shihyen Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages. 13 figures. Revisions: minor changes</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.00611v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00611v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1612.03343">
    <id>http://arxiv.org/abs/1612.03343v1</id>
    <updated>2016-12-10T21:04:55Z</updated>
    <published>2016-12-10T21:04:55Z</published>
    <title>Oblivious Sorting and Queues</title>
    <summary>  We present a deterministic oblivious LIFO (Stack), FIFO, double-ended and
double-ended priority queue as well as an oblivious mergesort and quicksort
algorithm. Our techniques and ideas include concatenating queues end-to-end,
size balancing of multiple arrays, several multi-level partitionings of an
array. Our queues are the first to enable executions of pop and push operations
without any change of the data structure (controlled by a parameter). This
enables interesting applications in computing on encrypted data such as hiding
confidential expressions. Mergesort becomes practical using our LIFO queue, ie.
it improves prior work (STOC '14) by a factor of (more than) 1000 in terms of
comparisons for all practically relevant queue sizes. We are the first to
present double-ended (priority) and LIFO queues as well as oblivious quicksort
which is asymptotically optimal. Aside from theortical analysis, we also
provide an empirical evaluation of all queues.
</summary>
    <author>
      <name>Johannes Schneider</name>
    </author>
    <link href="http://arxiv.org/abs/1612.03343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1611.01655">
    <id>http://arxiv.org/abs/1611.01655v3</id>
    <updated>2017-04-25T10:44:06Z</updated>
    <published>2016-11-05T13:55:25Z</published>
    <title>Twenty (simple) questions</title>
    <summary>  A basic combinatorial interpretation of Shannon's entropy function is via the
"20 questions" game. This cooperative game is played by two players, Alice and
Bob: Alice picks a distribution $\pi$ over the numbers $\{1,\ldots,n\}$, and
announces it to Bob. She then chooses a number $x$ according to $\pi$, and Bob
attempts to identify $x$ using as few Yes/No queries as possible, on average.
  An optimal strategy for the "20 questions" game is given by a Huffman code
for $\pi$: Bob's questions reveal the codeword for $x$ bit by bit. This
strategy finds $x$ using fewer than $H(\pi)+1$ questions on average. However,
the questions asked by Bob could be arbitrary. In this paper, we investigate
the following question: Are there restricted sets of questions that match the
performance of Huffman codes, either exactly or approximately?
  Our first main result shows that for every distribution $\pi$, Bob has a
strategy that uses only questions of the form "$x &lt; c$?" and "$x = c$?", and
uncovers $x$ using at most $H(\pi)+1$ questions on average, matching the
performance of Huffman codes in this sense. We also give a natural set of
$O(rn^{1/r})$ questions that achieve a performance of at most $H(\pi)+r$, and
show that $\Omega(rn^{1/r})$ questions are required to achieve such a
guarantee.
  Our second main result gives a set $\mathcal{Q}$ of $1.25^{n+o(n)}$ questions
such that for every distribution $\pi$, Bob can implement an optimal strategy
for $\pi$ using only questions from $\mathcal{Q}$. We also show that
$1.25^{n-o(n)}$ questions are needed, for infinitely many $n$. If we allow a
small slack of $r$ over the optimal strategy, then roughly $(rn)^{\Theta(1/r)}$
questions are necessary and sufficient.
</summary>
    <author>
      <name>Yuval Dagan</name>
    </author>
    <author>
      <name>Yuval Filmus</name>
    </author>
    <author>
      <name>Ariel Gabizon</name>
    </author>
    <author>
      <name>Shay Moran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages; to appear in STOC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.01655v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01655v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1611.01137">
    <id>http://arxiv.org/abs/1611.01137v2</id>
    <updated>2017-05-19T12:22:16Z</updated>
    <published>2016-11-03T19:33:06Z</published>
    <title>A Memory Bandwidth-Efficient Hybrid Radix Sort on GPUs</title>
    <summary>  Sorting is at the core of many database operations, such as index creation,
sort-merge joins, and user-requested output sorting. As GPUs are emerging as a
promising platform to accelerate various operations, sorting on GPUs becomes a
viable endeavour. Over the past few years, several improvements have been
proposed for sorting on GPUs, leading to the first radix sort implementations
that achieve a sorting rate of over one billion 32-bit keys per second. Yet,
state-of-the-art approaches are heavily memory bandwidth-bound, as they require
substantially more memory transfers than their CPU-based counterparts.
  Our work proposes a novel approach that almost halves the amount of memory
transfers and, therefore, considerably lifts the memory bandwidth limitation.
Being able to sort two gigabytes of eight-byte records in as little as 50
milliseconds, our approach achieves a 2.32-fold improvement over the
state-of-the-art GPU-based radix sort for uniform distributions, sustaining a
minimum speed-up of no less than a factor of 1.66 for skewed distributions.
  To address inputs that either do not reside on the GPU or exceed the
available device memory, we build on our efficient GPU sorting approach with a
pipelined heterogeneous sorting algorithm that mitigates the overhead
associated with PCIe data transfers. Comparing the end-to-end sorting
performance to the state-of-the-art CPU-based radix sort running 16 threads,
our heterogeneous approach achieves a 2.06-fold and a 1.53-fold improvement for
sorting 64 GB key-value pairs with a skewed and a uniform distribution,
respectively.
</summary>
    <author>
      <name>Elias Stehle</name>
    </author>
    <author>
      <name>Hans-Arno Jacobsen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3035918.3064043</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3035918.3064043" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, accepted at SIGMOD 2017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIGMOD (2017) 417-432</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.01137v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01137v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1611.00258">
    <id>http://arxiv.org/abs/1611.00258v2</id>
    <updated>2017-11-27T09:59:03Z</updated>
    <published>2016-11-01T14:57:32Z</published>
    <title>Dual-Pivot Quicksort: Optimality, Analysis and Zeros of Associated
  Lattice Paths</title>
    <summary>  We present an average case analysis of a variant of dual-pivot quicksort. We
show that the used algorithmic partitioning strategy is optimal, i.e., it
minimizes the expected number of key comparisons. For the analysis, we
calculate the expected number of comparisons exactly as well as asymptotically,
in particular, we provide exact expressions for the linear, logarithmic, and
constant terms.
  An essential step is the analysis of zeros of lattice paths in a certain
probability model. Along the way a combinatorial identity is proven.
</summary>
    <author>
      <name>Martin Aumüller</name>
    </author>
    <author>
      <name>Martin Dietzfelbinger</name>
    </author>
    <author>
      <name>Clemens Heuberger</name>
    </author>
    <author>
      <name>Daniel Krenn</name>
    </author>
    <author>
      <name>Helmut Prodinger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1017/S096354831800041X</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1017/S096354831800041X" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article supersedes arXiv:1602.04031</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Combin. Probab. Comput. 28 (2019), no. 4, 485-518</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.00258v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00258v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05A16, 68R05, 68P10, 68Q25, 68W40" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1611.09664">
    <id>http://arxiv.org/abs/1611.09664v1</id>
    <updated>2016-11-29T14:55:30Z</updated>
    <published>2016-11-29T14:55:30Z</published>
    <title>Optimizing run-length algorithm using octonary repetition tree</title>
    <summary>  Compression is beneficial because it helps detract resource usage. It reduces
data storage space as well as transmission traffic and improves web pages
loading. Run-length coding (RLC) is a lossless data compression algorithm. Data
are stored as a data value and counts. This is useful on data that contains
many consecutive runs. This paper proposes a compression algorithm using
octonary repetition tree (ORT), based on RLC. ORT is used to overcome the
duplication problem in primary RLC algorithms, instead of using flag or
codeword. It's the first method of run-length encoding which has the
compression ratio greater than one in all tested cases. Experimental results,
show average improvement of roughly 3 times, 3 times and 2 times in compression
ratio field of study comparing to PRLC1, PRLC2, DF-RLC respectively. By using
this approach of run-length encoding we can compress wider types of data, such
as multimedia, document, executive files, etc.
</summary>
    <author>
      <name>Kaveh Geyratmand Haghighi</name>
    </author>
    <author>
      <name>Mirkamal Mirnia</name>
    </author>
    <author>
      <name>Ahmad Habibizad Navin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information Security
  (IJCSIS) Vol. 14, No. 8, August 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.09664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1503.05977">
    <id>http://arxiv.org/abs/1503.05977v1</id>
    <updated>2015-03-20T01:31:25Z</updated>
    <published>2015-03-20T01:31:25Z</published>
    <title>Dynamic Data Structures for Document Collections and Graphs</title>
    <summary>  In the dynamic indexing problem, we must maintain a changing collection of
text documents so that we can efficiently support insertions, deletions, and
pattern matching queries. We are especially interested in developing efficient
data structures that store and query the documents in compressed form. All
previous compressed solutions to this problem rely on answering rank and select
queries on a dynamic sequence of symbols. Because of the lower bound in
[Fredman and Saks, 1989], answering rank queries presents a bottleneck in
compressed dynamic indexing. In this paper we show how this lower bound can be
circumvented using our new framework. We demonstrate that the gap between
static and dynamic variants of the indexing problem can be almost closed. Our
method is based on a novel framework for adding dynamism to static compressed
data structures. Our framework also applies more generally to dynamizing other
problems. We show, for example, how our framework can be applied to develop
compressed representations of dynamic graphs and binary relations.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <author>
      <name>Jeffrey Scott Vitter</name>
    </author>
    <link href="http://arxiv.org/abs/1503.05977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.05977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1503.06271">
    <id>http://arxiv.org/abs/1503.06271v1</id>
    <updated>2015-03-21T06:25:02Z</updated>
    <published>2015-03-21T06:25:02Z</published>
    <title>Binary Coding in Stream</title>
    <summary>  Big data is becoming ever more ubiquitous, ranging over massive video
repositories, document corpuses, image sets and Internet routing history.
Proximity search and clustering are two algorithmic primitives fundamental to
data analysis, but suffer from the "curse of dimensionality" on these gigantic
datasets. A popular attack for this problem is to convert object
representations into short binary codewords, while approximately preserving
near neighbor structure. However, there has been limited research on
constructing codewords in the "streaming" or "online" settings often applicable
to this scale of data, where one may only make a single pass over data too
massive to fit in local memory.
  In this paper, we apply recent advances in matrix sketching techniques to
construct binary codewords in both streaming and online setting. Our
experimental results compete outperform several of the most popularly used
algorithms, and we prove theoretical guarantees on performance in the streaming
setting under mild assumptions on the data and randomness of the training set.
</summary>
    <author>
      <name>Mina Ghashami</name>
    </author>
    <author>
      <name>Amirali Abdullah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 figures, 9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.06271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W40" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1503.01093">
    <id>http://arxiv.org/abs/1503.01093v3</id>
    <updated>2015-03-11T19:10:20Z</updated>
    <published>2015-03-03T20:37:41Z</published>
    <title>A note on the longest common Abelian factor problem</title>
    <summary>  Abelian string matching problems are becoming an object of considerable
interest in last years. Very recently, Alatabbi et al. \cite{AILR2015}
presented the first solution for the longest common Abelian factor problem for
a pair of strings, reaching $O(\sigma n^2)$ time with $O(\sigma n \log n)$ bits
of space, where $n$ is the length of the strings and $\sigma$ is the alphabet
size. In this note we show how the time complexity can be preserved while the
space is reduced by a factor of $\sigma$, and then how the time complexity can
be improved, if the alphabet is not too small, when superlinear space is
allowed.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v3 is vastly different to the previous one</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.01093v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.01093v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1503.00049">
    <id>http://arxiv.org/abs/1503.00049v1</id>
    <updated>2015-02-28T01:20:55Z</updated>
    <published>2015-02-28T01:20:55Z</published>
    <title>Algorithms for Longest Common Abelian Factors</title>
    <summary>  In this paper we consider the problem of computing the longest common abelian
factor (LCAF) between two given strings. We present a simple $O(\sigma~ n^2)$
time algorithm, where $n$ is the length of the strings and $\sigma$ is the
alphabet size, and a sub-quadratic running time solution for the binary string
case, both having linear space requirement. Furthermore, we present a modified
algorithm applying some interesting tricks and experimentally show that the
resulting algorithm runs faster.
</summary>
    <author>
      <name>Ali Alatabbi</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Alessio Langiu</name>
    </author>
    <author>
      <name>M. Sohel Rahman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.00049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1502.04625">
    <id>http://arxiv.org/abs/1502.04625v1</id>
    <updated>2015-02-16T16:57:17Z</updated>
    <published>2015-02-16T16:57:17Z</published>
    <title>Compressed Tree Canonization</title>
    <summary>  Straight-line (linear) context-free tree (SLT) grammars have been used to
compactly represent ordered trees. It is well known that equivalence of SLT
grammars is decidable in polynomial time. Here we extend this result and show
that isomorphism of unordered trees given as SLT grammars is decidable in
polynomial time. The proof constructs a compressed version of the canonical
form of the tree represented by the input SLT grammar. The result is
generalized to unrooted trees by "re-rooting" the compressed trees in
polynomial time. We further show that bisimulation equivalence of unrooted
unordered trees represented by SLT grammars is decidable in polynomial time.
For non-linear SLT grammars which can have double-exponential compression
ratios, we prove that unordered isomorphism is PSPACE-hard and in EXPTIME. The
same complexity bounds are shown for bisimulation equivalence.
</summary>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Sebastian Maneth</name>
    </author>
    <author>
      <name>Fabian Peternek</name>
    </author>
    <link href="http://arxiv.org/abs/1502.04625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q17, 68Q42" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1502.03845">
    <id>http://arxiv.org/abs/1502.03845v1</id>
    <updated>2015-02-12T22:12:54Z</updated>
    <published>2015-02-12T22:12:54Z</published>
    <title>Adaptive Search over Sorted Sets</title>
    <summary>  We revisit the classical algorithms for searching over sorted sets to
introduce an algorithm refinement, called Adaptive Search, that combines the
good features of Interpolation search and those of Binary search. W.r.t.
Interpolation search, only a constant number of extra comparisons is
introduced. Yet, under diverse input data distributions our algorithm shows
costs comparable to that of Interpolation search, i.e., O(log log n) while the
worst-case cost is always in O(log n), as with Binary search. On benchmarks
drawn from large datasets, both synthetic and real-life, Adaptive search scores
better times and lesser memory accesses even than Santoro and Sidney's
Interpolation-Binary search.
</summary>
    <author>
      <name>Biagio Bonasera</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <author>
      <name>Giacomo Fiumara</name>
    </author>
    <author>
      <name>Francesco Pagano</name>
    </author>
    <author>
      <name>Alessandro Provetti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jda.2014.12.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jda.2014.12.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Discrete Algorithms, Volume 30, 2015, pp. 128--133</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1502.03845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1502.01461">
    <id>http://arxiv.org/abs/1502.01461v1</id>
    <updated>2015-02-05T08:52:44Z</updated>
    <published>2015-02-05T08:52:44Z</published>
    <title>Parameterized Complexity of Superstring Problems</title>
    <summary>  In the Shortest Superstring problem we are given a set of strings $S=\{s_1,
\ldots, s_n\}$ and integer $\ell$ and the question is to decide whether there
is a superstring $s$ of length at most $\ell$ containing all strings of $S$ as
substrings. We obtain several parameterized algorithms and complexity results
for this problem.
  In particular, we give an algorithm which in time $2^{O(k)}
\operatorname{poly}(n)$ finds a superstring of length at most $\ell$ containing
at least $k$ strings of $S$. We complement this by the lower bound showing that
such a parameterization does not admit a polynomial kernel up to some
complexity assumption. We also obtain several results about "below guaranteed
values" parameterization of the problem. We show that parameterization by
compression admits a polynomial kernel while parameterization "below matching"
is hard.
</summary>
    <author>
      <name>Ivan Bliznets</name>
    </author>
    <author>
      <name>Fedor V. Fomin</name>
    </author>
    <author>
      <name>Petr A. Golovach</name>
    </author>
    <author>
      <name>Nikolay Karpov</name>
    </author>
    <author>
      <name>Alexander S. Kulikov</name>
    </author>
    <author>
      <name>Saket Saurabh</name>
    </author>
    <link href="http://arxiv.org/abs/1502.01461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1502.01063">
    <id>http://arxiv.org/abs/1502.01063v2</id>
    <updated>2015-04-02T21:30:10Z</updated>
    <published>2015-02-03T23:27:26Z</published>
    <title>Quadratic Conditional Lower Bounds for String Problems and Dynamic Time
  Warping</title>
    <summary>  Classic similarity measures of strings are longest common subsequence and
Levenshtein distance (i.e., the classic edit distance). A classic similarity
measure of curves is dynamic time warping. These measures can be computed by
simple $O(n^2)$ dynamic programming algorithms, and despite much effort no
algorithms with significantly better running time are known.
  We prove that, even restricted to binary strings or one-dimensional curves,
respectively, these measures do not have strongly subquadratic time algorithms,
i.e., no algorithms with running time $O(n^{2-\varepsilon})$ for any
$\varepsilon > 0$, unless the Strong Exponential Time Hypothesis fails. We
generalize the result to edit distance for arbitrary fixed costs of the four
operations (deletion in one of the two strings, matching, substitution), by
identifying trivial cases that can be solved in constant time, and proving
quadratic-time hardness on binary strings for all other cost choices. This
improves and generalizes the known hardness result for Levenshtein distance
[Backurs, Indyk STOC'15] by the restriction to binary strings and the
generalization to arbitrary costs, and adds important problems to a recent line
of research showing conditional lower bounds for a growing number of quadratic
time problems.
  As our main technical contribution, we introduce a framework for proving
quadratic-time hardness of similarity measures. To apply the framework it
suffices to construct a single gadget, which encapsulates all the expressive
power necessary to emulate a reduction from satisfiability.
  Finally, we prove quadratic-time hardness for longest palindromic subsequence
and longest tandem subsequence via reductions from longest common subsequence,
showing that conditional lower bounds based on the Strong Exponential Time
Hypothesis also apply to string problems that are not necessarily similarity
measures.
</summary>
    <author>
      <name>Karl Bringmann</name>
    </author>
    <author>
      <name>Marvin Künnemann</name>
    </author>
    <link href="http://arxiv.org/abs/1502.01063v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01063v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1501.07053">
    <id>http://arxiv.org/abs/1501.07053v2</id>
    <updated>2015-01-29T22:57:04Z</updated>
    <published>2015-01-28T10:12:58Z</published>
    <title>Quadratic-Time Hardness of LCS and other Sequence Similarity Measures</title>
    <summary>  Two important similarity measures between sequences are the longest common
subsequence (LCS) and the dynamic time warping distance (DTWD). The
computations of these measures for two given sequences are central tasks in a
variety of applications. Simple dynamic programming algorithms solve these
tasks in $O(n^2)$ time, and despite an extensive amount of research, no
algorithms with significantly better worst case upper bounds are known.
  In this paper, we show that an $O(n^{2-\epsilon})$ time algorithm, for some
$\epsilon>0$, for computing the LCS or the DTWD of two sequences of length $n$
over a constant size alphabet, refutes the popular Strong Exponential Time
Hypothesis (SETH). Moreover, we show that computing the LCS of $k$ strings over
an alphabet of size $O(k)$ cannot be done in $O(n^{k-\epsilon})$ time, for any
$\epsilon>0$, under SETH. Finally, we also address the time complexity of
approximating the DTWD of two strings in truly subquadratic time.
</summary>
    <author>
      <name>Amir Abboud</name>
    </author>
    <author>
      <name>Arturs Backurs</name>
    </author>
    <author>
      <name>Virginia Vassilevska Williams</name>
    </author>
    <link href="http://arxiv.org/abs/1501.07053v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07053v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1501.06461">
    <id>http://arxiv.org/abs/1501.06461v4</id>
    <updated>2017-02-08T17:35:52Z</updated>
    <published>2015-01-26T16:18:27Z</published>
    <title>On The Average-Case Complexity of Shellsort</title>
    <summary>  We prove a lower bound expressed in the increment sequence on the
average-case complexity of the number of inversions of Shellsort. This lower
bound is sharp in every case where it could be checked. A special case of this
lower bound yields the general Jiang-Li-Vit\'anyi lower bound. We obtain new
results e.g. determining the average-case complexity precisely in the
Yao-Janson-Knuth 3-pass case.
</summary>
    <author>
      <name>Paul M. B. Vitanyi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Research Center for Mathematics and Computer Science in the Netherlands</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages LaTeX</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Random Structures and Algorithms, 52:2(2018), 354-363</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1501.06461v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.06461v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1505.00558">
    <id>http://arxiv.org/abs/1505.00558v2</id>
    <updated>2015-05-11T11:01:43Z</updated>
    <published>2015-05-04T08:45:38Z</published>
    <title>Triple State QuickSort, A replacement for the C/C++ library qsort</title>
    <summary>  An industrial grade Quicksort function along with its new algorithm is
presented. Compared to 4 other well known implementations of Quicksort, the new
algorithm reduces both the number of comparisons and swaps in most cases while
staying close to the best of the 4 in worst cases. We trade space for
performance, at the price of n/2 temporary extra spaces in the worst case. Run
time tests reveal an overall improvement of at least 15.8% compared to the
overall best of the other 4 functions. Furthermore, our function scores a 32.7%
run time improvement against Yaroslavskiy's new Dual Pivot Quicksort. Our
function is pointer based, which is meant as a replacement for the C/C++
library qsort(). But we also provide an array based function of the same
algorithm for easy porting to different programming languages.
</summary>
    <author>
      <name>Ammar Muqaddas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 49 Figures. Minor fix in page 15 and a typo</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00558v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00558v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1505.00147">
    <id>http://arxiv.org/abs/1505.00147v1</id>
    <updated>2015-05-01T10:40:07Z</updated>
    <published>2015-05-01T10:40:07Z</published>
    <title>Strictly Implicit Priority Queues: On the Number of Moves and Worst-Case
  Time</title>
    <summary>  The binary heap of Williams (1964) is a simple priority queue characterized
by only storing an array containing the elements and the number of elements $n$
- here denoted a strictly implicit priority queue. We introduce two new
strictly implicit priority queues. The first structure supports amortized
$O(1)$ time Insert and $O(\log n)$ time ExtractMin operations, where both
operations require amortized $O(1)$ element moves. No previous implicit heap
with $O(1)$ time Insert supports both operations with $O(1)$ moves. The second
structure supports worst-case $O(1)$ time Insert and $O(\log n)$ time (and
moves) ExtractMin operations. Previous results were either amortized or needed
$O(\log n)$ bits of additional state information between operations.
</summary>
    <author>
      <name>Gerth Stølting Brodal</name>
    </author>
    <author>
      <name>Jesper Sindahl Nielsen</name>
    </author>
    <author>
      <name>Jakob Truelsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at WADS 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1505.04911">
    <id>http://arxiv.org/abs/1505.04911v4</id>
    <updated>2016-05-31T08:09:35Z</updated>
    <published>2015-05-19T09:01:43Z</published>
    <title>Read Mapping on de Bruijn graph</title>
    <summary>  Background Next Generation Sequencing (NGS) has dramatically enhanced our
ability to sequence genomes, but not to assemble them. In practice, many
published genome sequences remain in the state of a large set of contigs. Each
contig describes the sequence found along some path of the assembly graph,
however, the set of contigs does not record all the sequence information
contained in that graph. Although many subsequent analyses can be performed
with the set of contigs, one may ask whether mapping reads on the contigs is as
informative as mapping them on the paths of the assembly graph. Currently, one
lacks practical tools to perform mapping on such graphs. Results Here, we
propose a formal definition of mapping on a de Bruijn graph, analyse the
problem complexity which turns out to be NP-complete, and provide a practical
solution.We propose a pipeline called GGMAP (Greedy Graph MAPping). Its novelty
is a procedure to map reads on branching paths of the graph, for which we
designed a heuristic algorithm called BGREAT (de Bruijn Graph REAd mapping
Tool). For the sake of efficiency, BGREAT rewrites a read sequence as a
succession of unitigs sequences. GGMAP can map millions of reads per CPU hour
on a de Bruijn graph built from a large set of human genomic reads.
Surprisingly, results show that up to 22% more reads can be mapped on the graph
but not on the contig set. Conclusions Although mapping reads on a de Bruijn
graph is complex task, our proposal offers a practical solution combining
efficiency with an improved mapping capacity compared to assembly-based mapping
even for complex eukaryotic data. Availability: github.com/Malfoy/BGREAT
Keywords: Read mapping; De bruijn graphs; NGS; NP-completeness
</summary>
    <author>
      <name>Antoine Limasset</name>
    </author>
    <author>
      <name>Bastien Cazaux</name>
    </author>
    <author>
      <name>Eric Rivals</name>
    </author>
    <author>
      <name>Pierre Peterlongo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1186/s12859-016-1103-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1186/s12859-016-1103-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BMC Bioinformatics</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.04911v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.04911v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1505.06529">
    <id>http://arxiv.org/abs/1505.06529v1</id>
    <updated>2015-05-25T03:13:32Z</updated>
    <published>2015-05-25T03:13:32Z</published>
    <title>An efficient dynamic programming algorithm for the generalized LCS
  problem with multiple substring inclusive constraints</title>
    <summary>  In this paper, we consider a generalized longest common subsequence problem
with multiple substring inclusive constraints. For the two input sequences $X$
and $Y$ of lengths $n$ and $m$, and a set of $d$ constraints
$P=\{P_1,\cdots,P_d\}$ of total length $r$, the problem is to find a common
subsequence $Z$ of $X$ and $Y$ including each of constraint string in $P$ as a
substring and the length of $Z$ is maximized. A new dynamic programming
solution to this problem is presented in this paper. The correctness of the new
algorithm is proved. The time complexity of our algorithm is $O(d2^dnmr)$. In
the case of the number of constraint strings is fixed, our new algorithm for
the generalized longest common subsequence problem with multiple substring
inclusive constraints requires $O(nmr)$ time and space.
</summary>
    <author>
      <name>Daxin Zhu</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Yingjie Wu</name>
    </author>
    <author>
      <name>Xiaodong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1303.1872</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.06529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.06529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1506.00063">
    <id>http://arxiv.org/abs/1506.00063v1</id>
    <updated>2015-05-30T03:14:38Z</updated>
    <published>2015-05-30T03:14:38Z</published>
    <title>An Efficient Dynamic Programming Algorithm for STR-IC-SEQ-EC-LCS Problem</title>
    <summary>  In this paper, we consider a generalized longest common subsequence problem,
in which a constraining sequence of length $s$ must be included as a substring
and the other constraining sequence of length $t$ must be excluded as a
subsequence of two main sequences and the length of the result must be maximal.
For the two input sequences $X$ and $Y$ of lengths $n$ and $m$, and the given
two constraining sequences of length $s$ and $t$, we present an $O(nmst)$ time
dynamic programming algorithm for solving the new generalized longest common
subsequence problem. The time complexity can be reduced further to cubic time
in a more detailed analysis. The correctness of the new algorithm is proved.
</summary>
    <author>
      <name>Daxin Zhu</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Yingjie Wu</name>
    </author>
    <author>
      <name>Xiaodong Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1506.00063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1504.05535">
    <id>http://arxiv.org/abs/1504.05535v2</id>
    <updated>2015-09-28T04:12:34Z</updated>
    <published>2015-04-21T18:25:29Z</published>
    <title>Tree compression using string grammars</title>
    <summary>  We study the compressed representation of a ranked tree by a (string)
straight-line program (SLP) for its preorder traversal, and compare it with the
well-studied representation by straight-line context free tree grammars (which
are also known as tree straight-line programs or TSLPs). Although SLPs turn out
to be exponentially more succinct than TSLPs, we show that many simple tree
queries can still be performed efficiently on SLPs, such as computing the
height and Horton-Strahler number of a tree, tree navigation, or evaluation of
Boolean expressions. Other problems on tree traversals turn out to be
intractable, e.g. pattern matching and evaluation of tree automata.
</summary>
    <author>
      <name>Moses Ganardi</name>
    </author>
    <author>
      <name>Danny Hucke</name>
    </author>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Eric Noeth</name>
    </author>
    <link href="http://arxiv.org/abs/1504.05535v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.05535v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P30, 68Q42, 68Q17" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1504.00834">
    <id>http://arxiv.org/abs/1504.00834v1</id>
    <updated>2015-04-03T13:10:58Z</updated>
    <published>2015-04-03T13:10:58Z</published>
    <title>The complexity of computation in bit streams</title>
    <summary>  We revisit the complexity of online computation in the cell probe model. We
consider a class of problems where we are first given a fixed pattern or vector
$F$ of $n$ symbols and then one symbol arrives at a time in a stream. After
each symbol has arrived we must output some function of $F$ and the $n$-length
suffix of the arriving stream. Cell probe bounds of $\Omega(\delta\lg{n}/w)$
have previously been shown for both convolution and Hamming distance in this
setting, where $\delta$ is the size of a symbol in bits and
$w\in\Omega(\lg{n})$ is the cell size in bits. However, when $\delta$ is a
constant, as it is in many natural situations, these previous results no longer
give us non-trivial bounds.
  We introduce a new lop-sided information transfer proof technique which
enables us to prove meaningful lower bounds even for constant size input
alphabets. We use our new framework to prove an amortised cell probe lower
bound of $\Omega(\lg^2 n/(w\cdot \lg \lg n))$ time per arriving bit for an
online version of a well studied problem known as pattern matching with address
errors. This is the first non-trivial cell probe lower bound for any online
problem on bit streams that still holds when the cell sizes are large. We also
show the same bound for online convolution conditioned on a new combinatorial
conjecture related to Toeplitz matrices.
</summary>
    <author>
      <name>Raphael Clifford</name>
    </author>
    <author>
      <name>Markus Jalsenius</name>
    </author>
    <author>
      <name>Benjamin Sach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.00834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.00834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1101.4068">
    <id>http://arxiv.org/abs/1101.4068v1</id>
    <updated>2011-01-21T03:23:57Z</updated>
    <published>2011-01-21T03:23:57Z</published>
    <title>Linear-Space Data Structures for Range Mode Query in Arrays</title>
    <summary>  A mode of a multiset $S$ is an element $a \in S$ of maximum multiplicity;
that is, $a$ occurs at least as frequently as any other element in $S$. Given a
list $A[1:n]$ of $n$ items, we consider the problem of constructing a data
structure that efficiently answers range mode queries on $A$. Each query
consists of an input pair of indices $(i, j)$ for which a mode of $A[i:j]$ must
be returned. We present an $O(n^{2-2\epsilon})$-space static data structure
that supports range mode queries in $O(n^\epsilon)$ time in the worst case, for
any fixed $\epsilon \in [0,1/2]$. When $\epsilon = 1/2$, this corresponds to
the first linear-space data structure to guarantee $O(\sqrt{n})$ query time. We
then describe three additional linear-space data structures that provide
$O(k)$, $O(m)$, and $O(|j-i|)$ query time, respectively, where $k$ denotes the
number of distinct elements in $A$ and $m$ denotes the frequency of the mode of
$A$. Finally, we examine generalizing our data structures to higher dimensions.
</summary>
    <author>
      <name>Stephane Durocher</name>
    </author>
    <author>
      <name>Jason Morrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.4068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.4068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1101.3448">
    <id>http://arxiv.org/abs/1101.3448v1</id>
    <updated>2011-01-18T12:58:02Z</updated>
    <published>2011-01-18T12:58:02Z</published>
    <title>Inducing the LCP-Array</title>
    <summary>  We show how to modify the linear-time construction algorithm for suffix
arrays based on induced sorting (Nong et al., DCC'09) such that it computes the
array of longest common prefixes (LCP-array) as well. Practical tests show that
this outperforms recent LCP-array construction algorithms (Gog and Ohlebusch,
ALENEX'11).
</summary>
    <author>
      <name>Johannes Fischer</name>
    </author>
    <link href="http://arxiv.org/abs/1101.3448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.3448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1503.08498">
    <id>http://arxiv.org/abs/1503.08498v1</id>
    <updated>2015-03-29T21:37:24Z</updated>
    <published>2015-03-29T21:37:24Z</published>
    <title>Dual pivot Quicksort</title>
    <summary>  In this paper, we analyse the dual pivot Quicksort, a variant of the standard
Quicksort algorithm, in which two pivots are used for the partitioning of the
array. We are solving recurrences of the expected number of key comparisons and
exchanges performed by the algorithm, obtaining the exact and asymptotic total
average values contributing to its time complexity. Further, we compute the
average number of partitioning stages and the variance of the number of key
comparisons. In terms of mean values, dual pivot Quicksort does not appear to
be faster than ordinary algorithm.
</summary>
    <author>
      <name>Vasileios Iliopoulos</name>
    </author>
    <author>
      <name>David B. Penman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S1793830912500413</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S1793830912500413" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Post print of the article "Dual pivot Quicksort" published on 1
  August of 2012 in the journal of Discrete Mathematics, Algorithms and
  Applications</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Discrete Math. Algorithm. Appl. 04, 1250041 (2012) [13 pages]</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.08498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.08498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W40, 60F05" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1506.08518">
    <id>http://arxiv.org/abs/1506.08518v2</id>
    <updated>2015-12-22T08:31:43Z</updated>
    <published>2015-06-29T06:24:08Z</published>
    <title>Fast Computation of Abelian Runs</title>
    <summary>  Given a word $w$ and a Parikh vector $\mathcal{P}$, an abelian run of period
$\mathcal{P}$ in $w$ is a maximal occurrence of a substring of $w$ having
abelian period $\mathcal{P}$. Our main result is an online algorithm that,
given a word $w$ of length $n$ over an alphabet of cardinality $\sigma$ and a
Parikh vector $\mathcal{P}$, returns all the abelian runs of period
$\mathcal{P}$ in $w$ in time $O(n)$ and space $O(\sigma+p)$, where $p$ is the
norm of $\mathcal{P}$, i.e., the sum of its components. We also present an
online algorithm that computes all the abelian runs with periods of norm $p$ in
$w$ in time $O(np)$, for any given norm $p$. Finally, we give an $O(n^2)$-time
offline randomized algorithm for computing all the abelian runs of $w$. Its
deterministic counterpart runs in $O(n^2\log\sigma)$ time.
</summary>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Thierry Lecroq</name>
    </author>
    <author>
      <name>Arnaud Lefebvre</name>
    </author>
    <author>
      <name>Elise Prieur-Gaston</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2015.12.010</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2015.12.010" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Theoretical Computer Science</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Theoretical Computer Science, 656 Part B: 256-264, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.08518v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08518v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1506.06983">
    <id>http://arxiv.org/abs/1506.06983v1</id>
    <updated>2015-06-23T13:29:26Z</updated>
    <published>2015-06-23T13:29:26Z</published>
    <title>Linear Algorithms for Computing the Lyndon Border Array and the Lyndon
  Suffix Array</title>
    <summary>  We consider the problem of finding repetitive structures and inherent
patterns in a given string $\s{s}$ of length $n$ over a finite totally ordered
alphabet. A border $\s{u}$ of a string $\s{s}$ is both a prefix and a suffix of
$\s{s}$ such that $\s{u} \not= \s{s}$. The computation of the border array of a
string $\s{s}$, namely the borders of each prefix of $\s{s}$, is strongly
related to the string matching problem: given a string $\s{w}$, find all of its
occurrences in $\s{s}$. A {\itshape Lyndon word} is a primitive word (i.e., it
is not a power of another word) which is minimal for the lexicographical order
of its conjugacy class (i.e., the set of words obtained by cyclic rotations of
the letters). In this paper we combine these concepts to introduce the
\emph{Lyndon Border Array} $\mathcal L \beta$ of $\s{s}$, whose $i$-th entry
$\mathcal L \beta(\s{s})[i]$ is the length of the longest border of $\s{s}[1
\dd i]$ which is also a Lyndon word. We propose linear-time and linear-space
algorithms for computing $\mathcal L \beta (\s{s})$. %in the case of both
binary and bounded alphabets. Further, we introduce the \emph{Lyndon Suffix
Array}, and by modifying the efficient suffix array technique of Ko and Aluru
\cite{KA03} outline a linear time and space algorithm for its construction.
</summary>
    <author>
      <name>Ali Alatabbi</name>
    </author>
    <author>
      <name>Jacqueline W. Daykin</name>
    </author>
    <author>
      <name>M. Sohel Rahman</name>
    </author>
    <link href="http://arxiv.org/abs/1506.06983v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06983v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1506.06793">
    <id>http://arxiv.org/abs/1506.06793v1</id>
    <updated>2015-06-22T21:13:36Z</updated>
    <published>2015-06-22T21:13:36Z</published>
    <title>Enhanced Covers of Regular &amp; Indeterminate Strings using Prefix Tables</title>
    <summary>  A \itbf{cover} of a string $x = x[1..n]$ is a proper substring $u$ of $x$
such that $x$ can be constructed from possibly overlapping instances of $u$. A
recent paper \cite{FIKPPST13} relaxes this definition --- an \itbf{enhanced
cover} $u$ of $x$ is a border of $x$ (that is, a proper prefix that is also a
suffix) that covers a {\it maximum} number of positions in $x$ (not necessarily
all) --- and proposes efficient algorithms for the computation of enhanced
covers. These algorithms depend on the prior computation of the \itbf{border
array} $\beta[1..n]$, where $\beta[i]$ is the length of the longest border of
$x[1..i]$, $1 \le i \le n$. In this paper, we first show how to compute
enhanced covers using instead the \itbf{prefix table}: an array $\pi[1..n]$
such that $\pi[i]$ is the length of the longest substring of $x$ beginning at
position $i$ that matches a prefix of $x$. Unlike the border array, the prefix
table is robust: its properties hold also for \itbf{indeterminate strings} ---
that is, strings defined on {\it subsets} of the alphabet $\Sigma$ rather than
individual elements of $\Sigma$. Thus, our algorithms, in addition to being
faster in practice and more space-efficient than those of \cite{FIKPPST13},
allow us to easily extend the computation of enhanced covers to indeterminate
strings. Both for regular and indeterminate strings, our algorithms execute in
expected linear time. Along the way we establish an important theoretical
result: that the expected maximum length of any border of any prefix of a
regular string $x$ is approximately 1.64 for binary alphabets, less for larger
ones.
</summary>
    <author>
      <name>Ali Alatabbi</name>
    </author>
    <author>
      <name>A. S. Sohidull Islam</name>
    </author>
    <author>
      <name>M. Sohel Rahman</name>
    </author>
    <author>
      <name>Jamie Simpson</name>
    </author>
    <author>
      <name>W. F. Smyth</name>
    </author>
    <link href="http://arxiv.org/abs/1506.06793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1506.04896">
    <id>http://arxiv.org/abs/1506.04896v2</id>
    <updated>2015-10-26T18:51:02Z</updated>
    <published>2015-06-16T09:53:29Z</published>
    <title>FM-index for dummies</title>
    <summary>  The FM-index is a celebrated compressed data structure for full-text pattern
searching. After the first wave of interest in its theoretical developments, we
can observe a surge of interest in practical FM-index variants in the last few
years. These enhancements are often related to a bit-vector representation,
augmented with an efficient rank-handling data structure. In this work, we
propose a new, cache-friendly, implementation of the rank primitive and
advocate for a very simple architecture of the FM-index, which trades
compression ratio for speed. Experimental results show that our variants are
2--3 times faster than the fastest known ones, for the price of using typically
1.5--5 times more space.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Marcin Raniszewski</name>
    </author>
    <author>
      <name>Sebastian Deorowicz</name>
    </author>
    <link href="http://arxiv.org/abs/1506.04896v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04896v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1506.04559">
    <id>http://arxiv.org/abs/1506.04559v1</id>
    <updated>2015-06-15T11:45:53Z</updated>
    <published>2015-06-15T11:45:53Z</published>
    <title>Linear Algorithm for Conservative Degenerate Pattern Matching</title>
    <summary>  A degenerate symbol x* over an alphabet A is a non-empty subset of A, and a
sequence of such symbols is a degenerate string. A degenerate string is said to
be conservative if its number of non-solid symbols is upper-bounded by a fixed
positive constant k. We consider here the matching problem of conservative
degenerate strings and present the first linear-time algorithm that can find,
for given degenerate strings P* and T* of total length n containing k non-solid
symbols in total, the occurrences of P* in T* in O(nk) time.
</summary>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Ritu Kundu</name>
    </author>
    <author>
      <name>Manal Mohamed</name>
    </author>
    <author>
      <name>Fatima Vayani</name>
    </author>
    <link href="http://arxiv.org/abs/1506.04559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1506.04486">
    <id>http://arxiv.org/abs/1506.04486v1</id>
    <updated>2015-06-15T06:08:39Z</updated>
    <published>2015-06-15T06:08:39Z</published>
    <title>Error Tree: A Tree Structure for Hamming &amp; Edit Distances &amp; Wildcards
  Matching</title>
    <summary>  Error Tree is a novel tree structure that is mainly oriented to solve the
approximate pattern matching problems, Hamming and edit distances, as well as
the wildcards matching problem. The input is a text of length $n$ over a fixed
alphabet of length $\Sigma$, a pattern of length $m$, and $k$. The output is to
find all positions that have $\leq$ $k$ Hamming distance, edit distance, or
wildcards matching with $P$. The algorithm proposes for Hamming distance and
wildcards matching a tree structure that needs $O(n\frac{log_\Sigma
^{k}n}{k!})$ words and takes $O(\frac {m^k}{k!} + occ$)($O(m + \frac
{log_\Sigma ^kn}{k!} + occ$) in the average case) of query time for any
online/offline pattern, where $occ$ is the number of outputs. As well, a tree
structure of $O(2^{k}n\frac{log_\Sigma ^{k}n}{k!})$ words and $O(\frac
{m^k}{k!} + 3^{k}occ$)($O(m + \frac {log_\Sigma ^kn}{k!} + 3^{k}occ$) in the
average case) query time for edit distance for any online/offline pattern.
</summary>
    <author>
      <name>Anas Al-Okaily</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1089/cmb.2015.0132</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1089/cmb.2015.0132" rel="related"/>
    <link href="http://arxiv.org/abs/1506.04486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1506.04499">
    <id>http://arxiv.org/abs/1506.04499v1</id>
    <updated>2015-06-15T07:51:29Z</updated>
    <published>2015-06-15T07:51:29Z</published>
    <title>Tree Compression with Top Trees Revisited</title>
    <summary>  We revisit tree compression with top trees (Bille et al, ICALP'13) and
present several improvements to the compressor and its analysis. By
significantly reducing the amount of information stored and guiding the
compression step using a RePair-inspired heuristic, we obtain a fast compressor
achieving good compression ratios, addressing an open problem posed by Bille et
al. We show how, with relatively small overhead, the compressed file can be
converted into an in-memory representation that supports basic navigation
operations in worst-case logarithmic time without decompression. We also show a
much improved worst-case bound on the size of the output of top-tree
compression (answering an open question posed in a talk on this algorithm by
Weimann in 2012).
</summary>
    <author>
      <name>Lorenz Hübschle-Schneider</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SEA 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.04499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; E.2; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1506.03528">
    <id>http://arxiv.org/abs/1506.03528v1</id>
    <updated>2015-06-11T01:50:18Z</updated>
    <published>2015-06-11T01:50:18Z</published>
    <title>Amortized Rotation Cost in AVL Trees</title>
    <summary>  An AVL tree is the original type of balanced binary search tree. An insertion
in an $n$-node AVL tree takes at most two rotations, but a deletion in an
$n$-node AVL tree can take $\Theta(\log n)$. A natural question is whether
deletions can take many rotations not only in the worst case but in the
amortized case as well. A sequence of $n$ successive deletions in an $n$-node
tree takes $O(n)$ rotations, but what happens when insertions are intermixed
with deletions? Heaupler, Sen, and Tarjan conjectured that alternating
insertions and deletions in an $n$-node AVL tree can cause each deletion to do
$\Omega(\log n)$ rotations, but they provided no construction to justify their
claim. We provide such a construction: we show that, for infinitely many $n$,
there is a set $E$ of {\it expensive} $n$-node AVL trees with the property
that, given any tree in $E$, deleting a certain leaf and then reinserting it
produces a tree in $E$, with the deletion having done $\Theta(\log n)$
rotations. One can do an arbitrary number of such expensive deletion-insertion
pairs. The difficulty in obtaining such a construction is that in general the
tree produced by an expensive deletion-insertion pair is not the original tree.
Indeed, if the trees in $E$ have even height $k$, $2^{k/2}$ deletion-insertion
pairs are required to reproduce the original tree.
</summary>
    <author>
      <name>Mahdi Amani</name>
    </author>
    <author>
      <name>Kevin A. Lai</name>
    </author>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <link href="http://arxiv.org/abs/1506.03528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1505.01523">
    <id>http://arxiv.org/abs/1505.01523v5</id>
    <updated>2017-01-03T16:51:14Z</updated>
    <published>2015-05-06T21:47:25Z</published>
    <title>Fast and Powerful Hashing using Tabulation</title>
    <summary>  Randomized algorithms are often enjoyed for their simplicity, but the hash
functions employed to yield the desired probabilistic guarantees are often too
complicated to be practical. Here we survey recent results on how simple
hashing schemes based on tabulation provide unexpectedly strong guarantees.
  Simple tabulation hashing dates back to Zobrist [1970]. Keys are viewed as
consisting of $c$ characters and we have precomputed character tables
$h_1,...,h_c$ mapping characters to random hash values. A key $x=(x_1,...,x_c)$
is hashed to $h_1[x_1] \oplus h_2[x_2].....\oplus h_c[x_c]$. This schemes is
very fast with character tables in cache. While simple tabulation is not even
4-independent, it does provide many of the guarantees that are normally
obtained via higher independence, e.g., linear probing and Cuckoo hashing.
  Next we consider twisted tabulation where one input character is "twisted" in
a simple way. The resulting hash function has powerful distributional
properties: Chernoff-Hoeffding type tail bounds and a very small bias for
min-wise hashing. This also yields an extremely fast pseudo-random number
generator that is provably good for many classic randomized algorithms and
data-structures.
  Finally, we consider double tabulation where we compose two simple tabulation
functions, applying one to the output of the other, and show that this yields
very high independence in the classic framework of Carter and Wegman [1977]. In
fact, w.h.p., for a given set of size proportional to that of the space
consumed, double tabulation gives fully-random hashing. We also mention some
more elaborate tabulation schemes getting near-optimal independence for given
time and space.
  While these tabulation schemes are all easy to implement and use, their
analysis is not.
</summary>
    <author>
      <name>Mikkel Thorup</name>
    </author>
    <link href="http://arxiv.org/abs/1505.01523v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.01523v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1505.01210">
    <id>http://arxiv.org/abs/1505.01210v1</id>
    <updated>2015-05-05T22:40:19Z</updated>
    <published>2015-05-05T22:40:19Z</published>
    <title>Implementation of BT-trees</title>
    <summary>  This document presents the full implementation details of BT-trees, a highly
efficient ordered map, and an evaluation which compares BT-trees with unordered
maps. BT- trees are often much faster than other ordered maps, and have
comparable performance to unordered map implementations. However, in benchmarks
which favor unordered maps, BT-trees are not faster than the fastest unordered
map implementations we know of.
</summary>
    <author>
      <name>Lars F. Bonnichsen</name>
    </author>
    <author>
      <name>Christian W. Probst</name>
    </author>
    <author>
      <name>Sven Karlsson</name>
    </author>
    <link href="http://arxiv.org/abs/1505.01210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.01210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1509.06257">
    <id>http://arxiv.org/abs/1509.06257v1</id>
    <updated>2015-09-21T14:59:05Z</updated>
    <published>2015-09-21T14:59:05Z</published>
    <title>Communication Complexity (for Algorithm Designers)</title>
    <summary>  This document collects the lecture notes from my course "Communication
Complexity (for Algorithm Designers),'' taught at Stanford in the winter
quarter of 2015. The two primary goals of the course are: 1. Learn several
canonical problems that have proved the most useful for proving lower bounds
(Disjointness, Index, Gap-Hamming, etc.). 2. Learn how to reduce lower bounds
for fundamental algorithmic problems to communication complexity lower bounds.
Along the way, we'll also: 3. Get exposure to lots of cool computational models
and some famous results about them --- data streams and linear sketches,
compressive sensing, space-query time trade-offs in data structures,
sublinear-time algorithms, and the extension complexity of linear programs. 4.
Scratch the surface of techniques for proving communication complexity lower
bounds (fooling sets, corruption bounds, etc.).
</summary>
    <author>
      <name>Tim Roughgarden</name>
    </author>
    <link href="http://arxiv.org/abs/1509.06257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1509.06167">
    <id>http://arxiv.org/abs/1509.06167v2</id>
    <updated>2015-09-29T11:08:42Z</updated>
    <published>2015-09-21T10:02:39Z</published>
    <title>Parallel Query in the Suffix Tree</title>
    <summary>  Given the query string of length $m$, we explore a parallel query in a static
suffix tree based data structure for $p \ll n$, where $p$ is the number of
processors and $n$ is the length of the text. We present three results on CREW
PRAM. The parallel query in the suffix trie requires $O(m + p)$ work, $O(m/p +
\lg p)$ time and $O(n^2)$ space in the worst case. We extend the same technique
to the suffix tree where we show it is, by design, inherently sequential in the
worst case. Finally we perform the parallel query using an interleaved approach
and achieve $O(m \lg p)$ work, $O(\frac{m}{p} \lg p)$ time and $O(n \lg p)$
space in the worst case.
</summary>
    <author>
      <name>Matevž Jekovec</name>
    </author>
    <author>
      <name>Andrej Brodnik</name>
    </author>
    <link href="http://arxiv.org/abs/1509.06167v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06167v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1509.05809">
    <id>http://arxiv.org/abs/1509.05809v1</id>
    <updated>2015-09-18T21:54:26Z</updated>
    <published>2015-09-18T21:54:26Z</published>
    <title>Lower bounds for approximation schemes for Closest String</title>
    <summary>  In the Closest String problem one is given a family $\mathcal S$ of
equal-length strings over some fixed alphabet, and the task is to find a string
$y$ that minimizes the maximum Hamming distance between $y$ and a string from
$\mathcal S$. While polynomial-time approximation schemes (PTASes) for this
problem are known for a long time [Li et al., J. ACM'02], no efficient
polynomial-time approximation scheme (EPTAS) has been proposed so far. In this
paper, we prove that the existence of an EPTAS for Closest String is in fact
unlikely, as it would imply that $\mathrm{FPT}=\mathrm{W}[1]$, a highly
unexpected collapse in the hierarchy of parameterized complexity classes. Our
proof also shows that the existence of a PTAS for Closest String with running
time $f(\varepsilon)\cdot n^{o(1/\varepsilon)}$, for any computable function
$f$, would contradict the Exponential Time Hypothesis.
</summary>
    <author>
      <name>Marek Cygan</name>
    </author>
    <author>
      <name>Daniel Lokshtanov</name>
    </author>
    <author>
      <name>Marcin Pilipczuk</name>
    </author>
    <author>
      <name>Michał Pilipczuk</name>
    </author>
    <author>
      <name>Saket Saurabh</name>
    </author>
    <link href="http://arxiv.org/abs/1509.05809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.05809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1508.06610">
    <id>http://arxiv.org/abs/1508.06610v1</id>
    <updated>2015-08-26T19:11:09Z</updated>
    <published>2015-08-26T19:11:09Z</published>
    <title>Full-text and Keyword Indexes for String Searching</title>
    <summary>  In this work, we present a literature review for full-text and keyword
indexes as well as our contributions (which are mostly practice-oriented).
  The first contribution is the FM-bloated index, which is a modification of
the well-known FM-index (a compressed, full-text index) that trades space for
speed. In our approach, the count table and the occurrence lists store
information about selected $q$-grams in addition to the individual characters.
Two variants are described, namely one using $O(n \log^2 n)$ bits of space with
$O(m + \log m \log \log n)$ average query time, and one with linear space and
$O(m \log \log n)$ average query time, where $n$ is the input text length and
$m$ is the pattern length. We experimentally show that a significant speedup
can be achieved by operating on $q$-grams (albeit at the cost of very high
space requirements, hence the name "bloated").
  In the category of keyword indexes we present the so-called split index,
which can efficiently solve the $k$-mismatches problem, especially for 1 error.
Our implementation in the C++ language is focused mostly on data compaction,
which is beneficial for the search speed (by being cache friendly). We compare
our solution with other algorithms and we show that it is faster when the
Hamming distance is used. Query times in the order of 1 microsecond were
reported for one mismatch for a few-megabyte natural language dictionary on a
medium-end PC.
  A minor contribution includes string sketches which aim to speed up
approximate string comparison at the cost of additional space ($O(1)$ per
string). They can be used in the context of keyword indexes in order to deduce
that two strings differ by at least $k$ mismatches with the use of fast bitwise
operations rather than an explicit verification.
</summary>
    <author>
      <name>Aleksander Cisłak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master's thesis, 107 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.06610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1508.05553">
    <id>http://arxiv.org/abs/1508.05553v1</id>
    <updated>2015-08-23T00:56:03Z</updated>
    <published>2015-08-23T00:56:03Z</published>
    <title>A Practical O(R\log\log n+n) time Algorithm for Computing the Longest
  Common Subsequence</title>
    <summary>  In this paper, we revisit the much studied LCS problem for two given
sequences. Based on the algorithm of Iliopoulos and Rahman for solving the LCS
problem, we have suggested 3 new improved algorithms. We first reformulate the
problem in a very succinct form. The problem LCS is abstracted to an abstract
data type DS on an ordered positive integer set with a special operation
Update(S,x). For the two input sequences X and Y of equal length n, the first
improved algorithm uses a van Emde Boas tree for DS and its time and space
complexities are O(R\log\log n+n) and O(R), where R is the number of matched
pairs of the two input sequences. The second algorithm uses a balanced binary
search tree for DS and its time and space complexities are O(R\log L+n) and
O(R), where L is the length of the longest common subsequence of X and Y. The
third algorithm uses an ordered vector for DS and its time and space
complexities are O(nL) and O(R).
</summary>
    <author>
      <name>Daxin Zhu</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Yingjie Wu</name>
    </author>
    <author>
      <name>Xiaodong Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1508.05553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1507.06953">
    <id>http://arxiv.org/abs/1507.06953v1</id>
    <updated>2015-07-24T18:43:59Z</updated>
    <published>2015-07-24T18:43:59Z</published>
    <title>Pattern-avoiding access in binary search trees</title>
    <summary>  The dynamic optimality conjecture is perhaps the most fundamental open
question about binary search trees (BST). It postulates the existence of an
asymptotically optimal online BST, i.e. one that is constant factor competitive
with any BST on any input access sequence. The two main candidates for dynamic
optimality in the literature are splay trees [Sleator and Tarjan, 1985], and
Greedy [Lucas, 1988; Munro, 2000; Demaine et al. 2009] [..]
  Dynamic optimality is trivial for almost all sequences: the optimum access
cost of most length-n sequences is Theta(n log n), achievable by any balanced
BST. Thus, the obvious missing step towards the conjecture is an understanding
of the "easy" access sequences. [..] The difficulty of proving dynamic
optimality is witnessed by highly restricted special cases that remain
unresolved; one prominent example is the traversal conjecture [Sleator and
Tarjan, 1985], which states that preorder sequences (whose optimum is linear)
are linear-time accessed by splay trees; no online BST is known to satisfy this
conjecture.
  In this paper, we prove two different relaxations of the traversal conjecture
for Greedy: (i) Greedy is almost linear for preorder traversal, (ii) if a
linear-time preprocessing is allowed, Greedy is in fact linear. These
statements are corollaries of our more general results that express the
complexity of access sequences in terms of a pattern avoidance parameter k.
[..] To our knowledge, these are the first upper bounds for Greedy that are not
known to hold for any other online BST. To obtain these results we identify an
input-revealing property of Greedy. Informally, this means that the execution
log partially reveals the structure of the access sequence. This property
facilitates the use of rich technical tools from forbidden submatrix theory.
  [Abridged]
</summary>
    <author>
      <name>Parinya Chalermsook</name>
    </author>
    <author>
      <name>Mayank Goswami</name>
    </author>
    <author>
      <name>Laszlo Kozma</name>
    </author>
    <author>
      <name>Kurt Mehlhorn</name>
    </author>
    <author>
      <name>Thatchaphol Saranurak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at FOCS 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.06953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.06953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1507.02989">
    <id>http://arxiv.org/abs/1507.02989v1</id>
    <updated>2015-07-10T19:03:00Z</updated>
    <published>2015-07-10T19:03:00Z</published>
    <title>A Bloom filter based semi-index on $q$-grams</title>
    <summary>  We present a simple $q$-gram based semi-index, which allows to look for a
pattern typically only in a small fraction of text blocks. Several space-time
tradeoffs are presented. Experiments on Pizza &amp; Chili datasets show that our
solution is up to three orders of magnitude faster than the Claude et al.
\cite{CNPSTjda10} semi-index at a comparable space usage.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Robert Susik</name>
    </author>
    <author>
      <name>Marcin Raniszewski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/spe.2431</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/spe.2431" rel="related"/>
    <link href="http://arxiv.org/abs/1507.02989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1512.07494">
    <id>http://arxiv.org/abs/1512.07494v1</id>
    <updated>2015-12-23T14:33:47Z</updated>
    <published>2015-12-23T14:33:47Z</published>
    <title>A Quadratic Assignment Formulation of the Graph Edit Distance</title>
    <summary>  Computing efficiently a robust measure of similarity or dissimilarity between
graphs is a major challenge in Pattern Recognition. The Graph Edit Distance
(GED) is a flexible measure of dissimilarity between graphs which arises in
error-tolerant graph matching. It is defined from an optimal sequence of edit
operations (edit path) transforming one graph into an other. Unfortunately, the
exact computation of this measure is NP-hard. In the last decade, several
approaches have been proposed to approximate the GED in polynomial time, mainly
by solving linear programming problems. Among them, the bipartite GED has
received much attention. It is deduced from a linear sum assignment of the
nodes of the two graphs, which can be efficiently computed by Hungarian-type
algorithms. However, edit operations on nodes and edges are not handled
simultaneously, which limits the accuracy of the approximation. To overcome
this limitation, we propose to extend the linear assignment model to a
quadratic one, for directed or undirected graphs having labelized nodes and
edges. This is realized through the definition of a family of edit paths
induced by assignments between nodes. We formally show that the GED, restricted
to the paths in this family, is equivalent to a quadratic assignment problem.
Since this problem is NP-hard, we propose to compute an approximate solution by
an adaptation of the Integer Projected Fixed Point method. Experiments show
that the proposed approach is generally able to reach a more accurate
approximation of the optimal GED than the bipartite GED, with a computational
cost that is still affordable for graphs of non trivial sizes.
</summary>
    <author>
      <name>Sébastien Bougleux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Luc Brun</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Vincenzo Carletti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Pasquale Foggia</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Benoit Gaüzère</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Mario Vento</name>
    </author>
    <link href="http://arxiv.org/abs/1512.07494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.07494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1506.08620">
    <id>http://arxiv.org/abs/1506.08620v3</id>
    <updated>2017-09-26T06:10:03Z</updated>
    <published>2015-06-29T13:44:21Z</published>
    <title>Fast and Vectorizable Alternative to Binary Search in O(1) Applicable to
  a Wide Domain of Sorted Arrays of Floating Point Numbers</title>
    <summary>  Given an array $X$ of $N+1$ strictly ordered floating point numbers and a
floating point number $z$ in the interval $[X_0,X_N)$, a common problem is to
find the index $i$ of the interval $[X_{i},X_{i+1})$ containing $z$. This
problem arises for instance in the context of spline interpolation or the
computation of empirical probability distribution from empirical data. Often it
needs to be solved for a large number of different values $z$ and the same
array $X$, which makes it worth investing resources upfront in pre-processing
the array $X$ with the goal of speeding up subsequent search operations. In
some cases the values $z$ to be processed are known simultaneously in blocks of
size $M$, which offers the opportunity to solve the problem vectorially,
exploiting the parallel capabilities of modern CPUs. The common solution is to
sequentially invoke $M$ times the well known binary search algorithm, which has
complexity $O(log_2 N)$ per individual search and, in its classic formulation,
is not vectorizable, i.e. is not SIMD friendly. This paper describes technical
improvements to the binary search algorithm, which make it faster and
vectorizable. Next it proposes a new vectorizable algorithm, based on an
indexing technique, applicable to a wide family of $X$ partitions, which solves
the problem with complexity $O(1)$ per individual search at the cost of
introducing an initial overhead to compute the index and requiring extra memory
for its storage. Test results using streaming SIMD extensions compare the
performance of the algorithm versus various benchmarks and demonstrate its
effectiveness. Depending on the test case, the algorithm can produce a
throughput up to two orders of magnitude larger than the classic binary search.
Applicability limitations and cache-friendliness related aspects are also
discussed.
</summary>
    <author>
      <name>Fabio Cannizzo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jpdc.2017.10.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jpdc.2017.10.007" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Parallel and Distributed Computing, Volume 113, March
  2018, Pages 37-5</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.08620v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08620v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1510.04676">
    <id>http://arxiv.org/abs/1510.04676v3</id>
    <updated>2016-05-31T13:15:53Z</updated>
    <published>2015-10-15T19:30:29Z</published>
    <title>How Good is Multi-Pivot Quicksort?</title>
    <summary>  Multi-Pivot Quicksort refers to variants of classical quicksort where in the
partitioning step $k$ pivots are used to split the input into $k + 1$ segments.
For many years, multi-pivot quicksort was regarded as impractical, but in 2009
a 2-pivot approach by Yaroslavskiy, Bentley, and Bloch was chosen as the
standard sorting algorithm in Sun's Java 7. In 2014 at ALENEX, Kushagra et al.
introduced an even faster algorithm that uses three pivots. This paper studies
what possible advantages multi-pivot quicksort might offer in general. The
contributions are as follows: Natural comparison-optimal algorithms for
multi-pivot quicksort are devised and analyzed. The analysis shows that the
benefits of using multiple pivots with respect to the average comparison count
are marginal and these strategies are inferior to simpler strategies such as
the well known median-of-$k$ approach. A substantial part of the partitioning
cost is caused by rearranging elements. A rigorous analysis of an algorithm for
rearranging elements in the partitioning step is carried out, observing mainly
how often array cells are accessed during partitioning. The algorithm behaves
best if 3 to 5 pivots are used. Experiments show that this translates into good
cache behavior and is closest to predicting observed running times of
multi-pivot quicksort algorithms. Finally, it is studied how choosing pivots
from a sample affects sorting cost. The study is theoretical in the sense that
although the findings motivate design recommendations for multipivot quicksort
algorithms that lead to running time improvements over known algorithms in an
experimental setting, these improvements are small.
</summary>
    <author>
      <name>Martin Aumüller</name>
    </author>
    <author>
      <name>Martin Dietzfelbinger</name>
    </author>
    <author>
      <name>Pascal Klaue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to a journal, v2: Fixed statement of Gibb's inequality, v3:
  Revised version, especially improving on the experiments in Section 9</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04676v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04676v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1510.02637">
    <id>http://arxiv.org/abs/1510.02637v1</id>
    <updated>2015-10-09T11:28:49Z</updated>
    <published>2015-10-09T11:28:49Z</published>
    <title>Efficient Ranking of Lyndon Words and Decoding Lexicographically Minimal
  de Bruijn Sequence</title>
    <summary>  We give efficient algorithms for ranking Lyndon words of length n over an
alphabet of size {\sigma}. The rank of a Lyndon word is its position in the
sequence of lexicographically ordered Lyndon words of the same length. The
outputs are integers of exponential size, and complexity of arithmetic
operations on such large integers cannot be ignored. Our model of computations
is the word-RAM, in which basic arithmetic operations on (large) numbers of
size at most {\sigma}^n take O(n) time. Our algorithm for ranking Lyndon words
makes O(n^2) arithmetic operations (this would imply directly cubic time on
word-RAM). However, using an algebraic approach we are able to reduce the total
time complexity on the word-RAM to O(n^2 log {\sigma}). We also present an
O(n^3 log^2 {\sigma})-time algorithm that generates the Lyndon word of a given
length and rank in lexicographic order. Finally we use the connections between
Lyndon words and lexicographically minimal de Bruijn sequences (theorem of
Fredricksen and Maiorana) to develop the first polynomial-time algorithm for
decoding minimal de Bruijn sequence of any rank n (it determines the position
of an arbitrary word of length n within the de Bruijn sequence).
</summary>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Improved version of a paper presented at CPM 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.02637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1510.03367">
    <id>http://arxiv.org/abs/1510.03367v1</id>
    <updated>2015-10-12T17:03:33Z</updated>
    <published>2015-10-12T17:03:33Z</published>
    <title>Layered Heaps Beating Standard and Fibonacci Heaps in Practice</title>
    <summary>  We consider the classic problem of designing heaps. Standard binary heaps run
faster in practice than Fibonacci heaps but have worse time guarantees. Here we
present a new type of heap, a layered heap, that runs faster in practice than
both standard binary and Fibonacci heaps, but has asymptotic insert times
better than that of binary heaps. Our heap is defined recursively and maximum
run time speed up occurs when a recursion depth of 1 is used, i.e. a heap of
heaps.
</summary>
    <author>
      <name>Peter Huggins</name>
    </author>
    <link href="http://arxiv.org/abs/1510.03367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.03367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1510.00634">
    <id>http://arxiv.org/abs/1510.00634v1</id>
    <updated>2015-10-02T16:17:11Z</updated>
    <published>2015-10-02T16:17:11Z</published>
    <title>A Note on Easy and Efficient Computation of Full Abelian Periods of a
  Word</title>
    <summary>  Constantinescu and Ilie (Bulletin of the EATCS 89, 167-170, 2006) introduced
the idea of an Abelian period with head and tail of a finite word. An Abelian
period is called full if both the head and the tail are empty. We present a
simple and easy-to-implement $O(n\log\log n)$-time algorithm for computing all
the full Abelian periods of a word of length $n$ over a constant-size alphabet.
Experiments show that our algorithm significantly outperforms the $O(n)$
algorithm proposed by Kociumaka et al. (Proc. of STACS, 245-256, 2013) for the
same problem.
</summary>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Thierry Lecroq</name>
    </author>
    <author>
      <name>Arnaud Lefebvre</name>
    </author>
    <author>
      <name>Élise Prieur-Gaston</name>
    </author>
    <author>
      <name>William F. Smyth</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.dam.2015.09.024</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.dam.2015.09.024" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Discrete Applied Mathematics</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Discrete Applied Mathematics, 212: 88-95, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1510.00634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1509.09228">
    <id>http://arxiv.org/abs/1509.09228v1</id>
    <updated>2015-09-30T15:42:42Z</updated>
    <published>2015-09-30T15:42:42Z</published>
    <title>Fast Algorithms for Exact String Matching</title>
    <summary>  Given a pattern string $P$ of length $n$ and a query string $T$ of length
$m$, where the characters of $P$ and $T$ are drawn from an alphabet of size
$\Delta$, the {\em exact string matching} problem consists of finding all
occurrences of $P$ in $T$. For this problem, we present algorithms that in
$O(n\Delta^2)$ time pre-process $P$ to essentially identify $sparse(P)$, a
rarely occurring substring of $P$, and then use it to find occurrences of $P$
in $T$ efficiently. Our algorithms require a worst case search time of $O(m)$,
and expected search time of $O(m/min(|sparse(P)|, \Delta))$, where
$|sparse(P)|$ is at least $\delta$ (i.e. the number of distinct characters in
$P$), and for most pattern strings it is observed to be $\Omega(n^{1/2})$.
</summary>
    <author>
      <name>Srikrishnan Divakaran</name>
    </author>
    <link href="http://arxiv.org/abs/1509.09228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.09228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1509.08608">
    <id>http://arxiv.org/abs/1509.08608v1</id>
    <updated>2015-09-29T06:49:32Z</updated>
    <published>2015-09-29T06:49:32Z</published>
    <title>Probabilistic Threshold Indexing for Uncertain Strings</title>
    <summary>  Strings form a fundamental data type in computer systems. String searching
has been extensively studied since the inception of computer science.
Increasingly many applications have to deal with imprecise strings or strings
with fuzzy information in them. String matching becomes a probabilistic event
when a string contains uncertainty, i.e. each position of the string can have
different probable characters with associated probability of occurrence for
each character. Such uncertain strings are prevalent in various applications
such as biological sequence data, event monitoring and automatic ECG
annotations. We explore the problem of indexing uncertain strings to support
efficient string searching. In this paper we consider two basic problems of
string searching, namely substring searching and string listing. In substring
searching, the task is to find the occurrences of a deterministic string in an
uncertain string. We formulate the string listing problem for uncertain
strings, where the objective is to output all the strings from a collection of
strings, that contain probable occurrence of a deterministic query string.
Indexing solution for both these problems are significantly more challenging
for uncertain strings than for deterministic strings. Given a construction time
probability value $\tau$, our indexes can be constructed in linear space and
supports queries in near optimal time for arbitrary values of probability
threshold parameter greater than $\tau$. To the best of our knowledge, this is
the first indexing solution for searching in uncertain strings that achieves
strong theoretical bound and supports arbitrary values of probability threshold
parameter. We also propose an approximate substring search index that can
answer substring search queries with an additive error in optimal time. We
conduct experiments to evaluate the performance of our indexes.
</summary>
    <author>
      <name>Sharma V. Thankachan</name>
    </author>
    <author>
      <name>Manish Patil</name>
    </author>
    <author>
      <name>Rahul Shah</name>
    </author>
    <author>
      <name>Sudip Biswas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.08608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.08608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1509.08216">
    <id>http://arxiv.org/abs/1509.08216v4</id>
    <updated>2017-03-17T02:21:24Z</updated>
    <published>2015-09-28T07:08:27Z</published>
    <title>Fast Algorithms for Finding Pattern Avoiders and Counting Pattern
  Occurrences in Permutations</title>
    <summary>  Given a set $\Pi$ of permutation patterns of length at most $k$, we present
an algorithm for building $S_{\le n}(\Pi)$, the set of permutations of length
at most $n$ avoiding the patterns in $\Pi$, in time $O(|S_{\le n - 1}(\Pi)|
\cdot k + |S_{n}(\Pi)|)$. Additionally, we present an $O(n!k)$-time algorithm
for counting the number of copies of patterns from $\Pi$ in each permutation in
$S_n$. Surprisingly, when $|\Pi| = 1$, this runtime can be improved to $O(n!)$,
spending only constant time per permutation. Whereas the previous best
algorithms, based on generate-and-check, take exponential time per permutation
analyzed, all of our algorithms take time at most polynomial per outputted
permutation.
  If we want to solve only the enumerative variant of each problem, computing
$|S_{\le n}(\Pi)|$ or tallying permutations according to $\Pi$-patterns, rather
than to store information about every permutation, then all of our algorithms
can be implemented in $O(n^{k+1}k)$ space.
  Using our algorithms, we generated $|S_5(\Pi)|, \ldots, |S_{16}(\Pi)|$ for
each $\Pi \subseteq S_4$ with $|\Pi| > 4$, and analyzed OEIS matches. We
obtained a number of potentially novel pattern-avoidance conjectures.
  Our algorithms extend to considering permutations in any set closed under
standardization of subsequences. Our algorithms also partially adapt to
considering vincular patterns.
</summary>
    <author>
      <name>William Kuszmaul</name>
    </author>
    <link href="http://arxiv.org/abs/1509.08216v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.08216v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1509.08240">
    <id>http://arxiv.org/abs/1509.08240v1</id>
    <updated>2015-09-28T09:02:54Z</updated>
    <published>2015-09-28T09:02:54Z</published>
    <title>External Memory Three-Sided Range Reporting and Top-$k$ Queries with
  Sublogarithmic Updates</title>
    <summary>  An external memory data structure is presented for maintaining a dynamic set
of $N$ two-dimensional points under the insertion and deletion of points, and
supporting 3-sided range reporting queries and top-$k$ queries, where top-$k$
queries report the $k$~points with highest $y$-value within a given $x$-range.
For any constant $0&lt;\varepsilon\leq \frac{1}{2}$, a data structure is
constructed that supports updates in amortized $O(\frac{1}{\varepsilon
B^{1-\varepsilon}}\log_B N)$ IOs and queries in amortized
$O(\frac{1}{\varepsilon}\log_B N+K/B)$ IOs, where $B$ is the external memory
block size, and $K$ is the size of the output to the query (for top-$k$ queries
$K$ is the minimum of $k$ and the number of points in the query interval). The
data structure uses linear space. The update bound is a significant factor
$B^{1-\varepsilon}$ improvement over the previous best update bounds for the
two query problems, while staying within the same query and space bounds.
</summary>
    <author>
      <name>Gerth Stølting Brodal</name>
    </author>
    <link href="http://arxiv.org/abs/1509.08240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.08240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1509.07053">
    <id>http://arxiv.org/abs/1509.07053v1</id>
    <updated>2015-09-23T16:35:38Z</updated>
    <published>2015-09-23T16:35:38Z</published>
    <title>Practical Concurrent Priority Queues</title>
    <summary>  Priority queues are abstract data structures which store a set of key/value
pairs and allow efficient access to the item with the minimal (maximal) key.
Such queues are an important element in various areas of computer science such
as algorithmics (i.e. Dijkstra's shortest path algorithm) and operating system
(i.e. priority schedulers).
  The recent trend towards multiprocessor computing requires new
implementations of basic data structures which are able to be used concurrently
and scale well to a large number of threads. In particular, lock-free
structures promise superior scalability by avoiding the use of blocking
synchronization primitives.
  Concurrent priority queues have been extensively researched over the past
decades. In this paper, we discuss three major ideas within the field:
fine-grained locking employs multiple locks to avoid a single bottleneck within
the queue; SkipLists are search structures which use randomization and
therefore do not require elaborate reorganization schemes; and relaxed data
structures trade semantic guarantees for improved scalability.
</summary>
    <author>
      <name>Jakob Gruber</name>
    </author>
    <link href="http://arxiv.org/abs/1509.07053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1509.06948">
    <id>http://arxiv.org/abs/1509.06948v1</id>
    <updated>2015-09-23T12:53:59Z</updated>
    <published>2015-09-23T12:53:59Z</published>
    <title>Dynamic concurrent van Emde Boas array</title>
    <summary>  The growing popularity of shared-memory multiprocessor machines has caused
significant changes in the design of concurrent software. In this approach, the
concurrently running threads communicate and synchronize with each other
through data structures in shared memory. Hence, the efficiency of these
structures is essential for the performance of concurrent applications. The
need to find new concurrent data structures prompted the author some time ago
to propose the cvEB array modeled on the van Emde Boas Tree structure as a
dynamic set alternative. This paper describes an improved version of that
structure - the dcvEB array (Dynamic Concurrent van Emde Boas Array). One of
the improvements involves memory usage optimization. This enhancement required
the design of a tree which grows and shrinks at both: the top (root) and the
bottom (leaves) level. Another enhancement concerns the successor (and
predecessor) search strategy. The tests performed seem to confirm the high
performance of the dcvEB array. They are especially visible when the range of
keys is significantly larger than the number of elements in the collection.
</summary>
    <author>
      <name>Konrad Kułakowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.06948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1401.5143">
    <id>http://arxiv.org/abs/1401.5143v2</id>
    <updated>2014-01-29T06:37:29Z</updated>
    <published>2014-01-21T01:41:17Z</published>
    <title>Fully Online Grammar Compression in Constant Space</title>
    <summary>  We present novel variants of fully online LCA (FOLCA), a fully online grammar
compression that builds a straight line program (SLP) and directly encodes it
into a succinct representation in an online manner. FOLCA enables a direct
encoding of an SLP into a succinct representation that is asymptotically
equivalent to an information theoretic lower bound for representing an SLP
(Maruyama et al., SPIRE'13). The compression of FOLCA takes linear time
proportional to the length of an input text and its working space depends only
on the size of the SLP, which enables us to apply FOLCA to large-scale
repetitive texts. Recent repetitive texts, however, include some noise. For
example, current sequencing technology has significant error rates, which
embeds noise into genome sequences. For such noisy repetitive texts, FOLCA
working in the SLP size consumes a large amount of memory. We present two
variants of FOLCA working in constant space by leveraging the idea behind
stream mining techniques. Experiments using 100 human genomes corresponding to
about 300GB from the 1000 human genomes project revealed the applicability of
our method to large-scale, noisy repetitive texts.
</summary>
    <author>
      <name>Shirou Maruyama</name>
    </author>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extended version of a proceeding accepted to Data
  Compression Conference (DCC), 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.5143v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.5143v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1401.2065">
    <id>http://arxiv.org/abs/1401.2065v4</id>
    <updated>2014-06-29T14:18:47Z</updated>
    <published>2014-01-09T16:38:21Z</published>
    <title>Binary Jumbled Pattern Matching via All-Pairs Shortest Paths</title>
    <summary>  In binary jumbled pattern matching we wish to preprocess a binary string $S$
in order to answer queries $(i,j)$ which ask for a substring of $S$ that is of
size $i$ and has exactly $j$ 1-bits. The problem naturally generalizes to
node-labeled trees and graphs by replacing "substring" with "connected
subgraph".
  In this paper, we give an ${n^2}/{2^{\Omega(\log n/\log \log n)^{1/2}}}$ time
solution for both strings and trees. This odd-looking time complexity improves
the state of the art $O(n^2/\log^2 n)$ solutions by more than any
poly-logarithmic factor. It originates from the recent seminal algorithm of
Williams for min-plus matrix multiplication. We obtain the result by giving a
black box reduction from trees to strings. This is then combined with a
reduction from strings to min-plus matrix multiplications.
</summary>
    <author>
      <name>Danny Hermelin</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Yuri Rabinovich</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <link href="http://arxiv.org/abs/1401.2065v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2065v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1512.03512">
    <id>http://arxiv.org/abs/1512.03512v1</id>
    <updated>2015-12-11T03:39:45Z</updated>
    <published>2015-12-11T03:39:45Z</published>
    <title>A Fast Heuristic for Exact String Matching</title>
    <summary>  Given a pattern string $P$ of length $n$ consisting of $\delta$ distinct
characters and a query string $T$ of length $m$, where the characters of $P$
and $T$ are drawn from an alphabet $\Sigma$ of size $\Delta$, the {\em exact
string matching} problem consists of finding all occurrences of $P$ in $T$. For
this problem, we present a randomized heuristic that in $O(n\delta)$ time
preprocesses $P$ to identify $sparse(P)$, a rarely occurring substring of $P$,
and then use it to find all occurrences of $P$ in $T$ efficiently. This
heuristic has an expected search time of $O( \frac{m}{min(|sparse(P)|,
\Delta)})$, where $|sparse(P)|$ is at least $\delta$. We also show that for a
pattern string $P$ whose characters are chosen uniformly at random from an
alphabet of size $\Delta$, $E[|sparse(P)|]$ is $\Omega(\Delta log
(\frac{2\Delta}{2\Delta-\delta}))$.
</summary>
    <author>
      <name>Srikrishnan Divakaran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1509.09228</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.03512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1601.04248">
    <id>http://arxiv.org/abs/1601.04248v1</id>
    <updated>2015-11-27T12:37:22Z</updated>
    <published>2015-11-27T12:37:22Z</published>
    <title>Word Existence Algorithm</title>
    <summary>  The current scenario in the field of computing is largely affected by the
speed at which data can be accessed and recalled. In this paper, we present the
word existence algorithm which is used to check if the word given as an input
is part of a particular database or not. We have taken the English language as
an example here. This algorithm tries to solve the problem of lookup by using a
uniformly distributed hash function. We have also addressed the problem of
clustering and collision. A further contribution is that we follow a direct
hashed model where each hash value is linked to another table if the continuity
for the function holds true. The core of the algorithm lies in the data model
being used during preordering. Our focus lies on the formation of a continuity
series and validating the words that exists in the database. This algorithm can
be used in applications where we there is a requirement to search for just the
existence of a word, example Artificial Intelligence responding to input ,look
up for neural networks and dictionary lookups and more. We have observed that
this algorithm provides a faster search time
</summary>
    <author>
      <name>Tejeswini Sundaram</name>
    </author>
    <author>
      <name>Vyom Chabbra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.04248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1511.08431">
    <id>http://arxiv.org/abs/1511.08431v2</id>
    <updated>2015-12-07T14:40:40Z</updated>
    <published>2015-11-26T15:55:50Z</published>
    <title>On the Greedy Algorithm for the Shortest Common Superstring Problem with
  Reversals</title>
    <summary>  We study a variation of the classical Shortest Common Superstring (SCS)
problem in which a shortest superstring of a finite set of strings $S$ is
sought containing as a factor every string of $S$ or its reversal. We call this
problem Shortest Common Superstring with Reversals (SCS-R). This problem has
been introduced by Jiang et al., who designed a greedy-like algorithm with
length approximation ratio $4$. In this paper, we show that a natural
adaptation of the classical greedy algorithm for SCS has (optimal) compression
ratio $\frac12$, i.e., the sum of the overlaps in the output string is at least
half the sum of the overlaps in an optimal solution. We also provide a
linear-time implementation of our algorithm.
</summary>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ipl.2015.11.015</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ipl.2015.11.015" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Information Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.08431v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08431v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1511.03148">
    <id>http://arxiv.org/abs/1511.03148v3</id>
    <updated>2020-04-07T13:37:03Z</updated>
    <published>2015-11-10T15:38:45Z</published>
    <title>A Study on Splay Trees</title>
    <summary>  We study the dynamic optimality conjecture, which predicts that splay trees
are a form of universally efficient binary search tree, for any access
sequence. We reduce this claim to a regular access bound, which seems plausible
and might be easier to prove. This approach may be useful to establish dynamic
optimality.
</summary>
    <author>
      <name>Luís M. S. Russo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2018.12.020</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2018.12.020" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Theor. Comput. Sci. 776: 1-18 (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.03148v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03148v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68P10, 05C05, 94A17, 68Q25, 68P20, 68W27, 68W40, 68Q25" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1511.02141">
    <id>http://arxiv.org/abs/1511.02141v2</id>
    <updated>2015-11-10T05:11:53Z</updated>
    <published>2015-11-06T16:29:47Z</published>
    <title>Traversing Grammar-Compressed Trees with Constant Delay</title>
    <summary>  A grammar-compressed ranked tree is represented with a linear space overhead
so that a single traversal step, i.e., the move to the parent or the i-th
child, can be carried out in constant time. Moreover, we extend our data
structure such that equality of subtrees can be checked in constant time.
</summary>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Sebastian Maneth</name>
    </author>
    <author>
      <name>Carl Philipp Reh</name>
    </author>
    <link href="http://arxiv.org/abs/1511.02141v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.02141v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1510.08663">
    <id>http://arxiv.org/abs/1510.08663v2</id>
    <updated>2016-05-26T01:11:31Z</updated>
    <published>2015-10-29T12:19:42Z</published>
    <title>Permutations sortable by two stacks in series</title>
    <summary>  We address the problem of the number of permutations that can be sorted by
two stacks in series. We do this by first counting all such permutations of
length less than 20 exactly, then using a numerical technique to obtain
nineteen further coefficients approximately. Analysing these coefficients by a
variety of methods we conclude that the OGF behaves as $$S(z) \sim A (1 - \mu
\cdot z)^\gamma,$$ where $\mu =12.45 \pm 0.15,$ $\gamma= 1.5 \pm 0.3,$ and $A
\approx 0.02$.
</summary>
    <author>
      <name>Andrew Elvey Price</name>
    </author>
    <author>
      <name>Anthony J Guttmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 7 figures. Improved analysis, updated references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Applied Mathematics (2017): 81-96</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1510.08663v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.08663v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05Axx" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1510.09037">
    <id>http://arxiv.org/abs/1510.09037v2</id>
    <updated>2015-11-15T09:58:02Z</updated>
    <published>2015-10-30T10:16:44Z</published>
    <title>Multiple sequence alignment for short sequences</title>
    <summary>  Multiple sequence alignment (MSA) has been one of the most important problems
in bioinformatics for more decades and it is still heavily examined by many
mathematicians and biologists. However, mostly because of the practical
motivation of this problem, the research on this topic is focused on aligning
long sequences. It is understandable, since the sequences that need to be
aligned (usually DNA or protein sequences) are generally quite long (e. g., at
least 30-40 characters). Nevertheless, it is a challenging question that
exactly where MSA starts to become a real hard problem (since it is known that
MSA is NP-complete [2]), and the key to answer this question is to examine
short sequences. If the optimal alignment for short sequences could be
determined in polynomial time, then these results may help to develop faster or
more accurate heuristic algorithms for aligning long sequences. In this work,
it is shown that for length-1 sequences using arbitrary metric, as well as for
length-2 sequences using unit metric, the optimum of the MSA problem can be
achieved by the trivial alignment.
</summary>
    <author>
      <name>Kristóf Takács</name>
    </author>
    <link href="http://arxiv.org/abs/1510.09037v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.09037v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1510.06051">
    <id>http://arxiv.org/abs/1510.06051v2</id>
    <updated>2016-12-18T22:43:30Z</updated>
    <published>2015-10-20T20:43:18Z</published>
    <title>The Complexity of Pattern Matching for $321$-Avoiding and Skew-Merged
  Permutations</title>
    <summary>  The Permutation Pattern Matching problem, asking whether a pattern
permutation $\pi$ is contained in a permutation $\tau$, is known to be
NP-complete. In this paper we present two polynomial time algorithms for
special cases. The first algorithm is applicable if both $\pi$ and $\tau$ are
$321$-avoiding; the second is applicable if $\pi$ and $\tau$ are skew-merged.
Both algorithms have a runtime of $O(kn)$, where $k$ is the length of $\pi$ and
$n$ the length of $\tau$.
</summary>
    <author>
      <name>Michael H. Albert</name>
    </author>
    <author>
      <name>Marie-Louise Lackner</name>
    </author>
    <author>
      <name>Martin Lackner</name>
    </author>
    <author>
      <name>Vincent Vatter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.46298/dmtcs.1308</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.46298/dmtcs.1308" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Discrete Mathematics &amp; Theoretical Computer Science, Vol. 18 no.
  2, Permutation Patterns 2015, Permutation Patterns (December 21, 2016)
  dmtcs:2607</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1510.06051v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.06051v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05A05, 68Q25" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1402.2097">
    <id>http://arxiv.org/abs/1402.2097v1</id>
    <updated>2014-02-10T10:54:52Z</updated>
    <published>2014-02-10T10:54:52Z</published>
    <title>Longest Common Subsequence in k-length substrings</title>
    <summary>  In this paper we define a new problem, motivated by computational biology,
$LCSk$ aiming at finding the maximal number of $k$ length $substrings$,
matching in both input strings while preserving their order of appearance. The
traditional LCS definition is a special case of our problem, where $k = 1$. We
provide an algorithm, solving the general case in $O(n^2)$ time, where $n$ is
the length of the input strings, equaling the time required for the special
case of $k=1$. The space requirement of the algorithm is $O(kn)$. %, however,
in order to enable %backtracking of the solution, $O(n^2)$ space is needed.
  We also define a complementary $EDk$ distance measure and show that
$EDk(A,B)$ can be computed in $O(nm)$ time and $O(km)$ space, where $m$, $n$
are the lengths of the input sequences $A$ and $B$ respectively.
</summary>
    <author>
      <name>Gary Benson</name>
    </author>
    <author>
      <name>Avivit Levy</name>
    </author>
    <author>
      <name>Riva Shalom</name>
    </author>
    <link href="http://arxiv.org/abs/1402.2097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1402.2741">
    <id>http://arxiv.org/abs/1402.2741v2</id>
    <updated>2021-10-23T14:50:40Z</updated>
    <published>2014-02-12T05:37:11Z</published>
    <title>Static Level Ancestors in Practice</title>
    <summary>  Given a rooted tree T, the level ancestor problem aims to answer queries of
the form LA(v, d), which identify the level d ancestor of a node v in the tree.
Several algorithms of varied complexity have been proposed for this problem in
the literature, including optimal solutions that preprocess the tree $T$ in
linear bounded time and proceed to answer queries in constant time. Despite its
significance and numerous applications, to date there have been no comparative
studies of the performance of these algorithms and few implementations are
widely available. In our experimental study we implemented and compared several
solutions to the level ancestor problem, including three theoretically optimal
algorithms, and examined their space requirements and time performance in
practice.
</summary>
    <author>
      <name>Matthew Mabrey</name>
    </author>
    <author>
      <name>Thomas Caputi</name>
    </author>
    <author>
      <name>Georgios Papamichail</name>
    </author>
    <author>
      <name>Dimitris Papamichail</name>
    </author>
    <link href="http://arxiv.org/abs/1402.2741v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2741v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1402.2712">
    <id>http://arxiv.org/abs/1402.2712v3</id>
    <updated>2014-04-18T02:29:23Z</updated>
    <published>2014-02-12T01:36:36Z</published>
    <title>Dynamic Partial Sorting</title>
    <summary>  The dynamic partial sorting problem asks for an algorithm that maintains
lists of numbers under the link, cut and change value operations, and queries
the sorted sequence of the $k$ least numbers in one of the lists. We first
solve the problem in $O(k\log (n))$ time for queries and $O(\log (n))$ time for
updates using the tournament tree data structure, where $n$ is the number of
elements in the lists. We then introduce a layered tournament tree data
structure and solve the same problem in $O(\log_\varphi^* (n) k\log (k))$ time
for queries and $O\left(\log (n)\cdot\log^2\log (n)\right)$ for updates, where
$\varphi$ is the golden ratio and $\log_\varphi^*(n)$ is the iterated
logarithmic function with base $\varphi$.
</summary>
    <author>
      <name>Jiamou Liu</name>
    </author>
    <author>
      <name>Kostya Ross</name>
    </author>
    <link href="http://arxiv.org/abs/1402.2712v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2712v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1402.2508">
    <id>http://arxiv.org/abs/1402.2508v1</id>
    <updated>2014-02-11T14:48:27Z</updated>
    <published>2014-02-11T14:48:27Z</published>
    <title>Data Compaction - Compression without Decompression</title>
    <summary>  Data compaction is a new approach for lossless and lossy compression of
read-only array data. The biggest advantage over existing approaches is the
possibility to access compressed data without any decompression. This makes
data compaction most suitable for systems that could currently not apply
compression techniques due to real-time or memory constraints. This is true for
the majority of all computers, i.e. a wide range of embedded systems.
</summary>
    <author>
      <name>Steffen Görzig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.2508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.2; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1402.1811">
    <id>http://arxiv.org/abs/1402.1811v1</id>
    <updated>2014-02-08T02:47:25Z</updated>
    <published>2014-02-08T02:47:25Z</published>
    <title>How Fast Can We Multiply Large Integers on an Actual Computer?</title>
    <summary>  We provide two complexity measures that can be used to measure the running
time of algorithms to compute multiplications of long integers. The random
access machine with unit or logarithmic cost is not adequate for measuring the
complexity of a task like multiplication of long integers. The Turing machine
is more useful here, but fails to take into account the multiplication
instruction for short integers, which is available on physical computing
devices. An interesting outcome is that the proposed refined complexity
measures do not rank the well known multiplication algorithms the same way as
the Turing machine model.
</summary>
    <author>
      <name>Martin Fürer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the proceedings of Latin 2014. Springer LNCS 8392</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.1811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="03D15, 68Q25, 11Y16" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.3; F.2.1; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1401.7416">
    <id>http://arxiv.org/abs/1401.7416v1</id>
    <updated>2014-01-29T05:39:11Z</updated>
    <published>2014-01-29T05:39:11Z</published>
    <title>A Comparative Study on String Matching Algorithm of Biological Sequences</title>
    <summary>  String matching algorithm plays the vital role in the Computational Biology.
The functional and structural relationship of the biological sequence is
determined by similarities on that sequence. For that, the researcher is
supposed to aware of similarities on the biological sequences. Pursuing of
similarity among biological sequences is an important research area of that can
bring insight into the evolutionary and genetic relationships among the genes.
In this paper, we have studied different kinds of string matching algorithms
and observed their time and space complexities. For this study, we have
assessed the performance of algorithms tested with biological sequences.
</summary>
    <author>
      <name>Pandiselvam. P</name>
    </author>
    <author>
      <name>Marimuthu. T</name>
    </author>
    <author>
      <name>Lawrance. R</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Selected For International Conference on Intelligent Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.7416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.7416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1401.7457">
    <id>http://arxiv.org/abs/1401.7457v1</id>
    <updated>2014-01-29T10:06:07Z</updated>
    <published>2014-01-29T10:06:07Z</published>
    <title>GPU-Accelerated BWT Construction for Large Collection of Short Reads</title>
    <summary>  Advances in DNA sequencing technology have stimulated the development of
algorithms and tools for processing very large collections of short strings
(reads). Short-read alignment and assembly are among the most well-studied
problems. Many state-of-the-art aligners, at their core, have used the
Burrows-Wheeler transform (BWT) as a main-memory index of a reference genome
(typical example, NCBI human genome). Recently, BWT has also found its use in
string-graph assembly, for indexing the reads (i.e., raw data from DNA
sequencers). In a typical data set, the volume of reads is tens of times of the
sequenced genome and can be up to 100 Gigabases. Note that a reference genome
is relatively stable and computing the index is not a frequent task. For reads,
the index has to computed from scratch for each given input. The ability of
efficient BWT construction becomes a much bigger concern than before. In this
paper, we present a practical method called CX1 for constructing the BWT of
very large string collections. CX1 is the first tool that can take advantage of
the parallelism given by a graphics processing unit (GPU, a relative cheap
device providing a thousand or more primitive cores), as well as simultaneously
the parallelism from a multi-core CPU and more interestingly, from a cluster of
GPU-enabled nodes. Using CX1, the BWT of a short-read collection of up to 100
Gigabases can be constructed in less than 2 hours using a machine equipped with
a quad-core CPU and a GPU, or in about 43 minutes using a cluster with 4 such
machines (the speedup is almost linear after excluding the first 16 minutes for
loading the reads from the hard disk). The previously fastest tool BRC is
measured to take 12 hours to process 100 Gigabases on one machine; it is
non-trivial how BRC can be parallelized to take advantage a cluster of
machines, let alone GPUs.
</summary>
    <author>
      <name>Chi-Man Liu</name>
    </author>
    <author>
      <name>Ruibang Luo</name>
    </author>
    <author>
      <name>Tak-Wah Lam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.7457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.7457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1401.7110">
    <id>http://arxiv.org/abs/1401.7110v1</id>
    <updated>2014-01-28T08:27:19Z</updated>
    <published>2014-01-28T08:27:19Z</published>
    <title>A Fast String Matching Algorithm Based on Lowlight Characters in the
  Pattern</title>
    <summary>  We put forth a new string matching algorithm which matches the pattern from
neither the left nor the right end, instead a special position. Comparing with
the Knuth-Morris-Pratt algorithm and the Boyer-Moore algorithm, the new
algorithm is more flexible to pick the position for starting comparisons. The
option really brings it a saving in cost.
</summary>
    <author>
      <name>Zhengjun Cao</name>
    </author>
    <author>
      <name>Lihua Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1401.7110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.7110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1401.6346">
    <id>http://arxiv.org/abs/1401.6346v1</id>
    <updated>2014-01-24T14:30:35Z</updated>
    <published>2014-01-24T14:30:35Z</published>
    <title>On Combinatorial Generation of Prefix Normal Words</title>
    <summary>  A prefix normal word is a binary word with the property that no substring has
more 1s than the prefix of the same length. This class of words is important in
the context of binary jumbled pattern matching. In this paper we present an
efficient algorithm for exhaustively listing the prefix normal words with a
fixed length. The algorithm is based on the fact that the language of prefix
normal words is a bubble language, a class of binary languages with the
property that, for any word w in the language, exchanging the first occurrence
of 01 by 10 in w results in another word in the language. We prove that each
prefix normal word is produced in O(n) amortized time, and conjecture, based on
experimental evidence, that the true amortized running time is O(polylog(n)).
</summary>
    <author>
      <name>Péter Burcsi</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <author>
      <name>Frank Ruskey</name>
    </author>
    <author>
      <name>Joe Sawada</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-07566-2_7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-07566-2_7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Combinatorial Pattern Matching 2014, LNCS 8464, 60-69</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.6346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.6346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1401.5383">
    <id>http://arxiv.org/abs/1401.5383v4</id>
    <updated>2014-10-06T12:39:56Z</updated>
    <published>2014-01-21T16:55:02Z</published>
    <title>On the representation of de Bruijn graphs</title>
    <summary>  The de Bruijn graph plays an important role in bioinformatics, especially in
the context of de novo assembly. However, the representation of the de Bruijn
graph in memory is a computational bottleneck for many assemblers. Recent
papers proposed a navigational data structure approach in order to improve
memory usage. We prove several theoretical space lower bounds to show the
limitation of these types of approaches. We further design and implement a
general data structure (DBGFM) and demonstrate its use on a human whole-genome
dataset, achieving space usage of 1.5 GB and a 46% improvement over previous
approaches. As part of DBGFM, we develop the notion of frequency-based
minimizers and show how it can be used to enumerate all maximal simple paths of
the de Bruijn graph using only 43 MB of memory. Finally, we demonstrate that
our approach can be integrated into an existing assembler by modifying the
ABySS software to use DBGFM.
</summary>
    <author>
      <name>Rayan Chikhi</name>
    </author>
    <author>
      <name>Antoine Limasset</name>
    </author>
    <author>
      <name>Shaun Jackman</name>
    </author>
    <author>
      <name>Jared Simpson</name>
    </author>
    <author>
      <name>Paul Medvedev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal version (JCB). A preliminary version of this article was
  published in the proceedings of RECOMB 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.5383v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.5383v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1405.4892">
    <id>http://arxiv.org/abs/1405.4892v2</id>
    <updated>2014-07-11T13:07:58Z</updated>
    <published>2014-05-19T20:54:56Z</published>
    <title>Alternative Algorithms for Lyndon Factorization</title>
    <summary>  We present two variations of Duval's algorithm for computing the Lyndon
factorization of a word. The first algorithm is designed for the case of small
alphabets and is able to skip a significant portion of the characters of the
string, for strings containing runs of the smallest character in the alphabet.
Experimental results show that it is faster than Duval's original algorithm,
more than ten times in the case of long DNA strings. The second algorithm
computes, given a run-length encoded string $R$ of length $\rho$, the Lyndon
factorization of $R$ in $O(\rho)$ time and constant space.
</summary>
    <author>
      <name>Sukhpal Singh Ghuman</name>
    </author>
    <author>
      <name>Emanuele Giaquinta</name>
    </author>
    <author>
      <name>Jorma Tarhio</name>
    </author>
    <link href="http://arxiv.org/abs/1405.4892v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4892v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1405.2846">
    <id>http://arxiv.org/abs/1405.2846v7</id>
    <updated>2014-12-15T15:43:44Z</updated>
    <published>2014-05-12T17:32:27Z</published>
    <title>Introduction to Dynamic Unary Encoding</title>
    <summary>  Dynamic unary encoding takes unary encoding to the next level. Every n-bit
binary string is an encoding of dynamic unary and every n-bit binary string is
encodable by dynamic unary. By utilizing both forms of unary code and a single
bit of parity information dynamic unary encoding partitions 2^n non-negative
integers into n sets of disjoint cycles of n-bit elements. These cycles have
been employed as virtual data sets, binary transforms and as a mathematical
object. Characterization of both the cycles and of the cycle spectrum is given.
Examples of encoding and decoding algorithms are given. Examples of other
constructs utilizing the principles of dynamic unary encoding are presented.
The cycle as a mathematical object is demonstrated.
</summary>
    <author>
      <name>Ernst D. Berg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Seven pages of text, two pages of flow charts and two pages of data.
  Introduces an encoding scheme and a mathematical object</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.2846v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.2846v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1404.5475">
    <id>http://arxiv.org/abs/1404.5475v2</id>
    <updated>2014-11-01T13:29:52Z</updated>
    <published>2014-04-22T12:44:42Z</published>
    <title>Combining pattern-based CRFs and weighted context-free grammars</title>
    <summary>  We consider two models for the sequence labeling (tagging) problem. The first
one is a {\em Pattern-Based Conditional Random Field }(\PB), in which the
energy of a string (chain labeling) $x=x_1\ldots x_n\in D^n$ is a sum of terms
over intervals $[i,j]$ where each term is non-zero only if the substring
$x_i\ldots x_j$ equals a prespecified word $w\in \Lambda$. The second model is
a {\em Weighted Context-Free Grammar }(\WCFG) frequently used for natural
language processing. \PB and \WCFG encode local and non-local interactions
respectively, and thus can be viewed as complementary.
  We propose a {\em Grammatical Pattern-Based CRF model }(\GPB) that combines
the two in a natural way. We argue that it has certain advantages over existing
approaches such as the {\em Hybrid model} of Bened{\'i} and Sanchez that
combines {\em $\mbox{$N$-grams}$} and \WCFGs. The focus of this paper is to
analyze the complexity of inference tasks in a \GPB such as computing MAP. We
present a polynomial-time algorithm for general \GPBs and a faster version for
a special case that we call {\em Interaction Grammars}.
</summary>
    <author>
      <name>Rustem Takhanov</name>
    </author>
    <author>
      <name>Vladimir Kolmogorov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.5475v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5475v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1404.2824">
    <id>http://arxiv.org/abs/1404.2824v1</id>
    <updated>2014-04-01T07:55:57Z</updated>
    <published>2014-04-01T07:55:57Z</published>
    <title>Normal, Abby Normal, Prefix Normal</title>
    <summary>  A prefix normal word is a binary word with the property that no substring has
more 1s than the prefix of the same length. This class of words is important in
the context of binary jumbled pattern matching. In this paper we present
results about the number $pnw(n)$ of prefix normal words of length $n$, showing
that $pnw(n) =\Omega\left(2^{n - c\sqrt{n\ln n}}\right)$ for some $c$ and
$pnw(n) = O \left(\frac{2^n (\ln n)^2}{n}\right)$. We introduce efficient
algorithms for testing the prefix normal property and a "mechanical algorithm"
for computing prefix normal forms. We also include games which can be played
with prefix normal words. In these games Alice wishes to stay normal but Bob
wants to drive her "abnormal" -- we discuss which parameter settings allow
Alice to succeed.
</summary>
    <author>
      <name>Péter Burcsi</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <author>
      <name>Frank Ruskey</name>
    </author>
    <author>
      <name>Joe Sawada</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-07890-8_7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-07890-8_7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at FUN '14</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS 8496, pages 74-88 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.2824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.2824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1403.2777">
    <id>http://arxiv.org/abs/1403.2777v1</id>
    <updated>2014-03-11T23:07:41Z</updated>
    <published>2014-03-11T23:07:41Z</published>
    <title>Zig-zag Sort: A Simple Deterministic Data-Oblivious Sorting Algorithm
  Running in O(n log n) Time</title>
    <summary>  We describe and analyze Zig-zag Sort--a deterministic data-oblivious sorting
algorithm running in O(n log n) time that is arguably simpler than previously
known algorithms with similar properties, which are based on the AKS sorting
network. Because it is data-oblivious and deterministic, Zig-zag Sort can be
implemented as a simple O(n log n)-size sorting network, thereby providing a
solution to an open problem posed by Incerpi and Sedgewick in 1985. In
addition, Zig-zag Sort is a variant of Shellsort, and is, in fact, the first
deterministic Shellsort variant running in O(n log n) time. The existence of
such an algorithm was posed as an open problem by Plaxton et al. in 1992 and
also by Sedgewick in 1996. More relevant for today, however, is the fact that
the existence of a simple data-oblivious deterministic sorting algorithm
running in O(n log n) time simplifies the inner-loop computation in several
proposed oblivious-RAM simulation methods (which utilize AKS sorting networks),
and this, in turn, implies simplified mechanisms for privacy-preserving data
outsourcing in several cloud computing applications. We provide both
constructive and non-constructive implementations of Zig-zag Sort, based on the
existence of a circuit known as an epsilon-halver, such that the constant
factors in our constructive implementations are orders of magnitude smaller
than those for constructive variants of the AKS sorting network, which are also
based on the use of epsilon-halvers.
</summary>
    <author>
      <name>Michael T. Goodrich</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2591796.2591830</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2591796.2591830" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appearing in ACM Symp. on Theory of Computing (STOC) 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.2777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.2777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1403.2439">
    <id>http://arxiv.org/abs/1403.2439v1</id>
    <updated>2014-03-10T23:55:53Z</updated>
    <published>2014-03-10T23:55:53Z</published>
    <title>String Reconstruction from Substring Compositions</title>
    <summary>  Motivated by mass-spectrometry protein sequencing, we consider a
simply-stated problem of reconstructing a string from the multiset of its
substring compositions. We show that all strings of length 7, one less than a
prime, or one less than twice a prime, can be reconstructed uniquely up to
reversal. For all other lengths we show that reconstruction is not always
possible and provide sometimes-tight bounds on the largest number of strings
with given substring compositions. The lower bounds are derived by
combinatorial arguments and the upper bounds by algebraic considerations that
precisely characterize the set of strings with the same substring compositions
in terms of the factorization of bivariate polynomials. The problem can be
viewed as a combinatorial simplification of the turnpike problem, and its
solution may shed light on this long-standing problem as well. Using well known
results on transience of multi-dimensional random walks, we also provide a
reconstruction algorithm that reconstructs random strings over alphabets of
size $\ge4$ in optimal near-quadratic time.
</summary>
    <author>
      <name>Jayadev Acharya</name>
    </author>
    <author>
      <name>Hirakendu Das</name>
    </author>
    <author>
      <name>Olgica Milenkovic</name>
    </author>
    <author>
      <name>Alon Orlitsky</name>
    </author>
    <author>
      <name>Shengjun Pan</name>
    </author>
    <link href="http://arxiv.org/abs/1403.2439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.2439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1403.2056">
    <id>http://arxiv.org/abs/1403.2056v1</id>
    <updated>2014-03-09T13:43:32Z</updated>
    <published>2014-03-09T13:43:32Z</published>
    <title>Engineering Parallel String Sorting</title>
    <summary>  We discuss how string sorting algorithms can be parallelized on modern
multi-core shared memory machines. As a synthesis of the best sequential string
sorting algorithms and successful parallel sorting algorithms for atomic
objects, we first propose string sample sort. The algorithm makes effective use
of the memory hierarchy, uses additional word level parallelism, and largely
avoids branch mispredictions. Then we focus on NUMA architectures, and develop
parallel multiway LCP-merge and -mergesort to reduce the number of random
memory accesses to remote nodes. Additionally, we parallelize variants of
multikey quicksort and radix sort that are also useful in certain situations.
Comprehensive experiments on five current multi-core platforms are then
reported and discussed. The experiments show that our implementations scale
very well on real-world inputs and modern machines.
</summary>
    <author>
      <name>Timo Bingmann</name>
    </author>
    <author>
      <name>Andreas Eberle</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages, extension of "Parallel String Sample Sort" arXiv:1305.1157</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.2056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.2056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; E.5; D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1403.0457">
    <id>http://arxiv.org/abs/1403.0457v3</id>
    <updated>2014-07-14T15:48:36Z</updated>
    <published>2014-03-03T15:01:23Z</published>
    <title>Efficient Representation for Online Suffix Tree Construction</title>
    <summary>  Suffix tree construction algorithms based on suffix links are popular because
they are simple to implement, can operate online in linear time, and because
the suffix links are often convenient for pattern matching. We present an
approach using edge-oriented suffix links, which reduces the number of branch
lookup operations (known to be a bottleneck in construction time) with some
additional techniques to reduce construction cost. We discuss various effects
of our approach and compare it to previous techniques. An experimental
evaluation shows that we are able to reduce construction time to around half
that of the original algorithm, and about two thirds that of previously known
branch-reduced construction.
</summary>
    <author>
      <name>N. Jesper Larsson</name>
    </author>
    <author>
      <name>Kasper Fuglsang</name>
    </author>
    <author>
      <name>Kenneth Karlsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at SEA 2014, Copenhagen</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.0457v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.0457v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1403.0800">
    <id>http://arxiv.org/abs/1403.0800v3</id>
    <updated>2014-07-14T15:35:25Z</updated>
    <published>2014-03-04T14:32:23Z</published>
    <title>Most Recent Match Queries in On-Line Suffix Trees (with appendix)</title>
    <summary>  A suffix tree is able to efficiently locate a pattern in an indexed string,
but not in general the most recent copy of the pattern in an online stream,
which is desirable in some applications. We study the most general version of
the problem of locating a most recent match: supporting queries for arbitrary
patterns, at each step of processing an online stream. We present augmentations
to Ukkonen's suffix tree construction algorithm for optimal-time queries,
maintaining indexing time within a logarithmic factor in the size of the
indexed string. We show that the algorithm is applicable to sliding-window
indexing, and sketch a possible optimization for use in the special case of
Lempel-Ziv compression.
</summary>
    <author>
      <name>N. Jesper Larsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Result presented at CPM 2014, Moscow</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.0800v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.0800v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1402.1936">
    <id>http://arxiv.org/abs/1402.1936v1</id>
    <updated>2014-02-09T11:28:41Z</updated>
    <published>2014-02-09T11:28:41Z</published>
    <title>Integer Set Compression and Statistical Modeling</title>
    <summary>  Compression of integer sets and sequences has been extensively studied for
settings where elements follow a uniform probability distribution. In addition,
methods exist that exploit clustering of elements in order to achieve higher
compression performance. In this work, we address the case where enumeration of
elements may be arbitrary or random, but where statistics is kept in order to
estimate probabilities of elements. We present a recursive subset-size encoding
method that is able to benefit from statistics, explore the effects of
permuting the enumeration order based on element probabilities, and discuss
general properties and possibilities for this class of compression problem.
</summary>
    <author>
      <name>N. Jesper Larsson</name>
    </author>
    <link href="http://arxiv.org/abs/1402.1936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1406.1077">
    <id>http://arxiv.org/abs/1406.1077v1</id>
    <updated>2014-06-03T19:13:17Z</updated>
    <published>2014-06-03T19:13:17Z</published>
    <title>How inefficient can a sort algorithm be?</title>
    <summary>  We find large lower bounds for a certain family of algorithms, and prove that
such bounds are limited only by natural computability arguments.
</summary>
    <author>
      <name>Miguel A. Lerma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.1077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.1077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="03D99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1406.0426">
    <id>http://arxiv.org/abs/1406.0426v1</id>
    <updated>2014-06-02T16:06:04Z</updated>
    <published>2014-06-02T16:06:04Z</published>
    <title>Fast construction of FM-index for long sequence reads</title>
    <summary>  Summary: We present a new method to incrementally construct the FM-index for
both short and long sequence reads, up to the size of a genome. It is the first
algorithm that can build the index while implicitly sorting the sequences in
the reverse (complement) lexicographical order without a separate sorting step.
The implementation is among the fastest for indexing short reads and the only
one that practically works for reads of averaged kilobases in length.
  Availability and implementation: https://github.com/lh3/ropebwt2
  Contact: hengli@broadinstitute.org
</summary>
    <author>
      <name>Heng Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/bioinformatics/btu541</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/bioinformatics/btu541" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.0426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1405.7520">
    <id>http://arxiv.org/abs/1405.7520v2</id>
    <updated>2015-06-11T15:08:26Z</updated>
    <published>2014-05-29T11:09:55Z</published>
    <title>An External-Memory Algorithm for String Graph Construction</title>
    <summary>  Some recent results have introduced external-memory algorithms to compute
self-indexes of a set of strings, mainly via computing the Burrows-Wheeler
Transform (BWT) of the input strings. The motivations for those results stem
from Bioinformatics, where a large number of short strings (called reads) are
routinely produced and analyzed. In that field, a fundamental problem is to
assemble a genome from a large set of much shorter samples extracted from the
unknown genome. The approaches that are currently used to tackle this problem
are memory-intensive. This fact does not bode well with the ongoing increase in
the availability of genomic data. A data structure that is used in genome
assembly is the string graph, where vertices correspond to samples and arcs
represent two overlapping samples. In this paper we address an open problem: to
design an external-memory algorithm to compute the string graph.
</summary>
    <author>
      <name>Paola Bonizzoni</name>
    </author>
    <author>
      <name>Gianluca Della Vedova</name>
    </author>
    <author>
      <name>Yuri Pirola</name>
    </author>
    <author>
      <name>Marco Previtali</name>
    </author>
    <author>
      <name>Raffaella Rizzi</name>
    </author>
    <link href="http://arxiv.org/abs/1405.7520v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7520v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1405.6874">
    <id>http://arxiv.org/abs/1405.6874v2</id>
    <updated>2014-09-18T17:41:36Z</updated>
    <published>2014-05-27T11:34:35Z</published>
    <title>Disk-based genome sequencing data compression</title>
    <summary>  Motivation: High-coverage sequencing data have significant, yet hard to
exploit, redundancy. Most FASTQ compressors cannot efficiently compress the DNA
stream of large datasets, since the redundancy between overlapping reads cannot
be easily captured in the (relatively small) main memory. More interesting
solutions for this problem are disk-based~(Yanovsky, 2011; Cox et al., 2012),
where the better of these two, from Cox~{\it et al.}~(2012), is based on the
Burrows--Wheeler transform (BWT) and achieves 0.518 bits per base for a 134.0
Gb human genome sequencing collection with almost 45-fold coverage.
  Results: We propose ORCOM (Overlapping Reads COmpression with Minimizers), a
compression algorithm dedicated to sequencing reads (DNA only). Our method
makes use of a conceptually simple and easily parallelizable idea of
minimizers, to obtain 0.317 bits per base as the compression ratio, allowing to
fit the 134.0 Gb dataset into only 5.31 GB of space.
  Availability: http://sun.aei.polsl.pl/orcom under a free license.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Sebastian Deorowicz</name>
    </author>
    <author>
      <name>Łukasz Roguski</name>
    </author>
    <link href="http://arxiv.org/abs/1405.6874v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6874v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1405.5919">
    <id>http://arxiv.org/abs/1405.5919v2</id>
    <updated>2016-05-23T17:04:14Z</updated>
    <published>2014-05-22T21:55:00Z</published>
    <title>Two simple full-text indexes based on the suffix array</title>
    <summary>  We propose two suffix array inspired full-text indexes. One, called SA-hash,
augments the suffix array with a hash table to speed up pattern searches due to
significantly narrowed search interval before the binary search phase. The
other, called FBCSA, is a compact data structure, similar to M{\"a}kinen's
compact suffix array, but working on fixed sized blocks. Experiments on the
Pizza~\&amp;~Chili 200\,MB datasets show that SA-hash is about 2--3 times faster in
pattern searches (counts) than the standard suffix array, for the price of
requiring $0.2n-1.1n$ bytes of extra space, where $n$ is the text length, and
setting a minimum pattern length. FBCSA is relatively fast in single cell
accesses (a few times faster than related indexes at about the same or better
compression), but not competitive if many consecutive cells are to be
extracted. Still, for the task of extracting, e.g., 10 successive cells its
time-space relation remains attractive.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Marcin Raniszewski</name>
    </author>
    <link href="http://arxiv.org/abs/1405.5919v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5919v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.5; F.2.2; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1405.5754">
    <id>http://arxiv.org/abs/1405.5754v3</id>
    <updated>2014-06-24T10:39:12Z</updated>
    <published>2014-05-22T13:42:19Z</published>
    <title>Twenty-Five Comparators is Optimal when Sorting Nine Inputs (and
  Twenty-Nine for Ten)</title>
    <summary>  This paper describes a computer-assisted non-existence proof of nine-input
sorting networks consisting of 24 comparators, hence showing that the
25-comparator sorting network found by Floyd in 1964 is optimal. As a
corollary, we obtain that the 29-comparator network found by Waksman in 1969 is
optimal when sorting ten inputs.
  This closes the two smallest open instances of the optimal size sorting
network problem, which have been open since the results of Floyd and Knuth from
1966 proving optimality for sorting networks of up to eight inputs.
  The proof involves a combination of two methodologies: one based on
exploiting the abundance of symmetries in sorting networks, and the other,
based on an encoding of the problem to that of satisfiability of propositional
logic. We illustrate that, while each of these can single handed solve smaller
instances of the problem, it is their combination which leads to an efficient
solution for nine inputs.
</summary>
    <author>
      <name>Michael Codish</name>
    </author>
    <author>
      <name>Luís Cruz-Filipe</name>
    </author>
    <author>
      <name>Michael Frank</name>
    </author>
    <author>
      <name>Peter Schneider-Kamp</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICTAI.2014.36</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICTAI.2014.36" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.5754v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5754v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1405.5873">
    <id>http://arxiv.org/abs/1405.5873v1</id>
    <updated>2014-05-22T15:01:07Z</updated>
    <published>2014-05-22T15:01:07Z</published>
    <title>Compressive Mining: Fast and Optimal Data Mining in the Compressed
  Domain</title>
    <summary>  Real-world data typically contain repeated and periodic patterns. This
suggests that they can be effectively represented and compressed using only a
few coefficients of an appropriate basis (e.g., Fourier, Wavelets, etc.).
However, distance estimation when the data are represented using different sets
of coefficients is still a largely unexplored area. This work studies the
optimization problems related to obtaining the \emph{tightest} lower/upper
bound on Euclidean distances when each data object is potentially compressed
using a different set of orthonormal coefficients. Our technique leads to
tighter distance estimates, which translates into more accurate search,
learning and mining operations \textit{directly} in the compressed domain.
  We formulate the problem of estimating lower/upper distance bounds as an
optimization problem. We establish the properties of optimal solutions, and
leverage the theoretical analysis to develop a fast algorithm to obtain an
\emph{exact} solution to the problem. The suggested solution provides the
tightest estimation of the $L_2$-norm or the correlation. We show that typical
data-analysis operations, such as k-NN search or k-Means clustering, can
operate more accurately using the proposed compression and distance
reconstruction technique. We compare it with many other prevalent compression
and reconstruction techniques, including random projections and PCA-based
techniques. We highlight a surprising result, namely that when the data are
highly sparse in some basis, our technique may even outperform PCA-based
compression.
  The contributions of this work are generic as our methodology is applicable
to any sequential or high-dimensional data as well as to any orthogonal data
transformation used for the underlying data compression scheme.
</summary>
    <author>
      <name>Michail Vlachos</name>
    </author>
    <author>
      <name>Nikolaos Freris</name>
    </author>
    <author>
      <name>Anastasios Kyrillidis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 20 figures, accepted in VLDB</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.5873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1405.5646">
    <id>http://arxiv.org/abs/1405.5646v1</id>
    <updated>2014-05-22T07:37:56Z</updated>
    <published>2014-05-22T07:37:56Z</published>
    <title>Mathematical Programming Strategies for Solving the Minimum Common
  String Partition Problem</title>
    <summary>  The minimum common string partition problem is an NP-hard combinatorial
optimization problem with applications in computational biology. In this work
we propose the first integer linear programming model for solving this problem.
Moreover, on the basis of the integer linear programming model we develop a
deterministic 2-phase heuristic which is applicable to larger problem
instances. The results show that provenly optimal solutions can be obtained for
problem instances of small and medium size from the literature by solving the
proposed integer linear programming model with CPLEX. Furthermore, new
best-known solutions are obtained for all considered problem instances from the
literature. Concerning the heuristic, we were able to show that it outperforms
heuristic competitors from the related literature.
</summary>
    <author>
      <name>Christian Blum</name>
    </author>
    <author>
      <name>José A. Lozano</name>
    </author>
    <author>
      <name>Pedro Pinacho Davidson</name>
    </author>
    <link href="http://arxiv.org/abs/1405.5646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90-08" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1405.5461">
    <id>http://arxiv.org/abs/1405.5461v1</id>
    <updated>2014-05-21T15:57:58Z</updated>
    <published>2014-05-21T15:57:58Z</published>
    <title>The LevelArray: A Fast, Practical Long-Lived Renaming Algorithm</title>
    <summary>  The long-lived renaming problem appears in shared-memory systems where a set
of threads need to register and deregister frequently from the computation,
while concurrent operations scan the set of currently registered threads.
Instances of this problem show up in concurrent implementations of
transactional memory, flat combining, thread barriers, and memory reclamation
schemes for lock-free data structures. In this paper, we analyze a randomized
solution for long-lived renaming. The algorithmic technique we consider, called
the LevelArray, has previously been used for hashing and one-shot (single-use)
renaming. Our main contribu- tion is to prove that, in long-lived executions,
where processes may register and deregister polynomially many times, the
technique guarantees constant steps on average and O(log log n) steps with high
probability for registering, unit cost for deregistering, and O(n) steps for
collect queries, where n is an upper bound on the number of processes that may
be active at any point in time. We also show that the algorithm has the
surprising property that it is self-healing: under reasonable assumptions on
the schedule, operations running while the data structure is in a degraded
state implicitly help the data structure re-balance itself. This subtle
mechanism obviates the need for expensive periodic rebuilding procedures. Our
benchmarks validate this approach, showing that, for typical use parameters,
the average number of steps a process takes to register is less than two and
the worst-case number of steps is bounded by six, even in executions with
billions of operations. We contrast this with other randomized implementations,
whose worst-case behavior we show to be unreliable, and with deterministic
implementations, whose cost is linear in n.
</summary>
    <author>
      <name>Dan Alistarh</name>
    </author>
    <author>
      <name>Justin Kopinsky</name>
    </author>
    <author>
      <name>Alexander Matveev</name>
    </author>
    <author>
      <name>Nir Shavit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICDCS 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.5461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1405.5483">
    <id>http://arxiv.org/abs/1405.5483v2</id>
    <updated>2014-05-22T08:11:02Z</updated>
    <published>2014-05-21T17:20:34Z</published>
    <title>Multiple pattern matching revisited</title>
    <summary>  We consider the classical exact multiple string matching problem. Our
solution is based on $q$-grams combined with pattern superimposition,
bit-parallelism and alphabet size reduction. We discuss the pros and cons of
the various alternatives of how to achieve best combination. Our method is
closely related to previous work by (Salmela et al., 2006). The experimental
results show that our method performs well on different alphabet sizes and that
they scale to large pattern sets.
</summary>
    <author>
      <name>Robert Susik</name>
    </author>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Kimmo Fredriksson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.31577/cai_2019_4_937</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.31577/cai_2019_4_937" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">http://www.cai.sk/ojs/index.php/cai/article/view/2019_4_937</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.5483v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5483v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1407.5750">
    <id>http://arxiv.org/abs/1407.5750v1</id>
    <updated>2014-07-22T06:30:53Z</updated>
    <published>2014-07-22T06:30:53Z</published>
    <title>Fibonacci Heaps Revisited</title>
    <summary>  The Fibonacci heap is a classic data structure that supports deletions in
logarithmic amortized time and all other heap operations in O(1) amortized
time. We explore the design space of this data structure. We propose a version
with the following improvements over the original: (i) Each heap is represented
by a single heap-ordered tree, instead of a set of trees. (ii) Each
decrease-key operation does only one cut and a cascade of rank changes, instead
of doing a cascade of cuts. (iii) The outcomes of all comparisons done by the
algorithm are explicitly represented in the data structure, so none are wasted.
We also give an example to show that without cascading cuts or rank changes,
both the original data structure and the new version fail to have the desired
efficiency, solving an open problem of Fredman. Finally, we illustrate the
richness of the design space by proposing several alternative ways to do
cascading rank changes, including a randomized strategy related to one
previously proposed by Karger. We leave the analysis of these alternatives as
intriguing open problems.
</summary>
    <author>
      <name>Haim Kaplan</name>
    </author>
    <author>
      <name>Robert E. Tarjan</name>
    </author>
    <author>
      <name>Uri Zwick</name>
    </author>
    <link href="http://arxiv.org/abs/1407.5750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.5750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1407.4286">
    <id>http://arxiv.org/abs/1407.4286v3</id>
    <updated>2015-09-21T13:55:03Z</updated>
    <published>2014-07-16T12:48:48Z</published>
    <title>Constructing small tree grammars and small circuits for formulas</title>
    <summary>  It is shown that every tree of size $n$ over a fixed set of $\sigma$
different ranked symbols can be decomposed (in linear time as well as in
logspace) into $O\big(\frac{n}{\log_\sigma n}\big) = O\big(\frac{n \log
\sigma}{\log n}\big)$ many hierarchically defined pieces. Formally, such a
hierarchical decomposition has the form of a straight-line linear context-free
tree grammar of size $O\big(\frac{n}{\log_\sigma n}\big)$, which can be used as
a compressed representation of the input tree. This generalizes an analogous
result for strings. Previous grammar-based tree compressors were not analyzed
for the worst-case size of the computed grammar, except for the top dag of
Bille et al., for which only the weaker upper bound of
$O\big(\frac{n}{\log_\sigma^{0.19} n}\big)$ (which was very recently improved
to $O\big(\frac{n \cdot \log \log_\sigma n}{\log_\sigma n}\big)$ by
H\"ubschle-Schneider and Raman) for unranked and unlabelled trees has been
derived. The main result is used to show that every arithmetical formula of
size $n$, in which only $m \leq n$ different variables occur, can be
transformed (in linear time as well as in logspace) into an arithmetical
circuit of size $O\big(\frac{n \cdot \log m}{\log n}\big)$ and depth $O(\log
n)$. This refines a classical result of Brent from 1974, according to which an
arithmetical formula of size $n$ can be transformed into a logarithmic depth
circuit of size $O(n)$.
</summary>
    <author>
      <name>Moses Ganardi</name>
    </author>
    <author>
      <name>Danny Hucke</name>
    </author>
    <author>
      <name>Artur Jez</name>
    </author>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Eric Noeth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A short version of this paper appeared in the Proceedings of FSTTCS
  2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.4286v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.4286v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P30, 68Q42" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1407.2407">
    <id>http://arxiv.org/abs/1407.2407v1</id>
    <updated>2014-07-09T09:42:36Z</updated>
    <published>2014-07-09T09:42:36Z</published>
    <title>$LCSk$++: Practical similarity metric for long strings</title>
    <summary>  In this paper we present $LCSk$++: a new metric for measuring the similarity
of long strings, and provide an algorithm for its efficient computation. With
ever increasing size of strings occuring in practice, e.g. large genomes of
plants and animals, classic algorithms such as Longest Common Subsequence (LCS)
fail due to demanding computational complexity. Recently, Benson et al. defined
a similarity metric named $LCSk$. By relaxing the requirement that the
$k$-length substrings should not overlap, we extend their definition into a new
metric. An efficient algorithm is presented which computes $LCSk$++ with
complexity of $O((|X|+|Y|)\log(|X|+|Y|))$ for strings $X$ and $Y$ under a
realistic random model. The algorithm has been designed with implementation
simplicity in mind. Additionally, we describe how it can be adjusted to compute
$LCSk$ as well, which gives an improvement of the $O(|X|\dot|Y|)$ algorithm
presented in the original $LCSk$ paper.
</summary>
    <author>
      <name>Filip Pavetić</name>
    </author>
    <author>
      <name>Goran Žužić</name>
    </author>
    <author>
      <name>Mile Šikić</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Prague Stringology Conference 2018, (2018)
  50-60</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.2407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.2407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1407.0950">
    <id>http://arxiv.org/abs/1407.0950v2</id>
    <updated>2016-01-14T13:19:24Z</updated>
    <published>2014-07-03T15:29:40Z</published>
    <title>On the Average-case Complexity of Pattern Matching with Wildcards</title>
    <summary>  Pattern matching with wildcards is the problem of finding all factors of a
text $t$ of length $n$ that match a pattern $x$ of length $m$, where wildcards
(characters that match everything) may be present. In this paper we present a
number of fast average-case algorithms for pattern matching where wildcards are
restricted to either the pattern or the text, however, the results are easily
adapted to the case where wildcards are allowed in both. We analyse the
\textit{average-case} complexity of these algorithms and show the first
non-trivial time bounds. These are the first results on the average-case
complexity of pattern matching with wildcards which, as a by product, provide
with first provable separation in complexity between exact pattern matching and
pattern matching with wildcards in the word RAM model.
</summary>
    <author>
      <name>Carl Barton</name>
    </author>
    <link href="http://arxiv.org/abs/1407.0950v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0950v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1406.3170">
    <id>http://arxiv.org/abs/1406.3170v1</id>
    <updated>2014-06-12T09:47:09Z</updated>
    <published>2014-06-12T09:47:09Z</published>
    <title>Compact Indexes for Flexible Top-k Retrieval</title>
    <summary>  We engineer a self-index based retrieval system capable of rank-safe
evaluation of top-k queries. The framework generalizes the GREEDY approach of
Culpepper et al. (ESA 2010) to handle multi-term queries, including over
phrases. We propose two techniques which significantly reduce the ranking time
for a wide range of popular Information Retrieval (IR) relevance measures, such
as TFxIDF and BM25. First, we reorder elements in the document array according
to document weight. Second, we introduce the repetition array, which
generalizes Sadakane's (JDA 2007) document frequency structure to document
subsets. Combining document and repetition array, we achieve attractive
functionality-space trade-offs. We provide an extensive evaluation of our
system on terabyte-sized IR collections.
</summary>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Matthias Petri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.3170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1406.3092">
    <id>http://arxiv.org/abs/1406.3092v1</id>
    <updated>2014-06-12T00:56:02Z</updated>
    <published>2014-06-12T00:56:02Z</published>
    <title>A note on the largest number of red nodes in red-black trees</title>
    <summary>  In this paper, we are interested in the number of red nodes in red-black
trees. We first present an $O(n^2\log n)$ time dynamic programming solution for
computing $r(n)$, the largest number of red internal nodes in a red-black tree
on $n$ keys. Then the algorithm is improved to some $O(\log n)$ time recursive
and nonrecursive algorithms. Based on these improved algorithms we finally find
a closed-form solution of $r(n)$.
</summary>
    <author>
      <name>Yingjie Wu</name>
    </author>
    <author>
      <name>Daxin Zhu</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Xiaodong Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1406.3092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1406.3058">
    <id>http://arxiv.org/abs/1406.3058v2</id>
    <updated>2015-07-17T13:03:08Z</updated>
    <published>2014-06-11T20:38:43Z</published>
    <title>Multilevel polynomial partitions and simplified range searching</title>
    <summary>  The polynomial partitioning method of Guth and Katz [arXiv:1011.4105] has
numerous applications in discrete and computational geometry. It partitions a
given $n$-point set $P\subset\mathbb{R}^d$ using the zero set $Z(f)$ of a
suitable $d$-variate polynomial $f$. Applications of this result are often
complicated by the problem, what should be done with the points of $P$ lying
within $Z(f)$? A natural approach is to partition these points with another
polynomial and continue further in a similar manner. So far it has been pursued
with limited success---several authors managed to construct and apply a second
partitioning polynomial, but further progress has been prevented by technical
obstacles. We provide a polynomial partitioning method with up to $d$
polynomials in dimension $d$, which allows for a complete decomposition of the
given point set. We apply it to obtain a new algorithm for the semialgebraic
range searching problem. Our algorithm has running time bounds similar to a
recent algorithm by Agarwal, Sharir, and the first author [SIAM~J.~Comput.
42(2013) 2039--2062], but it is simpler both conceptually and technically.
While this paper has been in preparation, Basu and Sombra, as well as Fox,
Pach, Sheffer, Suk, and Zahl, obtained results concerning polynomial partitions
which overlap with ours to some extent.
</summary>
    <author>
      <name>Jiri Matousek</name>
    </author>
    <author>
      <name>Zuzana Patakova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages; The proof that the Groebner basis can be effectively
  computed is stated in more detail</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Disc. Comput. Geom. 54(1):22-41, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.3058v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3058v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U05, 68W30, 14Q20" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1406.2348">
    <id>http://arxiv.org/abs/1406.2348v2</id>
    <updated>2014-12-03T19:39:41Z</updated>
    <published>2014-06-09T20:36:01Z</published>
    <title>Sampling the suffix array with minimizers</title>
    <summary>  Sampling (evenly) the suffixes from the suffix array is an old idea trading
the pattern search time for reduced index space. A few years ago Claude et al.
showed an alphabet sampling scheme allowing for more efficient pattern searches
compared to the sparse suffix array, for long enough patterns. A drawback of
their approach is the requirement that sought patterns need to contain at least
one character from the chosen subalphabet. In this work we propose an
alternative suffix sampling approach with only a minimum pattern length as a
requirement, which seems more convenient in practice. Experiments show that our
algorithm achieves competitive time-space tradeoffs on most standard benchmark
data.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Marcin Raniszewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">One new SamSAMi variant; extended experimental results</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2348v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2348v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1406.2262">
    <id>http://arxiv.org/abs/1406.2262v1</id>
    <updated>2014-06-06T12:17:11Z</updated>
    <published>2014-06-06T12:17:11Z</published>
    <title>ARC Sort: Enhanced and Time Efficient Sorting Algorithm</title>
    <summary>  This paper discusses about a sorting algorithm which uses the concept of
buckets where each bucket represents a certain number of digits. A two
dimensional data structure is used where one dimension represents buckets i. e;
number of digits and each bucket's corresponding dimensions represents the
input numbers that belong to that bucket. Each bucket is then individually
sorted. Since every preceding bucket elements will always be smaller than the
succeeding buckets no comparison between them is required. By doing this we can
significantly reduced the time complexity of any sorting algorithm used to sort
the given set of inputs.
</summary>
    <author>
      <name>Ankit Chadha</name>
    </author>
    <author>
      <name>Rishikesh Misal</name>
    </author>
    <author>
      <name>Tanaya Mokashi</name>
    </author>
    <author>
      <name>Aman Chadha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/ijais14-451130</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/ijais14-451130" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Applied Information Systems 7(2), 1-7,
  March 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.2262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1406.1158">
    <id>http://arxiv.org/abs/1406.1158v2</id>
    <updated>2015-01-11T23:58:02Z</updated>
    <published>2014-06-04T19:32:04Z</published>
    <title>Kernelization lower bound for Permutation Pattern Matching</title>
    <summary>  A permutation $\pi$ contains a permutation $\sigma$ as a pattern if it
contains a subsequence of length $|\sigma|$ whose elements are in the same
relative order as in the permutation $\sigma$. This notion plays a major role
in enumerative combinatorics. We prove that the problem does not have a
polynomial kernel (under the widely believed complexity assumption $\mbox{NP}
\not\subseteq \mbox{co-NP}/\mbox{poly}$) by introducing a new polynomial
reduction from the clique problem to permutation pattern matching.
</summary>
    <author>
      <name>Ivan Bliznets</name>
    </author>
    <author>
      <name>Marek Cygan</name>
    </author>
    <author>
      <name>Pawel Komosa</name>
    </author>
    <author>
      <name>Lukas Mach</name>
    </author>
    <link href="http://arxiv.org/abs/1406.1158v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.1158v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1410.0562">
    <id>http://arxiv.org/abs/1410.0562v1</id>
    <updated>2014-10-02T14:25:51Z</updated>
    <published>2014-10-02T14:25:51Z</published>
    <title>A massively parallel algorithm for constructing the BWT of large string
  sets</title>
    <summary>  We present a new scalable, lightweight algorithm to incrementally construct
the BWT and FM-index of large string sets such as those produced by Next
Generation Sequencing. The algorithm is designed for massive parallelism and
can effectively exploit the combination of low capacity high bandwidth memory
and slower external system memory typical of GPU accelerated systems.
Particularly, for a string set of n characters from an alphabet with \sigma
symbols, it uses a constant amount of high-bandwidth memory and at most 3n
log(\sigma) bits of system memory. Given that deep memory hierarchies are
becoming a pervasive trait of high performance computing architectures, we
believe this to be a relevant feature. The implementation can handle reads of
arbitrary length and is up to 2 and respectively 6.5 times faster than
state-of-the-art for short and long genomic reads
</summary>
    <author>
      <name>Jacopo Pantaleoni</name>
    </author>
    <link href="http://arxiv.org/abs/1410.0562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.0562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1409.2433">
    <id>http://arxiv.org/abs/1409.2433v1</id>
    <updated>2014-09-08T17:19:11Z</updated>
    <published>2014-09-08T17:19:11Z</published>
    <title>Approximating solution structure of the Weighted Sentence Alignment
  problem</title>
    <summary>  We study the complexity of approximating solution structure of the bijective
weighted sentence alignment problem of DeNero and Klein (2008). In particular,
we consider the complexity of finding an alignment that has a significant
overlap with an optimal alignment. We discuss ways of representing the solution
for the general weighted sentence alignment as well as phrases-to-words
alignment problem, and show that computing a string which agrees with the
optimal sentence partition on more than half (plus an arbitrarily small
polynomial fraction) positions for the phrases-to-words alignment is NP-hard.
For the general weighted sentence alignment we obtain such bound from the
agreement on a little over 2/3 of the bits. Additionally, we generalize the
Hamming distance approximation of a solution structure to approximating it with
respect to the edit distance metric, obtaining similar lower bounds.
</summary>
    <author>
      <name>Antonina Kolokolova</name>
    </author>
    <author>
      <name>Renesa Nizamee</name>
    </author>
    <link href="http://arxiv.org/abs/1409.2433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1409.2928">
    <id>http://arxiv.org/abs/1409.2928v1</id>
    <updated>2014-09-10T00:49:34Z</updated>
    <published>2014-09-10T00:49:34Z</published>
    <title>Path algebra algorithm for finding longest increasing subsequence</title>
    <summary>  New algorithm for finding longest increasing subsequence is discussed. This
algorithm is based on the ideas of idempotent mathematics and uses Max-Plus
idempotent semiring. Problem of finding longest increasing sub- sequence is
reformulated in a matrix form and solved with linear algebra.
</summary>
    <author>
      <name>Anatoly Rodionov</name>
    </author>
    <link href="http://arxiv.org/abs/1409.2928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1409.2398">
    <id>http://arxiv.org/abs/1409.2398v1</id>
    <updated>2014-09-08T15:34:32Z</updated>
    <published>2014-09-08T15:34:32Z</published>
    <title>A Parameterized Study of Maximum Generalized Pattern Matching Problems</title>
    <summary>  The generalized function matching (GFM) problem has been intensively studied
starting with [Ehrenfeucht and Rozenberg, 1979]. Given a pattern p and a text
t, the goal is to find a mapping from the letters of p to non-empty substrings
of t, such that applying the mapping to p results in t. Very recently, the
problem has been investigated within the framework of parameterized complexity
[Fernau, Schmid, and Villanger, 2013].
  In this paper we study the parameterized complexity of the optimization
variant of GFM (called Max-GFM), which has been introduced in [Amir and Nor,
2007]. Here, one is allowed to replace some of the pattern letters with some
special symbols "?", termed wildcards or don't cares, which can be mapped to an
arbitrary substring of the text. The goal is to minimize the number of
wildcards used.
  We give a complete classification of the parameterized complexity of Max-GFM
and its variants under a wide range of parameterizations, such as, the number
of occurrences of a letter in the text, the size of the text alphabet, the
number of occurrences of a letter in the pattern, the size of the pattern
alphabet, the maximum length of a string matched to any pattern letter, the
number of wildcards and the maximum size of a string that a wildcard can be
mapped to.
</summary>
    <author>
      <name>Sebastian Ordyniak</name>
    </author>
    <author>
      <name>Alexandru Popa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in Proc. IPEC'14</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.2398v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2398v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1409.1694">
    <id>http://arxiv.org/abs/1409.1694v2</id>
    <updated>2015-03-16T12:22:05Z</updated>
    <published>2014-09-05T08:57:02Z</published>
    <title>Longest common substrings with k mismatches</title>
    <summary>  The longest common substring with $k$-mismatches problem is to find, given
two strings $S_1$ and $S_2$, a longest substring $A_1$ of $S_1$ and $A_2$ of
$S_2$ such that the Hamming distance between $A_1$ and $A_2$ is $\le k$. We
introduce a practical $O(nm)$ time and $O(1)$ space solution for this problem,
where $n$ and $m$ are the lengths of $S_1$ and $S_2$, respectively. This
algorithm can also be used to compute the matching statistics with
$k$-mismatches of $S_1$ and $S_2$ in $O(nm)$ time and $O(m)$ space. Moreover,
we also present a theoretical solution for the $k = 1$ case which runs in $O(n
\log m)$ time, assuming $m\le n$, and uses $O(m)$ space, improving over the
existing $O(nm)$ time and $O(m)$ space bound of Babenko and Starikovskaya.
</summary>
    <author>
      <name>Tomas Flouri</name>
    </author>
    <author>
      <name>Emanuele Giaquinta</name>
    </author>
    <author>
      <name>Kassian Kobert</name>
    </author>
    <author>
      <name>Esko Ukkonen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ipl.2015.03.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ipl.2015.03.006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted version</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.1694v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.1694v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1408.5422">
    <id>http://arxiv.org/abs/1408.5422v1</id>
    <updated>2014-08-11T11:40:01Z</updated>
    <published>2014-08-11T11:40:01Z</published>
    <title>Analysis of String Sorting using Heapsort</title>
    <summary>  In this master thesis we analyze the complexity of sorting a set of strings.
It was shown that the complexity of sorting strings can be naturally expressed
in terms of the prefix trie induced by the set of strings. The model of
computation takes into account symbol comparisons and not just comparisons
between the strings. The analysis of upper and lower bounds for some classical
algorithms such as Quicksort and Mergesort in terms of such a model was shown.
Here we extend the analysis to another classical algorithm - Heapsort. We also
give analysis for the version of the algorithm that uses Binomial heaps as a
heap implementation.
</summary>
    <author>
      <name>Igor Stassiy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.5422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1408.1816">
    <id>http://arxiv.org/abs/1408.1816v3</id>
    <updated>2015-08-26T05:25:00Z</updated>
    <published>2014-08-08T11:03:35Z</published>
    <title>Quantum pattern matching fast on average</title>
    <summary>  The $d$-dimensional pattern matching problem is to find an occurrence of a
pattern of length $m \times \dots \times m$ within a text of length $n \times
\dots \times n$, with $n \ge m$. This task models various problems in text and
image processing, among other application areas. This work describes a quantum
algorithm which solves the pattern matching problem for random patterns and
texts in time $\widetilde{O}((n/m)^{d/2} 2^{O(d^{3/2}\sqrt{\log m})})$. For
large $m$ this is super-polynomially faster than the best possible classical
algorithm, which requires time $\widetilde{\Omega}( (n/m)^d + n^{d/2} )$. The
algorithm is based on the use of a quantum subroutine for finding hidden shifts
in $d$ dimensions, which is a variant of algorithms proposed by Kuperberg.
</summary>
    <author>
      <name>Ashley Montanaro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 2 figures; v3: further minor changes, essentially published
  version</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.1816v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.1816v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1407.8142">
    <id>http://arxiv.org/abs/1407.8142v7</id>
    <updated>2015-04-01T06:28:14Z</updated>
    <published>2014-07-30T17:55:42Z</published>
    <title>Parallel Wavelet Tree Construction</title>
    <summary>  We present parallel algorithms for wavelet tree construction with
polylogarithmic depth, improving upon the linear depth of the recent parallel
algorithms by Fuentes-Sepulveda et al. We experimentally show on a 40-core
machine with two-way hyper-threading that we outperform the existing parallel
algorithms by 1.3--5.6x and achieve up to 27x speedup over the sequential
algorithm on a variety of real-world and artificial inputs. Our algorithms show
good scalability with increasing thread count, input size and alphabet size. We
also discuss extensions to variants of the standard wavelet tree.
</summary>
    <author>
      <name>Julian Shun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DCC.2015.7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DCC.2015.7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a longer version of the paper that appears in the Proceedings
  of the IEEE Data Compression Conference, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.8142v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.8142v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1407.7459">
    <id>http://arxiv.org/abs/1407.7459v3</id>
    <updated>2018-09-04T16:26:01Z</updated>
    <published>2014-07-25T13:58:57Z</published>
    <title>A note on multipivot Quicksort</title>
    <summary>  We analyse a generalisation of the Quicksort algorithm, where k uniformly at
random chosen pivots are used for partitioning an array of n distinct keys.
Specifically, the expected cost of this scheme is obtained, under the
assumption of linearity of the cost needed for the partition process. The
integration constants of the expected cost are computed using Vandermonde
matrices.
</summary>
    <author>
      <name>Vasileios Iliopoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/02522667.2017.1303947</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/02522667.2017.1303947" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Author's accepted manuscript, 7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Inform. Optim. Sci. 39 (5): 1139-1147, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.7459v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.7459v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P10, 68W20" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1407.6794">
    <id>http://arxiv.org/abs/1407.6794v1</id>
    <updated>2014-07-25T06:55:58Z</updated>
    <published>2014-07-25T06:55:58Z</published>
    <title>GCD Computation of n Integers</title>
    <summary>  Greatest Common Divisor (GCD) computation is one of the most important
operation of algorithmic number theory. In this paper we present the algorithms
for GCD computation of $n$ integers. We extend the Euclid's algorithm and
binary GCD algorithm to compute the GCD of more than two integers.
</summary>
    <author>
      <name>Shri Prakash Dwivedi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/RAECS.2014.6799612</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/RAECS.2014.6799612" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RAECS 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.6794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.6794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1411.6581">
    <id>http://arxiv.org/abs/1411.6581v2</id>
    <updated>2015-06-15T11:49:54Z</updated>
    <published>2014-11-24T19:27:44Z</published>
    <title>Optimal Encodings for Range Top-k, Selection, and Min-Max</title>
    <summary>  We consider encoding problems for range queries on arrays. In these problems
the goal is to store a structure capable of recovering the answer to all
queries that occupies the information theoretic minimum space possible, to
within lower order terms. As input, we are given an array $A[1..n]$, and a
fixed parameter $k \in [1,n]$. A range top-$k$ query on an arbitrary range
$[i,j] \subseteq [1,n]$ asks us to return the ordered set of indices $\{\ell_1,
..., \ell_{k}\}$ such that $A[\ell_m]$ is the $m$-th largest element in
$A[i..j]$, for $1 \le m \le k$. A range selection query for an arbitrary range
$[i,j] \subseteq [1,n]$ and query parameter $k' \in [1,k]$ asks us to return
the index of the $k'$-th largest element in $A[i..j]$. We completely resolve
the space complexity of both of these heavily studied problems---to within
lower order terms---for all $k = o(n)$. Previously, the constant factor in the
space complexity was known only for $k=1$. We also resolve the space complexity
of another problem, that we call range min-max, in which the goal is to return
the indices of both the minimum and maximum elements in a range.
</summary>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <author>
      <name>Patrick K. Nicholson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages: a short version of this paper will be presented at ICALP
  2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.6581v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6581v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1411.5127">
    <id>http://arxiv.org/abs/1411.5127v2</id>
    <updated>2014-11-21T01:37:51Z</updated>
    <published>2014-11-19T06:48:20Z</published>
    <title>PivotCompress: Compression by Sorting</title>
    <summary>  Sorted data is usually easier to compress than unsorted permutations of the
same data. This motivates a simple compression scheme: specify the sorted
permutation of the data along with a representation of the sorted data
compressed recursively. The sorted permutation can be specified by recording
the decisions made by quicksort. If the size of the data is known, then the
quicksort decisions describe the data at a rate that is nearly as efficient as
the minimal prefix-free code for the distribution, which is bounded by the
entropy of the distribution. This is possible even though the distribution is
unknown ahead of time. Used in this way, quicksort acts as a universal code in
that it is asymptotically optimal for any stationary source. The Shannon
entropy is a lower bound when describing stochastic, independent symbols.
However, it is possible to encode non-uniform, finite strings below the entropy
of the sample distribution by also encoding symbol counts because the values in
the sequence are no longer independent once the counts are known. The key
insight is that sparse quicksort comparison vectors can also be compressed to
achieve an even lower rate when data is highly non-uniform while incurring only
a modest penalty when data is random.
</summary>
    <author>
      <name>Oscar Stiffelman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preprint, compression by sorting, quicksort as universal code; this
  version describes the permutation vector and its inverse</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.5127v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.5127v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1411.2059">
    <id>http://arxiv.org/abs/1411.2059v1</id>
    <updated>2014-11-07T23:20:37Z</updated>
    <published>2014-11-07T23:20:37Z</published>
    <title>Analysis of Branch Misses in Quicksort</title>
    <summary>  The analysis of algorithms mostly relies on counting classic elementary
operations like additions, multiplications, comparisons, swaps etc. This
approach is often sufficient to quantify an algorithm's efficiency. In some
cases, however, features of modern processor architectures like pipelined
execution and memory hierarchies have significant impact on running time and
need to be taken into account to get a reliable picture. One such example is
Quicksort: It has been demonstrated experimentally that under certain
conditions on the hardware the classically optimal balanced choice of the pivot
as median of a sample gets harmful. The reason lies in mispredicted branches
whose rollback costs become dominating.
  In this paper, we give the first precise analytical investigation of the
influence of pipelining and the resulting branch mispredictions on the
efficiency of (classic) Quicksort and Yaroslavskiy's dual-pivot Quicksort as
implemented in Oracle's Java 7 library. For the latter it is still not fully
understood why experiments prove it 10% faster than a highly engineered
implementation of a classic single-pivot version. For different branch
prediction strategies, we give precise asymptotics for the expected number of
branch misses caused by the aforementioned Quicksort variants when their pivots
are chosen from a sample of the input. We conclude that the difference in
branch misses is too small to explain the superiority of the dual-pivot
algorithm.
</summary>
    <author>
      <name>Conrado Martínez</name>
    </author>
    <author>
      <name>Markus E. Nebel</name>
    </author>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/1.9781611973761.11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/1.9781611973761.11" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to be presented at ANALCO 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.2059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1411.0644">
    <id>http://arxiv.org/abs/1411.0644v1</id>
    <updated>2014-11-03T20:10:57Z</updated>
    <published>2014-11-03T20:10:57Z</published>
    <title>Towards Tight Lower Bounds for Range Reporting on the RAM</title>
    <summary>  In the orthogonal range reporting problem, we are to preprocess a set of $n$
points with integer coordinates on a $U \times U$ grid. The goal is to support
reporting all $k$ points inside an axis-aligned query rectangle. This is one of
the most fundamental data structure problems in databases and computational
geometry. Despite the importance of the problem its complexity remains
unresolved in the word-RAM. On the upper bound side, three best tradeoffs
exists: (1.) Query time $O(\lg \lg n + k)$ with $O(nlg^{\varepsilon}n)$ words
of space for any constant $\varepsilon>0$. (2.) Query time $O((1 + k) \lg \lg
n)$ with $O(n \lg \lg n)$ words of space. (3.) Query time
$O((1+k)\lg^{\varepsilon} n)$ with optimal $O(n)$ words of space. However, the
only known query time lower bound is $\Omega(\log \log n +k)$, even for linear
space data structures.
  All three current best upper bound tradeoffs are derived by reducing range
reporting to a ball-inheritance problem. Ball-inheritance is a problem that
essentially encapsulates all previous attempts at solving range reporting in
the word-RAM. In this paper we make progress towards closing the gap between
the upper and lower bounds for range reporting by proving cell probe lower
bounds for ball-inheritance. Our lower bounds are tight for a large range of
parameters, excluding any further progress for range reporting using the
ball-inheritance reduction.
</summary>
    <author>
      <name>Allan Grønlund</name>
    </author>
    <author>
      <name>Kasper Green Larsen</name>
    </author>
    <link href="http://arxiv.org/abs/1411.0644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.0644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1411.0048">
    <id>http://arxiv.org/abs/1411.0048v1</id>
    <updated>2014-11-01T00:34:58Z</updated>
    <published>2014-11-01T00:34:58Z</published>
    <title>Fusion Tree Sorting</title>
    <summary>  The sorting problem is one of the most relevant problems in computer science.
Within the scope of modern computer science it has been studied for more than
70 years. In spite of these facts, new sorting algorithms have been developed
in recent years. Among several types of sorting algorithms, some are quicker;
others are more economic in relation to space, whereas others insert a few
restrictions in relation to data input. This paper is aimed at explaining the
fusion tree data structure, which is responsible for the first sorting
algorithm with complexity time smaller than nlgn. The nlgn time complexity has
led to some confusion and generated the wrong belief in part of the community
of being the minimum possible for this type of problem.
</summary>
    <author>
      <name>Luis A. A. Meira</name>
    </author>
    <author>
      <name>Rogério H. B. de Lima</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.0048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.0048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1410.6754">
    <id>http://arxiv.org/abs/1410.6754v2</id>
    <updated>2015-02-25T10:53:28Z</updated>
    <published>2014-10-24T17:56:48Z</published>
    <title>Practical Massively Parallel Sorting</title>
    <summary>  Previous parallel sorting algorithms do not scale to the largest available
machines, since they either have prohibitive communication volume or
prohibitive critical path length. We describe algorithms that are a viable
compromise and overcome this gap both in theory and practice. The algorithms
are multi-level generalizations of the known algorithms sample sort and
multiway mergesort. In particular our sample sort variant turns out to be very
scalable. Some tools we develop may be of independent interest -- a simple,
practical, and flexible sorting algorithm for small inputs working in
logarithmic time, a near linear time optimal algorithm for solving a
constrained bin packing problem, and an algorithm for data delivery, that
guarantees a small number of message startups on each processor.
</summary>
    <author>
      <name>Michael Axtmann</name>
    </author>
    <author>
      <name>Timo Bingmann</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <author>
      <name>Christian Schulz</name>
    </author>
    <link href="http://arxiv.org/abs/1410.6754v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6754v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.3; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1410.6433">
    <id>http://arxiv.org/abs/1410.6433v5</id>
    <updated>2016-04-07T10:28:22Z</updated>
    <published>2014-10-23T17:54:10Z</published>
    <title>Tight tradeoffs for approximating palindromes in streams</title>
    <summary>  We consider computing the longest palindrome in a text of length $n$ in the
streaming model, where the characters arrive one-by-one, and we do not have
random access to the input. While computing the answer exactly using sublinear
memory is not possible in such a setting, one can still hope for a good
approximation guarantee.
  We focus on the two most natural variants, where we aim for either additive
or multiplicative approximation of the length of the longest palindrome. We
first show that there is no point in considering Las Vegas algorithms in such a
setting, as they cannot achieve sublinear space complexity. For Monte Carlo
algorithms, we provide a lowerbound of $\Omega(\frac{n}{E})$ bits for
approximating the answer with additive error $E$, and $\Omega(\frac{\log
n}{\log(1+\varepsilon)})$ bits for approximating the answer with multiplicative
error $(1+\varepsilon)$ for the binary alphabet. Then, we construct a generic
Monte Carlo algorithm, which by choosing the parameters appropriately achieves
space complexity matching up to a logarithmic factor for both variants. This
substantially improves the previous results by Berenbrink et al. (STACS 2014)
and essentially settles the space complexity.
</summary>
    <author>
      <name>Paweł Gawrychowski</name>
    </author>
    <author>
      <name>Przemysław Uznański</name>
    </author>
    <link href="http://arxiv.org/abs/1410.6433v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6433v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1410.5420">
    <id>http://arxiv.org/abs/1410.5420v35</id>
    <updated>2023-03-20T15:53:43Z</updated>
    <published>2014-10-20T16:08:40Z</published>
    <title>Building a Balanced k-d Tree in O(kn log n) Time</title>
    <summary>  The original description of the k-d tree recognized that rebalancing
techniques, such as are used to build an AVL tree or a red-black tree, are not
applicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is
necessary to find the median of the data for each recursive subdivision of
those data. The sort or selection that is used to find the median for each
subdivision strongly influences the computational complexity of building a k-d
tree. This paper discusses an alternative algorithm that builds a balanced k-d
tree by presorting the data in each of k dimensions prior to building the tree.
It then preserves the order of these k sorts during tree construction and
thereby avoids the requirement for any further sorting. Moreover, this
algorithm is amenable to parallel execution via multiple threads. Compared to
an algorithm that finds the median for each recursive subdivision, this
presorting algorithm has equivalent performance for four dimensions and better
performance for three or fewer dimensions.
</summary>
    <author>
      <name>Russell A. Brown</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures, published at
  http://jcgt.org/published/0004/01/03/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computer Graphics Techniques (JCGT), vol. 4, no. 1,
  50-68, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.5420v35" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5420v35" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1410.4701">
    <id>http://arxiv.org/abs/1410.4701v4</id>
    <updated>2015-01-26T13:56:24Z</updated>
    <published>2014-10-17T11:59:45Z</published>
    <title>Optimal Time Random Access to Grammar-Compressed Strings in Small Space</title>
    <summary>  The random access problem for compressed strings is to build a data structure
that efficiently supports accessing the character in position $i$ of a string
given in compressed form. Given a grammar of size $n$ compressing a string of
size $N$, we present a data structure using $O(n\Delta \log_\Delta \frac N n
\log N)$ bits of space that supports accessing position $i$ in $O(\log_\Delta
N)$ time for $\Delta \leq \log^{O(1)} N$. The query time is optimal for
polynomially compressible strings, i.e., when $n=O(N^{1-\epsilon})$.
</summary>
    <author>
      <name>Patrick Hagge Cording</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Withdrawn because of errors in proofs. Fixed versions will be
  incorporated into a paper by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.4701v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.4701v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1410.2736">
    <id>http://arxiv.org/abs/1410.2736v1</id>
    <updated>2014-10-10T10:55:38Z</updated>
    <published>2014-10-10T10:55:38Z</published>
    <title>Faster Sorting Networks for $17$, $19$ and $20$ Inputs</title>
    <summary>  We present new parallel sorting networks for $17$ to $20$ inputs. For $17,
19,$ and $20$ inputs these new networks are faster (i.e., they require less
computation steps) than the previously known best networks. Therefore, we
improve upon the known upper bounds for minimal depth sorting networks on $17,
19,$ and $20$ channels. The networks were obtained using a combination of
hand-crafted first layers and a SAT encoding of sorting networks.
</summary>
    <author>
      <name>Thorsten Ehlers</name>
    </author>
    <author>
      <name>Mike Müller</name>
    </author>
    <link href="http://arxiv.org/abs/1410.2736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1412.3688">
    <id>http://arxiv.org/abs/1412.3688v2</id>
    <updated>2014-12-17T15:50:27Z</updated>
    <published>2014-12-11T15:49:44Z</published>
    <title>Run-Length Encoded Nondeterministic KMP and Suffix Automata</title>
    <summary>  We present a novel bit-parallel representation, based on the run-length
encoding, of the nondeterministic KMP and suffix automata for a string $P$ with
at least two distinct symbols. Our method is targeted to the case of long
strings over small alphabets and complements the method of Cantone et al.
(2012), which is effective for long strings over large alphabets. Our encoding
requires $O((\sigma + m)\lceil \rho / w\rceil)$ space and allows one to
simulate the automata on a string in time $O(\lceil \rho / w\rceil)$ per
transition, where $\sigma$ is the alphabet size, $m$ is the length of $P$,
$\rho$ is the length of the run-length encoding of $P$ and $w$ is the machine
word size in bits. The input string can be given in either unencoded or
run-length encoded form.
</summary>
    <author>
      <name>Emanuele Giaquinta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Various fixes</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.3688v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3688v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1412.3696">
    <id>http://arxiv.org/abs/1412.3696v1</id>
    <updated>2014-12-11T16:00:26Z</updated>
    <published>2014-12-11T16:00:26Z</published>
    <title>Covering Problems for Partial Words and for Indeterminate Strings</title>
    <summary>  We consider the problem of computing a shortest solid cover of an
indeterminate string. An indeterminate string may contain non-solid symbols,
each of which specifies a subset of the alphabet that could be present at the
corresponding position. We also consider covering partial words, which are a
special case of indeterminate strings where each non-solid symbol is a don't
care symbol. We prove that indeterminate string covering problem and partial
word covering problem are NP-complete for binary alphabet and show that both
problems are fixed-parameter tractable with respect to $k$, the number of
non-solid symbols. For the indeterminate string covering problem we obtain a
$2^{O(k \log k)} + n k^{O(1)}$-time algorithm. For the partial word covering
problem we obtain a $2^{O(\sqrt{k}\log k)} + nk^{O(1)}$-time algorithm. We
prove that, unless the Exponential Time Hypothesis is false, no
$2^{o(\sqrt{k})} n^{O(1)}$-time solution exists for either problem, which shows
that our algorithm for this case is close to optimal. We also present an
algorithm for both problems which is feasible in practice.
</summary>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">full version (simplified and corrected); preliminary version appeared
  at ISAAC 2014; 14 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.3696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32 (Primary), 68Q25 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1412.3016">
    <id>http://arxiv.org/abs/1412.3016v2</id>
    <updated>2015-02-27T11:53:36Z</updated>
    <published>2014-12-09T16:38:25Z</published>
    <title>Computing Covers Using Prefix Tables</title>
    <summary>  An \emph{indeterminate string} $x = x[1..n]$ on an alphabet $\Sigma$ is a
sequence of nonempty subsets of $\Sigma$; $x$ is said to be \emph{regular} if
every subset is of size one. A proper substring $u$ of regular $x$ is said to
be a \emph{cover} of $x$ iff for every $i \in 1..n$, an occurrence of $u$ in
$x$ includes $x[i]$. The \emph{cover array} $\gamma = \gamma[1..n]$ of $x$ is
an integer array such that $\gamma[i]$ is the longest cover of $x[1..i]$.
Fifteen years ago a complex, though nevertheless linear-time, algorithm was
proposed to compute the cover array of regular $x$ based on prior computation
of the border array of $x$. In this paper we first describe a linear-time
algorithm to compute the cover array of regular string $x$ based on the prefix
table of $x$. We then extend this result to indeterminate strings.
</summary>
    <author>
      <name>Ali Alatabbi</name>
    </author>
    <author>
      <name>M. Sohel Rahman</name>
    </author>
    <author>
      <name>W. F. Smyth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.3016v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3016v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1412.4882">
    <id>http://arxiv.org/abs/1412.4882v1</id>
    <updated>2014-12-16T05:14:44Z</updated>
    <published>2014-12-16T05:14:44Z</published>
    <title>Simple Balanced Binary Search Trees</title>
    <summary>  Efficient implementations of sets and maps (dictionaries) are important in
computer science, and balanced binary search trees are the basis of the best
practical implementations. Pedagogically, however, they are often quite
complicated, especially with respect to deletion. I present complete code (with
justification and analysis not previously available in the literature) for a
purely-functional implementation based on AA trees, which is the simplest
treatment of the subject of which I am aware.
</summary>
    <author>
      <name>Prabhakar Ragde</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Waterloo, Waterloo, Ontario, Canada</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.170.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.170.6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TFPIE 2014, arXiv:1412.4738</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 170, 2014, pp. 78-87</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1412.4882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1412.5932">
    <id>http://arxiv.org/abs/1412.5932v1</id>
    <updated>2014-12-18T16:37:12Z</updated>
    <published>2014-12-18T16:37:12Z</published>
    <title>Compression of high throughput sequencing data with probabilistic de
  Bruijn graph</title>
    <summary>  Motivation: Data volumes generated by next-generation sequencing technolo-
gies is now a major concern, both for storage and transmission. This triggered
the need for more efficient methods than general purpose compression tools,
such as the widely used gzip. Most reference-free tools developed for NGS data
compression still use general text compression methods and fail to benefit from
algorithms already designed specifically for the analysis of NGS data. The goal
of our new method Leon is to achieve compression of DNA sequences of high
throughput sequencing data, without the need of a reference genome, with
techniques derived from existing assembly principles, that possibly better
exploit NGS data redundancy. Results: We propose a novel method, implemented in
the software Leon, for compression of DNA sequences issued from high throughput
sequencing technologies. This is a lossless method that does not need a
reference genome. Instead, a reference is built de novo from the set of reads
as a probabilistic de Bruijn Graph, stored in a Bloom filter. Each read is
encoded as a path in this graph, storing only an anchoring kmer and a list of
bifurcations indicating which path to follow in the graph. This new method will
allow to have compressed read files that also already contain its underlying de
Bruijn Graph, thus directly re-usable by many tools relying on this structure.
Leon achieved encoding of a C. elegans reads set with 0.7 bits/base,
outperforming state of the art reference-free methods. Availability: Open
source, under GNU affero GPL License, available for download at
http://gatb.inria.fr/software/leon/
</summary>
    <author>
      <name>Gaëtan Benoit</name>
    </author>
    <author>
      <name>Claire Lemaitre</name>
    </author>
    <author>
      <name>Dominique Lavenier</name>
    </author>
    <author>
      <name>Guillaume Rizk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1412.4646">
    <id>http://arxiv.org/abs/1412.4646v3</id>
    <updated>2015-12-23T08:06:19Z</updated>
    <published>2014-12-15T15:54:46Z</published>
    <title>Fewer runs than word length</title>
    <summary>  The work takes another look at the number of runs that a string might contain
and provides an alternative proof for the bound. We also propose another
stronger conjecture that states that, for a fixed order on the alphabet, within
every factor of a word there are at most as many occurrences of Lyndon roots
corresponding to runs in a word as the length of the factor (only first such
occurrences for each run are considered).
</summary>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Robert Mercas</name>
    </author>
    <link href="http://arxiv.org/abs/1412.4646v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4646v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1412.8246">
    <id>http://arxiv.org/abs/1412.8246v3</id>
    <updated>2015-01-01T06:59:26Z</updated>
    <published>2014-12-29T02:23:39Z</published>
    <title>Pattern Matching and Local Alignment for RNA Structures</title>
    <summary>  The primary structure of a ribonucleic acid (RNA) molecule can be represented
as a sequence of nucleotides (bases) over the alphabet {A, C, G, U}. The
secondary or tertiary structure of an RNA is a set of base pairs which form
bonds between A-U and G-C. For secondary structures, these bonds have been
traditionally assumed to be one-to-one and non-crossing. This paper considers
pattern matching as well as local alignment between two RNA structures. For
pattern matching, we present two algorithms, one for obtaining an exact match,
the other for approximate match. We then present an algorithm for RNA local
structural alignment.
</summary>
    <author>
      <name>Shihyen Chen</name>
    </author>
    <author>
      <name>Zhuozhi Wang</name>
    </author>
    <author>
      <name>Kaizhong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages. V2: changed first names initials to full names in metadata.
  V3: added info of conference proceedings, updated email address</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2002 International Conference on Mathematics
  and Engineering Techniques in Medicine and Biological Sciences (METMBS),
  55-61, 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1412.8246v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.8246v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1412.0193">
    <id>http://arxiv.org/abs/1412.0193v2</id>
    <updated>2015-08-10T07:03:08Z</updated>
    <published>2014-11-30T07:49:11Z</published>
    <title>Analysis of Pivot Sampling in Dual-Pivot Quicksort</title>
    <summary>  The new dual-pivot Quicksort by Vladimir Yaroslavskiy - used in Oracle's Java
runtime library since version 7 - features intriguing asymmetries. They make a
basic variant of this algorithm use less comparisons than classic single-pivot
Quicksort. In this paper, we extend the analysis to the case where the two
pivots are chosen as fixed order statistics of a random sample. Surprisingly,
dual-pivot Quicksort then needs more comparisons than a corresponding version
of classic Quicksort, so it is clear that counting comparisons is not
sufficient to explain the running time advantages observed for Yaroslavskiy's
algorithm in practice. Consequently, we take a more holistic approach and give
also the precise leading term of the average number of swaps, the number of
executed Java Bytecode instructions and the number of scanned elements, a new
simple cost measure that approximates I/O costs in the memory hierarchy. We
determine optimal order statistics for each of the cost measures. It turns out
that the asymmetries in Yaroslavskiy's algorithm render pivots with a
systematic skew more efficient than the symmetric choice. Moreover, we finally
have a convincing explanation for the success of Yaroslavskiy's algorithm in
practice: Compared with corresponding versions of classic single-pivot
Quicksort, dual-pivot Quicksort needs significantly less I/Os, both with and
without pivot sampling.
</summary>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <author>
      <name>Markus E. Nebel</name>
    </author>
    <author>
      <name>Conrado Martínez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00453-015-0041-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00453-015-0041-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article is identical (up to typograhical details) to the
  Algorithmica version available from Springerlink (see DOI). It is an extended
  and improved version of our corresponding article at the AofA 2014 conference
  [arXiv:1403.6602]</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.0193v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.0193v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1411.7315">
    <id>http://arxiv.org/abs/1411.7315v3</id>
    <updated>2015-10-19T08:41:23Z</updated>
    <published>2014-11-26T17:57:04Z</published>
    <title>Language Edit Distance &amp; Maximum Likelihood Parsing of Stochastic
  Grammars: Faster Algorithms &amp; Connection to Fundamental Graph Problems</title>
    <summary>  Given a context free language $L(G)$ over alphabet $\Sigma$ and a string $s
\in \Sigma^*$, the language edit distance (Lan-ED) problem seeks the minimum
number of edits (insertions, deletions and substitutions) required to convert
$s$ into a valid member of $L(G)$. The well-known dynamic programming algorithm
solves this problem in $O(n^3)$ time (ignoring grammar size) where $n$ is the
string length [Aho, Peterson 1972, Myers 1985]. Despite its vast number of
applications, there is no algorithm known till date that computes or
approximates Lan-ED in true sub-cubic time.
  In this paper we give the first such algorithm that computes Lan-ED almost
optimally. For any arbitrary $\epsilon > 0$, our algorithm runs in
$\tilde{O}(\frac{n^{\omega}}{poly(\epsilon)})$ time and returns an estimate
within a multiplicative approximation factor of $(1+\epsilon)$, where $\omega$
is the exponent of ordinary matrix multiplication of $n$ dimensional square
matrices. It also computes the edit script. Further, for all substrings of $s$,
we can estimate their Lan-ED within $(1\pm \epsilon)$ factor in
$\tilde{O}(\frac{n^{\omega}}{poly(\epsilon)})$ time with high probability. We
also design the very first sub-cubic ($\tilde{O}(n^\omega)$) algorithm to
handle arbitrary stochastic context free grammar (SCFG) parsing. SCFGs lie at
the foundation of statistical natural language processing, they generalize
hidden Markov models, and have found widespread applications.
  To complement our upper bound result, we show that exact computation of SCFG
parsing, or Lan-ED with insertion as only edit operation in true sub-cubic time
will imply a truly sub-cubic algorithm for all-pairs shortest paths, and hence
to a large range of problems in graphs and matrices. Known lower bound results
on parsing implies no improvement over our time bound of $O(n^\omega)$ is
possible for any nontrivial multiplicative approximation.
</summary>
    <author>
      <name>Barna Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages: This is an updated version of the previous submission
  "Faster Language Edit Distance, Connection to All-pairs Shortest Paths and
  Related Problems". Introduction is rewritten, an error in a previous lower
  bound proof corrected, and the Sidon sequence construction is elaborated</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.7315v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.7315v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1411.7359">
    <id>http://arxiv.org/abs/1411.7359v2</id>
    <updated>2014-12-07T17:36:42Z</updated>
    <published>2014-11-26T20:25:27Z</published>
    <title>Algorithms in the Ultra-Wide Word Model</title>
    <summary>  The effective use of parallel computing resources to speed up algorithms in
current multi-core parallel architectures remains a difficult challenge, with
ease of programming playing a key role in the eventual success of various
parallel architectures. In this paper we consider an alternative view of
parallelism in the form of an ultra-wide word processor. We introduce the
Ultra-Wide Word architecture and model, an extension of the word-RAM model that
allows for constant time operations on thousands of bits in parallel. Word
parallelism as exploited by the word-RAM model does not suffer from the more
difficult aspects of parallel programming, namely synchronization and
concurrency. For the standard word-RAM algorithms, the speedups obtained are
moderate, as they are limited by the word size. We argue that a large class of
word-RAM algorithms can be implemented in the Ultra-Wide Word model, obtaining
speedups comparable to multi-threaded computations while keeping the simplicity
of programming of the sequential RAM model. We show that this is the case by
describing implementations of Ultra-Wide Word algorithms for dynamic
programming and string searching. In addition, we show that the Ultra-Wide Word
model can be used to implement a nonstandard memory architecture, which enables
the sidestepping of lower bounds of important data structure problems such as
priority queues and dynamic prefix sums. While similar ideas about operating on
large words have been mentioned before in the context of multimedia processors
[Thorup 2003], it is only recently that an architecture like the one we propose
has become feasible and that details can be worked out.
</summary>
    <author>
      <name>Arash Farzan</name>
    </author>
    <author>
      <name>Alejandro López-Ortiz</name>
    </author>
    <author>
      <name>Patrick K. Nicholson</name>
    </author>
    <author>
      <name>Alejandro Salinger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 5 figures; minor changes</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.7359v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.7359v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; F.1.2; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1302.4016">
    <id>http://arxiv.org/abs/1302.4016v3</id>
    <updated>2013-07-06T20:34:41Z</updated>
    <published>2013-02-17T00:01:20Z</published>
    <title>Full-fledged Real-Time Indexing for Constant Size Alphabets</title>
    <summary>  In this paper we describe a data structure that supports pattern matching
queries on a dynamically arriving text over an alphabet ofconstant size. Each
new symbol can be prepended to $T$ in O(1) worst-case time. At any moment, we
can report all occurrences of a pattern $P$ in the current text in $O(|P|+k)$
time, where $|P|$ is the length of $P$ and $k$ is the number of occurrences.
This resolves, under assumption of constant-size alphabet, a long-standing open
problem of existence of a real-time indexing method for string matching (see
\cite{AmirN08}).
</summary>
    <author>
      <name>Gregory Kucherov</name>
    </author>
    <author>
      <name>Yakov Nekrich</name>
    </author>
    <link href="http://arxiv.org/abs/1302.4016v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.4016v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1302.3481">
    <id>http://arxiv.org/abs/1302.3481v2</id>
    <updated>2014-01-22T16:15:16Z</updated>
    <published>2013-02-14T17:24:36Z</published>
    <title>One-variable word equations in linear time</title>
    <summary>  In this paper we consider word equations with one variable (and arbitrary
many appearances of it). A recent technique of recompression, which is
applicable to general word equations, is shown to be suitable also in this
case. While in general case it is non-deterministic, it determinises in case of
one variable and the obtained running time is O(n + #_X log n), where #_X is
the number of appearances of the variable in the equation. This matches the
previously-best algorithm due to D\k{a}browski and Plandowski. Then, using a
couple of heuristics as well as more detailed time analysis the running time is
lowered to O(n) in RAM model. Unfortunately no new properties of solutions are
shown.
</summary>
    <author>
      <name>Artur Jeż</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to a journal, general overhaul over the previous version</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.3481v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.3481v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; F.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1302.3437">
    <id>http://arxiv.org/abs/1302.3437v2</id>
    <updated>2013-07-25T16:45:40Z</updated>
    <published>2013-02-14T15:44:49Z</published>
    <title>Modulated String Searching</title>
    <summary>  In his 1987 paper entitled "Generalized String Matching", Abrahamson
introduced {\em pattern matching with character classes} and provided the first
efficient algorithm to solve it. The best known solution to date is due to
Linhart and Shamir (2009).
  Another broad yet comparatively less studied class of string matching
problems is that of numerical string searching, such as, e.g., the `less-than'
or $L_1$-norm string searching. The best known solutions for problems in this
class are based on FFT convolution after some suitable re-encoding.
  The present paper introduces {\em modulated string searching} as a unified
framework for string matching problems where the numerical conditions can be
combined with some Boolean/numerical decision conditions on the character
classes. One example problem in this class is the {\em locally bounded
$L_1$-norm} matching problem on character classes: here the "match" between a
character at some position in the text and a set of characters at some position
in the pattern is assessed based on the smallest $L_1$ distance between the
text character and one of those pattern characters. The two positions "match"
if the (absolute value of the) difference between the two characters does not
exceed a predefined constant. The pattern has an occurrence in an alignment
with the text if the sum of all such differences does not exceed a second
predefined constant value. This problem requires a pointwise evaluation of the
quality of each match and has no known solution based on the previously
mentioned algorithms.
</summary>
    <author>
      <name>Alberto Apostolico</name>
    </author>
    <author>
      <name>Péter L. Erdős</name>
    </author>
    <author>
      <name>István Miklós</name>
    </author>
    <author>
      <name>Johannes Siemons</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2013.10.013</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2013.10.013" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.3437v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.3437v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1302.0072">
    <id>http://arxiv.org/abs/1302.0072v1</id>
    <updated>2013-02-01T04:14:08Z</updated>
    <published>2013-02-01T04:14:08Z</published>
    <title>Dynamic 2D Dictionary Matching in Small Space</title>
    <summary>  The dictionary matching problem preprocesses a set of patterns and finds all
occurrences of each of the patterns in a text when it is provided. We focus on
the dynamic setting, in which patterns can be inserted to and removed from the
dictionary, without reprocessing the entire dictionary. This article presents
the first algorithm that performs \emph{dynamic} dictionary matching on
two-dimensional data within small space. The time complexity of our algorithm
is almost linear. The only slowdown is incurred by querying the compressed
self-index that replaces the dictionary. The dictionary is updated in time
proportional to the size of the pattern that is being inserted to or removed
from the dictionary. Our algorithm is suitable for rectangular patterns that
are of uniform size in one dimension.
</summary>
    <author>
      <name>Shoshana Marcus</name>
    </author>
    <author>
      <name>Dina Sokol</name>
    </author>
    <link href="http://arxiv.org/abs/1302.0072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.0072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1301.7183">
    <id>http://arxiv.org/abs/1301.7183v1</id>
    <updated>2013-01-30T10:03:10Z</updated>
    <published>2013-01-30T10:03:10Z</published>
    <title>A Dynamic Programming Solution to a Generalized LCS Problem</title>
    <summary>  In this paper, we consider a generalized longest common subsequence problem,
the string-excluding constrained LCS problem. For the two input sequences $X$
and $Y$ of lengths $n$ and $m$, and a constraint string $P$ of length $r$, the
problem is to find a common subsequence $Z$ of $X$ and $Y$ excluding $P$ as a
substring and the length of $Z$ is maximized. The problem and its solution were
first proposed by Chen and Chao\cite{1}, but we found that their algorithm can
not solve the problem correctly. A new dynamic programming solution for the
STR-EC-LCS problem is then presented in this paper. The correctness of the new
algorithm is proved. The time complexity of the new algorithm is $O(nmr)$.
</summary>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Xiaodong Wang</name>
    </author>
    <author>
      <name>Yingjie Wu</name>
    </author>
    <author>
      <name>Daxin Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1301.7183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.7183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1301.6428">
    <id>http://arxiv.org/abs/1301.6428v1</id>
    <updated>2013-01-28T02:25:33Z</updated>
    <published>2013-01-28T02:25:33Z</published>
    <title>Engineering Small Space Dictionary Matching</title>
    <summary>  The dictionary matching problem is to locate occurrences of any pattern among
a set of patterns in a given text. Massive data sets abound and at the same
time, there are many settings in which working space is extremely limited. We
introduce dictionary matching software for the space-constrained environment
whose running time is close to linear. We use the compressed suffix tree as the
underlying data structure of our algorithm, thus, the working space of our
algorithm is proportional to the optimal compression of the dictionary. We also
contribute a succinct tool for performing constant-time lowest marked ancestor
queries on a tree that is succinctly encoded as a sequence of balanced
parentheses, with linear time preprocessing of the tree. This tool should be
useful in many other applications. Our source code is available at
http://www.sci.brooklyn.cuny.edu/~sokol/dictmatch.html
</summary>
    <author>
      <name>Shoshana Marcus Dina Sokol</name>
    </author>
    <link href="http://arxiv.org/abs/1301.6428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.6428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1301.5842">
    <id>http://arxiv.org/abs/1301.5842v3</id>
    <updated>2013-11-07T09:42:13Z</updated>
    <published>2013-01-24T17:03:41Z</published>
    <title>Approximation of grammar-based compression via recompression</title>
    <summary>  In this paper we present a simple linear-time algorithm constructing a
context-free grammar of size O(g log(N/g)) for the input string, where N is the
size of the input string and g the size of the optimal grammar generating this
string. The algorithm works for arbitrary size alphabets, but the running time
is linear assuming that the alphabet \Sigma of the input string can be
identified with numbers from {1, ..., N^c} for some constant c. Otherwise,
additional cost of O(n log|\Sigma|) is needed.
  Algorithms with such approximation guarantees and running time are known, the
novelty of this paper is a particular simplicity of the algorithm as well as
the analysis of the algorithm, which uses a general technique of recompression
recently introduced by the author. Furthermore, contrary to the previous
results, this work does not use the LZ representation of the input string in
the construction, nor in the analysis.
</summary>
    <author>
      <name>Artur Jeż</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, some many small improvements, to be submited to a journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.5842v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.5842v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; F.4.2; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1301.2495">
    <id>http://arxiv.org/abs/1301.2495v1</id>
    <updated>2013-01-11T13:44:27Z</updated>
    <published>2013-01-11T13:44:27Z</published>
    <title>A simple online competitive adaptation of Lempel-Ziv compression with
  efficient random access support</title>
    <summary>  We present a simple adaptation of the Lempel Ziv 78' (LZ78) compression
scheme ({\em IEEE Transactions on Information Theory, 1978}) that supports
efficient random access to the input string. Namely, given query access to the
compressed string, it is possible to efficiently recover any symbol of the
input string. The compression algorithm is given as input a parameter $\eps
>0$, and with very high probability increases the length of the compressed
string by at most a factor of $(1+\eps)$. The access time is $O(\log n +
1/\eps^2)$ in expectation, and $O(\log n/\eps^2)$ with high probability. The
scheme relies on sparse transitive-closure spanners. Any (consecutive)
substring of the input string can be retrieved at an additional additive cost
in the running time of the length of the substring. We also formally establish
the necessity of modifying LZ78 so as to allow efficient random access.
Specifically, we construct a family of strings for which $\Omega(n/\log n)$
queries to the LZ78-compressed string are required in order to recover a single
symbol in the input string. The main benefit of the proposed scheme is that it
preserves the online nature and simplicity of LZ78, and that for {\em every}
input string, the length of the compressed string is only a small factor larger
than that obtained by running LZ78.
</summary>
    <author>
      <name>Akashnil Dutta</name>
    </author>
    <author>
      <name>Reut Levi</name>
    </author>
    <author>
      <name>Dana Ron</name>
    </author>
    <author>
      <name>Ronitt Rubinfeld</name>
    </author>
    <link href="http://arxiv.org/abs/1301.2495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1301.0114">
    <id>http://arxiv.org/abs/1301.0114v1</id>
    <updated>2013-01-01T18:29:39Z</updated>
    <published>2013-01-01T18:29:39Z</published>
    <title>Tree-based Arithmetic and Compressed Representations of Giant Numbers</title>
    <summary>  Can we do arithmetic in a completely different way, with a radically
different data structure? Could this approach provide practical benefits, like
operations on giant numbers while having an average performance similar to
traditional bitstring representations?
  While answering these questions positively, our tree based representation
described in this paper comes with a few extra benefits: it compresses giant
numbers such that, for instance, the largest known prime number as well as its
related perfect number are represented as trees of small sizes. The same also
applies to Fermat numbers and important computations like exponentiation of two
become constant time operations.
  At the same time, succinct representations of sparse sets, multisets and
sequences become possible through bijections to our tree-represented natural
numbers.
</summary>
    <author>
      <name>Paul Tarau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">UNPUBLISHED DRAFT, 26 pages, 2 figures, literate Haskell code
  included</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.0114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1301.0103">
    <id>http://arxiv.org/abs/1301.0103v1</id>
    <updated>2013-01-01T16:24:46Z</updated>
    <published>2013-01-01T16:24:46Z</published>
    <title>2D Lyndon Words and Applications</title>
    <summary>  A Lyndon word is a primitive string which is lexicographically smallest among
cyclic permutations of its characters. Lyndon words are used for constructing
bases in free Lie algebras, constructing de Bruijn sequences, finding the
lexicographically smallest or largest substring in a string, and succinct
suffix-prefix matching of highly periodic strings. In this paper, we extend the
concept of the Lyndon word to two dimensions. We introduce the 2D Lyndon word
and use it to capture 2D horizontal periodicity of a matrix in which each row
is highly periodic, and to efficiently solve 2D horizontal suffix-prefix
matching among a set of patterns. This yields a succinct and efficient
algorithm for 2D dictionary matching.
  We present several algorithms that compute the 2D Lyndon word that represents
a matrix. The final algorithm achieves linear time complexity even when the
least common multiple of the periods of the rows is exponential in the matrix
width.
</summary>
    <author>
      <name>Shoshana Marcus</name>
    </author>
    <author>
      <name>Dina Sokol</name>
    </author>
    <link href="http://arxiv.org/abs/1301.0103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1305.1157">
    <id>http://arxiv.org/abs/1305.1157v1</id>
    <updated>2013-05-06T12:03:56Z</updated>
    <published>2013-05-06T12:03:56Z</published>
    <title>Parallel String Sample Sort</title>
    <summary>  We discuss how string sorting algorithms can be parallelized on modern
multi-core shared memory machines. As a synthesis of the best sequential string
sorting algorithms and successful parallel sorting algorithms for atomic
objects, we propose string sample sort. The algorithm makes effective use of
the memory hierarchy, uses additional word level parallelism, and largely
avoids branch mispredictions. Additionally, we parallelize variants of multikey
quicksort and radix sort that are also useful in certain situations.
</summary>
    <author>
      <name>Timo Bingmann</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 7 figures and 12 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.1157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; E.5; D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1304.7355">
    <id>http://arxiv.org/abs/1304.7355v2</id>
    <updated>2013-05-01T10:28:49Z</updated>
    <published>2013-04-27T10:51:34Z</published>
    <title>Web graph compression with fast access</title>
    <summary>  In recent years studying the content of the World Wide Web became a very
important yet rather difficult task. There is a need for a compression
technique that would allow a web graph representation to be put into the memory
while maintaining random access time competitive to the time needed to access
uncompressed web graph on a hard drive.
  There are already available techniques that accomplish this task, but there
is still room for improvements and this thesis attempts to prove it. It
includes a comparison of two methods contained in state of art of this field
(BV and k2partitioned) to two already implemented algorithms (rewritten,
however, in C++ programming language to maximize speed and resource management
efficiency), which are LM and 2D, and introduces the new variant of the latter
one, called 2D stripes.
  This thesis serves as well as a proof of concept. The final considerations
show positive and negative aspects of all presented methods, expose the
feasibility of the new variant as well as indicate future direction for
development.
</summary>
    <author>
      <name>Filip Proborszcz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MSc thesis, May 2012, advisors: Szymon Grabowski, Wojciech Bieniecki;
  65 pages, 16 figures, 6 tables, 5 code snippets, source code available at:
  http://sourceforge.net/projects/webgraphcompres/files/</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.7355v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7355v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1304.0988">
    <id>http://arxiv.org/abs/1304.0988v3</id>
    <updated>2015-02-13T16:38:39Z</updated>
    <published>2013-04-03T15:37:50Z</published>
    <title>Average Case and Distributional Analysis of Dual-Pivot Quicksort</title>
    <summary>  In 2009, Oracle replaced the long-serving sorting algorithm in its Java 7
runtime library by a new dual-pivot Quicksort variant due to Vladimir
Yaroslavskiy. The decision was based on the strikingly good performance of
Yaroslavskiy's implementation in running time experiments. At that time, no
precise investigations of the algorithm were available to explain its superior
performance - on the contrary: Previous theoretical studies of other dual-pivot
Quicksort variants even discouraged the use of two pivots. Only in 2012, two of
the authors gave an average case analysis of a simplified version of
Yaroslavskiy's algorithm, proving that savings in the number of comparisons are
possible. However, Yaroslavskiy's algorithm needs more swaps, which renders the
analysis inconclusive.
  To force the issue, we herein extend our analysis to the fully detailed style
of Knuth: We determine the exact number of executed Java Bytecode instructions.
Surprisingly, Yaroslavskiy's algorithm needs sightly more Bytecode instructions
than a simple implementation of classic Quicksort - contradicting observed
running times. Like in Oracle's library implementation we incorporate the use
of Insertionsort on small subproblems and show that it indeed speeds up
Yaroslavskiy's Quicksort in terms of Bytecodes; but even with optimal
Insertionsort thresholds the new Quicksort variant needs slightly more Bytecode
instructions on average.
  Finally, we show that the (suitably normalized) costs of Yaroslavskiy's
algorithm converge to a random variable whose distribution is characterized by
a fixed-point equation. From that, we compute variances of costs and show that
for large n, costs are concentrated around their mean.
</summary>
    <author>
      <name>Sebastian Wild</name>
    </author>
    <author>
      <name>Markus E. Nebel</name>
    </author>
    <author>
      <name>Ralph Neininger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2629340</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2629340" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v3 is content-wise identical to TALG version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Algorithms 11, 3, Article 22 (Jan 2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.0988v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.0988v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; G.2.1; G.3; F.2.3; D.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1304.0917">
    <id>http://arxiv.org/abs/1304.0917v3</id>
    <updated>2013-06-15T02:13:38Z</updated>
    <published>2013-04-03T11:13:00Z</published>
    <title>A Succinct Grammar Compression</title>
    <summary>  We solve an open problem related to an optimal encoding of a straight line
program (SLP), a canonical form of grammar compression deriving a single string
deterministically. We show that an information-theoretic lower bound for
representing an SLP with n symbols requires at least 2n+logn!+o(n) bits. We
then present a succinct representation of an SLP; this representation is
asymptotically equivalent to the lower bound. The space is at most 2n log
{rho}(1 + o(1)) bits for rho leq 2sqrt{n}, while supporting random access to
any production rule of an SLP in O(log log n) time. In addition, we present a
novel dynamic data structure associating a digram with a unique symbol. Such a
data structure is called a naming function and has been implemented using a
hash table that has a space-time tradeoff. Thus, the memory space is mainly
occupied by the hash table during the development of production rules.
Alternatively, we build a dynamic data structure for the naming function by
leveraging the idea behind the wavelet tree. The space is strictly bounded by
2n log n(1 + o(1)) bits, while supporting O(log n) query and update time.
</summary>
    <author>
      <name>Yasuo Tabei</name>
    </author>
    <author>
      <name>Yoshimasa Takabatake</name>
    </author>
    <author>
      <name>Hiroshi Sakamoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper is accepted to 24th Annual Symposium on Combinatorial
  Pattern Matching (CPM2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.0917v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.0917v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1303.6481">
    <id>http://arxiv.org/abs/1303.6481v1</id>
    <updated>2013-03-26T13:28:31Z</updated>
    <published>2013-03-26T13:28:31Z</published>
    <title>Large-Scale Pattern Search Using Reduced-Space On-Disk Suffix Arrays</title>
    <summary>  The suffix array is an efficient data structure for in-memory pattern search.
Suffix arrays can also be used for external-memory pattern search, via
two-level structures that use an internal index to identify the correct block
of suffix pointers. In this paper we describe a new two-level suffix
array-based index structure that requires significantly less disk space than
previous approaches. Key to the saving is the use of disk blocks that are based
on prefixes rather than the more usual uniform-sampling approach, allowing
reductions between blocks and subparts of other blocks. We also describe a new
in-memory structure based on a condensed BWT string, and show that it allows
common patterns to be resolved without access to the text. Experiments using 64
GB of English web text and a laptop computer with just 4 GB of main memory
demonstrate the speed and versatility of the new approach. For this data the
index is around one- third the size of previous two-level mechanisms; and the
memory footprint of as little as 1% of the text size means that queries can be
processed more quickly than is possible with a compact FM-INDEX.
</summary>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Alistair Moffat</name>
    </author>
    <author>
      <name>J. Shane Culpepper</name>
    </author>
    <author>
      <name>Andrew Turpin</name>
    </author>
    <author>
      <name>Anthony Wirth</name>
    </author>
    <link href="http://arxiv.org/abs/1303.6481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.6481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1303.5217">
    <id>http://arxiv.org/abs/1303.5217v3</id>
    <updated>2015-10-13T13:21:52Z</updated>
    <published>2013-03-21T10:20:51Z</published>
    <title>Optimal Partitioning for Dual-Pivot Quicksort</title>
    <summary>  Dual-pivot quicksort refers to variants of classical quicksort where in the
partitioning step two pivots are used to split the input into three segments.
This can be done in different ways, giving rise to different algorithms.
Recently, a dual-pivot algorithm proposed by Yaroslavskiy received much
attention, because a variant of it replaced the well-engineered quicksort
algorithm in Sun's Java 7 runtime library. Nebel and Wild (ESA 2012) analyzed
this algorithm and showed that on average it uses 1.9n ln n + O(n) comparisons
to sort an input of size n, beating standard quicksort, which uses 2n ln n +
O(n) comparisons. We introduce a model that captures all dual-pivot algorithms,
give a unified analysis, and identify new dual-pivot algorithms that minimize
the average number of key comparisons among all possible algorithms up to a
linear term. This minimum is 1.8n ln n + O(n). For the case that the pivots are
chosen from a small sample, we include a comparison of dual-pivot quicksort and
classical quicksort. Specifically, we show that dual-pivot quicksort benefits
from a skewed choice of pivots. We experimentally evaluate our algorithms and
compare them to Yaroslavskiy's algorithm and the recently described three-pivot
quicksort algorithm of Kushagra et al. (ALENEX 2014).
</summary>
    <author>
      <name>Martin Aumüller</name>
    </author>
    <author>
      <name>Martin Dietzfelbinger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in ACM Transactions on Algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.5217v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.5217v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1303.3692">
    <id>http://arxiv.org/abs/1303.3692v4</id>
    <updated>2015-05-03T20:03:29Z</updated>
    <published>2013-03-15T07:00:14Z</published>
    <title>Ultra-fast Multiple Genome Sequence Matching Using GPU</title>
    <summary>  In this paper, a contrastive evaluation of massively parallel implementations
of suffix tree and suffix array to accelerate genome sequence matching are
proposed based on Intel Core i7 3770K quad-core and NVIDIA GeForce GTX680 GPU.
Besides suffix array only held approximately 20%~30% of the space relative to
suffix tree, the coalesced binary search and tile optimization make suffix
array clearly outperform suffix tree using GPU. Consequently, the experimental
results show that multiple genome sequence matching based on suffix array is
more than 99 times speedup than that of CPU serial implementation. There is no
doubt that massively parallel matching algorithm based on suffix array is an
efficient approach to high-performance bioinformatics applications.
</summary>
    <author>
      <name>Gang Liao</name>
    </author>
    <author>
      <name>Qi Sun</name>
    </author>
    <author>
      <name>Longfei Ma</name>
    </author>
    <author>
      <name>Sha Ding</name>
    </author>
    <author>
      <name>Wen Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 2013 International Conference on High Performance Computing &amp;
  Simulation (ACM/IEEE HPCS 2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.3692v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.3692v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1303.1872">
    <id>http://arxiv.org/abs/1303.1872v1</id>
    <updated>2013-03-08T02:15:59Z</updated>
    <published>2013-03-08T02:15:59Z</published>
    <title>An Efficient Dynamic Programming Algorithm for the Generalized LCS
  Problem with Multiple Substring Exclusion Constrains</title>
    <summary>  In this paper, we consider a generalized longest common subsequence problem
with multiple substring exclusion constrains. For the two input sequences $X$
and $Y$ of lengths $n$ and $m$, and a set of $d$ constrains $P=\{P_1,...,P_d\}$
of total length $r$, the problem is to find a common subsequence $Z$ of $X$ and
$Y$ excluding each of constrain string in $P$ as a substring and the length of
$Z$ is maximized. The problem was declared to be NP-hard\cite{1}, but we
finally found that this is not true. A new dynamic programming solution for
this problem is presented in this paper. The correctness of the new algorithm
is proved. The time complexity of our algorithm is $O(nmr)$.
</summary>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Xiaodong Wang</name>
    </author>
    <author>
      <name>Yingjie Wu</name>
    </author>
    <author>
      <name>Daxin Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1301.7183</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.1872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.1872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1302.7278">
    <id>http://arxiv.org/abs/1302.7278v2</id>
    <updated>2013-05-21T15:25:19Z</updated>
    <published>2013-02-28T18:35:21Z</published>
    <title>Using cascading Bloom filters to improve the memory usage for de Brujin
  graphs</title>
    <summary>  De Brujin graphs are widely used in bioinformatics for processing
next-generation sequencing data. Due to a very large size of NGS datasets, it
is essential to represent de Bruijn graphs compactly, and several approaches to
this problem have been proposed recently. In this work, we show how to reduce
the memory required by the algorithm of [3] that represents de Brujin graphs
using Bloom filters. Our method requires 30% to 40% less memory with respect to
the method of [3], with insignificant impact to construction time. At the same
time, our experiments showed a better query time compared to [3]. This is, to
our knowledge, the best practical representation for de Bruijn graphs.
</summary>
    <author>
      <name>Kamil Salikhov</name>
    </author>
    <author>
      <name>Gustavo Sacomoto</name>
    </author>
    <author>
      <name>Gregory Kucherov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.7278v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.7278v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.2; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1302.5851">
    <id>http://arxiv.org/abs/1302.5851v1</id>
    <updated>2013-02-23T22:39:03Z</updated>
    <published>2013-02-23T22:39:03Z</published>
    <title>Parallel Suffix Array Construction by Accelerated Sampling</title>
    <summary>  A deterministic BSP algorithm for constructing the suffix array of a given
string is presented, based on a technique which we call accelerated sampling.
It runs in optimal O(n/p) local computation and communication, and requires a
near optimal O(log log p) synchronisation steps. The algorithm provides an
improvement over the synchronisation costs of existing algorithms, and
reinforces the importance of the sampling technique.
</summary>
    <author>
      <name>Matthew Felice Pace</name>
    </author>
    <author>
      <name>Alexander Tiskin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.5851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.2; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.0099">
    <id>http://arxiv.org/abs/1307.0099v1</id>
    <updated>2013-06-29T12:53:49Z</updated>
    <published>2013-06-29T12:53:49Z</published>
    <title>On a compact encoding of the swap automaton</title>
    <summary>  Given a string $P$ of length $m$ over an alphabet $\Sigma$ of size $\sigma$,
a swapped version of $P$ is a string derived from $P$ by a series of local
swaps, i.e., swaps of adjacent symbols, such that each symbol can participate
in at most one swap. We present a theoretical analysis of the nondeterministic
finite automaton for the language $\bigcup_{P'\in\Pi_P}\Sigma^*P'$ (swap
automaton for short), where $\Pi_P$ is the set of swapped versions of $P$. Our
study is based on the bit-parallel simulation of the same automaton due to
Fredriksson, and reveals an interesting combinatorial property that links the
automaton to the one for the language $\Sigma^*P$. By exploiting this property
and the method presented by Cantone et al. (2010), we obtain a bit-parallel
encoding of the swap automaton which takes $O(\sigma^2\ceil{k/w})$ space and
allows one to simulate the automaton on a string of length $n$ in time
$O(n\ceil{k/w})$, where $\ceil{m/\sigma}\le k\le m$.
</summary>
    <author>
      <name>Kimmo Fredriksson</name>
    </author>
    <author>
      <name>Emanuele Giaquinta</name>
    </author>
    <link href="http://arxiv.org/abs/1307.0099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.0099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1306.4627">
    <id>http://arxiv.org/abs/1306.4627v1</id>
    <updated>2013-06-19T17:45:32Z</updated>
    <published>2013-06-19T17:45:32Z</published>
    <title>Parallel Algorithm for Longest Common Subsequence in a String</title>
    <summary>  In the area of Pattern Recognition and Matching, finding a Longest Common
Subsequence plays an important role. In this paper, we have proposed one
algorithm based on parallel computation. We have used OpenMP API package as
middleware to send the data to different processors. We have tested our
algorithm in a system having four processors and 2 GB physical memory. The best
result showed that the parallel algorithm increases the performance (speed of
computation) by 3.22.
</summary>
    <author>
      <name>Tirtharaj Dash</name>
    </author>
    <author>
      <name>Tanistha Nayak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">appeared in: Proceedings of National Conference on Artificial
  Intelligence, Robotics and Embedded Systems (AIRES) - 2012, Andhra
  University, Visakhapatnam (29-30 June, 2012), pp. 66-69</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.4627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.4627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1306.3772">
    <id>http://arxiv.org/abs/1306.3772v1</id>
    <updated>2013-06-17T08:47:05Z</updated>
    <published>2013-06-17T08:47:05Z</published>
    <title>Minimal Indices for Successor Search</title>
    <summary>  We give a new successor data structure which improves upon the index size of
the P\v{a}tra\c{s}cu-Thorup data structures, reducing the index size from $O(n
w^{4/5})$ bits to $O(n \log w)$ bits, with optimal probe complexity.
Alternatively, our new data structure can be viewed as matching the space
complexity of the (probe-suboptimal) $z$-fast trie of Belazzougui et al. Thus,
we get the best of both approaches with respect to both probe count and index
size. The penalty we pay is an extra $O(\log w)$ inter-register operations. Our
data structure can also be used to solve the weak prefix search problem, the
index size of $O(n \log w)$ bits is known to be optimal for any such data
structure.
  The technical contributions include highly efficient single word indices,
with out-degree $w/\log w$ (compared to the $w^{1/5}$ out-degree of fusion tree
based indices). To construct such high efficiency single word indices we device
highly efficient bit selectors which, we believe, are of independent interest.
</summary>
    <author>
      <name>Sarel Cohen</name>
    </author>
    <author>
      <name>Amos Fiat</name>
    </author>
    <author>
      <name>Moshik Hershcovitch</name>
    </author>
    <author>
      <name>Haim Kaplan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, full version, extended abstract submitted to MFCS 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.3772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.3772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1306.3482">
    <id>http://arxiv.org/abs/1306.3482v1</id>
    <updated>2013-06-14T18:50:47Z</updated>
    <published>2013-06-14T18:50:47Z</published>
    <title>Set-Difference Range Queries</title>
    <summary>  We introduce the problem of performing set-difference range queries, where
answers to queries are set-theoretic symmetric differences between sets of
items in two geometric ranges. We describe a general framework for answering
such queries based on a novel use of data-streaming sketches we call signed
symmetric-difference sketches. We show that such sketches can be realized using
invertible Bloom filters (IBFs), which can be composed, differenced, and
searched so as to solve set-difference range queries in a wide range of
scenarios.
</summary>
    <author>
      <name>David Eppstein</name>
    </author>
    <author>
      <name>Michael T. Goodrich</name>
    </author>
    <author>
      <name>Joseph A. Simons</name>
    </author>
    <link href="http://arxiv.org/abs/1306.3482v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.3482v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1306.2483">
    <id>http://arxiv.org/abs/1306.2483v2</id>
    <updated>2014-07-07T14:03:33Z</updated>
    <published>2013-06-11T10:42:56Z</published>
    <title>Motif matching using gapped patterns</title>
    <summary>  We present new algorithms for the problem of multiple string matching of
gapped patterns, where a gapped pattern is a sequence of strings such that
there is a gap of fixed length between each two consecutive strings. The
problem has applications in the discovery of transcription factor binding sites
in DNA sequences when using generalized versions of the Position Weight Matrix
model to describe transcription factor specificities. In these models a motif
can be matched as a set of gapped patterns with unit-length keywords. The
existing algorithms for matching a set of gapped patterns are worst-case
efficient but not practical, or vice versa, in this particular case. The novel
algorithms that we present are based on dynamic programming and
bit-parallelism, and lie in a middle-ground among the existing algorithms. In
fact, their time complexity is close to the best existing bound and, yet, they
are also practical. We also provide experimental results which show that the
presented algorithms are fast in practice, and preferable if all the strings in
the patterns have unit-length.
</summary>
    <author>
      <name>Emanuele Giaquinta</name>
    </author>
    <author>
      <name>Kimmo Fredriksson</name>
    </author>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Alexandru I. Tomescu</name>
    </author>
    <author>
      <name>Esko Ukkonen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tcs.2014.06.032</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tcs.2014.06.032" rel="related"/>
    <link href="http://arxiv.org/abs/1306.2483v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.2483v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1306.1366">
    <id>http://arxiv.org/abs/1306.1366v1</id>
    <updated>2013-06-06T10:27:22Z</updated>
    <published>2013-06-06T10:27:22Z</published>
    <title>Sorting suffixes of a text via its Lyndon Factorization</title>
    <summary>  The process of sorting the suffixes of a text plays a fundamental role in
Text Algorithms. They are used for instance in the constructions of the
Burrows-Wheeler transform and the suffix array, widely used in several fields
of Computer Science. For this reason, several recent researches have been
devoted to finding new strategies to obtain effective methods for such a
sorting. In this paper we introduce a new methodology in which an important
role is played by the Lyndon factorization, so that the local suffixes inside
factors detected by this factorization keep their mutual order when extended to
the suffixes of the whole word. This property suggests a versatile technique
that easily can be adapted to different implementative scenarios.
</summary>
    <author>
      <name>Sabrina Mantaci</name>
    </author>
    <author>
      <name>Antonio Restivo</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the Prague Stringology Conference 2013 (PSC 2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.1366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.1366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1306.0615">
    <id>http://arxiv.org/abs/1306.0615v1</id>
    <updated>2013-06-03T22:37:43Z</updated>
    <published>2013-06-03T22:37:43Z</published>
    <title>Orthogonal Range Searching for Text Indexing</title>
    <summary>  Text indexing, the problem in which one desires to preprocess a (usually
large) text for future (shorter) queries, has been researched ever since the
suffix tree was invented in the early 70's. With textual data continuing to
increase and with changes in the way it is accessed, new data structures and
new algorithmic methods are continuously required. Therefore, text indexing is
of utmost importance and is a very active research domain.
  Orthogonal range searching, classically associated with the computational
geometry community, is one of the tools that has increasingly become important
for various text indexing applications. Initially, in the mid 90's there were a
couple of results recognizing this connection. In the last few years we have
seen an increase in use of this method and are reaching a deeper understanding
of the range searching uses for text indexing.
  In this monograph we survey some of these results.
</summary>
    <author>
      <name>Moshe Lewenstein</name>
    </author>
    <link href="http://arxiv.org/abs/1306.0615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.0615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1305.4883">
    <id>http://arxiv.org/abs/1305.4883v1</id>
    <updated>2013-05-21T17:04:21Z</updated>
    <published>2013-05-21T17:04:21Z</published>
    <title>Repetition-free longest common subsequence of random sequences</title>
    <summary>  A repetition free Longest Common Subsequence (LCS) of two sequences x and y
is an LCS of x and y where each symbol may appear at most once. Let R denote
the length of a repetition free LCS of two sequences of n symbols each one
chosen randomly, uniformly, and independently over a k-ary alphabet. We study
the asymptotic, in n and k, behavior of R and establish that there are three
distinct regimes, depending on the relative speed of growth of n and k. For
each regime we establish the limiting behavior of R. In fact, we do more, since
we actually establish tail bounds for large deviations of R from its limiting
behavior.
  Our study is motivated by the so called exemplar model proposed by Sankoff
(1999) and the related similarity measure introduced by Adi et al. (2007). A
natural question that arises in this context, which as we show is related to
long standing open problems in the area of probabilistic combinatorics, is to
understand the asymptotic, in n and k, behavior of parameter R.
</summary>
    <author>
      <name>Marcos Kiwi</name>
    </author>
    <author>
      <name>Cristina G. Fernandes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.4883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.4883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05A19, 05A15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1305.1744">
    <id>http://arxiv.org/abs/1305.1744v1</id>
    <updated>2013-05-08T08:36:07Z</updated>
    <published>2013-05-08T08:36:07Z</published>
    <title>Suffix Tree of Alignment: An Efficient Index for Similar Data</title>
    <summary>  We consider an index data structure for similar strings. The generalized
suffix tree can be a solution for this. The generalized suffix tree of two
strings $A$ and $B$ is a compacted trie representing all suffixes in $A$ and
$B$. It has $|A|+|B|$ leaves and can be constructed in $O(|A|+|B|)$ time.
However, if the two strings are similar, the generalized suffix tree is not
efficient because it does not exploit the similarity which is usually
represented as an alignment of $A$ and $B$.
  In this paper we propose a space/time-efficient suffix tree of alignment
which wisely exploits the similarity in an alignment. Our suffix tree for an
alignment of $A$ and $B$ has $|A| + l_d + l_1$ leaves where $l_d$ is the sum of
the lengths of all parts of $B$ different from $A$ and $l_1$ is the sum of the
lengths of some common parts of $A$ and $B$. We did not compromise the pattern
search to reduce the space. Our suffix tree can be searched for a pattern $P$
in $O(|P|+occ)$ time where $occ$ is the number of occurrences of $P$ in $A$ and
$B$. We also present an efficient algorithm to construct the suffix tree of
alignment. When the suffix tree is constructed from scratch, the algorithm
requires $O(|A| + l_d + l_1 + l_2)$ time where $l_2$ is the sum of the lengths
of other common substrings of $A$ and $B$. When the suffix tree of $A$ is
already given, it requires $O(l_d + l_1 + l_2)$ time.
</summary>
    <author>
      <name>Joong Chae Na</name>
    </author>
    <author>
      <name>Heejin Park</name>
    </author>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Jan Holub</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Laurent Mouchard</name>
    </author>
    <author>
      <name>Kunsoo Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.1744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1305.0160">
    <id>http://arxiv.org/abs/1305.0160v1</id>
    <updated>2013-05-01T12:51:45Z</updated>
    <published>2013-05-01T12:51:45Z</published>
    <title>Lightweight LCP Construction for Next-Generation Sequencing Datasets</title>
    <summary>  The advent of "next-generation" DNA sequencing (NGS) technologies has meant
that collections of hundreds of millions of DNA sequences are now commonplace
in bioinformatics. Knowing the longest common prefix array (LCP) of such a
collection would facilitate the rapid computation of maximal exact matches,
shortest unique substrings and shortest absent words. CPU-efficient algorithms
for computing the LCP of a string have been described in the literature, but
require the presence in RAM of large data structures. This prevents such
methods from being feasible for NGS datasets.
  In this paper we propose the first lightweight method that simultaneously
computes, via sequential scans, the LCP and BWT of very large collections of
sequences. Computational results on collections as large as 800 million
100-mers demonstrate that our algorithm scales to the vast sequence collections
encountered in human whole genome sequencing experiments.
</summary>
    <author>
      <name>Markus J. Bauer</name>
    </author>
    <author>
      <name>Anthony J. Cox</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <author>
      <name>Marinella Sciortino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-33122-0_26</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-33122-0_26" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Springer LNCS (Lecture Notes in Computer Science) should be
  considered as the original place of publication, please cite accordingly. The
  final version of this manuscript is available at
  http://link.springer.com/chapter/10.1007/978-3-642-33122-0_26</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science Volume 7534, 2012, pp 326-337</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.0160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.0160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.5296">
    <id>http://arxiv.org/abs/1307.5296v2</id>
    <updated>2013-10-07T20:24:32Z</updated>
    <published>2013-07-19T17:56:24Z</published>
    <title>First-Come-First-Served for Online Slot Allocation and Huffman Coding</title>
    <summary>  Can one choose a good Huffman code on the fly, without knowing the underlying
distribution? Online Slot Allocation (OSA) models this and similar problems:
There are n slots, each with a known cost. There are n items. Requests for
items are drawn i.i.d. from a fixed but hidden probability distribution p.
After each request, if the item, i, was not previously requested, then the
algorithm (knowing the slot costs and the requests so far, but not p) must
place the item in some vacant slot j(i). The goal is to minimize the sum, over
the items, of the probability of the item times the cost of its assigned slot.
  The optimal offline algorithm is trivial: put the most probable item in the
cheapest slot, the second most probable item in the second cheapest slot, etc.
The optimal online algorithm is First Come First Served (FCFS): put the first
requested item in the cheapest slot, the second (distinct) requested item in
the second cheapest slot, etc. The optimal competitive ratios for any online
algorithm are 1+H(n-1) ~ ln n for general costs and 2 for concave costs. For
logarithmic costs, the ratio is, asymptotically, 1: FCFS gives cost opt + O(log
opt).
  For Huffman coding, FCFS yields an online algorithm (one that allocates
codewords on demand, without knowing the underlying probability distribution)
that guarantees asymptotically optimal cost: at most opt + 2 log(1+opt) + 2.
</summary>
    <author>
      <name>Monik Khare</name>
    </author>
    <author>
      <name>Claire Mathieu</name>
    </author>
    <author>
      <name>Neal E. Young</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM-SIAM Symposium on Discrete Algorithms (SODA) 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.5296v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.5296v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W40, 68Q87" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.2; F.2.0; H.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.3699">
    <id>http://arxiv.org/abs/1307.3699v1</id>
    <updated>2013-07-14T04:32:12Z</updated>
    <published>2013-07-14T04:32:12Z</published>
    <title>Statistically-secure ORAM with $\tilde{O}(\log^2 n)$ Overhead</title>
    <summary>  We demonstrate a simple, statistically secure, ORAM with computational
overhead $\tilde{O}(\log^2 n)$; previous ORAM protocols achieve only
computational security (under computational assumptions) or require
$\tilde{\Omega}(\log^3 n)$ overheard. An additional benefit of our ORAM is its
conceptual simplicity, which makes it easy to implement in both software and
(commercially available) hardware.
  Our construction is based on recent ORAM constructions due to Shi, Chan,
Stefanov, and Li (Asiacrypt 2011) and Stefanov and Shi (ArXiv 2012), but with
some crucial modifications in the algorithm that simplifies the ORAM and enable
our analysis. A central component in our analysis is reducing the analysis of
our algorithm to a "supermarket" problem; of independent interest (and of
importance to our analysis,) we provide an upper bound on the rate of "upset"
customers in the "supermarket" problem.
</summary>
    <author>
      <name>Kai-Min Chung</name>
    </author>
    <author>
      <name>Zhenming Liu</name>
    </author>
    <author>
      <name>Rafael Pass</name>
    </author>
    <link href="http://arxiv.org/abs/1307.3699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.3073">
    <id>http://arxiv.org/abs/1307.3073v2</id>
    <updated>2013-10-31T10:30:13Z</updated>
    <published>2013-07-11T11:50:48Z</published>
    <title>Finding small patterns in permutations in linear time</title>
    <summary>  Given two permutations $\sigma$ and $\pi$, the \textsc{Permutation Pattern}
problem asks if $\sigma$ is a subpattern of $\pi$. We show that the problem can
be solved in time $2^{O(\ell^2\log \ell)}\cdot n$, where $\ell=|\sigma|$ and
$n=|\pi|$. In other words, the problem is fixed-parameter tractable
parameterized by the size of the subpattern to be found.
  We introduce a novel type of decompositions for permutations and a
corresponding width measure. We present a linear-time algorithm that either
finds $\sigma$ as a subpattern of $\pi$, or finds a decomposition of $\pi$
whose width is bounded by a function of $|\sigma|$. Then we show how to solve
the \textsc{Permutation Pattern} problem in linear time if a bounded-width
decomposition is given in the input.
</summary>
    <author>
      <name>Sylvain Guillemot</name>
    </author>
    <author>
      <name>Dániel Marx</name>
    </author>
    <link href="http://arxiv.org/abs/1307.3073v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3073v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.3033">
    <id>http://arxiv.org/abs/1307.3033v1</id>
    <updated>2013-07-11T09:46:33Z</updated>
    <published>2013-07-11T09:46:33Z</published>
    <title>QuickXsort: Efficient Sorting with n log n - 1.399n +o(n) Comparisons on
  Average</title>
    <summary>  In this paper we generalize the idea of QuickHeapsort leading to the notion
of QuickXsort. Given some external sorting algorithm X, QuickXsort yields an
internal sorting algorithm if X satisfies certain natural conditions.
  With QuickWeakHeapsort and QuickMergesort we present two examples for the
QuickXsort-construction. Both are efficient algorithms that incur approximately
n log n - 1.26n +o(n) comparisons on the average. A worst case of n log n +
O(n) comparisons can be achieved without significantly affecting the average
case.
  Furthermore, we describe an implementation of MergeInsertion for small n.
Taking MergeInsertion as a base case for QuickMergesort, we establish a
worst-case efficient sorting algorithm calling for n log n - 1.3999n + o(n)
comparisons on average. QuickMergesort with constant size base cases shows the
best performance on practical inputs: when sorting integers it is slower by
only 15% to STL-Introsort.
</summary>
    <author>
      <name>Stefan Edelkamp</name>
    </author>
    <author>
      <name>Armin Weiß</name>
    </author>
    <link href="http://arxiv.org/abs/1307.3033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.2724">
    <id>http://arxiv.org/abs/1307.2724v1</id>
    <updated>2013-07-10T09:13:35Z</updated>
    <published>2013-07-10T09:13:35Z</published>
    <title>The technique of in-place associative sorting</title>
    <summary>  In the first place, a novel, yet straightforward in-place integer
value-sorting algorithm is presented. It sorts in linear time using constant
amount of additional memory for storing counters and indices beside the input
array. The technique is inspired from the principal idea behind one of the
ordinal theories of "serial order in behavior" and explained by the analogy
with the three main stages in the formation and retrieval of memory in
cognitive neuroscience: (i) practicing, (ii) storage and (iii) retrieval. It is
further improved in terms of time complexity as well as specialized for
distinct integers, though still improper for rank-sorting.
  Afterwards, another novel, yet straightforward technique is introduced which
makes this efficient value-sorting technique proper for rank-sorting. Hence,
given an array of n elements each have an integer key, the technique sorts the
elements according to their integer keys in linear time using only constant
amount of additional memory. The devised technique is very practical and
efficient outperforming bucket sort, distribution counting sort and address
calculation sort family of algorithms making it attractive in almost every case
even when space is not a critical resource.
</summary>
    <author>
      <name>A. Emre Cetin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 Pages. arXiv admin note: substantial text overlap with
  arXiv:1209.0572, arXiv:1210.1771, arXiv:1209.3668, arXiv:1209.1942,
  arXiv:1209.4714</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.2724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.2724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68P10" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.1915">
    <id>http://arxiv.org/abs/1307.1915v6</id>
    <updated>2015-10-15T09:24:04Z</updated>
    <published>2013-07-07T21:02:11Z</published>
    <title>Complexity of the FIFO Stack-Up Problem</title>
    <summary>  We study the combinatorial FIFO stack-up problem. In delivery industry, bins
have to be stacked-up from conveyor belts onto pallets with respect to customer
orders. Given k sequences q_1, ..., q_k of labeled bins and a positive integer
p, the aim is to stack-up the bins by iteratively removing the first bin of one
of the k sequences and put it onto an initially empty pallet of unbounded
capacity located at one of p stack-up places. Bins with different pallet labels
have to be placed on different pallets, bins with the same pallet label have to
be placed on the same pallet. After all bins for a pallet have been removed
from the given sequences, the corresponding stack-up place will be cleared and
becomes available for a further pallet. The FIFO stack-up problem is to find a
stack-up sequence such that all pallets can be build-up with the available p
stack-up places. In this paper, we introduce two digraph models for the FIFO
stack-up problem, namely the processing graph and the sequence graph. We show
that there is a processing of some list of sequences with at most p stack-up
places if and only if the sequence graph of this list has directed pathwidth at
most p-1. This connection implies that the FIFO stack-up problem is NP-complete
in general, even if there are at most 6 bins for every pallet and that the
problem can be solved in polynomial time, if the number p of stack-up places is
assumed to be fixed. Further the processing graph allows us to show that the
problem can be solved in polynomial time, if the number k of sequences is
assumed to be fixed.
</summary>
    <author>
      <name>Frank Gurski</name>
    </author>
    <author>
      <name>Jochen Rethmann</name>
    </author>
    <author>
      <name>Egon Wanke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.1915v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1915v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.1560">
    <id>http://arxiv.org/abs/1307.1560v1</id>
    <updated>2013-07-05T09:28:07Z</updated>
    <published>2013-07-05T09:28:07Z</published>
    <title>Compressed Pattern-Matching with Ranked Variables in Zimin Words</title>
    <summary>  Zimin words are very special finite words which are closely related to the
pattern-avoidability problem. This problem consists in testing if an instance
of a given pattern with variables occurs in almost all words over any finite
alphabet. The problem is not well understood, no polynomial time algorithm is
known and its NP-hardness is also not known. The pattern-avoidability problem
is equivalent to searching for a pattern (with variables) in a Zimin word. The
main difficulty is potentially exponential size of Zimin words. We use special
properties of Zimin words, especially that they are highly compressible, to
design efficient algorithms for special version of the pattern-matching, called
here ranked matching. It gives a new interpretation of Zimin algorithm in
compressed setting. We discuss the structure of rankings of variables and
compressed representations of values of variables. Moreover, for a ranked
matching we present efficient algorithms to find the shortest instance and the
number of valuations of instances of the pattern.
</summary>
    <author>
      <name>Radosław Głowinski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.1560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.1417">
    <id>http://arxiv.org/abs/1307.1417v1</id>
    <updated>2013-07-04T17:10:08Z</updated>
    <published>2013-07-04T17:10:08Z</published>
    <title>An Elegant Algorithm for the Construction of Suffix Arrays</title>
    <summary>  The suffix array is a data structure that finds numerous applications in
string processing problems for both linguistic texts and biological data. It
has been introduced as a memory efficient alternative for suffix trees. The
suffix array consists of the sorted suffixes of a string. There are several
linear time suffix array construction algorithms (SACAs) known in the
literature. However, one of the fastest algorithms in practice has a worst case
run time of $O(n^2)$. The problem of designing practically and theoretically
efficient techniques remains open. In this paper we present an elegant
algorithm for suffix array construction which takes linear time with high
probability; the probability is on the space of all possible inputs. Our
algorithm is one of the simplest of the known SACAs and it opens up a new
dimension of suffix array construction that has not been explored until now.
Our algorithm is easily parallelizable. We offer parallel implementations on
various parallel models of computing. We prove a lemma on the $\ell$-mers of a
random string which might find independent applications. We also present
another algorithm that utilizes the above algorithm. This algorithm is called
RadixSA and has a worst case run time of $O(n\log{n})$. RadixSA introduces an
idea that may find independent applications as a speedup technique for other
SACAs. An empirical comparison of RadixSA with other algorithms on various
datasets reveals that our algorithm is one of the fastest algorithms to date.
The C++ source code is freely available at
http://www.engr.uconn.edu/~man09004/radixSA.zip
</summary>
    <author>
      <name>Sanguthevar Rajasekaran</name>
    </author>
    <author>
      <name>Marius Nicolae</name>
    </author>
    <link href="http://arxiv.org/abs/1307.1417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.1406">
    <id>http://arxiv.org/abs/1307.1406v1</id>
    <updated>2013-07-04T16:56:09Z</updated>
    <published>2013-07-04T16:56:09Z</published>
    <title>On string matching with k mismatches</title>
    <summary>  In this paper we consider several variants of the pattern matching problem.
In particular, we investigate the following problems: 1) Pattern matching with
k mismatches; 2) Approximate counting of mismatches; and 3) Pattern matching
with mismatches. The distance metric used is the Hamming distance. We present
some novel algorithms and techniques for solving these problems. Both
deterministic and randomized algorithms are offered. Variants of these problems
where there could be wild cards in either the text or the pattern or both are
considered. An experimental evaluation of these algorithms is also presented.
The source code is available at http://www.engr.uconn.edu/~man09004/kmis.zip.
</summary>
    <author>
      <name>Marius Nicolae</name>
    </author>
    <author>
      <name>Sanguthevar Rajasekaran</name>
    </author>
    <link href="http://arxiv.org/abs/1307.1406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.0920">
    <id>http://arxiv.org/abs/1307.0920v1</id>
    <updated>2013-07-03T06:19:08Z</updated>
    <published>2013-07-03T06:19:08Z</published>
    <title>Domain Specific Hierarchical Huffman Encoding</title>
    <summary>  In this paper, we revisit the classical data compression problem for domain
specific texts. It is well-known that classical Huffman algorithm is optimal
with respect to prefix encoding and the compression is done at character level.
Since many data transfer are domain specific, for example, downloading of
lecture notes, web-blogs, etc., it is natural to think of data compression in
larger dimensions (i.e. word level rather than character level). Our framework
employs a two-level compression scheme in which the first level identifies
frequent patterns in the text using classical frequent pattern algorithms. The
identified patterns are replaced with special strings and to acheive a better
compression ratio the length of a special string is ensured to be shorter than
the length of the corresponding pattern. After this transformation, on the
resultant text, we employ classical Huffman data compression algorithm. In
short, in the first level compression is done at word level and in the second
level it is at character level. Interestingly, this two level compression
technique for domain specific text outperforms classical Huffman technique. To
support our claim, we have presented both theoretical and simulation results
for domain specific texts.
</summary>
    <author>
      <name>K. Ilambharathi</name>
    </author>
    <author>
      <name>G. S. N. V. Venkata Manik</name>
    </author>
    <author>
      <name>N. Sadagopan</name>
    </author>
    <author>
      <name>B. Sivaselvan</name>
    </author>
    <link href="http://arxiv.org/abs/1307.0920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.0920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1309.1981">
    <id>http://arxiv.org/abs/1309.1981v2</id>
    <updated>2013-09-18T15:15:24Z</updated>
    <published>2013-09-08T17:15:21Z</published>
    <title>The Swap Matching Problem Revisited</title>
    <summary>  In this paper, we revisit the much studied problem of Pattern Matching with
Swaps (Swap Matching problem, for short). We first present a graph-theoretic
model, which opens a new and so far unexplored avenue to solve the problem.
Then, using the model, we devise two efficient algorithms to solve the swap
matching problem. The resulting algorithms are adaptations of the classic
shift-and algorithm. For patterns having length similar to the word-size of the
target machine, both the algorithms run in linear time considering a fixed
alphabet.
</summary>
    <author>
      <name>Pritom Ahmed</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>A. S. M. Sohidull Islam</name>
    </author>
    <author>
      <name>M. Sohel Rahman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 3 Figures and 17 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.1981v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1981v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1309.2476">
    <id>http://arxiv.org/abs/1309.2476v1</id>
    <updated>2013-09-10T12:24:50Z</updated>
    <published>2013-09-10T12:24:50Z</published>
    <title>TRANS outperforms MTF for two special types of request sequences without
  locality of reference</title>
    <summary>  Various list accessing algorithms have been proposed in the literature and
their performances have been analyzed theoretically and experimentally.
Move-To-Front (MTF) and Transpose (TRANS) are two well known primitive list
accessing algorithms. MTF has been proved to be the best performing online
algorithm till date in the literature for real life inputs and practical
applications with locality of reference. It has been shown that when storage
space is extremely limited and pointers for lists cannot be used, then array
implementation of TRANS gives efficient reorganization. Use of MTF is extensive
in the literature whereas, the use of TRANS is rare. As mentioned as an open
problem in literature, direct bounds on the behavior and performance of various
list accessing algorithms are needed to allow realistic comparisons. Since it
has been shown that no single optimal permutation algorithm exists, it becomes
necessary to characterize the circumstances that indicate the advantage in
using a particular list accessing algorithm. Motivated by above challenging
research issue, in this paper we have made an analytical study for evaluating
the performance of TRANS list accessing algorithm using two special types of
request sequences without locality of reference. We have compared the
performance of TRANS with MTF and observed that TRANS outperforms MTF for these
considered types of request sequences.
</summary>
    <author>
      <name>Rakesh Mohanty</name>
    </author>
    <author>
      <name>Sangita Patel</name>
    </author>
    <author>
      <name>Shiba Prasad Dash</name>
    </author>
    <author>
      <name>Burle Sharma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages, Proceedings of International Conference on Communication,
  Computing and Security (ICCCS)-2012, India.
  http://www.sciencedirect.com/science/article/pii/S2212017312006123</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Procedia Technology, Elsevier, Vol 6, pages 556-563, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.2476v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.2476v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1309.3210">
    <id>http://arxiv.org/abs/1309.3210v32</id>
    <updated>2016-10-29T19:46:38Z</updated>
    <published>2013-09-12T16:21:46Z</published>
    <title>O-notation in algorithm analysis</title>
    <summary>  We provide an extensive list of desirable properties for an O-notation --- as
used in algorithm analysis --- and reduce them to 8 primitive properties. We
prove that the primitive properties are equivalent to the definition of the
O-notation as linear dominance. We abstract the existing definitions of the
O-notation under local linear dominance, and show that it has a
characterization by limits over filters for positive functions. We define the
O-mappings as a general tool for manipulating the O-notation, and show that
Master theorems hold under linear dominance.
</summary>
    <author>
      <name>Kalle Rutanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proved a minimal axiom-set for O-notation</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.3210v32" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.3210v32" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1308.6145">
    <id>http://arxiv.org/abs/1308.6145v1</id>
    <updated>2013-08-28T12:45:42Z</updated>
    <published>2013-08-28T12:45:42Z</published>
    <title>ELB-Trees, An Efficient and Lock-free B-tree Derivative</title>
    <summary>  This technical report is an extension of the paper of the same title, which
is to appear at MUCOCOS'13. The technical report proves correctness of the
ELB-trees operations' semantics and that the operations are lock-free.
</summary>
    <author>
      <name>Lars F. Bonnichsen</name>
    </author>
    <author>
      <name>Sven Karlsson</name>
    </author>
    <author>
      <name>Christian W. Probst</name>
    </author>
    <link href="http://arxiv.org/abs/1308.6145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.6145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1308.3822">
    <id>http://arxiv.org/abs/1308.3822v3</id>
    <updated>2015-01-15T18:23:44Z</updated>
    <published>2013-08-18T01:45:04Z</published>
    <title>Sublinear Matching With Finite Automata Using Reverse Suffix Scanning</title>
    <summary>  We give algorithms to accelerate the computation of deterministic finite
automata (DFA) by calculating the state of a DFA n positions ahead utilizing a
reverse scan of the next n characters. Often this requires scanning fewer than
n characters resulting in a fraction of the input being skipped and a
commensurate increase in processing speed. The skipped fraction is > 80% in
several of our examples. We introduce offsetting finite automata (OFA) to
encode the accelerated computation. OFA generalize DFA by adding an integer
offset to the current input index at each state transition. We give algorithms
for constructing an OFA that accepts the same language as a DFA while possibly
skipping input, and for matching with an OFA. Compared to previous algorithms
that attempt to skip some of the input, the new matching algorithm can skip
more often and can skip farther. In the worst case the new matching algorithm
scans the same number of characters as a simple forward scan, whereas previous
approaches often scan more, so the new algorithm can be used as a reliable
replacement for the simple forward scan. Additionally, the new algorithm adapts
to available memory and time constraints.
</summary>
    <author>
      <name>Steven M. Kearns</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version of the paper is a streamlined presentation that includes
  the definition of Offsetting Finite Automata, which replaces the name
  Accelerated Finite Automata in previous version</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.3822v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3822v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1308.3326">
    <id>http://arxiv.org/abs/1308.3326v1</id>
    <updated>2013-08-15T07:38:16Z</updated>
    <published>2013-08-15T07:38:16Z</published>
    <title>Sorted Range Reporting Revisited</title>
    <summary>  We consider the two-dimensional sorted range reporting problem. Our data
structure requires O(n lglg n) words of space and O(lglg n + k lglg n) query
time, where k is the number of points in the query range. This data structure
improves a recent result of Nekrich and Navarro [8] by a factor of O(lglg n) in
query time, and matches the state of the art for unsorted range reporting [1].
</summary>
    <author>
      <name>Gelin Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.3326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1308.0626">
    <id>http://arxiv.org/abs/1308.0626v1</id>
    <updated>2013-08-02T21:57:52Z</updated>
    <published>2013-08-02T21:57:52Z</published>
    <title>Estimating the longest increasing sequence in polylogarithmic time</title>
    <summary>  Finding the length of the longest increasing subsequence (LIS) is a classic
algorithmic problem. Let $n$ denote the size of the array. Simple $O(n\log n)$
algorithms are known for this problem. We develop a polylogarithmic time
randomized algorithm that for any constant $\delta > 0$, estimates the length
of the LIS of an array to within an additive error of $\delta n$. More
precisely, the running time of the algorithm is $(\log n)^c
(1/\delta)^{O(1/\delta)}$ where the exponent $c$ is independent of $\delta$.
Previously, the best known polylogarithmic time algorithms could only achieve
an additive $n/2$ approximation. With a suitable choice of parameters, our
algorithm also gives, for any fixed $\tau>0$, a multiplicative
$(1+\tau)$-approximation to the distance to monotonicity $\varepsilon_f$ (the
fraction of entries not in the LIS), whose running time is polynomial in
$\log(n)$ and $1/varepsilon_f$. The best previously known algorithm could only
guarantee an approximation within a factor (arbitrarily close to) 2.
</summary>
    <author>
      <name>M. Saks</name>
    </author>
    <author>
      <name>C. Seshadhri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of FOCS 2010 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.0626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; G.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1308.0833">
    <id>http://arxiv.org/abs/1308.0833v1</id>
    <updated>2013-08-04T18:31:30Z</updated>
    <published>2013-08-04T18:31:30Z</published>
    <title>Data Structures in Classical and Quantum Computing</title>
    <summary>  This survey summarizes several results about quantum computing related to
(mostly static) data structures. First, we describe classical data structures
for the set membership and the predecessor search problems: Perfect Hash tables
for set membership by Fredman, Koml\'{o}s and Szemer\'{e}di and a data
structure by Beame and Fich for predecessor search. We also prove results about
their space complexity (how many bits are required) and time complexity (how
many bits have to be read to answer a query). After that, we turn our attention
to classical data structures with quantum access. In the quantum access model,
data is stored in classical bits, but they can be accessed in a quantum way: We
may read several bits in superposition for unit cost. We give proofs for lower
bounds in this setting that show that the classical data structures from the
first section are, in some sense, asymptotically optimal - even in the quantum
model. In fact, these proofs are simpler and give stronger results than
previous proofs for the classical model of computation. The lower bound for set
membership was proved by Radhakrishnan, Sen and Venkatesh and the result for
the predecessor problem by Sen and Venkatesh. Finally, we examine fully quantum
data structures. Instead of encoding the data in classical bits, we now encode
it in qubits. We allow any unitary operation or measurement in order to answer
queries. We describe one data structure by de Wolf for the set membership
problem and also a general framework using fully quantum data structures in
quantum walks by Jeffery, Kothari and Magniez.
</summary>
    <author>
      <name>Maximilian Fillinger</name>
    </author>
    <link href="http://arxiv.org/abs/1308.0833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1307.7842">
    <id>http://arxiv.org/abs/1307.7842v1</id>
    <updated>2013-07-30T06:59:15Z</updated>
    <published>2013-07-30T06:59:15Z</published>
    <title>A Fixed-Parameter Algorithm for Minimum Common String Partition with Few
  Duplications</title>
    <summary>  Motivated by the study of genome rearrangements, the NP-hard Minimum Common
String Partition problems asks, given two strings, to split both strings into
an identical set of blocks. We consider an extension of this problem to
unbalanced strings, so that some elements may not be covered by any block. We
present an efficient fixed-parameter algorithm for the parameters number k of
blocks and maximum occurrence d of a letter in either string. We then evaluate
this algorithm on bacteria genomes and synthetic data.
</summary>
    <author>
      <name>Laurent Bulteau</name>
    </author>
    <author>
      <name>Guillaume Fertin</name>
    </author>
    <author>
      <name>Christian Komusiewicz</name>
    </author>
    <author>
      <name>Irena Rusu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.7842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1311.6093">
    <id>http://arxiv.org/abs/1311.6093v6</id>
    <updated>2016-08-03T22:19:11Z</updated>
    <published>2013-11-24T08:18:04Z</published>
    <title>A New Algorithm for Updating and Querying Sub-arrays of Multidimensional
  Arrays</title>
    <summary>  Given a $d$-dimensional array $A$, an update operation adds a given constant
$C$ to each element within a continuous sub-array of $A$. A query operation
computes the sum of all the elements within a continuous sub-array of $A$. The
one-dimensional update and query handling problem has been studied intensively
and is usually solved using segment trees with lazy propagation technique. In
this paper, we present a new algorithm incorporating Binary Indexed Trees and
Inclusion-Exclusion Principle to accomplish the same task. We extend the
algorithm to update and query sub-matrices of matrices (two-dimensional array).
Finally, we propose a general form of the algorithm for $d$-dimensions which
achieves $\mathcal{O}(4^d*\log^{d}n)$ time complexity for both updates and
queries. This is an improvement over the previously known algorithms which
utilize hierarchical data structures like quadtrees and octrees and have a
worst-case time complexity of $\Omega(n^{d-1})$ per update/query.
</summary>
    <author>
      <name>Pushkar Mishra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.1.2394.2485</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.1.2394.2485" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 Pages, 3 Figures, 1 Table</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.6093v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6093v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1311.4552">
    <id>http://arxiv.org/abs/1311.4552v1</id>
    <updated>2013-11-18T21:02:38Z</updated>
    <published>2013-11-18T21:02:38Z</published>
    <title>Efficient algorithms for the longest common subsequence in $k$-length
  substrings</title>
    <summary>  Finding the longest common subsequence in $k$-length substrings (LCS$k$) is a
recently proposed problem motivated by computational biology. This is a
generalization of the well-known LCS problem in which matching symbols from two
sequences $A$ and $B$ are replaced with matching non-overlapping substrings of
length $k$ from $A$ and $B$. We propose several algorithms for LCS$k$, being
non-trivial incarnations of the major concepts known from LCS research (dynamic
programming, sparse dynamic programming, tabulation). Our algorithms make use
of a linear-time and linear-space preprocessing finding the occurrences of all
the substrings of length $k$ from one sequence in the other sequence.
</summary>
    <author>
      <name>Sebastian Deorowicz</name>
    </author>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to a journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.4552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.4552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1311.2557">
    <id>http://arxiv.org/abs/1311.2557v2</id>
    <updated>2013-11-12T07:56:08Z</updated>
    <published>2013-11-11T20:05:08Z</published>
    <title>Efficiently Computing Edit Distance to Dyck Language</title>
    <summary>  Given a string $\sigma$ over alphabet $\Sigma$ and a grammar $G$ defined over
the same alphabet, how many minimum number of repairs: insertions, deletions
and substitutions are required to map $\sigma$ into a valid member of $G$ ? We
investigate this basic question in this paper for $Dyck(s)$. $Dyck(s)$ is a
fundamental context free grammar representing the language of well-balanced
parentheses with s different types of parentheses and has played a pivotal role
in the development of theory of context free languages. Computing edit distance
to $Dyck(s)$ significantly generalizes string edit distance problem and has
numerous applications ranging from repairing semi-structured documents such as
XML to memory checking, automated compiler optimization, natural language
processing etc.
  In this paper we give the first near-linear time algorithm for edit distance
computation to $Dyck(s)$ that achieves a nontrivial approximation factor of
$O(\frac{1}{\epsilon}\log{OPT}(\log{n})^{\frac{1}{\epsilon}})$ in
$O(n^{1+\epsilon}\log{n})$ time. In fact, given there exists an algorithm for
computing string edit distance on input of size $n$ in $\alpha(n)$ time with
$\beta(n)$-approximation factor, we can devise an algorithm for edit distance
problem to $Dyck(s)$ running in $\tilde{O}(n^{1+\epsilon}+\alpha(n))$ and
achieving an approximation factor of $O(\frac{1}{\epsilon}\beta(n)\log{OPT})$.
  We show that the framework for efficiently approximating edit distance to
$Dyck(s)$ can be applied to many other languages. We illustrate this by
considering various memory checking languages which comprise of valid
transcripts of stacks, queues, priority queues, double-ended queues etc.
Therefore, any language that can be recognized by these data structures, can
also be repaired efficiently by our algorithm.
</summary>
    <author>
      <name>Barna Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.2557v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.2557v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1311.1762">
    <id>http://arxiv.org/abs/1311.1762v1</id>
    <updated>2013-11-07T17:44:02Z</updated>
    <published>2013-11-07T17:44:02Z</published>
    <title>Suffix Trays and Suffix Trists: Structures for Faster Text Indexing</title>
    <summary>  Suffix trees and suffix arrays are two of the most widely used data
structures for text indexing. Each uses linear space and can be constructed in
linear time for polynomially sized alphabets. However, when it comes to
answering queries with worst-case deterministic time bounds, the prior does so
in $O(m\log|\Sigma|)$ time, where $m$ is the query size, $|\Sigma|$ is the
alphabet size, and the latter does so in $O(m+\log n)$ time, where $n$ is the
text size. If one wants to output all appearances of the query, an additive
cost of $O(occ)$ time is sufficient, where $occ$ is the size of the output.
  We propose a novel way of combining the two into, what we call, a {\em suffix
tray}. The space and construction time remain linear and the query time
improves to $O(m+\log|\Sigma|)$ for integer alphabets from a linear range, i.e.
$\Sigma \subset \{1,\cdots, cn\}$, for an arbitrary constant $c$. The
construction and query are deterministic. Here also an additive $O(occ)$ time
is sufficient if one desires to output all appearances of the query.
  We also consider the online version of indexing, where the text arrives
online, one character at a time, and indexing queries are answered in tandem.
In this variant we create a cross between a suffix tree and a suffix list (a
dynamic variant of suffix array) to be called a {\em suffix trist}; it supports
queries in $O(m+\log|\Sigma|)$ time. The suffix trist also uses linear space.
Furthermore, if there exists an online construction for a linear-space suffix
tree such that the cost of adding a character is worst-case deterministic
$f(n,|\Sigma|)$ ($n$ is the size of the current text), then one can further
update the suffix trist in $O(f(n,|\Sigma|)+\log |\Sigma|)$ time. The best
currently known worst-case deterministic bound for $f(n,|\Sigma|)$ is $O(\log
n)$ time.
</summary>
    <author>
      <name>Richard Cole</name>
    </author>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <author>
      <name>Moshe Lewenstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Results from this paper have appeared as an extended abstract in
  ICALP 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.1762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.1762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1310.6887">
    <id>http://arxiv.org/abs/1310.6887v1</id>
    <updated>2013-10-25T11:55:30Z</updated>
    <published>2013-10-25T11:55:30Z</published>
    <title>Bin Packing and Related Problems: General Arc-flow Formulation with
  Graph Compression</title>
    <summary>  We present an exact method, based on an arc-flow formulation with side
constraints, for solving bin packing and cutting stock problems --- including
multi-constraint variants --- by simply representing all the patterns in a very
compact graph. Our method includes a graph compression algorithm that usually
reduces the size of the underlying graph substantially without weakening the
model. As opposed to our method, which provides strong models, conventional
models are usually highly symmetric and provide very weak lower bounds.
  Our formulation is equivalent to Gilmore and Gomory's, thus providing a very
strong linear relaxation. However, instead of using column-generation in an
iterative process, the method constructs a graph, where paths from the source
to the target node represent every valid packing pattern.
  The same method, without any problem-specific parameterization, was used to
solve a large variety of instances from several different cutting and packing
problems. In this paper, we deal with vector packing, graph coloring, bin
packing, cutting stock, cardinality constrained bin packing, cutting stock with
cutting knife limitation, cutting stock with binary patterns, bin packing with
conflicts, and cutting stock with binary patterns and forbidden pairs. We
report computational results obtained with many benchmark test data sets, all
of them showing a large advantage of this formulation with respect to the
traditional ones.
</summary>
    <author>
      <name>Filipe Brandão</name>
    </author>
    <author>
      <name>João Pedro Pedroso</name>
    </author>
    <link href="http://arxiv.org/abs/1310.6887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.6887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="80M50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1310.1440">
    <id>http://arxiv.org/abs/1310.1440v3</id>
    <updated>2015-09-06T04:16:41Z</updated>
    <published>2013-10-05T04:24:28Z</published>
    <title>Approximate String Matching using a Bidirectional Index</title>
    <summary>  We study strategies of approximate pattern matching that exploit
bidirectional text indexes, extending and generalizing ideas of Lam et al. We
introduce a formalism, called search schemes, to specify search strategies of
this type, then develop a probabilistic measure for the efficiency of a search
scheme, prove several combinatorial results on efficient search schemes, and
finally, provide experimental computations supporting the superiority of our
strategies.
</summary>
    <author>
      <name>Gregory Kucherov</name>
    </author>
    <author>
      <name>Kamil Salikhov</name>
    </author>
    <author>
      <name>Dekel Tsur</name>
    </author>
    <link href="http://arxiv.org/abs/1310.1440v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1440v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1310.0178">
    <id>http://arxiv.org/abs/1310.0178v1</id>
    <updated>2013-10-01T08:15:28Z</updated>
    <published>2013-10-01T08:15:28Z</published>
    <title>Dynamic Gomory-Hu Tree Construction -- fast and simple</title>
    <summary>  A cut tree (or Gomory-Hu tree) of an undirected weighted graph G=(V,E)
encodes a minimum s-t-cut for each vertex pair {s,t} \subseteq V and can be
iteratively constructed by n-1 maximum flow computations. They solve the
multiterminal network flow problem, which asks for the all-pairs maximum flow
values in a network and at the same time they represent n-1 non-crossing,
linearly independent cuts that constitute a minimum cut basis of G. Hence, cut
trees are resident in at least two fundamental fields of network analysis and
graph theory, which emphasizes their importance for many applications. In this
work we present a fully-dynamic algorithm that efficiently maintains a cut tree
for a changing graph. The algorithm is easy to implement and has a high
potential for saving cut computations under the assumption that a local change
in the underlying graph does rarely affect the global cut structure. We
document the good practicability of our approach in a brief experiment on real
world data.
</summary>
    <author>
      <name>Tanja Hartmann</name>
    </author>
    <author>
      <name>Dorothea Wagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, Full version of the ISAAC 2012 publication "Fast
  and Simple Fully-Dynamic Cut Tree Construction"</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.0178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1309.7724">
    <id>http://arxiv.org/abs/1309.7724v4</id>
    <updated>2013-12-15T22:57:43Z</updated>
    <published>2013-09-30T05:24:37Z</published>
    <title>The Dynamic Longest Increasing Subsequence Problem</title>
    <summary>  In this paper, we construct a data structure to efficiently compute the
longest increasing subsequence of a sequence subject to dynamic updates. Our
data structure supports a query for the longest increasing subsequence in
$O(r+\log n)$ worst-case time and supports inserts anywhere in the sequence in
$O \left(r\log{n/r}\right)$ worst-case time (where $r$ is the length of the
longest increasing subsequence). The same data structure with a minor
modification supports $O(\log n)$ worst-case time insertions if the insertions
are performed at the end of the sequence. The data structure presented can also
be augmented to support delete operations in the same worst-case time as
insertions.
</summary>
    <author>
      <name>Alex Chen</name>
    </author>
    <author>
      <name>Timothy Chu</name>
    </author>
    <author>
      <name>Nathan Pinsker</name>
    </author>
    <link href="http://arxiv.org/abs/1309.7724v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.7724v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1309.5927">
    <id>http://arxiv.org/abs/1309.5927v1</id>
    <updated>2013-09-23T19:26:16Z</updated>
    <published>2013-09-23T19:26:16Z</published>
    <title>XML Compression via DAGs</title>
    <summary>  Unranked trees can be represented using their minimal dag (directed acyclic
graph). For XML this achieves high compression ratios due to their repetitive
mark up. Unranked trees are often represented through first child/next sibling
(fcns) encoded binary trees. We study the difference in size (= number of
edges) of minimal dag versus minimal dag of the fcns encoded binary tree. One
main finding is that the size of the dag of the binary tree can never be
smaller than the square root of the size of the minimal dag, and that there are
examples that match this bound. We introduce a new combined structure, the
hybrid dag, which is guaranteed to be smaller than (or equal in size to) both
dags. Interestingly, we find through experiments that last child/previous
sibling encodings are much better for XML compression via dags, than fcns
encodings. We determine the average sizes of unranked and binary dags over a
given set of labels (under uniform distribution) in terms of their exact
generating functions, and in terms of their asymptotical behavior.
</summary>
    <author>
      <name>Mireille Bousquet-Melou</name>
    </author>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <author>
      <name>Sebastian Maneth</name>
    </author>
    <author>
      <name>Eric Noeth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A short version of this paper appeared in the Proceedings of ICDT
  2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.5927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P30, 68P15, 68R05" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1309.4958">
    <id>http://arxiv.org/abs/1309.4958v4</id>
    <updated>2018-10-06T08:50:12Z</updated>
    <published>2013-09-19T13:04:49Z</published>
    <title>Approximation of smallest linear tree grammar</title>
    <summary>  A simple linear-time algorithm for constructing a linear context-free tree
grammar of size O(rg + r g log (n/r g))for a given input tree T of size n is
presented, where g is the size of a minimal linear context-free tree grammar
for T, and r is the maximal rank of symbols in T (which is a constant in many
applications). This is the first example of a grammar-based tree compression
algorithm with a good, i.e. logarithmic in terms of the size of the input tree,
approximation ratio. The analysis of the algorithm uses an extension of the
recompression technique from strings to trees.
</summary>
    <author>
      <name>Artur Jeż</name>
    </author>
    <author>
      <name>Markus Lohrey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">45 pages, published in Information and Computation. Approximation
  ratio improved since the first version, figures improved, some examples
  added. A small calculation error corrected since the previous version (all
  claims hold as previously)</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.4958v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.4958v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.2; F.2.2; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1201.3802">
    <id>http://arxiv.org/abs/1201.3802v1</id>
    <updated>2012-01-16T09:11:18Z</updated>
    <published>2012-01-16T09:11:18Z</published>
    <title>An Entertaining Example for the Usage of Bitwise Operations in
  Programming</title>
    <summary>  The present study is meant to fill in some information gaps occurring in the
most widespread and well-known educational and reference literature about
programming. The stress is laid on a very useful instrument - the bitwise
operations, topic which is, unfortunately, seldom dealt with in most of the
well-known books on programming. In addition, the research is very useful as
regards the topic of overloading operators in any Object-oriented programming
course. Given some appropriate examples, with the emphasis being laid on some
particular and data structures language constructions, the results are quite
interesting. The algorithm of solving the popular Sudoku puzzle is one such
entertaining example.
</summary>
    <author>
      <name>Hristina Kostadinova</name>
    </author>
    <author>
      <name>Krasimir Yordzhev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Fourth International Scientific Conference -
  FMNS2011, 8 - 11 June, 2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mathematics and natural science, v. 1, SWU "N. Rilski", 2011,
  159-168</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.3802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.3802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1201.3077">
    <id>http://arxiv.org/abs/1201.3077v1</id>
    <updated>2012-01-15T10:17:36Z</updated>
    <published>2012-01-15T10:17:36Z</published>
    <title>A Bijective String Sorting Transform</title>
    <summary>  Given a string of characters, the Burrows-Wheeler Transform rearranges the
characters in it so as to produce another string of the same length which is
more amenable to compression techniques such as move to front, run-length
encoding, and entropy encoders. We present a variant of the transform which
gives rise to similar or better compression value, but, unlike the original,
the transform we present is bijective, in that the inverse transformation
exists for all strings. Our experiments indicate that using our variant of the
transform gives rise to better compression ratio than the original
Burrows-Wheeler transform. We also show that both the transform and its inverse
can be computed in linear time and consuming linear storage.
</summary>
    <author>
      <name>Joseph Yossi Gil</name>
    </author>
    <author>
      <name>David Allen Scott</name>
    </author>
    <link href="http://arxiv.org/abs/1201.3077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.3077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="cs/9608105">
    <id>http://arxiv.org/abs/cs/9608105v1</id>
    <updated>1996-08-22T00:00:00Z</updated>
    <published>1996-08-22T00:00:00Z</published>
    <title>Shellsort with three increments</title>
    <summary>  A perturbation technique can be used to simplify and sharpen A. C. Yao's
theorems about the behavior of shellsort with increments $(h,g,1)$. In
particular, when $h=\Theta(n^{7/15})$ and $g=\Theta(h^{1/5})$, the average
running time is $O(n^{23/15})$. The proof involves interesting properties of
the inversions in random permutations that have been $h$-sorted and $g$-sorted.
</summary>
    <author>
      <name>Svante Janson</name>
    </author>
    <author>
      <name>Donald E. Knuth</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Random Structures Algorithms 10 (1997), no. 1-2, 125--142</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9608105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9608105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1201.0365">
    <id>http://arxiv.org/abs/1201.0365v1</id>
    <updated>2012-01-01T17:38:06Z</updated>
    <published>2012-01-01T17:38:06Z</published>
    <title>Lower bounding edit distances between permutations</title>
    <summary>  A number of fields, including the study of genome rearrangements and the
design of interconnection networks, deal with the connected problems of sorting
permutations in "as few moves as possible", using a given set of allowed
operations, or computing the number of moves the sorting process requires,
often referred to as the \emph{distance} of the permutation. These operations
often act on just one or two segments of the permutation, e.g. by reversing one
segment or exchanging two segments. The \emph{cycle graph} of the permutation
to sort is a fundamental tool in the theory of genome rearrangements, and has
proved useful in settling the complexity of many variants of the above
problems. In this paper, we present an algebraic reinterpretation of the cycle
graph of a permutation $\pi$ as an even permutation $\bar{\pi}$, and show how
to reformulate our sorting problems in terms of particular factorisations of
the latter permutation. Using our framework, we recover known results in a
simple and unified way, and obtain a new lower bound on the \emph{prefix
transposition distance} (where a \emph{prefix transposition} displaces the
initial segment of a permutation), which is shown to outperform previous
results. Moreover, we use our approach to improve the best known lower bound on
the \emph{prefix transposition diameter} from $2n/3$ to $\lfloor3n/4\rfloor$,
and investigate a few relations between some statistics on $\pi$ and
$\bar{\pi}$.
</summary>
    <author>
      <name>Anthony Labarre</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/13090897X</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/13090897X" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIAM Journal on Discrete Mathematics 27 (3), 1410-1428 (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.0365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.0365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1312.6039">
    <id>http://arxiv.org/abs/1312.6039v1</id>
    <updated>2013-12-20T16:56:28Z</updated>
    <published>2013-12-20T16:56:28Z</published>
    <title>Succinct representation of labeled trees</title>
    <summary>  We give a representation for labeled ordered trees that supports labeled
queries such as finding the i-th ancestor of a node with a given label. Our
representation is succinct, namely the redundancy is small-o of the optimal
space for storing the tree. This improves the representation of He et al. which
is succinct unless the entropy of the labels is small.
</summary>
    <author>
      <name>Dekel Tsur</name>
    </author>
    <link href="http://arxiv.org/abs/1312.6039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1312.4666">
    <id>http://arxiv.org/abs/1312.4666v1</id>
    <updated>2013-12-17T07:06:57Z</updated>
    <published>2013-12-17T07:06:57Z</published>
    <title>A Functional Approach to Standard Binary Heaps</title>
    <summary>  This paper describes a new and purely functional implementation technique of
binary heaps. A binary heap is a tree-based data structure that implements
priority queue operations (insert, remove, minimum/maximum) and guarantees at
worst logarithmic running time for them. Approaches and ideas described in this
paper present a simple and asymptotically optimal implementation of immutable
binary heap.
</summary>
    <author>
      <name>Vladimir Kostyukov</name>
    </author>
    <link href="http://arxiv.org/abs/1312.4666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1312.4413">
    <id>http://arxiv.org/abs/1312.4413v1</id>
    <updated>2013-12-16T15:55:03Z</updated>
    <published>2013-12-16T15:55:03Z</published>
    <title>Near-optimal labeling schemes for nearest common ancestors</title>
    <summary>  We consider NCA labeling schemes: given a rooted tree $T$, label the nodes of
$T$ with binary strings such that, given the labels of any two nodes, one can
determine, by looking only at the labels, the label of their nearest common
ancestor.
  For trees with $n$ nodes we present upper and lower bounds establishing that
labels of size $(2\pm \epsilon)\log n$, $\epsilon&lt;1$ are both sufficient and
necessary. (All logarithms in this paper are in base 2.)
  Alstrup, Bille, and Rauhe (SIDMA'05) showed that ancestor and NCA labeling
schemes have labels of size $\log n +\Omega(\log \log n)$. Our lower bound
increases this to $\log n + \Omega(\log n)$ for NCA labeling schemes. Since
Fraigniaud and Korman (STOC'10) established that labels in ancestor labeling
schemes have size $\log n +\Theta(\log \log n)$, our new lower bound separates
ancestor and NCA labeling schemes. Our upper bound improves the $10 \log n$
upper bound by Alstrup, Gavoille, Kaplan and Rauhe (TOCS'04), and our
theoretical result even outperforms some recent experimental studies by Fischer
(ESA'09) where variants of the same NCA labeling scheme are shown to all have
labels of size approximately $8 \log n$.
</summary>
    <author>
      <name>Stephen Alstrup</name>
    </author>
    <author>
      <name>Esben Bistrup Halvorsen</name>
    </author>
    <author>
      <name>Kasper Green Larsen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/1.9781611973402.72</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/1.9781611973402.72" rel="related"/>
    <link href="http://arxiv.org/abs/1312.4413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; G.2.2; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1312.2738">
    <id>http://arxiv.org/abs/1312.2738v3</id>
    <updated>2014-01-10T23:10:22Z</updated>
    <published>2013-12-10T10:06:13Z</published>
    <title>Shortest Unique Substring Query Revisited</title>
    <summary>  We revisit the problem of finding shortest unique substring (SUS) proposed
recently by [6]. We propose an optimal $O(n)$ time and space algorithm that can
find an SUS for every location of a string of size $n$. Our algorithm
significantly improves the $O(n^2)$ time complexity needed by [6]. We also
support finding all the SUSes covering every location, whereas the solution in
[6] can find only one SUS for every location. Further, our solution is simpler
and easier to implement and can also be more space efficient in practice, since
we only use the inverse suffix array and longest common prefix array of the
string, while the algorithm in [6] uses the suffix tree of the string and other
auxiliary data structures. Our theoretical results are validated by an
empirical study that shows our algorithm is much faster and more space-saving
than the one in [6].
</summary>
    <author>
      <name>Atalay Mert İleri</name>
    </author>
    <author>
      <name>M. Oğuzhan Külekci</name>
    </author>
    <author>
      <name>Bojian Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1312.2738v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2738v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1312.2381">
    <id>http://arxiv.org/abs/1312.2381v1</id>
    <updated>2013-12-09T11:10:11Z</updated>
    <published>2013-12-09T11:10:11Z</published>
    <title>A Note on the Longest Common Compatible Prefix Problem for Partial Words</title>
    <summary>  For a partial word $w$ the longest common compatible prefix of two positions
$i,j$, denoted $lccp(i,j)$, is the largest $k$ such that $w[i,i+k-1]\uparrow
w[j,j+k-1]$, where $\uparrow$ is the compatibility relation of partial words
(it is not an equivalence relation). The LCCP problem is to preprocess a
partial word in such a way that any query $lccp(i,j)$ about this word can be
answered in $O(1)$ time. It is a natural generalization of the longest common
prefix (LCP) problem for regular words, for which an $O(n)$ preprocessing time
and $O(1)$ query time solution exists.
  Recently an efficient algorithm for this problem has been given by F.
Blanchet-Sadri and J. Lazarow (LATA 2013). The preprocessing time was
$O(nh+n)$, where $h$ is the number of "holes" in $w$. The algorithm was
designed for partial words over a constant alphabet and was quite involved.
  We present a simple solution to this problem with slightly better runtime
that works for any linearly-sortable alphabet. Our preprocessing is in time
$O(n\mu+n)$, where $\mu$ is the number of blocks of holes in $w$. Our algorithm
uses ideas from alignment algorithms and dynamic programming.
</summary>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Marcin Kubica</name>
    </author>
    <author>
      <name>Alessio Langiu</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Bartosz Szreder</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <link href="http://arxiv.org/abs/1312.2381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1312.2018">
    <id>http://arxiv.org/abs/1312.2018v2</id>
    <updated>2013-12-10T17:43:13Z</updated>
    <published>2013-12-06T21:26:44Z</published>
    <title>RAM-Efficient External Memory Sorting</title>
    <summary>  In recent years a large number of problems have been considered in external
memory models of computation, where the complexity measure is the number of
blocks of data that are moved between slow external memory and fast internal
memory (also called I/Os). In practice, however, internal memory time often
dominates the total running time once I/O-efficiency has been obtained. In this
paper we study algorithms for fundamental problems that are simultaneously
I/O-efficient and internal memory efficient in the RAM model of computation.
</summary>
    <author>
      <name>Lars Arge</name>
    </author>
    <author>
      <name>Mikkel Thorup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of ISAAC 2013, getting the Best Paper Award</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.2018v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2018v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1311.1249">
    <id>http://arxiv.org/abs/1311.1249v1</id>
    <updated>2013-11-05T23:37:14Z</updated>
    <published>2013-11-05T23:37:14Z</published>
    <title>From Theory to Practice: Plug and Play with Succinct Data Structures</title>
    <summary>  Engineering efficient implementations of compact and succinct structures is a
time-consuming and challenging task, since there is no standard library of
easy-to- use, highly optimized, and composable components. One consequence is
that measuring the practical impact of new theoretical proposals is a difficult
task, since older base- line implementations may not rely on the same basic
components, and reimplementing from scratch can be very time-consuming. In this
paper we present a framework for experimentation with succinct data structures,
providing a large set of configurable components, together with tests,
benchmarks, and tools to analyze resource requirements. We demonstrate the
functionality of the framework by recomposing succinct solutions for document
retrieval.
</summary>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Timo Beller</name>
    </author>
    <author>
      <name>Alistair Moffat</name>
    </author>
    <author>
      <name>Matthias Petri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.1249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.1249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1203.4836">
    <id>http://arxiv.org/abs/1203.4836v1</id>
    <updated>2012-03-21T21:15:44Z</updated>
    <published>2012-03-21T21:15:44Z</published>
    <title>On a New Method of Storing a Variable Size Array</title>
    <summary>  This paper introduces a new data structure, log_vector, with the following
properties: constant time random access to individual elements; constant time
element addition to the end; constant time element removal from the end;
constant time empty data structure creation; amortized constant space per
individual elements; constant additional space used.
</summary>
    <author>
      <name>Anatolijs Gorbunovs</name>
    </author>
    <link href="http://arxiv.org/abs/1203.4836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.4836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1203.2822">
    <id>http://arxiv.org/abs/1203.2822v2</id>
    <updated>2014-12-12T18:43:33Z</updated>
    <published>2012-03-13T14:29:24Z</published>
    <title>A Fast Algorithm Finding the Shortest Reset Words</title>
    <summary>  In this paper we present a new fast algorithm finding minimal reset words for
finite synchronizing automata. The problem is know to be computationally hard,
and our algorithm is exponential. Yet, it is faster than the algorithms used so
far and it works well in practice. The main idea is to use a bidirectional BFS
and radix (Patricia) tries to store and compare resulted subsets. We give both
theoretical and practical arguments showing that the branching factor is
reduced efficiently. As a practical test we perform an experimental study of
the length of the shortest reset word for random automata with $n$ states and 2
input letters. We follow Skvorsov and Tipikin, who have performed such a study
using a SAT solver and considering automata up to $n=100$ states. With our
algorithm we are able to consider much larger sample of automata with up to
$n=300$ states. In particular, we obtain a new more precise estimation of the
expected length of the shortest reset word $\approx 2.5\sqrt{n-5}$.
</summary>
    <author>
      <name>Andrzej Kisielewicz</name>
    </author>
    <author>
      <name>Jakub Kowalski</name>
    </author>
    <author>
      <name>Marek Szykuła</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-38768-5_18</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-38768-5_18" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">COCOON 2013. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F978-3-642-38768-5_18</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Computing and Combinatorics, volume 7936 of LNCS, pages
  182-196, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.2822v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.2822v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1203.2437">
    <id>http://arxiv.org/abs/1203.2437v1</id>
    <updated>2012-03-12T09:43:31Z</updated>
    <published>2012-03-12T09:43:31Z</published>
    <title>Sorting and preimages of pattern classes</title>
    <summary>  We introduce an algorithm to determine when a sorting operation, such as
stack-sort or bubble-sort, outputs a given pattern. The algorithm provides a
new proof of the description of West-2-stack-sortable permutations, that is
permutations that are completely sorted when passed twice through a stack, in
terms of patterns. We also solve the long-standing problem of describing
West-3-stack-sortable permutations. This requires a new type of generalized
permutation pattern we call a decorated pattern.
</summary>
    <author>
      <name>Anders Claesson</name>
    </author>
    <author>
      <name>Henning Úlfarsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures, to appear at FPSAC 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.2437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.2437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05A05" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1202.6575">
    <id>http://arxiv.org/abs/1202.6575v3</id>
    <updated>2012-06-29T12:17:38Z</updated>
    <published>2012-02-29T15:32:34Z</published>
    <title>Simplified, stable parallel merging</title>
    <summary>  This note makes an observation that significantly simplifies a number of
previous parallel, two-way merge algorithms based on binary search and
sequential merge in parallel. First, it is shown that the additional merge step
of distinguished elements as found in previous algorithms is not necessary,
thus simplifying the implementation and reducing constant factors. Second, by
fixating the requirements to the binary search, the merge algorithm becomes
stable, provided that the sequential merge subroutine is stable. The stable,
parallel merge algorithm can easily be used to implement a stable, parallel
merge sort.
  For ordered sequences with $n$ and $m$ elements, $m\leq n$, the simplified
merge algorithm runs in $O(n/p+\log n)$ operations using $p$ processing
elements. It can be implemented on an EREW PRAM, but since it requires only a
single synchronization step, it is also a candidate for implementation on other
parallel, shared-memory computers.
</summary>
    <author>
      <name>Jesper Larsson Träff</name>
    </author>
    <link href="http://arxiv.org/abs/1202.6575v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.6575v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1202.3470">
    <id>http://arxiv.org/abs/1202.3470v2</id>
    <updated>2012-04-25T13:54:14Z</updated>
    <published>2012-02-15T23:11:48Z</published>
    <title>Pattern Matching in Multiple Streams</title>
    <summary>  We investigate the problem of deterministic pattern matching in multiple
streams. In this model, one symbol arrives at a time and is associated with one
of s streaming texts. The task at each time step is to report if there is a new
match between a fixed pattern of length m and a newly updated stream. As is
usual in the streaming context, the goal is to use as little space as possible
while still reporting matches quickly. We give almost matching upper and lower
space bounds for three distinct pattern matching problems. For exact matching
we show that the problem can be solved in constant time per arriving symbol and
O(m+s) words of space. For the k-mismatch and k-difference problems we give
O(k) time solutions that require O(m+ks) words of space. In all three cases we
also give space lower bounds which show our methods are optimal up to a single
logarithmic factor. Finally we set out a number of open problems related to
this new model for pattern matching.
</summary>
    <author>
      <name>Raphael Clifford</name>
    </author>
    <author>
      <name>Markus Jalsenius</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <author>
      <name>Benjamin Sach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.3470v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3470v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1202.2820">
    <id>http://arxiv.org/abs/1202.2820v1</id>
    <updated>2012-02-13T19:09:26Z</updated>
    <published>2012-02-13T19:09:26Z</published>
    <title>On Approximating String Selection Problems with Outliers</title>
    <summary>  Many problems in bioinformatics are about finding strings that approximately
represent a collection of given strings. We look at more general problems where
some input strings can be classified as outliers. The Close to Most Strings
problem is, given a set S of same-length strings, and a parameter d, find a
string x that maximizes the number of "non-outliers" within Hamming distance d
of x. We prove this problem has no PTAS unless ZPP=NP, correcting a decade-old
mistake. The Most Strings with Few Bad Columns problem is to find a
maximum-size subset of input strings so that the number of non-identical
positions is at most k; we show it has no PTAS unless P=NP. We also observe
Closest to k Strings has no EPTAS unless W[1]=FPT. In sum, outliers help model
problems associated with using biological data, but we show the problem of
finding an approximate solution is computationally difficult.
</summary>
    <author>
      <name>Christina Boucher</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Avivit Levy</name>
    </author>
    <author>
      <name>David Pritchard</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <link href="http://arxiv.org/abs/1202.2820v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.2820v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1201.6358">
    <id>http://arxiv.org/abs/1201.6358v1</id>
    <updated>2012-01-30T18:31:15Z</updated>
    <published>2012-01-30T18:31:15Z</published>
    <title>Deterministic Polynomial-Time Algorithms for Designing Short DNA Words</title>
    <summary>  Designing short DNA words is a problem of constructing a set (i.e., code) of
n DNA strings (i.e., words) with the minimum length such that the Hamming
distance between each pair of words is at least k and the n words satisfy a set
of additional constraints. This problem has applications in, e.g., DNA
self-assembly and DNA arrays. Previous works include those that extended
results from coding theory to obtain bounds on code and word sizes for
biologically motivated constraints and those that applied heuristic local
searches, genetic algorithms, and randomized algorithms. In particular, Kao,
Sanghi, and Schweller (2009) developed polynomial-time randomized algorithms to
construct n DNA words of length within a multiplicative constant of the
smallest possible word length (e.g., 9 max{log n, k}) that satisfy various sets
of constraints with high probability. In this paper, we give deterministic
polynomial-time algorithms to construct DNA words based on derandomization
techniques. Our algorithms can construct n DNA words of shorter length (e.g.,
2.1 log n + 6.28 k) and satisfy the same sets of constraints as the words
constructed by the algorithms of Kao et al. Furthermore, we extend these new
algorithms to construct words that satisfy a larger set of constraints for
which the algorithms of Kao et al. do not work.
</summary>
    <author>
      <name>Ming-Yang Kao</name>
    </author>
    <author>
      <name>Henry C. M. Leung</name>
    </author>
    <author>
      <name>He Sun</name>
    </author>
    <author>
      <name>Yong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.6358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.6358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1201.6162">
    <id>http://arxiv.org/abs/1201.6162v1</id>
    <updated>2012-01-30T10:43:59Z</updated>
    <published>2012-01-30T10:43:59Z</published>
    <title>Quasiperiodicities in Fibonacci strings</title>
    <summary>  We consider the problem of finding quasiperiodicities in a Fibonacci string.
A factor u of a string y is a cover of y if every letter of y falls within some
occurrence of u in y. A string v is a seed of y, if it is a cover of a
superstring of y. A left seed of a string y is a prefix of y that it is a cover
of a superstring of y. Similarly a right seed of a string y is a suffix of y
that it is a cover of a superstring of y. In this paper, we present some
interesting results regarding quasiperiodicities in Fibonacci strings, we
identify all covers, left/right seeds and seeds of a Fibonacci string and all
covers of a circular Fibonacci string.
</summary>
    <author>
      <name>Michalis Christou</name>
    </author>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Costas Iliopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Local Proceedings of "The 38th International Conference on Current
  Trends in Theory and Practice of Computer Science" (SOFSEM 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.6162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.6162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1201.5603">
    <id>http://arxiv.org/abs/1201.5603v1</id>
    <updated>2012-01-26T18:53:00Z</updated>
    <published>2012-01-26T18:53:00Z</published>
    <title>BIN@ERN: Binary-Ternary Compressing Data Coding</title>
    <summary>  This paper describes a new method of data encoding which may be used in
various modern digital, computer and telecommunication systems and devices. The
method permits the compression of data for storage or transmission, allowing
the exact original data to be reconstructed without any loss of content. The
method is characterized by the simplicity of implementation, as well as high
speed and compression ratio. The method is based on a unique scheme of
binary-ternary prefix-free encoding of characters of the original data. This
scheme does not require the transmission of the code tables from encoder to
decoder; allows for the linear presentation of the code lists; permits the
usage of computable indexes of the prefix codes in a linear list for decoding;
makes it possible to estimate the compression ratio prior to encoding; makes
the usage of multiplication and division operations, as well as operations with
the floating point unnecessary; proves to be effective for static as well as
adaptive coding; applicable to character sets of any size; allows for repeated
compression to improve the ratio.
</summary>
    <author>
      <name>Igor Nesiolovskiy</name>
    </author>
    <author>
      <name>Artem Nesiolovskiy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.5603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.5603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1206.1837">
    <id>http://arxiv.org/abs/1206.1837v2</id>
    <updated>2012-06-30T05:18:12Z</updated>
    <published>2012-06-08T18:54:33Z</published>
    <title>Efficient Algorithms for Finding Tucker Patterns</title>
    <summary>  The Consecutive Ones Property is an important notion for binary matrices,
both from a theoretical and applied point of view. Tucker gave in 1972 a
characterization of matrices that do not satisfy the Consecutive Ones Property
in terms of forbidden submatrices, the Tucker patterns. We describe here a
linear time algorithm to find a Tucker pattern in a non-C1P binary matrix,
which allows to extract in linear time a certificate for the non-C1P. We also
describe an output-sensitive algorithm to enumerate all Tucker patterns of a
non-C1P binary matrix.
  This paper had been withdrawn due to some missing cases in Algorithms 2 and
3.
</summary>
    <author>
      <name>Cedric Chauve</name>
    </author>
    <author>
      <name>Tamon Stephen</name>
    </author>
    <author>
      <name>Maria Tamayo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages. Preliminary version This paper had been withdrawn due to
  some missing cases in Algorithms 2 and 3</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.1837v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.1837v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05C85" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1205.6787">
    <id>http://arxiv.org/abs/1205.6787v1</id>
    <updated>2012-05-30T19:44:17Z</updated>
    <published>2012-05-30T19:44:17Z</published>
    <title>Lyndon Words and Short Superstrings</title>
    <summary>  In the Shortest-Superstring problem, we are given a set of strings S and want
to find a string that contains all strings in S as substrings and has minimum
length. This is a classical problem in approximation and the best known
approximation factor is 2 1/2, given by Sweedyk in 1999. Since then no
improvement has been made, howerever two other approaches yielding a 2
1/2-approximation algorithms have been proposed by Kaplan et al. and recently
by Paluch et al., both based on a reduction to maximum asymmetric TSP path
(Max-ATSP-Path) and structural results of Breslauer et al.
  In this paper we give an algorithm that achieves an approximation ratio of 2
11/23, breaking through the long-standing bound of 2 1/2.
  We use the standard reduction of Shortest-Superstring to Max-ATSP-Path. The
new, somewhat surprising, algorithmic idea is to take the better of the two
solutions obtained by using: (a) the currently best 2/3-approximation algorithm
for Max-ATSP-Path and (b) a naive cycle-cover based 1/2-approximation
algorithm. To prove that this indeed results in an improvement, we further
develop a theory of string overlaps, extending the results of Breslauer et al.
This theory is based on the novel use of Lyndon words, as a substitute for
generic unbordered rotations and critical factorizations, as used by Breslauer
et al.
</summary>
    <author>
      <name>Marcin Mucha</name>
    </author>
    <link href="http://arxiv.org/abs/1205.6787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.6787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1205.2632">
    <id>http://arxiv.org/abs/1205.2632v1</id>
    <updated>2012-05-09T15:49:12Z</updated>
    <published>2012-05-09T15:49:12Z</published>
    <title>Improving Compressed Counting</title>
    <summary>  Compressed Counting (CC) [22] was recently proposed for estimating the ath
frequency moments of data streams, where 0 &lt; a &lt;= 2. CC can be used for
estimating Shannon entropy, which can be approximated by certain functions of
the ath frequency moments as a -> 1. Monitoring Shannon entropy for anomaly
detection (e.g., DDoS attacks) in large networks is an important task. This
paper presents a new algorithm for improving CC. The improvement is most
substantial when a -> 1--. For example, when a = 0:99, the new algorithm
reduces the estimation variance roughly by 100-fold. This new algorithm would
make CC considerably more practical for estimating Shannon entropy.
Furthermore, the new algorithm is statistically optimal when a = 0.5.
</summary>
    <author>
      <name>Ping Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.2632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.2632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1205.0439">
    <id>http://arxiv.org/abs/1205.0439v1</id>
    <updated>2012-05-02T14:17:16Z</updated>
    <published>2012-05-02T14:17:16Z</published>
    <title>TH*:Scalable Distributed Trie Hashing</title>
    <summary>  In today's world of computers, dealing with huge amounts of data is not
unusual. The need to distribute this data in order to increase its availability
and increase the performance of accessing it is more urgent than ever. For
these reasons it is necessary to develop scalable distributed data structures.
In this paper we propose a TH* distributed variant of the Trie Hashing data
structure. First we propose Thsw new version of TH without node Nil in digital
tree (trie), then this version will be adapted to multicomputer environment.
The simulation results reveal that TH* is scalable in the sense that it grows
gracefully, one bucket at a time, to a large number of servers, also TH* offers
a good storage space utilization and high query efficiency special for ordering
operations.
</summary>
    <author>
      <name>Aridj Mohamed</name>
    </author>
    <author>
      <name>Zegour Djamel Eddine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 7, Issue
  6, November 2010 ISSN (Online): 1694-0814 http://www.IJCSI.org</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.0439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.0439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1205.0192">
    <id>http://arxiv.org/abs/1205.0192v2</id>
    <updated>2012-05-11T11:22:55Z</updated>
    <published>2012-05-01T15:39:50Z</published>
    <title>Large-scale compression of genomic sequence databases with the
  Burrows-Wheeler transform</title>
    <summary>  Motivation
  The Burrows-Wheeler transform (BWT) is the foundation of many algorithms for
compression and indexing of text data, but the cost of computing the BWT of
very large string collections has prevented these techniques from being widely
applied to the large sets of sequences often encountered as the outcome of DNA
sequencing experiments. In previous work, we presented a novel algorithm that
allows the BWT of human genome scale data to be computed on very moderate
hardware, thus enabling us to investigate the BWT as a tool for the compression
of such datasets.
  Results
  We first used simulated reads to explore the relationship between the level
of compression and the error rate, the length of the reads and the level of
sampling of the underlying genome and compare choices of second-stage
compression algorithm.
  We demonstrate that compression may be greatly improved by a particular
reordering of the sequences in the collection and give a novel `implicit
sorting' strategy that enables these benefits to be realised without the
overhead of sorting the reads. With these techniques, a 45x coverage of real
human genome sequence data compresses losslessly to under 0.5 bits per base,
allowing the 135.3Gbp of sequence to fit into only 8.2Gbytes of space (trimming
a small proportion of low-quality bases from the reads improves the compression
still further).
  This is more than 4 times smaller than the size achieved by a standard
BWT-based compressor (bzip2) on the untrimmed reads, but an important further
advantage of our approach is that it facilitates the building of compressed
full text indexes such as the FM-index on large-scale DNA sequence collections.
</summary>
    <author>
      <name>Anthony J. Cox</name>
    </author>
    <author>
      <name>Markus J. Bauer</name>
    </author>
    <author>
      <name>Tobias Jakobi</name>
    </author>
    <author>
      <name>Giovanna Rosone</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/bioinformatics/bts173</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/bioinformatics/bts173" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version here is as submitted to Bioinformatics and is same as the
  previously archived version. This submission registers the fact that the
  advanced access version is now available at
  http://bioinformatics.oxfordjournals.org/content/early/2012/05/02/bioinformatics.bts173.abstract
  . Bioinformatics should be considered as the original place of publication of
  this article, please cite accordingly</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.0192v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.0192v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1204.5224">
    <id>http://arxiv.org/abs/1204.5224v2</id>
    <updated>2015-03-16T16:47:42Z</updated>
    <published>2012-04-23T22:46:21Z</published>
    <title>A Fast Algorithm for Permutation Pattern Matching Based on Alternating
  Runs</title>
    <summary>  The NP-complete Permutation Pattern Matching problem asks whether a
$k$-permutation $P$ is contained in a $n$-permutation $T$ as a pattern. This is
the case if there exists an order-preserving embedding of $P$ into $T$. In this
paper, we present a fixed-parameter algorithm solving this problem with a
worst-case runtime of $\mathcal{O}(1.79^{\mathsf{run}(T)}\cdot n\cdot k)$,
where $\mathsf{run}(T)$ denotes the number of alternating runs of $T$. This
algorithm is particularly well-suited for instances where $T$ has few runs,
i.e., few ups and downs. Moreover, since $\mathsf{run}(T)&lt;n$, this can be seen
as a $\mathcal{O}(1.79^{n}\cdot n\cdot k)$ algorithm which is the first to beat
the exponential $2^n$ runtime of brute-force search. Furthermore, we prove that
under standard complexity theoretic assumptions such a fixed-parameter
tractability result is not possible for $\mathsf{run}(P)$.
</summary>
    <author>
      <name>Marie-Louise Bruner</name>
    </author>
    <author>
      <name>Martin Lackner</name>
    </author>
    <link href="http://arxiv.org/abs/1204.5224v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.5224v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1204.4765">
    <id>http://arxiv.org/abs/1204.4765v1</id>
    <updated>2012-04-21T00:36:28Z</updated>
    <published>2012-04-21T00:36:28Z</published>
    <title>String Trees</title>
    <summary>  A string-like compact data structure for unlabelled rooted trees is given
using 2n bits.
</summary>
    <author>
      <name>Julius D'souza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.4765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.4765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1204.3581">
    <id>http://arxiv.org/abs/1204.3581v1</id>
    <updated>2012-04-16T17:50:35Z</updated>
    <published>2012-04-16T17:50:35Z</published>
    <title>The Wavelet Trie: Maintaining an Indexed Sequence of Strings in
  Compressed Space</title>
    <summary>  An indexed sequence of strings is a data structure for storing a string
sequence that supports random access, searching, range counting and analytics
operations, both for exact matches and prefix search. String sequences lie at
the core of column-oriented databases, log processing, and other storage and
query tasks. In these applications each string can appear several times and the
order of the strings in the sequence is relevant. The prefix structure of the
strings is relevant as well: common prefixes are sought in strings to extract
interesting features from the sequence. Moreover, space-efficiency is highly
desirable as it translates directly into higher performance, since more data
can fit in fast memory.
  We introduce and study the problem of compressed indexed sequence of strings,
representing indexed sequences of strings in nearly-optimal compressed space,
both in the static and dynamic settings, while preserving provably good
performance for the supported operations.
  We present a new data structure for this problem, the Wavelet Trie, which
combines the classical Patricia Trie with the Wavelet Tree, a succinct data
structure for storing a compressed sequence. The resulting Wavelet Trie
smoothly adapts to a sequence of strings that changes over time. It improves on
the state-of-the-art compressed data structures by supporting a dynamic
alphabet (i.e. the set of distinct strings) and prefix queries, both crucial
requirements in the aforementioned applications, and on traditional indexes by
reducing space occupancy to close to the entropy of the sequence.
</summary>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Giuseppe Ottaviano</name>
    </author>
    <link href="http://arxiv.org/abs/1204.3581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.3581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1204.1957">
    <id>http://arxiv.org/abs/1204.1957v2</id>
    <updated>2012-04-22T19:43:32Z</updated>
    <published>2012-04-09T19:38:58Z</published>
    <title>Succinct Posets</title>
    <summary>  We describe an algorithm for compressing a partially ordered set, or
\emph{poset}, so that it occupies space matching the information theory lower
bound (to within lower order terms), in the worst case. Using this algorithm,
we design a succinct data structure for representing a poset that, given two
elements, can report whether one precedes the other in constant time. This is
equivalent to succinctly representing the transitive closure graph of the
poset, and we note that the same method can also be used to succinctly
represent the transitive reduction graph. For an $n$ element poset, the data
structure occupies $n^2/4 + o(n^2)$ bits, in the worst case, which is roughly
half the space occupied by an upper triangular matrix. Furthermore, a slight
extension to this data structure yields a succinct oracle for reachability in
arbitrary directed graphs. Thus, using roughly a quarter of the space required
to represent an arbitrary directed graph, reachability queries can be supported
in constant time.
</summary>
    <author>
      <name>J. Ian Munro</name>
    </author>
    <author>
      <name>Patrick K. Nicholson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages lncs format + short appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.1957v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1957v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1012.4263">
    <id>http://arxiv.org/abs/1012.4263v1</id>
    <updated>2010-12-20T09:08:05Z</updated>
    <published>2010-12-20T09:08:05Z</published>
    <title>Lightweight LCP-Array Construction in Linear Time</title>
    <summary>  The suffix tree is a very important data structure in string processing, but
it suffers from a huge space consumption. In large-scale applications,
compressed suffix trees (CSTs) are therefore used instead. A CST consists of
three (compressed) components: the suffix array, the LCP-array, and data
structures for simulating navigational operations on the suffix tree. The
LCP-array stores the lengths of the longest common prefixes of
lexicographically adjacent suffixes, and it can be computed in linear time. In
this paper, we present new LCP-array construction algorithms that are fast and
very space efficient. In practice, our algorithms outperform the currently best
algorithms.
</summary>
    <author>
      <name>Simon Gog</name>
    </author>
    <author>
      <name>Enno Ohlebusch</name>
    </author>
    <link href="http://arxiv.org/abs/1012.4263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.4263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1207.3532">
    <id>http://arxiv.org/abs/1207.3532v1</id>
    <updated>2012-07-15T19:45:19Z</updated>
    <published>2012-07-15T19:45:19Z</published>
    <title>Memory Efficient De Bruijn Graph Construction</title>
    <summary>  Massively parallel DNA sequencing technologies are revolutionizing genomics
research. Billions of short reads generated at low costs can be assembled for
reconstructing the whole genomes. Unfortunately, the large memory footprint of
the existing de novo assembly algorithms makes it challenging to get the
assembly done for higher eukaryotes like mammals. In this work, we investigate
the memory issue of constructing de Bruijn graph, a core task in leading
assembly algorithms, which often consumes several hundreds of gigabytes memory
for large genomes. We propose a disk-based partition method, called Minimum
Substring Partitioning (MSP), to complete the task using less than 10 gigabytes
memory, without runtime slowdown. MSP breaks the short reads into multiple
small disjoint partitions so that each partition can be loaded into memory,
processed individually and later merged with others to form a de Bruijn graph.
By leveraging the overlaps among the k-mers (substring of length k), MSP
achieves astonishing compression ratio: The total size of partitions is reduced
from $\Theta(kn)$ to $\Theta(n)$, where $n$ is the size of the short read
database, and $k$ is the length of a $k$-mer. Experimental results show that
our method can build de Bruijn graphs using a commodity computer for any
large-volume sequence dataset.
</summary>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Pegah Kamousi</name>
    </author>
    <author>
      <name>Fangqiu Han</name>
    </author>
    <author>
      <name>Shengqi Yang</name>
    </author>
    <author>
      <name>Xifeng Yan</name>
    </author>
    <author>
      <name>Subhash Suri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 19 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.3532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.3532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1207.4556">
    <id>http://arxiv.org/abs/1207.4556v2</id>
    <updated>2013-01-24T11:33:51Z</updated>
    <published>2012-07-19T05:47:42Z</published>
    <title>Refined Quicksort asymptotics</title>
    <summary>  The complexity of the Quicksort algorithm is usually measured by the number
of key comparisons used during its execution. When operating on a list of $n$
data, permuted uniformly at random, the appropriately normalized complexity
$Y_n$ is known to converge almost surely to a non-degenerate random limit $Y$.
This assumes a natural embedding of all $Y_n$ on one probability space, e.g.,
via random binary search trees. In this note a central limit theorem for the
error term in the latter almost sure convergence is shown:
$$\sqrt{\frac{n}{2\log n}}(Y_n-Y) \stackrel{d}{\longrightarrow} {\cal N} \qquad
(n\to\infty),$$ where ${\cal N}$ denotes a standard normal random variable.
</summary>
    <author>
      <name>Ralph Neininger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">revised version; title slightly changed; accepted for publication in
  Random Structures and Algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.4556v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4556v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60F05, 60F15, 68P10, 68Q25" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1207.2632">
    <id>http://arxiv.org/abs/1207.2632v2</id>
    <updated>2012-11-17T15:55:16Z</updated>
    <published>2012-07-11T13:30:06Z</published>
    <title>On Optimal Top-K String Retrieval</title>
    <summary>  Let ${\cal{D}}$ = $\{d_1, d_2, d_3, ..., d_D\}$ be a given set of $D$
(string) documents of total length $n$. The top-$k$ document retrieval problem
is to index $\cal{D}$ such that when a pattern $P$ of length $p$, and a
parameter $k$ come as a query, the index returns the $k$ most relevant
documents to the pattern $P$. Hon et. al. \cite{HSV09} gave the first linear
space framework to solve this problem in $O(p + k\log k)$ time. This was
improved by Navarro and Nekrich \cite{NN12} to $O(p + k)$. These results are
powerful enough to support arbitrary relevance functions like frequency,
proximity, PageRank, etc. In many applications like desktop or email search,
the data resides on disk and hence disk-bound indexes are needed. Despite of
continued progress on this problem in terms of theoretical, practical and
compression aspects, any non-trivial bounds in external memory model have so
far been elusive. Internal memory (or RAM) solution to this problem decomposes
the problem into $O(p)$ subproblems and thus incurs the additive factor of
$O(p)$. In external memory, these approaches will lead to $O(p)$ I/Os instead
of optimal $O(p/B)$ I/O term where $B$ is the block-size. We re-interpret the
problem independent of $p$, as interval stabbing with priority over tree-shaped
structure. This leads us to a linear space index in external memory supporting
top-$k$ queries (with unsorted outputs) in near optimal $O(p/B + \log_B n +
\log^{(h)} n + k/B)$ I/Os for any constant $h${$\log^{(1)}n =\log n$ and
$\log^{(h)} n = \log (\log^{(h-1)} n)$}. Then we get $O(n\log^*n)$ space index
with optimal $O(p/B+\log_B n + k/B)$ I/Os.
</summary>
    <author>
      <name>Rahul Shah</name>
    </author>
    <author>
      <name>Cheng Sheng</name>
    </author>
    <author>
      <name>Sharma V. Thankachan</name>
    </author>
    <author>
      <name>Jeffrey Scott Vitter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.2632v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.2632v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1206.6187">
    <id>http://arxiv.org/abs/1206.6187v1</id>
    <updated>2012-06-27T07:13:16Z</updated>
    <published>2012-06-27T07:13:16Z</published>
    <title>Some Novel Results From Analysis of Move To Front (MTF) List Accessing
  Algorithm</title>
    <summary>  List accessing problem has been studied as a problem of significant
theoretical and practical interest in the context of linear search. Various
list accessing algorithms have been proposed in the literature and their
performances have been analyzed theoretically and experimentally.
Move-To-Front(MTF),Transpose (TRANS) and Frequency Count (FC) are the three
primitive and widely used list accessing algorithms. Most of the other list
accessing algorithms are the variants of these three algorithms. As mentioned
in the literature as an open problem, direct bounds on the behavior and
performance of these list accessing algorithms are needed to allow realistic
comparisons. MTF has been proved to be the best performing online algorithm
till date in the literature for real life inputs with locality of reference.
Motivated by the above challenging research issue, in this paper, we have
generated four types of input request sequences corresponding to real life
inputs without locality of reference. Using these types of request sequences,
we have made an analytical study for evaluating the performance of MTF list
accessing algorithm to obtain some novel and interesting theoretical results.
</summary>
    <author>
      <name>Rakesh Mohanty</name>
    </author>
    <author>
      <name>Sangita Patel</name>
    </author>
    <author>
      <name>Shiba Prasad Dash</name>
    </author>
    <author>
      <name>Burle Sharma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, Proceedings of the International Conference on Recent
  Advances in Engineering and Technology, ICRAET, Hyderabad, India, April 2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Systems, Algorithms and Applications, May
  2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.6187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1206.4555">
    <id>http://arxiv.org/abs/1206.4555v4</id>
    <updated>2012-07-08T15:05:35Z</updated>
    <published>2012-06-20T16:53:04Z</published>
    <title>Optimal compression of hash-origin prefix trees</title>
    <summary>  There is a common problem of operating on hash values of elements of some
database. In this paper there will be analyzed informational content of such
general task and how to practically approach such found lower boundaries.
Minimal prefix tree which distinguish elements turns out to require
asymptotically only about 2.77544 bits per element, while standard approaches
use a few times more. While being certain of working inside the database, the
cost of distinguishability can be reduced further to about 2.33275 bits per
elements. Increasing minimal depth of nodes to reduce probability of false
positives leads to simple relation with average depth of such random tree,
which is asymptotically larger by about 1.33275 bits than lg(n) of the perfect
binary tree. This asymptotic case can be also seen as a way to optimally encode
n large unordered numbers - saving lg(n!) bits of information about their
ordering, which can be the major part of contained information. This ability
itself allows to reduce memory requirements even to about 0.693 of required in
Bloom filter for the same false positive probability.
</summary>
    <author>
      <name>Jarek Duda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4555v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4555v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1206.4300">
    <id>http://arxiv.org/abs/1206.4300v1</id>
    <updated>2012-06-19T19:47:12Z</updated>
    <published>2012-06-19T19:47:12Z</published>
    <title>Quasi-Succinct Indices</title>
    <summary>  Compressed inverted indices in use today are based on the idea of gap
compression: documents pointers are stored in increasing order, and the gaps
between successive document pointers are stored using suitable codes which
represent smaller gaps using less bits. Additional data such as counts and
positions is stored using similar techniques. A large body of research has been
built in the last 30 years around gap compression, including theoretical
modeling of the gap distribution, specialized instantaneous codes suitable for
gap encoding, and ad hoc document reorderings which increase the efficiency of
instantaneous codes. This paper proposes to represent an index using a
different architecture based on quasi-succinct representation of monotone
sequences. We show that, besides being theoretically elegant and simple, the
new index provides expected constant-time operations and, in practice,
significant performance improvements on conjunctive, phrasal and proximity
queries.
</summary>
    <author>
      <name>Sebastiano Vigna</name>
    </author>
    <link href="http://arxiv.org/abs/1206.4300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1206.3877">
    <id>http://arxiv.org/abs/1206.3877v1</id>
    <updated>2012-06-18T10:30:02Z</updated>
    <published>2012-06-18T10:30:02Z</published>
    <title>On the combinatorics of suffix arrays</title>
    <summary>  We prove several combinatorial properties of suffix arrays, including a
characterization of suffix arrays through a bijection with a certain
well-defined class of permutations. Our approach is based on the
characterization of Burrows-Wheeler arrays given in [1], that we apply by
reducing suffix sorting to cyclic shift sorting through the use of an
additional sentinel symbol. We show that the characterization of suffix arrays
for a special case of binary alphabet given in [2] easily follows from our
characterization. Based on our results, we also provide simple proofs for the
enumeration results for suffix arrays, obtained in [3]. Our approach to
characterizing suffix arrays is the first that exploits their relationship with
Burrows-Wheeler permutations.
</summary>
    <author>
      <name>Gregory Kucherov</name>
    </author>
    <author>
      <name>Lilla Tóthmérész</name>
    </author>
    <author>
      <name>Stéphane Vialette</name>
    </author>
    <link href="http://arxiv.org/abs/1206.3877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.3877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1206.3511">
    <id>http://arxiv.org/abs/1206.3511v1</id>
    <updated>2012-06-15T16:39:51Z</updated>
    <published>2012-06-15T16:39:51Z</published>
    <title>Comparison of Bucket Sort and RADIX Sort</title>
    <summary>  Bucket sort and RADIX sort are two well-known integer sorting algorithms.
This paper measures empirically what is the time usage and memory consumption
for different kinds of input sequences. The algorithms are compared both from a
theoretical standpoint but also on how well they do in six different use cases
using randomized sequences of numbers. The measurements provide data on how
good they are in different real-life situations.
  It was found that bucket sort was faster than RADIX sort, but that bucket
sort uses more memory in most cases. The sorting algorithms performed faster
with smaller integers. The RADIX sort was not quicker with already sorted
inputs, but the bucket sort was.
</summary>
    <author>
      <name>Panu Horsmalahti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.3511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.3511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1206.2523">
    <id>http://arxiv.org/abs/1206.2523v3</id>
    <updated>2013-05-31T17:32:12Z</updated>
    <published>2012-06-12T13:33:32Z</published>
    <title>Binary Jumbled String Matching for Highly Run-Length Compressible Texts</title>
    <summary>  The Binary Jumbled String Matching problem is defined as: Given a string $s$
over $\{a,b\}$ of length $n$ and a query $(x,y)$, with $x,y$ non-negative
integers, decide whether $s$ has a substring $t$ with exactly $x$ $a$'s and $y$
$b$'s. Previous solutions created an index of size O(n) in a pre-processing
step, which was then used to answer queries in constant time. The fastest
algorithms for construction of this index have running time $O(n^2/\log n)$
[Burcsi et al., FUN 2010; Moosa and Rahman, IPL 2010], or $O(n^2/\log^2 n)$ in
the word-RAM model [Moosa and Rahman, JDA 2012]. We propose an index
constructed directly from the run-length encoding of $s$. The construction time
of our index is $O(n+\rho^2\log \rho)$, where O(n) is the time for computing
the run-length encoding of $s$ and $\rho$ is the length of this encoding---this
is no worse than previous solutions if $\rho = O(n/\log n)$ and better if $\rho
= o(n/\log n)$. Our index $L$ can be queried in $O(\log \rho)$ time. While
$|L|= O(\min(n, \rho^{2}))$ in the worst case, preliminary investigations have
indicated that $|L|$ may often be close to $\rho$. Furthermore, the algorithm
for constructing the index is conceptually simple and easy to implement. In an
attempt to shed light on the structure and size of our index, we characterize
it in terms of the prefix normal forms of $s$ introduced in [Fici and Lipt\'ak,
DLT 2011].
</summary>
    <author>
      <name>Golnaz Badkobeh</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Steve Kroon</name>
    </author>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ipl.2013.05.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ipl.2013.05.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v2: only small cosmetic changes; v3: new title, weakened conjectures
  on size of Corner Index (we no longer conjecture it to be always linear in
  size of RLE); removed experimental part on random strings (these are valid
  but limited in their predictive power w.r.t. general strings); v3 published
  in IPL</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Processing Letters, 113: 604-608 (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.2523v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.2523v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32, 68P05, 68P20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1206.1877">
    <id>http://arxiv.org/abs/1206.1877v1</id>
    <updated>2012-06-08T21:01:03Z</updated>
    <published>2012-06-08T21:01:03Z</published>
    <title>On the Complexity of Minimum Labeling Alignment of Two Genomes</title>
    <summary>  In this note we investigate the complexity of the Minimum Label Alignment
problem and we show that such a problem is APX-hard.
</summary>
    <author>
      <name>Riccardo Dondi</name>
    </author>
    <author>
      <name>Nadia El-Mabrouk</name>
    </author>
    <link href="http://arxiv.org/abs/1206.1877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.1877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1209.4714">
    <id>http://arxiv.org/abs/1209.4714v1</id>
    <updated>2012-09-21T06:07:14Z</updated>
    <published>2012-09-21T06:07:14Z</published>
    <title>Sorting distinct integers using improved in-place associative sort</title>
    <summary>  In-place associative integer sorting technique was proposed for integer lists
which requires only constant amount of additional memory replacing bucket sort,
distribution counting sort and address calculation sort family of algorithms.
Afterwards, the technique was further improved and an in-place sorting
algorithm is proposed where n integers S[0...n-1] each in the range [0, n-1]
are sorted exactly in O(n) time while the complexity of the former technique
was the recursion T(n) = T(n/2) + O(n) yielding T(n) = O(n).
  The technique was specialized with two variants one for read-only distinct
integer keys and the other for modifiable distinct integers, as well. Assuming
w is the fixed word length, the variant for modifiable distinct integers was
capable of sorting n distinct integers S[0...n-1] each in the range [0, m-1] in
exactly O(n) time if m &lt; (w-logn)n. Otherwise, it sort in O(n + m/(w-logn))
time for the worst, O(m/(w-logn)) time for the average (uniformly distributed
keys) and O(n) time for the best case using only O(1) extra space.
  In this study, the variant for modifiable distinct integers is improved and
an algorithm is obtained that sorts n distinct integers S[0...n-1] each in the
range [0, m-1] in exactly O(n) time if m &lt; (w-1)n. Otherwise, it sort in O(n +
m/(w-1)) time for the worst, O(m/(w-1)) time for the average (uniformly
distributed keys) and O(n) time for the best case using only O(1) extra space.
</summary>
    <author>
      <name>A. Emre Cetin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages. arXiv admin note: substantial text overlap with
  arXiv:1209.3668, arXiv:1209.1942, arXiv:1209.0572</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.4714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68P10" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1209.3668">
    <id>http://arxiv.org/abs/1209.3668v1</id>
    <updated>2012-09-17T14:44:42Z</updated>
    <published>2012-09-17T14:44:42Z</published>
    <title>Improved in-place associative integer sorting</title>
    <summary>  A novel integer sorting technique was proposed replacing bucket sort,
distribution counting sort and address calculation sort family of algorithms
which requires only constant amount of additional memory. The technique was
inspired from one of the ordinal theories of "serial order in behavior" and
explained by the analogy with the three main stages in the formation and
retrieval of memory in cognitive neuroscience namely (i) practicing, (ii)
storing and (iii) retrieval.
  In this study, the technique is improved both theoretically and practically
and an algorithm is obtained which is faster than the former making it more
competitive. With the improved version, n integers S[0...n-1] each in the range
[0, n-1] are sorted exactly in O(n) time while the complexity of the former
technique was the recursion T(n) = T(n/2) + O(n) yielding T(n) = O(n).
</summary>
    <author>
      <name>A. Emre Cetin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages. arXiv admin note: substantial text overlap with
  arXiv:1209.0572, arXiv:1209.1942</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.3668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68P10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1209.1942">
    <id>http://arxiv.org/abs/1209.1942v2</id>
    <updated>2012-09-17T14:38:00Z</updated>
    <published>2012-09-10T11:04:02Z</published>
    <title>Sorting distinct integer keys using in-place associative sort</title>
    <summary>  In-place associative integer sorting technique was proposed for integer lists
which requires only constant amount of additional memory replacing bucket sort,
distribution counting sort and address calculation sort family of algorithms.
The technique was explained by the analogy with the three main stages in the
formation and retrieval of memory in cognitive neuroscience which are (i)
practicing, (ii) storing and (iii) retrieval.
  In this study, the technique is specialized with two variants one for
read-only integer keys and the other for modifiable integers. Hence, a novel
algorithm is obtained that does not require additional memory other than a
constant amount and sorts faster than all no matter how large is the list
provided that m = O (n logn) where m is the range and n is the number of keys
(or integers).
</summary>
    <author>
      <name>A. Emre Cetin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages. arXiv admin note: substantial text overlap with
  arXiv:1209.0572</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.1942v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1942v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68P10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1209.1045">
    <id>http://arxiv.org/abs/1209.1045v1</id>
    <updated>2012-09-05T17:05:21Z</updated>
    <published>2012-09-05T17:05:21Z</published>
    <title>A New Algorithm for Data Compression Optimization</title>
    <summary>  People tend to store a lot of files inside theirs storage. When the storage
nears it limit, they then try to reduce those files size to minimum by using
data compression software. In this paper we propose a new algorithm for data
compression, called j-bit encoding (JBE). This algorithm will manipulates each
bit of data inside file to minimize the size without losing any data after
decoding which is classified to lossless compression. This basic algorithm is
intended to be combining with other data compression algorithms to optimize the
compression ratio. The performance of this algorithm is measured by comparing
combination of different data compression algorithms.
</summary>
    <author>
      <name>I. Made Agus Dwi Suarjaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Science and
  Applications (IJACSA), Volume 3 Issue 8, 2012, 14-17</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.1045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1209.0572">
    <id>http://arxiv.org/abs/1209.0572v3</id>
    <updated>2012-11-01T14:28:30Z</updated>
    <published>2012-09-04T09:10:28Z</published>
    <title>In-place associative integer sorting</title>
    <summary>  A novel integer value-sorting technique is proposed replacing bucket sort,
distribution counting sort and address calculation sort family of algorithms.
It requires only constant amount of additional memory. The technique is
inspired from one of the ordinal theories of "serial order in behavior" and
explained by the analogy with the three main stages in the formation and
retrieval of memory in cognitive neuroscience namely (i) practicing, (ii)
storing and (iii) retrieval.
  Although not suitable for integer rank-sorting where the problem is to put an
array of elements into ascending or descending order by their numeric keys,
each of which is an integer, the technique seems to be efficient and applicable
to rank-sorting, as well as other problems such as hashing, searching, element
distinction, succinct data structures, gaining space, etc.
</summary>
    <author>
      <name>A. Emre Cetin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages. arXiv admin note: substantial text overlap with
  arXiv:1209.3668, arXiv:1210.1771, arXiv:1209.1942, arXiv:1209.4714</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.0572v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.0572v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68P10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1208.5713">
    <id>http://arxiv.org/abs/1208.5713v1</id>
    <updated>2012-08-28T16:49:59Z</updated>
    <published>2012-08-28T16:49:59Z</published>
    <title>Distance Measures for Sequences</title>
    <summary>  Given a set of sequences, the distance between pairs of them helps us to find
their similarity and derive structural relationship amongst them. For genomic
sequences such measures make it possible to construct the evolution tree of
organisms. In this paper we compare several distance measures and examine a
method that involves circular shifting one sequence against the other for
finding good alignment to minimize Hamming distance. We also use run-length
encoding together with LZ77 to characterize information in a binary sequence.
</summary>
    <author>
      <name>Sandeep Hosangadi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 PAGES</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.5713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.5713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1208.4516">
    <id>http://arxiv.org/abs/1208.4516v2</id>
    <updated>2014-03-26T07:28:55Z</updated>
    <published>2012-08-22T14:50:34Z</published>
    <title>A Dynamic I/O-Efficient Structure for One-Dimensional Top-k Range
  Reporting</title>
    <summary>  We present a structure in external memory for "top-k range reporting", which
uses linear space, answers a query in O(lg_B n + k/B) I/Os, and supports an
update in O(lg_B n) amortized I/Os, where n is the input size, and B is the
block size. This improves the state of the art which incurs O(lg^2_B n)
amortized I/Os per update.
</summary>
    <author>
      <name>Yufei Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In PODS'14</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.4516v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.4516v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1208.3313">
    <id>http://arxiv.org/abs/1208.3313v1</id>
    <updated>2012-08-16T08:33:36Z</updated>
    <published>2012-08-16T08:33:36Z</published>
    <title>A Note on Efficient Computation of All Abelian Periods in a String</title>
    <summary>  We derive a simple efficient algorithm for Abelian periods knowing all
Abelian squares in a string. An efficient algorithm for the latter problem was
given by Cummings and Smyth in 1997. By the way we show an alternative
algorithm for Abelian squares. We also obtain a linear time algorithm finding
all `long' Abelian periods. The aim of the paper is a (new) reduction of the
problem of all Abelian periods to that of (already solved) all Abelian squares
which provides new insight into both connected problems.
</summary>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Costas Iliopoulos</name>
    </author>
    <author>
      <name>Tomasz Kociumaka</name>
    </author>
    <author>
      <name>Marcin Kubica</name>
    </author>
    <author>
      <name>Jakub Pachocki</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Wojciech Tyczyński</name>
    </author>
    <author>
      <name>Tomasz Waleń</name>
    </author>
    <link href="http://arxiv.org/abs/1208.3313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.3313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1208.3798">
    <id>http://arxiv.org/abs/1208.3798v1</id>
    <updated>2012-08-19T01:26:33Z</updated>
    <published>2012-08-19T01:26:33Z</published>
    <title>On-line Indexing for General Alphabets via Predecessor Queries on
  Subsets of an Ordered List</title>
    <summary>  The problem of Text Indexing is a fundamental algorithmic problem in which
one wishes to preprocess a text in order to quickly locate pattern queries
within the text. In the ever evolving world of dynamic and on-line data, there
is also a need for developing solutions to index texts which arrive on-line,
i.e. a character at a time, and still be able to quickly locate said patterns.
In this paper, a new solution for on-line indexing is presented by providing an
on-line suffix tree construction in $O(\log \log n + \log\log |\Sigma|)$
worst-case expected time per character, where $n$ is the size of the string,
and $\Sigma$ is the alphabet. This improves upon all previously known on-line
suffix tree constructions for general alphabets, at the cost of having the run
time in expectation.
  The main idea is to reduce the problem of constructing a suffix tree on-line
to an interesting variant of the order maintenance problem, which may be of
independent interest. In the famous order maintenance problem, one wishes to
maintain a dynamic list $L$ of size $n$ under insertions, deletions, and order
queries. In an order query, one is given two nodes from $L$ and must determine
which node precedes the other in $L$. In the Predecessor search on Dynamic
Subsets of an Ordered Dynamic List problem (POLP) it is also necessary to
maintain dynamic subsets of $L$ such that given some $u\in L$ it will be
possible to quickly locate the predecessor of $u$ in any subset. This paper
provides an efficient data structure capable of solving the POLP with
worst-case expected bounds that match the currently best known bounds for
predecessor search in the RAM model, improving over a solution which may be
implicitly obtained from Dietz [Die89].
  Furthermore, this paper improves or simplifies bounds for several additional
applications, including fully-persistent arrays and the Order-Maintenance
Problem.
</summary>
    <author>
      <name>Tsvi Kopelowitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to FOCS 2012, 17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.3798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.3798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1207.6944">
    <id>http://arxiv.org/abs/1207.6944v2</id>
    <updated>2012-08-09T08:32:50Z</updated>
    <published>2012-07-30T14:30:58Z</published>
    <title>Efficient algorithms for highly compressed data: The Word Problem in
  Generalized Higman Groups is in P</title>
    <summary>  This paper continues the 2012 STACS contribution by Diekert, Ushakov, and the
author. We extend the results published in the proceedings in two ways.
  First, we show that the data structure of power circuits can be generalized
to work with arbitrary bases q>=2. This results in a data structure that can
hold huge integers, arising by iteratively forming powers of q. We show that
the properties of power circuits known for q=2 translate to the general case.
This generalization is non-trivial and additional techniques are required to
preserve the time bounds of arithmetic operations that were shown for the case
q=2.
  The extended power circuit model permits us to conduct operations in the
Baumslag-Solitar group BS(1,q) as efficiently as in BS(1,2). This allows us to
solve the word problem in the generalization H_4(1,q) of Higman's group, which
is an amalgamated product of four copies of the Baumslag-Solitar group BS(1,q)
rather than BS(1,2) in the original form.
  As a second result, we allow arbitrary numbers f>=4 of copies of BS(1,q),
leading to an even more generalized notion of Higman groups H_f(1,q). We prove
that the word problem of the latter can still be solved within the O(n^6) time
bound that was shown for H_4(1,2).
</summary>
    <author>
      <name>Jürn Laun</name>
    </author>
    <link href="http://arxiv.org/abs/1207.6944v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.6944v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1211.5108">
    <id>http://arxiv.org/abs/1211.5108v1</id>
    <updated>2012-11-21T18:22:39Z</updated>
    <published>2012-11-21T18:22:39Z</published>
    <title>The Rightmost Equal-Cost Position Problem</title>
    <summary>  LZ77-based compression schemes compress the input text by replacing factors
in the text with an encoded reference to a previous occurrence formed by the
couple (length, offset). For a given factor, the smallest is the offset, the
smallest is the resulting compression ratio. This is optimally achieved by
using the rightmost occurrence of a factor in the previous text. Given a cost
function, for instance the minimum number of bits used to represent an integer,
we define the Rightmost Equal-Cost Position (REP) problem as the problem of
finding one of the occurrences of a factor which cost is equal to the cost of
the rightmost one. We present the Multi-Layer Suffix Tree data structure that,
for a text of length n, at any time i, it provides REP(LPF) in constant time,
where LPF is the longest previous factor, i.e. the greedy phrase, a reference
to the list of REP({set of prefixes of LPF}) in constant time and REP(p) in
time O(|p| log log n) for any given pattern p.
</summary>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Alessio Langiu</name>
    </author>
    <author>
      <name>Filippo Mignosi</name>
    </author>
    <link href="http://arxiv.org/abs/1211.5108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1211.7110">
    <id>http://arxiv.org/abs/1211.7110v1</id>
    <updated>2012-11-29T22:38:57Z</updated>
    <published>2012-11-29T22:38:57Z</published>
    <title>Algorithms for discovering and proving theorems about permutation
  patterns</title>
    <summary>  We present an algorithm, called BiSC, that describes the patterns avoided by
a given set of permutations. It automatically conjectures the statements of
known theorems such as the descriptions of stack-sortable (Knuth 1975) and
West-2-stack-sortable permutations (West 1990), smooth (Lakshmibai and Sandhya
1990) and forest-like permutations (Bousquet-Melou and Butler 2007), and simsun
permutations (Branden and Claesson 2011). The algorithm has also been used to
discover new theorems and conjectures related to Young tableaux,
Wilf-equivalences and sorting devices. We further give algorithms to prove a
complete description of preimages of pattern classes under certain sorting
devices. These generalize an algorithm of Claesson and Ulfarsson (2012) and
allow us to prove a linear time algorithm for finding occurrences of the
pattern 4312.
</summary>
    <author>
      <name>Hjalti Magnusson</name>
    </author>
    <author>
      <name>Henning Ulfarsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.7110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.7110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05A05" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1211.2636">
    <id>http://arxiv.org/abs/1211.2636v1</id>
    <updated>2012-11-12T14:36:14Z</updated>
    <published>2012-11-12T14:36:14Z</published>
    <title>A memory versus compression ratio trade-off in PPM via compressed
  context modeling</title>
    <summary>  Since its introduction prediction by partial matching (PPM) has always been a
de facto gold standard in lossless text compression, where many variants
improving the compression ratio and speed have been proposed. However, reducing
the high space requirement of PPM schemes did not gain that much attention.
This study focuses on reducing the memory consumption of PPM via the recently
proposed compressed context modeling that uses the compressed representations
of contexts in the statistical model. Differently from the classical context
definition as the string of the preceding characters at a particular position,
CCM considers context as the amount of preceding information that is actually
the bit stream composed by compressing the previous symbols. We observe that by
using the CCM, the data structures, particularly the context trees, can be
implemented in smaller space, and present a trade-off between the compression
ratio and the space requirement. The experiments conducted showed that this
trade-off is especially beneficial in low orders with approximately 20 - 25
percent gain in memory by a sacrifice of up to nearly 7 percent loss in
compression ratio.
</summary>
    <author>
      <name>M. Oguzhan Kulekci</name>
    </author>
    <link href="http://arxiv.org/abs/1211.2636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.2636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1210.6176">
    <id>http://arxiv.org/abs/1210.6176v2</id>
    <updated>2013-05-01T06:09:51Z</updated>
    <published>2012-10-23T09:51:03Z</published>
    <title>New algorithms for binary jumbled pattern matching</title>
    <summary>  Given a pattern $P$ and a text $T$, both strings over a binary alphabet, the
binary jumbled string matching problem consists in telling whether any
permutation of $P$ occurs in $T$. The indexed version of this problem, i.e.,
preprocessing a string to efficiently answer such permutation queries, is hard
and has been studied in the last few years. Currently the best bounds for this
problem are $O(n^2/\log^2 n)$ (with O(n) space and O(1) query time) and
$O(r^2\log r)$ (with O(|L|) space and $O(\log|L|)$ query time), where $r$ is
the length of the run-length encoding of $T$ and $|L| = O(n)$ is the size of
the index. In this paper we present new results for this problem. Our first
result is an alternative construction of the index by Badkobeh et al. that
obtains a trade-off between the space and the time complexity. It has
$O(r^2\log k + n/k)$ complexity to build the index, $O(\log k)$ query time, and
uses $O(n/k + |L|)$ space, where $k$ is a parameter. The second result is an
$O(n^2 \log^2 w / w)$ algorithm (with O(n) space and O(1) query time), based on
word-level parallelism where $w$ is the word size in bits.
</summary>
    <author>
      <name>Emanuele Giaquinta</name>
    </author>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ipl.2013.04.013</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ipl.2013.04.013" rel="related"/>
    <link href="http://arxiv.org/abs/1210.6176v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6176v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1210.1771">
    <id>http://arxiv.org/abs/1210.1771v1</id>
    <updated>2012-10-05T14:30:37Z</updated>
    <published>2012-10-05T14:30:37Z</published>
    <title>In-place associative permutation sort</title>
    <summary>  In-place associative integer sorting technique was developed, improved and
specialized for distinct integers. The technique is suitable for integer
sorting. Hence, given a list S of n integers S[0...n-1], the technique sorts
the integers in ascending or descending order. It replaces bucket sort,
distribution counting sort and address calculation sort family of algorithms
and requires only constant amount of additional memory for storing counters and
indices beside the input list. The technique was inspired from one of the
ordinal theories of "serial order in behavior" and explained by the analogy
with the three main stages in the formation and retrieval of memory in
cognitive neuroscience: (i) practicing, (ii) storing and (iii) retrieval.
  In this study in-place associative permutation technique is introduced for
integer key sorting problem. Given a list S of n elements S[0...n-1] each have
an integer key in the range [0,m-1], the technique sorts the elements according
to their integer keys in O(n) time using only O(1) amount of memory if m&lt;=n. On
the other hand, if m>n, it sorts in O(n+m) time for the worst, O(m) time for
the average (uniformly distributed keys) and O(n) time for the best case using
O(1) extra space.
</summary>
    <author>
      <name>A. Emre Cetin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages. arXiv admin note: substantial text overlap with
  arXiv:1209.0572, arXiv:1209.3668, arXiv:1209.1942, arXiv:1209.4714</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.1771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.1771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P05, 68P10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1209.6449">
    <id>http://arxiv.org/abs/1209.6449v1</id>
    <updated>2012-09-28T08:28:43Z</updated>
    <published>2012-09-28T08:28:43Z</published>
    <title>Fast Packed String Matching for Short Patterns</title>
    <summary>  Searching for all occurrences of a pattern in a text is a fundamental problem
in computer science with applications in many other fields, like natural
language processing, information retrieval and computational biology. In the
last two decades a general trend has appeared trying to exploit the power of
the word RAM model to speed-up the performances of classical string matching
algorithms. In this model an algorithm operates on words of length w, grouping
blocks of characters, and arithmetic and logic operations on the words take one
unit of time. In this paper we use specialized word-size packed string matching
instructions, based on the Intel streaming SIMD extensions (SSE) technology, to
design very fast string matching algorithms in the case of short patterns. From
our experimental results it turns out that, despite their quadratic worst case
time complexity, the new presented algorithms become the clear winners on the
average for short patterns, when compared against the most effective algorithms
known in literature.
</summary>
    <author>
      <name>Simone Faro</name>
    </author>
    <author>
      <name>M. Oguzhan Külekci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.6449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.6449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1209.4971">
    <id>http://arxiv.org/abs/1209.4971v1</id>
    <updated>2012-09-22T08:37:17Z</updated>
    <published>2012-09-22T08:37:17Z</published>
    <title>Streaming Complexity of Checking Priority Queues</title>
    <summary>  This work is in the line of designing efficient checkers for testing the
reliability of some massive data structures. Given a sequential access to the
insert/extract operations on such a structure, one would like to decide, a
posteriori only, if it corresponds to the evolution of a reliable structure. In
a context of massive data, one would like to minimize both the amount of
reliable memory of the checker and the number of passes on the sequence of
operations. Chu, Kannan and McGregor initiated the study of checking priority
queues in this setting. They showed that use of timestamps allows to check a
priority queue with a single pass and memory space O(N^(1/2)), up to a
polylogarithmic factor. Later, Chakrabarti, Cormode, Kondapally and McGregor
removed the use of timestamps, and proved that more passes do not help. We show
that, even in the presence of timestamps, more passes do not help, solving a
previously open problem. On the other hand, we show that a second pass, but in
reverse direction, shrinks the memory space to O((log N)^2), extending a
phenomenon the first time observed by Magniez, Mathieu and Nayak for checking
well-parenthesized expressions.
</summary>
    <author>
      <name>Nathanaël François</name>
    </author>
    <author>
      <name>Frederic Magniez</name>
    </author>
    <link href="http://arxiv.org/abs/1209.4971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1209.4554">
    <id>http://arxiv.org/abs/1209.4554v1</id>
    <updated>2012-09-20T14:59:48Z</updated>
    <published>2012-09-20T14:59:48Z</published>
    <title>Bouma2 - A Quasi-Stateless, Tunable Multiple String-Match Algorithm</title>
    <summary>  The Bouma2 algorithm attempts to challenge the prevalent "stateful" exact
string-match paradigms by suggesting a "quasi-stateless" approach. We claim
that using state-machines to solve the multiple exact string-match problem
introduces a hidden artificial constraint, namely the Consume-Order Dependency,
which results in unnecessary overhead. Bouma2 is not restricted in this sense;
we postulate that this allows memory-efficiency and improved performance versus
its state-machine equivalents. The heart of the Bouma2 preprocessing problem is
formulated as a weighted Integer Linear Programming problem, that can be tuned
for memory footprint and performance optimization. Specifically, this allows
Bouma2 to be input-sensitive, as tuning can be based on input characteristics.
Evaluating Bouma2 against the Aho-Corasick variant of the popular Snort
Intrusion Prevention System, we demonstrate double the throughput while using
about 10% of the memory.
</summary>
    <author>
      <name>Erez M. Buchnik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.4554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1209.4214">
    <id>http://arxiv.org/abs/1209.4214v2</id>
    <updated>2013-03-06T10:43:03Z</updated>
    <published>2012-09-19T11:54:17Z</published>
    <title>QuickHeapsort: Modifications and improved analysis</title>
    <summary>  We present a new analysis for QuickHeapsort splitting it into the analysis of
the partition-phases and the analysis of the heap-phases. This enables us to
consider samples of non-constant size for the pivot selection and leads to
better theoretical bounds for the algorithm. Furthermore we introduce some
modifications of QuickHeapsort, both in-place and using n extra bits. We show
that on every input the expected number of comparisons is n lg n - 0.03n + o(n)
(in-place) respectively n lg n -0.997 n+ o (n). Both estimates improve the
previously known best results. (It is conjectured in Wegener93 that the
in-place algorithm Bottom-Up-Heapsort uses at most n lg n + 0.4 n on average
and for Weak-Heapsort which uses n extra-bits the average number of comparisons
is at most n lg n -0.42n in EdelkampS02.) Moreover, our non-in-place variant
can even compete with index based Heapsort variants (e.g. Rank-Heapsort in
WangW07) and Relaxed-Weak-Heapsort (n lg n -0.9 n+ o (n) comparisons in the
worst case) for which no O(n)-bound on the number of extra bits is known.
</summary>
    <author>
      <name>Volker Diekert</name>
    </author>
    <author>
      <name>Armin Weiss</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-38536-0_3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-38536-0_3" rel="related"/>
    <link href="http://arxiv.org/abs/1209.4214v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4214v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1209.4771">
    <id>http://arxiv.org/abs/1209.4771v1</id>
    <updated>2012-09-21T10:10:05Z</updated>
    <published>2012-09-21T10:10:05Z</published>
    <title>Counting common substrings effectively</title>
    <summary>  This article presents effective (dynamic) algorithm for solving a problem of
counting the number of substrings of given string which are also substrings of
second string. Presented algorithm can be used for example for quick
calculation of strings similarity measure using generalized $n$-gram method
(Niewiadomski measure), which are shown. Correctness and complexity analyses
are included.
  -----
  W artykule przedstawiono efektywny (dynamiczny) algorytm wyznaczaj\k{a}cy
miar\k{e} podobie\'nstwa wyraz\'ow za pomoc\k{a} uog\'olnionej metody
$n$-gram\'ow (miary Niewiadomskiego). Uzasadniono tak\.ze poprawno\'s\'c
dzia{\l}ania algorytmu i oszacowano jego z{\l}o\.zono\'s\'c obliczeniow\k{a}.
</summary>
    <author>
      <name>Stanisław Goldstein</name>
    </author>
    <author>
      <name>Piotr Beling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, in Polish</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.4771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1103.4282">
    <id>http://arxiv.org/abs/1103.4282v2</id>
    <updated>2011-03-30T14:07:56Z</updated>
    <published>2011-03-22T14:58:20Z</published>
    <title>Stratified B-trees and versioning dictionaries</title>
    <summary>  A classic versioned data structure in storage and computer science is the
copy-on-write (CoW) B-tree -- it underlies many of today's file systems and
databases, including WAFL, ZFS, Btrfs and more. Unfortunately, it doesn't
inherit the B-tree's optimality properties; it has poor space utilization,
cannot offer fast updates, and relies on random IO to scale. Yet, nothing
better has been developed since. We describe the `stratified B-tree', which
beats all known semi-external memory versioned B-trees, including the CoW
B-tree. In particular, it is the first versioned dictionary to achieve optimal
tradeoffs between space, query and update performance.
</summary>
    <author>
      <name>Andy Twigg</name>
    </author>
    <author>
      <name>Andrew Byde</name>
    </author>
    <author>
      <name>Grzegorz Milos</name>
    </author>
    <author>
      <name>Tim Moreton</name>
    </author>
    <author>
      <name>John Wilkes</name>
    </author>
    <author>
      <name>Tom Wilkie</name>
    </author>
    <link href="http://arxiv.org/abs/1103.4282v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.4282v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1102.4884">
    <id>http://arxiv.org/abs/1102.4884v3</id>
    <updated>2011-04-29T18:50:33Z</updated>
    <published>2011-02-24T00:19:46Z</published>
    <title>Upper Bounds for Maximally Greedy Binary Search Trees</title>
    <summary>  At SODA 2009, Demaine et al. presented a novel connection between binary
search trees (BSTs) and subsets of points on the plane. This connection was
independently discovered by Derryberry et al. As part of their results, Demaine
et al. considered GreedyFuture, an offline BST algorithm that greedily
rearranges the search path to minimize the cost of future searches. They showed
that GreedyFuture is actually an online algorithm in their geometric view, and
that there is a way to turn GreedyFuture into an online BST algorithm with only
a constant factor increase in total search cost. Demaine et al. conjectured
this algorithm was dynamically optimal, but no upper bounds were given in their
paper. We prove the first non-trivial upper bounds for the cost of search
operations using GreedyFuture including giving an access lemma similar to that
found in Sleator and Tarjan's classic paper on splay trees.
</summary>
    <author>
      <name>Kyle Fox</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, WADS 2011. rev 1: Fixed accidental upload of out-of-date
  Fig. 1; rev 2: Added figures and made updates based on reviewer comments</arxiv:comment>
    <link href="http://arxiv.org/abs/1102.4884v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.4884v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1102.4523">
    <id>http://arxiv.org/abs/1102.4523v1</id>
    <updated>2011-02-22T14:47:19Z</updated>
    <published>2011-02-22T14:47:19Z</published>
    <title>On Dynamic Optimality for Binary Search Trees</title>
    <summary>  Does there exist O(1)-competitive (self-adjusting) binary search tree (BST)
algorithms? This is a well-studied problem. A simple offline BST algorithm
GreedyFuture was proposed independently by Lucas and Munro, and they
conjectured it to be O(1)-competitive. Recently, Demaine et al. gave a
geometric view of the BST problem. This view allowed them to give an online
algorithm GreedyArb with the same cost as GreedyFuture. However, no
o(n)-competitive ratio was known for GreedyArb. In this paper we make progress
towards proving O(1)-competitive ratio for GreedyArb by showing that it is
O(\log n)-competitive.
</summary>
    <author>
      <name>Navin Goyal</name>
    </author>
    <author>
      <name>Manoj Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/1102.4523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.4523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1102.1783">
    <id>http://arxiv.org/abs/1102.1783v2</id>
    <updated>2011-03-27T17:20:00Z</updated>
    <published>2011-02-09T05:07:47Z</published>
    <title>Don't Rush into a Union: Take Time to Find Your Roots</title>
    <summary>  We present a new threshold phenomenon in data structure lower bounds where
slightly reduced update times lead to exploding query times. Consider
incremental connectivity, letting t_u be the time to insert an edge and t_q be
the query time. For t_u = Omega(t_q), the problem is equivalent to the
well-understood union-find problem: InsertEdge(s,t) can be implemented by
Union(Find(s), Find(t)). This gives worst-case time t_u = t_q = O(lg n / lglg
n) and amortized t_u = t_q = O(alpha(n)).
  By contrast, we show that if t_u = o(lg n / lglg n), the query time explodes
to t_q >= n^{1-o(1)}. In other words, if the data structure doesn't have time
to find the roots of each disjoint set (tree) during edge insertion, there is
no effective way to organize the information!
  For amortized complexity, we demonstrate a new inverse-Ackermann type
trade-off in the regime t_u = o(t_q).
  A similar lower bound is given for fully dynamic connectivity, where an
update time of o(\lg n) forces the query time to be n^{1-o(1)}. This lower
bound allows for amortization and Las Vegas randomization, and comes close to
the known O(lg n * poly(lglg n)) upper bound.
</summary>
    <author>
      <name>Mihai Patrascu</name>
    </author>
    <author>
      <name>Mikkel Thorup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in STOC'11</arxiv:comment>
    <link href="http://arxiv.org/abs/1102.1783v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.1783v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1102.1746">
    <id>http://arxiv.org/abs/1102.1746v1</id>
    <updated>2011-02-08T23:11:17Z</updated>
    <published>2011-02-08T23:11:17Z</published>
    <title>Algorithms for Jumbled Pattern Matching in Strings</title>
    <summary>  The Parikh vector p(s) of a string s is defined as the vector of
multiplicities of the characters. Parikh vector q occurs in s if s has a
substring t with p(t)=q. We present two novel algorithms for searching for a
query q in a text s. One solves the decision problem over a binary text in
constant time, using a linear size index of the text. The second algorithm, for
a general finite alphabet, finds all occurrences of a given Parikh vector q and
has sub-linear expected time complexity; we present two variants, which both
use a linear size index of the text.
</summary>
    <author>
      <name>Péter Burcsi</name>
    </author>
    <author>
      <name>Ferdinando Cicalese</name>
    </author>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Zsuzsanna Lipták</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0129054112400175</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0129054112400175" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 9 figures; article accepted for publication in the
  International Journal of Foundations of Computer Science</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Int. J. Found. Comput. Sci. 23(2): 357-374 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1102.1746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.1746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1101.5376">
    <id>http://arxiv.org/abs/1101.5376v1</id>
    <updated>2011-01-27T20:09:34Z</updated>
    <published>2011-01-27T20:09:34Z</published>
    <title>Succincter Text Indexing with Wildcards</title>
    <summary>  We study the problem of indexing text with wildcard positions, motivated by
the challenge of aligning sequencing data to large genomes that contain
millions of single nucleotide polymorphisms (SNPs)---positions known to differ
between individuals. SNPs modeled as wildcards can lead to more informed and
biologically relevant alignments. We improve the space complexity of previous
approaches by giving a succinct index requiring $(2 + o(1))n \log \sigma + O(n)
+ O(d \log n) + O(k \log k)$ bits for a text of length $n$ over an alphabet of
size $\sigma$ containing $d$ groups of $k$ wildcards. A key to the space
reduction is a result we give showing how any compressed suffix array can be
supplemented with auxiliary data structures occupying $O(n) + O(d \log
\frac{n}{d})$ bits to also support efficient dictionary matching queries. The
query algorithm for our wildcard index is faster than previous approaches using
reasonable working space. More importantly our new algorithm greatly reduces
the query working space to $O(d m + m \log n)$ bits. We note that compared to
previous results this reduces the working space by two orders of magnitude when
aligning short read data to the Human genome.
</summary>
    <author>
      <name>Chris Thachuk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 additional pages for supporting proofs</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.5376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.5376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1212.4771">
    <id>http://arxiv.org/abs/1212.4771v1</id>
    <updated>2012-12-19T18:07:20Z</updated>
    <published>2012-12-19T18:07:20Z</published>
    <title>Necklaces, Convolutions, and X+Y</title>
    <summary>  We give subquadratic algorithms that, given two necklaces each with n beads
at arbitrary positions, compute the optimal rotation of the necklaces to best
align the beads. Here alignment is measured according to the p norm of the
vector of distances between pairs of beads from opposite necklaces in the best
perfect matching. We show surprisingly different results for p = 1, p even, and
p = \infty. For p even, we reduce the problem to standard convolution, while
for p = \infty and p = 1, we reduce the problem to (min, +) convolution and
(median, +) convolution. Then we solve the latter two convolution problems in
subquadratic time, which are interesting results in their own right. These
results shed some light on the classic sorting X + Y problem, because the
convolutions can be viewed as computing order statistics on the antidiagonals
of the X + Y matrix. All of our algorithms run in o(n^2) time, whereas the
obvious algorithms for these problems run in \Theta(n^2) time.
</summary>
    <author>
      <name>David Bremner</name>
    </author>
    <author>
      <name>Timothy M. Chan</name>
    </author>
    <author>
      <name>Erik D. Demaine</name>
    </author>
    <author>
      <name>Jeff Erickson</name>
    </author>
    <author>
      <name>Ferran Hurtado</name>
    </author>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <author>
      <name>Mihai Patrascu</name>
    </author>
    <author>
      <name>Perouz Taslakian</name>
    </author>
    <link href="http://arxiv.org/abs/1212.4771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.4771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1211.5433">
    <id>http://arxiv.org/abs/1211.5433v3</id>
    <updated>2013-07-31T12:45:11Z</updated>
    <published>2012-11-23T08:30:45Z</published>
    <title>Approximate pattern matching with k-mismatches in packed text</title>
    <summary>  Given strings $P$ of length $m$ and $T$ of length $n$ over an alphabet of
size $\sigma$, the string matching with $k$-mismatches problem is to find the
positions of all the substrings in $T$ that are at Hamming distance at most $k$
from $P$. If $T$ can be read only one character at the time the best known
bounds are $O(n\sqrt{k\log k})$ and $O(n + n\sqrt{k/w}\log k)$ in the word-RAM
model with word length $w$. In the RAM models (including $AC^0$ and word-RAM)
it is possible to read up to $\floor{w / \log \sigma}$ characters in constant
time if the characters of $T$ are encoded using $\ceil{\log \sigma}$ bits. The
only solution for $k$-mismatches in packed text works in $O((n \log\sigma/\log
n)\ceil{m \log (k + \log n / \log\sigma) / w} + n^{\varepsilon})$ time, for any
$\varepsilon > 0$. We present an algorithm that runs in time
$O(\frac{n}{\floor{w/(m\log\sigma)}} (1 + \log \min(k,\sigma) \log m /
\log\sigma))$ in the $AC^0$ model if $m=O(w / \log\sigma)$ and $T$ is given
packed. We also describe a simpler variant that runs in time
$O(\frac{n}{\floor{w/(m\log\sigma)}}\log \min(m, \log w / \log\sigma))$ in the
word-RAM model. The algorithms improve the existing bound for $w =
\Omega(\log^{1+\epsilon}n)$, for any $\epsilon > 0$. Based on the introduced
technique, we present algorithms for several other approximate matching
problems.
</summary>
    <author>
      <name>Emanuele Giaquinta</name>
    </author>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Kimmo Fredriksson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is an extended version of the article that appeared in
  Information Processing Letters 113(19-21):693-697 (2013),
  http://dx.doi.org/10.1016/j.ipl.2013.07.002</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.5433v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5433v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1211.5389">
    <id>http://arxiv.org/abs/1211.5389v2</id>
    <updated>2013-06-10T17:24:29Z</updated>
    <published>2012-11-22T22:32:07Z</published>
    <title>Algorithms for Computing Abelian Periods of Words</title>
    <summary>  Constantinescu and Ilie (Bulletin EATCS 89, 167--170, 2006) introduced the
notion of an \emph{Abelian period} of a word. A word of length $n$ over an
alphabet of size $\sigma$ can have $\Theta(n^{2})$ distinct Abelian periods.
The Brute-Force algorithm computes all the Abelian periods of a word in time
$O(n^2 \times \sigma)$ using $O(n \times \sigma)$ space. We present an off-line
algorithm based on a $\sel$ function having the same worst-case theoretical
complexity as the Brute-Force one, but outperforming it in practice. We then
present on-line algorithms that also enable to compute all the Abelian periods
of all the prefixes of $w$.
</summary>
    <author>
      <name>Gabriele Fici</name>
    </author>
    <author>
      <name>Thierry Lecroq</name>
    </author>
    <author>
      <name>Arnaud Lefebvre</name>
    </author>
    <author>
      <name>Elise Prieur-Gaston</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.dam.2013.08.021</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.dam.2013.08.021" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Discrete Applied Mathematics</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Discrete Applied Mathematics 163: 287-297 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.5389v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5389v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1211.5350">
    <id>http://arxiv.org/abs/1211.5350v1</id>
    <updated>2012-11-22T18:25:56Z</updated>
    <published>2012-11-22T18:25:56Z</published>
    <title>Note on the Greedy Parsing Optimality for Dictionary-Based Text
  Compression</title>
    <summary>  Dynamic dictionary-based compression schemes are the most daily used data
compression schemes since they appeared in the foundational papers of Ziv and
Lempel in 1977, commonly referred to as LZ77. Their work is the base of
Deflate, gZip, WinZip, 7Zip and many others compression software. All of those
compression schemes use variants of the greedy approach to parse the text into
dictionary phrases. Greedy parsing optimality was proved by Cohn et al. (1996)
for fixed length code and unbounded dictionaries. The optimality of the greedy
parsing was never proved for bounded size dictionary which actually all of
those schemes require. We define the suffix-closed property for dynamic
dictionaries and we show that any LZ77-based dictionary, including the bounded
variants, satisfy this property. Under this condition we prove the optimality
of the greedy parsing as a variant of the proof by Cohn et al.
</summary>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Alessio Langiu</name>
    </author>
    <author>
      <name>Filippo Mignosi</name>
    </author>
    <link href="http://arxiv.org/abs/1211.5350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1105.5933">
    <id>http://arxiv.org/abs/1105.5933v3</id>
    <updated>2012-08-27T08:39:17Z</updated>
    <published>2011-05-30T10:25:03Z</published>
    <title>The Cell Probe Complexity of Dynamic Range Counting</title>
    <summary>  In this paper we develop a new technique for proving lower bounds on the
update time and query time of dynamic data structures in the cell probe model.
With this technique, we prove the highest lower bound to date for any explicit
problem, namely a lower bound of $t_q=\Omega((\lg n/\lg(wt_u))^2)$. Here $n$ is
the number of update operations, $w$ the cell size, $t_q$ the query time and
$t_u$ the update time. In the most natural setting of cell size $w=\Theta(\lg
n)$, this gives a lower bound of $t_q=\Omega((\lg n/\lg \lg n)^2)$ for any
polylogarithmic update time. This bound is almost a quadratic improvement over
the highest previous lower bound of $\Omega(\lg n)$, due to P\v{a}tra\c{s}cu
and Demaine [SICOMP'06].
  We prove the lower bound for the fundamental problem of weighted orthogonal
range counting. In this problem, we are to support insertions of
two-dimensional points, each assigned a $\Theta(\lg n)$-bit integer weight. A
query to this problem is specified by a point $q=(x,y)$, and the goal is to
report the sum of the weights assigned to the points dominated by $q$, where a
point $(x',y')$ is dominated by $q$ if $x' \leq x$ and $y' \leq y$. In addition
to being the highest cell probe lower bound to date, the lower bound is also
tight for data structures with update time $t_u = \Omega(\lg^{2+\eps}n)$, where
$\eps>0$ is an arbitrarily small constant.
</summary>
    <author>
      <name>Kasper Green Larsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an updated version of the paper which has been submitted to
  Journal of the ACM by invitation. The new version contains a new section
  which introduces an artificial problem for which it is significantly easier
  to apply the new lower bound technique</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.5933v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.5933v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1105.0187">
    <id>http://arxiv.org/abs/1105.0187v1</id>
    <updated>2011-05-01T17:17:17Z</updated>
    <published>2011-05-01T17:17:17Z</published>
    <title>An Improved Move-To-Front(IMTF) Off-line Algorithm for the List
  Accessing Problem</title>
    <summary>  For the List Accessing Problem, Move-To-Front(MTF) algorithm has been proved
to be the best performing online list accessing algorithm till date in the
literature[10]. In this paper, we have made a comprehensive analysis of MTF
algorithm and developed an Improved-MTF (IMTF) offline algorithm. We have
generated two new types of data set and devise a new method of experimental
analysis for our proposed algorithm. Our experimental analysis shows that IMTF
is performing better than MTF algorithm.
</summary>
    <author>
      <name>Rakesh Mohanty</name>
    </author>
    <author>
      <name>Sasmita Tripathy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computing and
  Communications(IJACC), January, 2011, Vol. 3, No. 1, pp-19-24</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1105.0187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.0187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1104.4353">
    <id>http://arxiv.org/abs/1104.4353v1</id>
    <updated>2011-04-21T20:25:03Z</updated>
    <published>2011-04-21T20:25:03Z</published>
    <title>Random input helps searching predecessors</title>
    <summary>  We solve the dynamic Predecessor Problem with high probability (whp) in
constant time, using only $n^{1+\delta}$ bits of memory, for any constant
$\delta > 0$. The input keys are random wrt a wider class of the well studied
and practically important class of $(f_1, f_2)$-smooth distributions introduced
in \cite{and:mat}. It achieves O(1) whp amortized time. Its worst-case time is
$O(\sqrt{\frac{\log n}{\log \log n}})$. Also, we prove whp $O(\log \log \log
n)$ time using only $n^{1+ \frac{1}{\log \log n}}= n^{1+o(1)}$ bits. Finally,
we show whp $O(\log \log n)$ time using O(n) space.
</summary>
    <author>
      <name>D. Belazzougui</name>
    </author>
    <author>
      <name>A. C. Kaporis</name>
    </author>
    <author>
      <name>P. G. Spirakis</name>
    </author>
    <link href="http://arxiv.org/abs/1104.4353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.4353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1104.3153">
    <id>http://arxiv.org/abs/1104.3153v1</id>
    <updated>2011-04-15T20:29:30Z</updated>
    <published>2011-04-15T20:29:30Z</published>
    <title>Efficient Seeds Computation Revisited</title>
    <summary>  The notion of the cover is a generalization of a period of a string, and
there are linear time algorithms for finding the shortest cover. The seed is a
more complicated generalization of periodicity, it is a cover of a superstring
of a given string, and the shortest seed problem is of much higher algorithmic
difficulty. The problem is not well understood, no linear time algorithm is
known. In the paper we give linear time algorithms for some of its versions ---
computing shortest left-seed array, longest left-seed array and checking for
seeds of a given length. The algorithm for the last problem is used to compute
the seed array of a string (i.e., the shortest seeds for all the prefixes of
the string) in $O(n^2)$ time. We describe also a simpler alternative algorithm
computing efficiently the shortest seeds. As a by-product we obtain an
$O(n\log{(n/m)})$ time algorithm checking if the shortest seed has length at
least $m$ and finding the corresponding seed. We also correct some important
details missing in the previously known shortest-seed algorithm (Iliopoulos et
al., 1996).
</summary>
    <author>
      <name>Michalis Christou</name>
    </author>
    <author>
      <name>Maxime Crochemore</name>
    </author>
    <author>
      <name>Costas S. Iliopoulos</name>
    </author>
    <author>
      <name>Marcin Kubica</name>
    </author>
    <author>
      <name>Solon P. Pissis</name>
    </author>
    <author>
      <name>Jakub Radoszewski</name>
    </author>
    <author>
      <name>Wojciech Rytter</name>
    </author>
    <author>
      <name>Bartosz Szreder</name>
    </author>
    <author>
      <name>Tomasz Walen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, accepted to CPM 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.3153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.3153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1104.3084">
    <id>http://arxiv.org/abs/1104.3084v1</id>
    <updated>2011-04-15T15:15:27Z</updated>
    <published>2011-04-15T15:15:27Z</published>
    <title>I/O-Efficient Data Structures for Colored Range and Prefix Reporting</title>
    <summary>  Motivated by information retrieval applications, we consider the
one-dimensional colored range reporting problem in rank space. The goal is to
build a static data structure for sets C_1,...,C_m \subseteq {1,...,sigma} that
supports queries of the kind: Given indices a,b, report the set Union_{a &lt;= i
&lt;= b} C_i.
  We study the problem in the I/O model, and show that there exists an optimal
linear-space data structure that answers queries in O(1+k/B) I/Os, where k
denotes the output size and B the disk block size in words. In fact, we obtain
the same bound for the harder problem of three-sided orthogonal range
reporting. In this problem, we are to preprocess a set of n two-dimensional
points in rank space, such that all points inside a query rectangle of the form
[x_1,x_2] x (-infinity,y] can be reported. The best previous bounds for this
problem is either O(n lg^2_B n) space and O(1+k/B) query I/Os, or O(n) space
and O(lg^(h)_B n +k/B) query I/Os, where lg^(h)_B n is the base B logarithm
iterated h times, for any constant integer h. The previous bounds are both
achieved under the indivisibility assumption, while our solution exploits the
full capabilities of the underlying machine. Breaking the indivisibility
assumption thus provides us with cleaner and optimal bounds.
  Our results also imply an optimal solution to the following colored prefix
reporting problem. Given a set S of strings, each O(1) disk blocks in length,
and a function c: S -> 2^{1,...,sigma}, support queries of the kind: Given a
string p, report the set Union_{x in S intersection p*} c(x), where p* denotes
the set of strings with prefix p. Finally, we consider the possibility of top-k
extensions of this result, and present a simple solution in a model that allows
non-blocked I/O.
</summary>
    <author>
      <name>Kasper Green Larsen</name>
    </author>
    <author>
      <name>Rasmus Pagh</name>
    </author>
    <link href="http://arxiv.org/abs/1104.3084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.3084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1104.1601">
    <id>http://arxiv.org/abs/1104.1601v5</id>
    <updated>2012-10-04T10:25:19Z</updated>
    <published>2011-04-08T15:46:44Z</published>
    <title>On-line construction of position heaps</title>
    <summary>  We propose a simple linear-time on-line algorithm for constructing a position
heap for a string [Ehrenfeucht et al, 2011]. Our definition of position heap
differs slightly from the one proposed in [Ehrenfeucht et al, 2011] in that it
considers the suffixes ordered from left to right. Our construction is based on
classic suffix pointers and resembles the Ukkonen's algorithm for suffix trees
[Ukkonen, 1995]. Using suffix pointers, the position heap can be extended into
the augmented position heap that allows for a linear-time string matching
algorithm [Ehrenfeucht et al, 2011].
</summary>
    <author>
      <name>Gregory Kucherov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in Journal of Discrete Algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.1601v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.1601v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1707.05095">
    <id>http://arxiv.org/abs/1707.05095v1</id>
    <updated>2017-07-17T11:13:16Z</updated>
    <published>2017-07-17T11:13:16Z</published>
    <title>Truly Sub-cubic Algorithms for Language Edit Distance and RNA Folding
  via Fast Bounded-Difference Min-Plus Product</title>
    <summary>  It is a major open problem whether the $(\min,+)$-product of two $n\times n$
matrices has a truly sub-cubic (i.e. $O(n^{3-\epsilon})$ for $\epsilon>0$) time
algorithm, in particular since it is equivalent to the famous
All-Pairs-Shortest-Paths problem (APSP) in $n$-vertex graphs. Some restrictions
of the $(\min,+)$-product to special types of matrices are known to admit truly
sub-cubic algorithms, each giving rise to a special case of APSP that can be
solved faster. In this paper we consider a new, different and powerful
restriction in which all matrix entries are integers and one matrix can be
arbitrary, as long as the other matrix has "bounded differences" in either its
columns or rows, i.e. any two consecutive entries differ by only a small
amount. We obtain the first truly sub-cubic algorithm for this
bounded-difference $(\min,+)$-product (answering an open problem of Chan and
Lewenstein).
  Our new algorithm, combined with a strengthening of an approach of L.~Valiant
for solving context-free grammar parsing with matrix multiplication, yields the
first truly sub-cubic algorithms for the following problems: Language Edit
Distance (a major problem in the parsing community), RNA-folding (a major
problem in bioinformatics) and Optimum Stack Generation (answering an open
problem of Tarjan).
</summary>
    <author>
      <name>Karl Bringmann</name>
    </author>
    <author>
      <name>Fabrizio Grandoni</name>
    </author>
    <author>
      <name>Barna Saha</name>
    </author>
    <author>
      <name>Virginia Vassilevska Williams</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of the conference paper, appeared in FOCS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.05095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2010.14181">
    <id>http://arxiv.org/abs/2010.14181v1</id>
    <updated>2020-10-27T10:33:01Z</updated>
    <published>2020-10-27T10:33:01Z</published>
    <title>Impossibility Results for Grammar-Compressed Linear Algebra</title>
    <summary>  To handle vast amounts of data, it is natural and popular to compress vectors
and matrices. When we compress a vector from size $N$ down to size $n \ll N$,
it certainly makes it easier to store and transmit efficiently, but does it
also make it easier to process?
  In this paper we consider lossless compression schemes, and ask if we can run
our computations on the compressed data as efficiently as if the original data
was that small. That is, if an operation has time complexity
$T(\rm{inputsize})$, can we perform it on the compressed representation in time
$T(n)$ rather than $T(N)$? We consider the most basic linear algebra
operations: inner product, matrix-vector multiplication, and matrix
multiplication. In particular, given two compressed vectors, can we compute
their inner product in time $O(n)$? Or perhaps we must decompress first and
then multiply, spending $\Omega(N)$ time?
  The answer depends on the compression scheme. While for simple ones such as
Run-Length-Encoding (RLE) the inner product can be done in $O(n)$ time, we
prove that this is impossible for compressions from a richer class: essentially
$n^2$ or even larger runtimes are needed in the worst case (under complexity
assumptions). This is the class of grammar-compressions containing most popular
methods such as the Lempel-Ziv family. These schemes are more compressing than
the simple RLE, but alas, we prove that performing computations on them is much
harder.
</summary>
    <author>
      <name>Amir Abboud</name>
    </author>
    <author>
      <name>Arturs Backurs</name>
    </author>
    <author>
      <name>Karl Bringmann</name>
    </author>
    <author>
      <name>Marvin Künnemann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS'20, 20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.14181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.14181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1103.5510">
    <id>http://arxiv.org/abs/1103.5510v1</id>
    <updated>2011-03-28T23:32:35Z</updated>
    <published>2011-03-28T23:32:35Z</published>
    <title>Orthogonal Range Searching on the RAM, Revisited</title>
    <summary>  We present several new results on one of the most extensively studied topics
in computational geometry, orthogonal range searching. All our results are in
the standard word RAM model for points in rank space:
  ** We present two data structures for 2-d orthogonal range emptiness. The
first achieves O(n lglg n) space and O(lglg n) query time. This improves the
previous results by Alstrup, Brodal, and Rauhe(FOCS'00), with O(n lg^eps n)
space and O(lglg n) query time, or with O(nlglg n) space and O(lg^2 lg n) query
time. Our second data structure uses O(n) space and answers queries in O(lg^eps
n) time. The best previous O(n)-space data structure, due to Nekrich (WADS'07),
answers queries in O(lg n/lglg n) time.
  ** For 3-d orthogonal range reporting, we obtain space O(n lg^{1+eps} n) and
query time O(lglg n + k), for any constant eps>0. This improves previous
results by Afshani (ESA'08), Karpinski and Nekrich (COCOON'09), and Chan
(SODA'11), with O(n lg^3 n) space and O(lglg n + k) query time, or with O(n
lg^{1+eps} n) space and O(lg^2 lg n + k) query time. This implies improved
bounds for orthogonal range reporting in all constant dimensions above 3.
  ** We give a randomized algorithm for 4-d offline dominance range
reporting/emptiness with running time O(n lg n + k). This resolves two open
problems from Preparata and Shamos' seminal book:
  **** given n axis-aligned rectangles in the plane, we can report all k
enclosure pairs in O(n lg n + k) expected time. The best known result was an
O([n lg n + k] lglg n) algorithm from SoCG'95 by Gupta, Janardan, Smid, and
Dasgupta.
  **** given n points in 4-d, we can find all maximal points in O(n lg n)
expected time. The best previous result was an O(n lg n lglg n) algorithm due
to Gabow, Bentley, and Tarjan (STOC'84). This implies record time bounds for
the maxima problem in all constant dimensions above 4.
</summary>
    <author>
      <name>Timothy M. Chan</name>
    </author>
    <author>
      <name>Kasper Green Larsen</name>
    </author>
    <author>
      <name>Mihai Patrascu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in SoCG 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.5510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.5510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1103.4521">
    <id>http://arxiv.org/abs/1103.4521v1</id>
    <updated>2011-03-23T13:57:18Z</updated>
    <published>2011-03-23T13:57:18Z</published>
    <title>An implementation of range trees with fractional cascading in C++</title>
    <summary>  Range trees are multidimensional binary trees which are used to perform
d-dimensional orthogonal range searching. In this technical report we study the
implementation issues of range trees with fractional cascading, named layered
range trees. We also document our implementation of range trees with fractional
cascading in C++ using STL and generic programming techniques.
</summary>
    <author>
      <name>Vissarion Fisikopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.4521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.4521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; I.3.5; D.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1108.5422">
    <id>http://arxiv.org/abs/1108.5422v1</id>
    <updated>2011-08-27T05:15:16Z</updated>
    <published>2011-08-27T05:15:16Z</published>
    <title>Linear Time Inference of Strings from Cover Arrays using a Binary
  Alphabet</title>
    <summary>  Covers being one of the most popular form of regularities in strings, have
drawn much attention over time. In this paper, we focus on the problem of
linear time inference of strings from cover arrays using the least sized
alphabet possible. We present an algorithm that can reconstruct a string $x$
over a two-letter alphabet whenever a valid cover array $C$ is given as an
input. This algorithm uses several interesting combinatorial properties of
cover arrays and an interesting relation between border array and cover array
to achieve this. Our algorithm runs in linear time.
</summary>
    <author>
      <name>Tanaeem M. Moosa</name>
    </author>
    <author>
      <name>Sumaiya Nazeen</name>
    </author>
    <author>
      <name>M. Sohel Rahman</name>
    </author>
    <author>
      <name>Rezwana Reaz</name>
    </author>
    <link href="http://arxiv.org/abs/1108.5422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1108.2157">
    <id>http://arxiv.org/abs/1108.2157v1</id>
    <updated>2011-08-10T11:36:22Z</updated>
    <published>2011-08-10T11:36:22Z</published>
    <title>Optimal Indexes for Sparse Bit Vectors</title>
    <summary>  We consider the problem of supporting Rank() and Select() operations on a bit
vector of length m with n 1 bits. The problem is considered in the succinct
index model, where the bit vector is stored in "read-only" memory and an
additional data structure, called the index, is created during pre-processing
to help answer the above queries. We give asymptotically optimal
density-sensitive trade-offs, involving both m and n, that relate the size of
the index to the number of accesses to the bit vector (and processing time)
needed to answer the above queries. The results are particularly interesting
for the case where n = o(m).
</summary>
    <author>
      <name>Alexander Golynski</name>
    </author>
    <author>
      <name>Alessio Orlandi</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <author>
      <name>S. Srinivasa Rao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Some of these results were published in preliminary form in the
  proceedings of SWAT 2008. There are new upper bounds not in the SWAT version,
  however</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.2157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.2157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1108.0554">
    <id>http://arxiv.org/abs/1108.0554v2</id>
    <updated>2012-03-30T23:04:11Z</updated>
    <published>2011-08-02T12:00:02Z</published>
    <title>Towards an Optimal Space-and-Query-Time Index for Top-k Document
  Retrieval</title>
    <summary>  Let $\D = $$ \{d_1,d_2,...d_D\}$ be a given set of $D$ string documents of
total length $n$, our task is to index $\D$, such that the $k$ most relevant
documents for an online query pattern $P$ of length $p$ can be retrieved
efficiently. We propose an index of size $|CSA|+n\log D(2+o(1))$ bits and
$O(t_{s}(p)+k\log\log n+poly\log\log n)$ query time for the basic relevance
metric \emph{term-frequency}, where $|CSA|$ is the size (in bits) of a
compressed full text index of $\D$, with $O(t_s(p))$ time for searching a
pattern of length $p$ . We further reduce the space to $|CSA|+n\log D(1+o(1))$
bits, however the query time will be $O(t_s(p)+k(\log \sigma \log\log
n)^{1+\epsilon}+poly\log\log n)$, where $\sigma$ is the alphabet size and
$\epsilon >0$ is any constant.
</summary>
    <author>
      <name>Wing-Kai Hon</name>
    </author>
    <author>
      <name>Rahul Shah</name>
    </author>
    <author>
      <name>Sharma V. Thankachan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.0554v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.0554v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1107.4223">
    <id>http://arxiv.org/abs/1107.4223v1</id>
    <updated>2011-07-21T10:11:15Z</updated>
    <published>2011-07-21T10:11:15Z</published>
    <title>Sorting Algorithms with Restrictions</title>
    <summary>  Sorting is one of the most used and well investigated algorithmic problem
[1]. Traditional postulation supposes the sorting data archived, and the
elementary operation as comparisons of two numbers. In a view of appearance of
new processors and applied problems with data streams, sorting changed its
face. This changes and generalizations are the subject of investigation in the
research below.
</summary>
    <author>
      <name>Hakob Aslanyan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Computer Science and Information
  Technologies. CSIT2005: 125-127, Yerevan, Armenia 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1107.4223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.4223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1107.3622">
    <id>http://arxiv.org/abs/1107.3622v1</id>
    <updated>2011-07-19T04:21:58Z</updated>
    <published>2011-07-19T04:21:58Z</published>
    <title>K-sort: A new sorting algorithm that beats Heap sort for n &lt;= 70 lakhs!</title>
    <summary>  Sundararajan and Chakraborty (2007) introduced a new version of Quick sort
removing the interchanges. Khreisat (2007) found this algorithm to be competing
well with some other versions of Quick sort. However, it uses an auxiliary
array thereby increasing the space complexity. Here, we provide a second
version of our new sort where we have removed the auxiliary array. This second
improved version of the algorithm, which we call K-sort, is found to sort
elements faster than Heap sort for an appreciably large array size (n &lt;=
70,00,000) for uniform U[0, 1] inputs.
</summary>
    <author>
      <name>Kiran Kumar Sundararajan</name>
    </author>
    <author>
      <name>Mita Pal</name>
    </author>
    <author>
      <name>Soubhik Chakraborty</name>
    </author>
    <author>
      <name>N. C. Mahanti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.3622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.3622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1107.3593">
    <id>http://arxiv.org/abs/1107.3593v1</id>
    <updated>2011-07-18T22:58:37Z</updated>
    <published>2011-07-18T22:58:37Z</published>
    <title>Privacy-Enhanced Methods for Comparing Compressed DNA Sequences</title>
    <summary>  In this paper, we study methods for improving the efficiency and privacy of
compressed DNA sequence comparison computations, under various querying
scenarios. For instance, one scenario involves a querier, Bob, who wants to
test if his DNA string, $Q$, is close to a DNA string, $Y$, owned by a data
owner, Alice, but Bob does not want to reveal $Q$ to Alice and Alice is willing
to reveal $Y$ to Bob \emph{only if} it is close to $Q$. We describe a
privacy-enhanced method for comparing two compressed DNA sequences, which can
be used to achieve the goals of such a scenario. Our method involves a
reduction to set differencing, and we describe a privacy-enhanced protocol for
set differencing that achieves absolute privacy for Bob (in the information
theoretic sense), and a quantifiable degree of privacy protection for Alice.
One of the important features of our protocols, which makes them ideally suited
to privacy-enhanced DNA sequence comparison problems, is that the communication
complexity of our solutions is proportional to a threshold that bounds the
cardinality of the set differences that are of interest, rather than the
cardinality of the sets involved (which correlates to the length of the DNA
sequences). Moreover, in our protocols, the querier, Bob, can easily compute
the set difference only if its cardinality is close to or below a specified
threshold.
</summary>
    <author>
      <name>David Eppstein</name>
    </author>
    <author>
      <name>Michael T. Goodrich</name>
    </author>
    <author>
      <name>Pierre Baldi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.3593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.3593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.0; K.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1107.1076">
    <id>http://arxiv.org/abs/1107.1076v1</id>
    <updated>2011-07-06T09:47:25Z</updated>
    <published>2011-07-06T09:47:25Z</published>
    <title>Genome Halving by Block Interchange</title>
    <summary>  We address the problem of finding the minimal number of block interchanges
(exchange of two intervals) required to transform a duplicated linear genome
into a tandem duplicated linear genome. We provide a formula for the distance
as well as a polynomial time algorithm for the sorting problem.
</summary>
    <author>
      <name>Antoine Thomas</name>
    </author>
    <author>
      <name>Aïda Ouangraoua</name>
    </author>
    <author>
      <name>Jean-Stéphane Varré</name>
    </author>
    <link href="http://arxiv.org/abs/1107.1076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.1076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1106.6342">
    <id>http://arxiv.org/abs/1106.6342v1</id>
    <updated>2011-06-30T19:04:18Z</updated>
    <published>2011-06-30T19:04:18Z</published>
    <title>Quadratic-time Algorithm for the String Constrained LCS Problem</title>
    <summary>  The problem of finding a longest common subsequence of two main sequences
with some constraint that must be a substring of the result (STR-IC-LCS) was
formulated recently. It is a variant of the constrained longest common
subsequence problem. As the known algorithms for the STR-IC-LCS problem are
cubic-time, the presented quadratic-time algorithm is significantly faster.
</summary>
    <author>
      <name>Sebastian Deorowicz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.6342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.6342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W32" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1106.4412">
    <id>http://arxiv.org/abs/1106.4412v1</id>
    <updated>2011-06-22T10:39:48Z</updated>
    <published>2011-06-22T10:39:48Z</published>
    <title>Space Lower Bounds for Online Pattern Matching</title>
    <summary>  We present space lower bounds for online pattern matching under a number of
different distance measures. Given a pattern of length m and a text that
arrives one character at a time, the online pattern matching problem is to
report the distance between the pattern and a sliding window of the text as
soon as the new character arrives. We require that the correct answer is given
at each position with constant probability. We give Omega(m) bit space lower
bounds for L_1, L_2, L_\infty, Hamming, edit and swap distances as well as for
any algorithm that computes the cross-correlation/convolution. We then show a
dichotomy between distance functions that have wildcard-like properties and
those that do not. In the former case which includes, as an example, pattern
matching with character classes, we give Omega(m) bit space lower bounds. For
other distance functions, we show that there exist space bounds of Omega(log m)
and O(log^2 m) bits. Finally we discuss space lower bounds for non-binary
inputs and show how in some cases they can be improved.
</summary>
    <author>
      <name>Raphael Clifford</name>
    </author>
    <author>
      <name>Markus Jalsenius</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <author>
      <name>Benjamin Sach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures, CPM 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.4412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.4412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1106.2603">
    <id>http://arxiv.org/abs/1106.2603v1</id>
    <updated>2011-06-14T04:06:06Z</updated>
    <published>2011-06-14T04:06:06Z</published>
    <title>SparseAssembler: de novo Assembly with the Sparse de Bruijn Graph</title>
    <summary>  de Bruijn graph-based algorithms are one of the two most widely used
approaches for de novo genome assembly. A major limitation of this approach is
the large computational memory space requirement to construct the de Bruijn
graph, which scales with k-mer length and total diversity (N) of unique k-mers
in the genome expressed in base pairs or roughly (2k+8)N bits. This limitation
is particularly important with large-scale genome analysis and for sequencing
centers that simultaneously process multiple genomes. We present a sparse de
Bruijn graph structure, based on which we developed SparseAssembler that
greatly reduces memory space requirements. The structure also allows us to
introduce a novel method for the removal of substitution errors introduced
during sequencing. The sparse de Bruijn graph structure skips g intermediate
k-mers, therefore reducing the theoretical memory space requirement to
~(2k/g+8)N. We have found that a practical value of g=16 consumes approximately
10% of the memory required by standard de Bruijn graph-based algorithms but
yields comparable results. A high error rate could potentially derail the
SparseAssembler. Therefore, we developed a sparse de Bruijn graph-based
denoising algorithm that can remove more than 99% of substitution errors from
datasets with a \leq 2% error rate. Given that substitution error rates for the
current generation of sequencers is lower than 1%, our denoising procedure is
sufficiently effective to safeguard the performance of our algorithm. Finally,
we also introduce a novel Dijkstra-like breadth-first search algorithm for the
sparse de Bruijn graph structure to circumvent residual errors and resolve
polymorphisms.
</summary>
    <author>
      <name>Chengxi Ye</name>
    </author>
    <author>
      <name>Zhanshan Sam Ma</name>
    </author>
    <author>
      <name>Charles H. Cannon</name>
    </author>
    <author>
      <name>Mihai Pop</name>
    </author>
    <author>
      <name>Douglas W. Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corresponding author: Douglas W. Yu, dougwyu@gmail.com</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.2603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.2603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1110.3225">
    <id>http://arxiv.org/abs/1110.3225v1</id>
    <updated>2011-10-14T14:47:18Z</updated>
    <published>2011-10-14T14:47:18Z</published>
    <title>Mining Patterns in Networks using Homomorphism</title>
    <summary>  In recent years many algorithms have been developed for finding patterns in
graphs and networks. A disadvantage of these algorithms is that they use
subgraph isomorphism to determine the support of a graph pattern; subgraph
isomorphism is a well-known NP complete problem. In this paper, we propose an
alternative approach which mines tree patterns in networks by using subgraph
homomorphism. The advantage of homomorphism is that it can be computed in
polynomial time, which allows us to develop an algorithm that mines tree
patterns in arbitrary graphs in incremental polynomial time. Homomorphism
however entails two problems not found when using isomorphism: (1) two patterns
of different size can be equivalent; (2) patterns of unbounded size can be
frequent. In this paper we formalize these problems and study solutions that
easily fit within our algorithm.
</summary>
    <author>
      <name>Anton Dries</name>
    </author>
    <author>
      <name>Siegfried Nijssen</name>
    </author>
    <link href="http://arxiv.org/abs/1110.3225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1110.0892">
    <id>http://arxiv.org/abs/1110.0892v1</id>
    <updated>2011-10-05T04:12:09Z</updated>
    <published>2011-10-05T04:12:09Z</published>
    <title>On Approximability of Block Sorting</title>
    <summary>  Block Sorting is a well studied problem, motivated by its applications in
Optical Character Recognition (OCR), and Computational Biology. Block Sorting
has been shown to be NP-Hard, and two separate polynomial time 2-approximation
algorithms have been designed for the problem. But questions like whether a
better approximation algorithm can be designed, and whether the problem is
APX-Hard have been open for quite a while now.
  In this work we answer the latter question by proving Block Sorting to be
Max-SNP-Hard (APX-Hard). The APX-Hardness result is based on a linear reduction
of Max-3SAT to Block Sorting. We also provide a new lower bound for the problem
via a new parametrized problem k-Block Merging.
</summary>
    <author>
      <name>N. S. Narayanaswamy</name>
    </author>
    <author>
      <name>Swapnoneel Roy</name>
    </author>
    <link href="http://arxiv.org/abs/1110.0892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.0892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1109.5635">
    <id>http://arxiv.org/abs/1109.5635v1</id>
    <updated>2011-09-26T16:48:20Z</updated>
    <published>2011-09-26T16:48:20Z</published>
    <title>Approximating Edit Distance in Near-Linear Time</title>
    <summary>  We show how to compute the edit distance between two strings of length n up
to a factor of 2^{\~O(sqrt(log n))} in n^(1+o(1)) time. This is the first
sub-polynomial approximation algorithm for this problem that runs in
near-linear time, improving on the state-of-the-art n^(1/3+o(1)) approximation.
Previously, approximation of 2^{\~O(sqrt(log n))} was known only for embedding
edit distance into l_1, and it is not known if that embedding can be computed
in less than quadratic time.
</summary>
    <author>
      <name>Alexandr Andoni</name>
    </author>
    <author>
      <name>Krzysztof Onak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version appeared in STOC 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.5635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.5635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1109.5615">
    <id>http://arxiv.org/abs/1109.5615v1</id>
    <updated>2011-09-26T15:43:45Z</updated>
    <published>2011-09-26T15:43:45Z</published>
    <title>A Regularity Measure for Context Free Grammars</title>
    <summary>  Parikh's theorem states that every Context Free Language (CFL) has the same
Parikh image as that of a regular language. A finite state automaton accepting
such a regular language is called a Parikh-equivalent automaton. In the worst
case, the number of states in any non-deterministic Parikh-equivalent automaton
is exponentially large in the size of the Context Free Grammar (CFG). We
associate a regularity width d with a CFG that measures the closeness of the
CFL with regular languages. The degree m of a CFG is one less than the maximum
number of variable occurrences in the right hand side of any production. Given
a CFG with n variables, we construct a Parikh-equivalent non-deterministic
automaton whose number of states is upper bounded by a polynomial in $n
(d^{2d(m+1)}), the degree of the polynomial being a small fixed constant. Our
procedure is constructive and runs in time polynomial in the size of the
automaton. In the terminology of parameterized complexity, we prove that
constructing a Parikh-equivalent automaton for a given CFG is Fixed Parameter
Tractable (FPT) when the degree m and regularity width d are parameters. We
also give an example from program verification domain where the degree and
regularity are small compared to the size of the grammar.
</summary>
    <author>
      <name>M. Praveen</name>
    </author>
    <link href="http://arxiv.org/abs/1109.5615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.5615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; F.4.2; F.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1109.4460">
    <id>http://arxiv.org/abs/1109.4460v1</id>
    <updated>2011-09-21T02:12:27Z</updated>
    <published>2011-09-21T02:12:27Z</published>
    <title>A Simple Linear-Space Data Structure for Constant-Time Range Minimum
  Query</title>
    <summary>  We revisit the range minimum query problem and present a new O(n)-space data
structure that supports queries in O(1) time. Although previous data structures
exist whose asymptotic bounds match ours, our goal is to introduce a new
solution that is simple, intuitive, and practical without increasing costs for
query time or space.
</summary>
    <author>
      <name>Stephane Durocher</name>
    </author>
    <link href="http://arxiv.org/abs/1109.4460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.4460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1109.1729">
    <id>http://arxiv.org/abs/1109.1729v1</id>
    <updated>2011-09-08T14:34:57Z</updated>
    <published>2011-09-08T14:34:57Z</published>
    <title>Anomaly Sequences Detection from Logs Based on Compression</title>
    <summary>  Mining information from logs is an old and still active research topic. In
recent years, with the rapid emerging of cloud computing, log mining becomes
increasingly important to industry. This paper focus on one major mission of
log mining: anomaly detection, and proposes a novel method for mining abnormal
sequences from large logs. Different from previous anomaly detection systems
which based on statistics, probabilities and Markov assumption, our approach
measures the strangeness of a sequence using compression. It first trains a
grammar about normal behaviors using grammar-based compression, then measures
the information quantities and densities of questionable sequences according to
incrementation of grammar length. We have applied our approach on mining some
real bugs from fine grained execution logs. We have also tested its ability on
intrusion detection using some publicity available system call traces. The
experiments show that our method successfully selects the strange sequences
which related to bugs or attacking.
</summary>
    <author>
      <name>Nan Wang</name>
    </author>
    <author>
      <name>Jizhong Han</name>
    </author>
    <author>
      <name>Jinyun Fang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.1729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1109.2885">
    <id>http://arxiv.org/abs/1109.2885v2</id>
    <updated>2012-04-25T16:58:51Z</updated>
    <published>2011-09-13T19:04:03Z</published>
    <title>Encoding 2-D Range Maximum Queries</title>
    <summary>  We consider the \emph{two-dimensional range maximum query (2D-RMQ)} problem:
given an array $A$ of ordered values, to pre-process it so that we can find the
position of the smallest element in the sub-matrix defined by a
(user-specified) range of rows and range of columns. We focus on determining
the \emph{effective} entropy of 2D-RMQ, i.e., how many bits are needed to
encode $A$ so that 2D-RMQ queries can be answered \emph{without} access to $A$.
We give tight upper and lower bounds on the expected effective entropy for the
case when $A$ contains independent identically-distributed random values, and
new upper and lower bounds for arbitrary $A$, for the case when $A$ contains
few rows. The latter results improve upon previous upper and lower bounds by
Brodal et al. (ESA 2010). In some cases we also give data structures whose
space usage is close to the effective entropy and answer 2D-RMQ queries
rapidly.
</summary>
    <author>
      <name>Mordecai J. Golin</name>
    </author>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Danny Krizanc</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <author>
      <name>S. Srinivasa Rao</name>
    </author>
    <author>
      <name>Sunil Shende</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of ISAAC 2011 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.2885v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.2885v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1109.2348">
    <id>http://arxiv.org/abs/1109.2348v1</id>
    <updated>2011-09-11T20:15:40Z</updated>
    <published>2011-09-11T20:15:40Z</published>
    <title>Lossless data compression on GPGPU architectures</title>
    <summary>  Modern graphics processors provide exceptional computa- tional power, but
only for certain computational models. While they have revolutionized
computation in many fields, compression has been largely unnaffected. This
paper aims to explain the current issues and possibili- ties in GPGPU
compression. This is done by a high level overview of the GPGPU computational
model in the context of compression algorithms; along with a more in-depth
analysis of how one would implement bzip2 on a GPGPU architecture.
</summary>
    <author>
      <name>Axel Eirola</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Aalto University special course on data compression course
  assignment. (http://www.cs.hut.fi/~travis/compression/)</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.2348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.2348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1109.1494">
    <id>http://arxiv.org/abs/1109.1494v2</id>
    <updated>2011-10-28T11:13:43Z</updated>
    <published>2011-09-07T15:46:51Z</published>
    <title>Pattern Matching under Polynomial Transformation</title>
    <summary>  We consider a class of pattern matching problems where a normalising
transformation is applied at every alignment. Normalised pattern matching plays
a key role in fields as diverse as image processing and musical information
processing where application specific transformations are often applied to the
input. By considering the class of polynomial transformations of the input, we
provide fast algorithms and the first lower bounds for both new and old
problems. Given a pattern of length m and a longer text of length n where both
are assumed to contain integer values only, we first show O(n log m) time
algorithms for pattern matching under linear transformations even when wildcard
symbols can occur in the input. We then show how to extend the technique to
polynomial transformations of arbitrary degree. Next we consider the problem of
finding the minimum Hamming distance under polynomial transformation. We show
that, for any epsilon>0, there cannot exist an O(n m^(1-epsilon)) time
algorithm for additive and linear transformations conditional on the hardness
of the classic 3SUM problem. Finally, we consider a version of the Hamming
distance problem under additive transformations with a bound k on the maximum
distance that need be reported. We give a deterministic O(nk log k) time
solution which we then improve by careful use of randomisation to O(n sqrt(k
log k) log n) time for sufficiently small k. Our randomised solution outputs
the correct answer at every position with high probability.
</summary>
    <author>
      <name>Ayelet Butman</name>
    </author>
    <author>
      <name>Peter Clifford</name>
    </author>
    <author>
      <name>Raphael Clifford</name>
    </author>
    <author>
      <name>Markus Jalsenius</name>
    </author>
    <author>
      <name>Noa Lewenstein</name>
    </author>
    <author>
      <name>Benny Porat</name>
    </author>
    <author>
      <name>Ely Porat</name>
    </author>
    <author>
      <name>Benjamin Sach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.1494v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1494v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1109.0783">
    <id>http://arxiv.org/abs/1109.0783v1</id>
    <updated>2011-09-05T01:57:07Z</updated>
    <published>2011-09-05T01:57:07Z</published>
    <title>Specific "scientific" data structures, and their processing</title>
    <summary>  Programming physicists use, as all programmers, arrays, lists, tuples,
records, etc., and this requires some change in their thought patterns while
converting their formulae into some code, since the "data structures" operated
upon, while elaborating some theory and its consequences, are rather: power
series and Pad\'e approximants, differential forms and other instances of
differential algebras, functionals (for the variational calculus), trajectories
(solutions of differential equations), Young diagrams and Feynman graphs, etc.
Such data is often used in a [semi-]numerical setting, not necessarily
"symbolic", appropriate for the computer algebra packages. Modules adapted to
such data may be "just libraries", but often they become specific, embedded
sub-languages, typically mapped into object-oriented frameworks, with
overloaded mathematical operations. Here we present a functional approach to
this philosophy. We show how the usage of Haskell datatypes and - fundamental
for our tutorial - the application of lazy evaluation makes it possible to
operate upon such data (in particular: the "infinite" sequences) in a natural
and comfortable manner.
</summary>
    <author>
      <name>Jerzy Karczmarczuk</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Caen, France</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.66.10</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.66.10" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings DSL 2011, arXiv:1109.0323</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 66, 2011, pp. 195-209</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.0783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.0783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1001.0827">
    <id>http://arxiv.org/abs/1001.0827v1</id>
    <updated>2010-01-06T07:51:23Z</updated>
    <published>2010-01-06T07:51:23Z</published>
    <title>Document Clustering with K-tree</title>
    <summary>  This paper describes the approach taken to the XML Mining track at INEX 2008
by a group at the Queensland University of Technology. We introduce the K-tree
clustering algorithm in an Information Retrieval context by adapting it for
document clustering. Many large scale problems exist in document clustering.
K-tree scales well with large inputs due to its low complexity. It offers
promising results both in terms of efficiency and quality. Document
classification was completed using Support Vector Machines.
</summary>
    <author>
      <name>Christopher M. De Vries</name>
    </author>
    <author>
      <name>Shlomo Geva</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-03761-0_43</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-03761-0_43" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, INEX 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1112.5636">
    <id>http://arxiv.org/abs/1112.5636v1</id>
    <updated>2011-12-23T19:02:01Z</updated>
    <published>2011-12-23T19:02:01Z</published>
    <title>Tight lower bounds for online labeling problem</title>
    <summary>  We consider the file maintenance problem (also called the online labeling
problem) in which n integer items from the set {1,...,r} are to be stored in an
array of size m >= n. The items are presented sequentially in an arbitrary
order, and must be stored in the array in sorted order (but not necessarily in
consecutive locations in the array). Each new item must be stored in the array
before the next item is received. If r&lt;=m then we can simply store item j in
location j but if r>m then we may have to shift the location of stored items to
make space for a newly arrived item. The algorithm is charged each time an item
is stored in the array, or moved to a new location. The goal is to minimize the
total number of such moves done by the algorithm. This problem is non-trivial
when n=&lt;m&lt;r.
  In the case that m=Cn for some C>1, algorithms for this problem with cost
O(log(n)^2) per item have been given [IKR81, Wil92, BCD+02]. When m=n,
algorithms with cost O(log(n)^3) per item were given [Zha93, BS07]. In this
paper we prove lower bounds that show that these algorithms are optimal, up to
constant factors. Previously, the only lower bound known for this range of
parameters was a lower bound of \Omega(log(n)^2) for the restricted class of
smooth algorithms [DSZ05a, Zha93].
  We also provide an algorithm for the sparse case: If the number of items is
polylogarithmic in the array size then the problem can be solved in amortized
constant time per item.
</summary>
    <author>
      <name>Jan Bulánek</name>
    </author>
    <author>
      <name>Michal Koucký</name>
    </author>
    <author>
      <name>Michael Saks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.5636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.5636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1112.4131">
    <id>http://arxiv.org/abs/1112.4131v2</id>
    <updated>2011-12-20T10:20:37Z</updated>
    <published>2011-12-18T08:01:08Z</published>
    <title>Uncommon Suffix Tries</title>
    <summary>  Common assumptions on the source producing the words inserted in a suffix
trie with $n$ leaves lead to a $\log n$ height and saturation level. We provide
an example of a suffix trie whose height increases faster than a power of $n$
and another one whose saturation level is negligible with respect to $\log n$.
Both are built from VLMC (Variable Length Markov Chain) probabilistic sources;
they are easily extended to families of sources having the same properties. The
first example corresponds to a "logarithmic infinite comb" and enjoys a non
uniform polynomial mixing. The second one corresponds to a "factorial infinite
comb" for which mixing is uniform and exponential.
</summary>
    <author>
      <name>Peggy Cénac</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMB</arxiv:affiliation>
    </author>
    <author>
      <name>Brigitte Chauvin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LM-Versailles</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Paccaut</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAMFA</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Pouyanne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LM-Versailles</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1112.4131v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.4131v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1112.0520">
    <id>http://arxiv.org/abs/1112.0520v4</id>
    <updated>2012-01-21T21:58:00Z</updated>
    <published>2011-12-02T17:50:10Z</published>
    <title>On the Complexity of Approximate Sum of Sorted List</title>
    <summary>  We consider the complexity for computing the approximate sum
$a_1+a_2+...+a_n$ of a sorted list of numbers $a_1\le a_2\le ...\le a_n$. We
show an algorithm that computes an $(1+\epsilon)$-approximation for the sum of
a sorted list of nonnegative numbers in an $O({1\over \epsilon}\min(\log n,
{\log ({x_{max}\over x_{min}})})\cdot (\log {1\over \epsilon}+\log\log n))$
time, where $x_{max}$ and $x_{min}$ are the largest and the least positive
elements of the input list, respectively. We prove a lower bound
$\Omega(\min(\log n,\log ({x_{max}\over x_{min}}))$ time for every
O(1)-approximation algorithm for the sum of a sorted list of nonnegative
elements. We also show that there is no sublinear time approximation algorithm
for the sum of a sorted list that contains at least one negative number.
</summary>
    <author>
      <name>Bin Fu</name>
    </author>
    <link href="http://arxiv.org/abs/1112.0520v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.0520v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1112.0278">
    <id>http://arxiv.org/abs/1112.0278v2</id>
    <updated>2012-01-01T11:18:29Z</updated>
    <published>2011-12-01T19:24:16Z</published>
    <title>Computing on Binary Strings</title>
    <summary>  Many problems in Computer Science can be abstracted to the following
question: given a set of objects and rules respectively, which new objects can
be produced? In the paper, we consider a succinct version of the question:
given a set of binary strings and several operations like conjunction and
disjunction, which new binary strings can be generated? Although it is a
fundamental problem, to the best of our knowledge, the problem hasn't been
studied yet. In this paper, an O(m^2n) algorithm is presented to determine
whether a string s is representable by a set W, where n is the number of
strings in W and each string has the same length m. However, looking for the
minimum subset from a set to represent a given string is shown to be NP-hard.
Also, finding the smallest subset from a set to represent each string in the
original set is NP-hard. We establishes inapproximability results and
approximation algorithms for them. In addition, we prove that counting the
number of strings representable is #P-complete. We then explore how the
problems change when the operator negation is available. For example, if the
operator negation can be used, the number is some power of 2. This difference
maybe help us understand the problem more profoundly.
</summary>
    <author>
      <name>Tian-Ming Bu</name>
    </author>
    <author>
      <name>Chen Yuan</name>
    </author>
    <author>
      <name>Peng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.0278v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.0278v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1111.5386">
    <id>http://arxiv.org/abs/1111.5386v1</id>
    <updated>2011-11-23T02:08:09Z</updated>
    <published>2011-11-23T02:08:09Z</published>
    <title>Edit Distance to Monotonicity in Sliding Windows</title>
    <summary>  Given a stream of items each associated with a numerical value, its edit
distance to monotonicity is the minimum number of items to remove so that the
remaining items are non-decreasing with respect to the numerical value. The
space complexity of estimating the edit distance to monotonicity of a data
stream is becoming well-understood over the past few years. Motivated by
applications on network quality monitoring, we extend the study to estimating
the edit distance to monotonicity of a sliding window covering the $w$ most
recent items in the stream for any $w \ge 1$. We give a deterministic algorithm
which can return an estimate within a factor of $(4+\eps)$ using
$O(\frac{1}{\eps^2} \log^2(\eps w))$ space.
  We also extend the study in two directions. First, we consider a stream where
each item is associated with a value from a partial ordered set. We give a
randomized $(4+\epsilon)$-approximate algorithm using $O(\frac{1}{\epsilon^2}
\log \epsilon^2 w \log w)$ space. Second, we consider an out-of-order stream
where each item is associated with a creation time and a numerical value, and
items may be out of order with respect to their creation times. The goal is to
estimate the edit distance to monotonicity with respect to the numerical value
of items arranged in the order of creation times. We show that any randomized
constant-approximate algorithm requires linear space.
</summary>
    <author>
      <name>Ho-Leung Chan</name>
    </author>
    <author>
      <name>Tak-Wah Lam</name>
    </author>
    <author>
      <name>Lap-Kei Lee</name>
    </author>
    <author>
      <name>Jiangwei Pan</name>
    </author>
    <author>
      <name>Hing-Fung Ting</name>
    </author>
    <author>
      <name>Qin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.5386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1111.3244">
    <id>http://arxiv.org/abs/1111.3244v4</id>
    <updated>2013-06-25T15:23:56Z</updated>
    <published>2011-11-14T15:26:56Z</published>
    <title>Faster fully compressed pattern matching by recompression</title>
    <summary>  In this paper, a fully compressed pattern matching problem is studied. The
compression is represented by straight-line programs (SLPs), i.e. a
context-free grammars generating exactly one string; the term fully means that
both the pattern and the text are given in the compressed form. The problem is
approached using a recently developed technique of local recompression: the
SLPs are refactored, so that substrings of the pattern and text are encoded in
both SLPs in the same way. To this end, the SLPs are locally decompressed and
then recompressed in a uniform way.
  This technique yields an O((n+m)log M) algorithm for compressed pattern
matching, assuming that M fits in O(1) machine words, where n (m) is the size
of the compressed representation of the text (pattern, respectively), while M
is the size of the decompressed pattern. If only m+n fits in O(1) machine
words, the running time increases to O((n+m)log M log(n+m)). The previous best
algorithm due to Lifshits had O(n^2m) running time.
</summary>
    <author>
      <name>Artur Jeż</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version, submitted to a journal as is. Overall improvements over
  the previous version</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.3244v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.3244v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1111.1665">
    <id>http://arxiv.org/abs/1111.1665v1</id>
    <updated>2011-11-07T18:16:59Z</updated>
    <published>2011-11-07T18:16:59Z</published>
    <title>De-amortizing Binary Search Trees</title>
    <summary>  We present a general method for de-amortizing essentially any Binary Search
Tree (BST) algorithm. In particular, by transforming Splay Trees, our method
produces a BST that has the same asymptotic cost as Splay Trees on any access
sequence while performing each search in O(log n) worst case time. By
transforming Multi-Splay Trees, we obtain a BST that is O(log log n)
competitive, satisfies the scanning theorem, the static optimality theorem, the
static finger theorem, the working set theorem, and performs each search in
O(log n) worst case time. Moreover, we prove that if there is a dynamically
optimal BST algorithm, then there is a dynamically optimal BST algorithm that
answers every search in O(log n) worst case time.
</summary>
    <author>
      <name>Prosenjit Bose</name>
    </author>
    <author>
      <name>Sébastien Collette</name>
    </author>
    <author>
      <name>Rolf Fagerberg</name>
    </author>
    <author>
      <name>Stefan Langerman</name>
    </author>
    <link href="http://arxiv.org/abs/1111.1665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.1665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1110.5296">
    <id>http://arxiv.org/abs/1110.5296v1</id>
    <updated>2011-10-24T18:20:43Z</updated>
    <published>2011-10-24T18:20:43Z</published>
    <title>Computing a Longest Common Palindromic Subsequence</title>
    <summary>  The {\em longest common subsequence (LCS)} problem is a classic and
well-studied problem in computer science. Palindrome is a word which reads the
same forward as it does backward. The {\em longest common palindromic
subsequence (LCPS)} problem is an interesting variant of the classic LCS
problem which finds the longest common subsequence between two given strings
such that the computed subsequence is also a palindrome. In this paper, we
study the LCPS problem and give efficient algorithms to solve this problem. To
the best of our knowledge, this is the first attempt to study and solve this
interesting problem.
</summary>
    <author>
      <name>Shihabur Rahman Chowdhury</name>
    </author>
    <author>
      <name>Md. Mahbubul Hasan</name>
    </author>
    <author>
      <name>Sumaiya Iqbal</name>
    </author>
    <author>
      <name>M. Sohel Rahman</name>
    </author>
    <link href="http://arxiv.org/abs/1110.5296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.5296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1110.3381">
    <id>http://arxiv.org/abs/1110.3381v1</id>
    <updated>2011-10-15T05:16:18Z</updated>
    <published>2011-10-15T05:16:18Z</published>
    <title>Partial Data Compression and Text Indexing via Optimal Suffix
  Multi-Selection</title>
    <summary>  Consider an input text string T[1,N] drawn from an unbounded alphabet. We
study partial computation in suffix-based problems for Data Compression and
Text Indexing such as
  (I) retrieve any segment of K&lt;=N consecutive symbols from the Burrows-Wheeler
transform of T, and
  (II) retrieve any chunk of K&lt;=N consecutive entries of the Suffix Array or
the Suffix Tree.
  Prior literature would take O(N log N) comparisons (and time) to solve these
problems by solving the total problem of building the entire Burrows-Wheeler
transform or Text Index for T, and performing a post-processing to single out
the wanted portion.
  We introduce a novel adaptive approach to partial computational problems
above, and solve both the partial problems in O(K log K + N) comparisons and
time, improving the best known running times of O(N log N) for K=o(N).
  These partial-computation problems are intimately related since they share a
common bottleneck: the suffix multi-selection problem, which is to output the
suffixes of rank r_1,r_2,...,r_K under the lexicographic order, where
r_1&lt;r_2&lt;...&lt;r_K, r_i in [1,N]. Special cases of this problem are well known:
K=N is the suffix sorting problem that is the workhorse in Stringology with
hundreds of applications, and K=1 is the recently studied suffix selection.
  We show that suffix multi-selection can be solved in Theta(N log N -
sum_{j=0}^K Delta_j log Delta_j+N) time and comparisons, where r_0=0,
r_{K+1}=N+1, and Delta_j=r_{j+1}-r_j for 0&lt;=j&lt;=K. This is asymptotically
optimal, and also matches the bound in [Dobkin, Munro, JACM 28(3)] for
multi-selection on atomic elements (not suffixes). Matching the bound known for
atomic elements for strings is a long running theme and challenge from 70's,
which we achieve for the suffix multi-selection problem. The partial suffix
problems as well as the suffix multi-selection problem have many applications.
</summary>
    <author>
      <name>Gianni Franceschini</name>
    </author>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>S. Muthukrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/1110.3381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1004.1194">
    <id>http://arxiv.org/abs/1004.1194v1</id>
    <updated>2010-04-07T21:24:27Z</updated>
    <published>2010-04-07T21:24:27Z</published>
    <title>Unified Compression-Based Acceleration of Edit-Distance Computation</title>
    <summary>  The edit distance problem is a classical fundamental problem in computer
science in general, and in combinatorial pattern matching in particular. The
standard dynamic programming solution for this problem computes the
edit-distance between a pair of strings of total length O(N) in O(N^2) time. To
this date, this quadratic upper-bound has never been substantially improved for
general strings. However, there are known techniques for breaking this bound in
case the strings are known to compress well under a particular compression
scheme. The basic idea is to first compress the strings, and then to compute
the edit distance between the compressed strings. As it turns out, practically
all known o(N^2) edit-distance algorithms work, in some sense, under the same
paradigm described above. It is therefore natural to ask whether there is a
single edit-distance algorithm that works for strings which are compressed
under any compression scheme. A rephrasing of this question is to ask whether a
single algorithm can exploit the compressibility properties of strings under
any compression method, even if each string is compressed using a different
compression. In this paper we set out to answer this question by using straight
line programs. These provide a generic platform for representing many popular
compression schemes including the LZ-family, Run-Length Encoding, Byte-Pair
Encoding, and dictionary methods. For two strings of total length N having
straight-line program representations of total size n, we present an algorithm
running in O(nN log(N/n)) time for computing the edit-distance of these two
strings under any rational scoring function, and an O(n^{2/3}N^{4/3}) time
algorithm for arbitrary scoring functions. Our new result, while providing a
signi cant speed up for highly compressible strings, does not surpass the
quadratic time bound even in the worst case scenario.
</summary>
    <author>
      <name>Danny Hermelin</name>
    </author>
    <author>
      <name>Gad M. Landau</name>
    </author>
    <author>
      <name>Shir Landau</name>
    </author>
    <author>
      <name>Oren Weimann</name>
    </author>
    <link href="http://arxiv.org/abs/1004.1194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.1194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1004.0424">
    <id>http://arxiv.org/abs/1004.0424v2</id>
    <updated>2010-06-27T18:56:01Z</updated>
    <published>2010-04-03T07:14:53Z</published>
    <title>Restricted Common Superstring and Restricted Common Supersequence</title>
    <summary>  The {\em shortest common superstring} and the {\em shortest common
supersequence} are two well studied problems having a wide range of
applications. In this paper we consider both problems with resource
constraints, denoted as the Restricted Common Superstring (shortly
\textit{RCSstr}) problem and the Restricted Common Supersequence (shortly
\textit{RCSseq}). In the \textit{RCSstr} (\textit{RCSseq}) problem we are given
a set $S$ of $n$ strings, $s_1$, $s_2$, $\ldots$, $s_n$, and a multiset $t =
\{t_1, t_2, \dots, t_m\}$, and the goal is to find a permutation $\pi : \{1,
\dots, m\} \to \{1, \dots, m\}$ to maximize the number of strings in $S$ that
are substrings (subsequences) of $\pi(t) = t_{\pi(1)}t_{\pi(2)}...t_{\pi(m)}$
(we call this ordering of the multiset, $\pi(t)$, a permutation of $t$). We
first show that in its most general setting the \textit{RCSstr} problem is {\em
NP-complete} and hard to approximate within a factor of $n^{1-\epsilon}$, for
any $\epsilon > 0$, unless P = NP. Afterwards, we present two separate
reductions to show that the \textit{RCSstr} problem remains NP-Hard even in the
case where the elements of $t$ are drawn from a binary alphabet or for the case
where all input strings are of length two. We then present some approximation
results for several variants of the \textit{RCSstr} problem. In the second part
of this paper, we turn to the \textit{RCSseq} problem, where we present some
hardness results, tight lower bounds and approximation algorithms.
</summary>
    <author>
      <name>Raphaël Clifford</name>
    </author>
    <author>
      <name>Zvi Gotthilf</name>
    </author>
    <author>
      <name>Moshe Lewenstein</name>
    </author>
    <author>
      <name>Alexandru Popa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to WAOA 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.0424v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.0424v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2008.03516">
    <id>http://arxiv.org/abs/2008.03516v2</id>
    <updated>2020-08-17T17:30:51Z</updated>
    <published>2020-08-08T13:13:37Z</published>
    <title>Blocksequences of k-local Words</title>
    <summary>  The locality of words is a relatively young structural complexity measure,
introduced by Day et al. in 2017 in order to define classes of patterns with
variables which can be matched in polynomial time. The main tool used to
compute the locality of a word is called marking sequence: an ordering of the
distinct letters occurring in the respective order. Once a marking sequence is
defined, the letters of the word are marked in steps: in the ith marking step,
all occurrences of the ith letter of the marking sequence are marked. As such,
after each marking step, the word can be seen as a sequence of blocks of marked
letters separated by blocks of non-marked letters. By keeping track of the
evolution of the marked blocks of the word through the marking defined by a
marking sequence, one defines the blocksequence of the respective marking
sequence. We first show that the words sharing the same blocksequence are only
loosely connected, so we consider the stronger notion of extended
blocksequence, which stores additional information on the form of each single
marked block. In this context, we present a series of combinatorial results for
words sharing the extended blocksequence.
</summary>
    <author>
      <name>Pamela Fleischmann</name>
    </author>
    <author>
      <name>Lukas Haschke</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Dirk Nowotka</name>
    </author>
    <author>
      <name>Cedric Tsatia Tsida</name>
    </author>
    <author>
      <name>Judith Wiedenbeck</name>
    </author>
    <link href="http://arxiv.org/abs/2008.03516v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.03516v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="2005.01112">
    <id>http://arxiv.org/abs/2005.01112v2</id>
    <updated>2021-03-15T16:31:53Z</updated>
    <published>2020-05-03T15:15:47Z</published>
    <title>Efficiently Testing Simon's Congruence</title>
    <summary>  Simon's congruence $\sim_k$ is defined as follows: two words are
$\sim_k$-equivalent if they have the same set of subsequences of length at most
$k$. We propose an algorithm which computes, given two words $s$ and $t$, the
largest $k$ for which $s\sim_k t$. Our algorithm runs in linear time
$O(|s|+|t|)$ when the input words are over the integer alphabet
$\{1,\ldots,|s|+|t|\}$ (or other alphabets which can be sorted in linear time).
This approach leads to an optimal algorithm in the case of general alphabets as
well. Our results are based on a novel combinatorial approach and a series of
efficient data structures.
</summary>
    <author>
      <name>Pawel Gawrychowski</name>
    </author>
    <author>
      <name>Maria Kosche</name>
    </author>
    <author>
      <name>Tore Koss</name>
    </author>
    <author>
      <name>Florin Manea</name>
    </author>
    <author>
      <name>Stefan Siemer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.STACS.2021.34</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.STACS.2021.34" rel="related"/>
    <link href="http://arxiv.org/abs/2005.01112v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.01112v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1003.2839">
    <id>http://arxiv.org/abs/1003.2839v2</id>
    <updated>2010-03-16T00:37:20Z</updated>
    <published>2010-03-15T02:33:43Z</published>
    <title>On the Border Length Minimization Problem (BLMP) on a Square Array</title>
    <summary>  Protein/Peptide microarrays are rapidly gaining momentum in the diagnosis of
cancer. High-density and highthroughput peptide arrays are being extensively
used to detect tumor biomarkers, examine kinase activity, identify antibodies
having low serum titers and locate antibody signatures. Improving the yield of
microarray fabrication involves solving a hard combinatorial optimization
problem called the Border Length Minimization Problem (BLMP). An important
question that remained open for the past seven years is if the BLMP is
tractable or not. We settle this open problem by proving that the BLMP is
NP-hard. We also present a hierarchical refinement algorithm which can refine
any heuristic solution for the BLMP problem. We also prove that the
TSP+1-threading heuristic is an O(N)- approximation. The hierarchical
refinement solver is available as an opensource code at
http://launchpad.net/blm-solve.
</summary>
    <author>
      <name>Vamsi Kundeti</name>
    </author>
    <author>
      <name>Sanguthevar Rajasekaran</name>
    </author>
    <author>
      <name>Hieu Dinh</name>
    </author>
    <link href="http://arxiv.org/abs/1003.2839v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.2839v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2; G.2.2; F.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1003.1940">
    <id>http://arxiv.org/abs/1003.1940v1</id>
    <updated>2010-03-09T17:54:01Z</updated>
    <published>2010-03-09T17:54:01Z</published>
    <title>Efficient Parallel and Out of Core Algorithms for Constructing Large
  Bi-directed de Bruijn Graphs</title>
    <summary>  Assembling genomic sequences from a set of overlapping reads is one of the
most fundamental problems in computational biology. Algorithms addressing the
assembly problem fall into two broad categories -- based on the data structures
which they employ. The first class uses an overlap/string graph and the second
type uses a de Bruijn graph. However with the recent advances in short read
sequencing technology, de Bruijn graph based algorithms seem to play a vital
role in practice.
  Efficient algorithms for building these massive de Bruijn graphs are very
essential in large sequencing projects based on short reads. In Jackson et. al.
ICPP-2008, an $O(n/p)$ time parallel algorithm has been given for this problem.
Here $n$ is the size of the input and $p$ is the number of processors. This
algorithm enumerates all possible bi-directed edges which can overlap with a
node and ends up generating $\Theta(n\Sigma)$ messages.
  In this paper we present a $\Theta(n/p)$ time parallel algorithm with a
communication complexity equal to that of parallel sorting and is not sensitive
to $\Sigma$. The generality of our algorithm makes it very easy to extend it
even to the out-of-core model and in this case it has an optimal I/O complexity
of $\Theta(\frac{n\log(n/B)}{B\log(M/B)})$. We demonstrate the scalability of
our parallel algorithm on a SGI/Altix computer. A comparison of our algorithm
with that of Jackson et. al. ICPP-2008 reveals that our algorithm is faster. We
also provide efficient algorithms for the bi-directed chain compaction problem.
</summary>
    <author>
      <name>Vamsi Kundeti</name>
    </author>
    <author>
      <name>Sanguthevar Rajasekaran</name>
    </author>
    <author>
      <name>Hieu Dinh</name>
    </author>
    <link href="http://arxiv.org/abs/1003.1940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.1940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1002.4248">
    <id>http://arxiv.org/abs/1002.4248v2</id>
    <updated>2012-11-17T20:08:09Z</updated>
    <published>2010-02-23T05:49:00Z</published>
    <title>Mergeable Dictionaries</title>
    <summary>  A data structure is presented for the Mergeable Dictionary abstract data
type, which supports the following operations on a collection of disjoint sets
of totally ordered data: Predecessor-Search, Split and Merge. While
Predecessor-Search and Split work in the normal way, the novel operation is
Merge. While in a typical mergeable dictionary (e.g. 2-4 Trees), the Merge
operation can only be performed on sets that span disjoint intervals in
keyspace, the structure here has no such limitation, and permits the merging of
arbitrarily interleaved sets. Tarjan and Brown present a data structure which
can handle arbitrary Merge operations in O(log n) amortized time per operation
if the set of operations is restricted to exclude the Split operation. In the
presence of Split operations, the amortized time complexity of their structure
becomes \Omega(n). A data structure which supports both Split and Merge
operations in O(log^2 n) amortized time per operation was given by Farach and
Thorup. In contrast, our data structure supports all operations, including
Split and Merge, in O(log n) amortized time, thus showing that interleaved
Merge operations can be supported at no additional cost vis-a-vis disjoint
Merge operations.
</summary>
    <author>
      <name>John Iacono</name>
    </author>
    <author>
      <name>Özgür Özkan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.4248v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.4248v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.2; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1001.2101">
    <id>http://arxiv.org/abs/1001.2101v3</id>
    <updated>2010-06-29T11:07:23Z</updated>
    <published>2010-01-13T09:18:24Z</published>
    <title>Sampled Longest Common Prefix Array</title>
    <summary>  When augmented with the longest common prefix (LCP) array and some other
structures, the suffix array can solve many string processing problems in
optimal time and space. A compressed representation of the LCP array is also
one of the main building blocks in many compressed suffix tree proposals. In
this paper, we describe a new compressed LCP representation: the sampled LCP
array. We show that when used with a compressed suffix array (CSA), the sampled
LCP array often offers better time/space trade-offs than the existing
alternatives. We also show how to construct the compressed representations of
the LCP array directly from a CSA.
</summary>
    <author>
      <name>Jouni Sirén</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-13509-5_21</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-13509-5_21" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a slightly extended version of the paper that was presented
  at CPM 2010. The implementation is available at
  http://www.cs.helsinki.fi/group/suds/rlcsa/</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.2101v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.2101v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1001.0833">
    <id>http://arxiv.org/abs/1001.0833v2</id>
    <updated>2010-02-02T02:46:22Z</updated>
    <published>2010-01-06T08:03:20Z</published>
    <title>Random Indexing K-tree</title>
    <summary>  Random Indexing (RI) K-tree is the combination of two algorithms for
clustering. Many large scale problems exist in document clustering. RI K-tree
scales well with large inputs due to its low complexity. It also exhibits
features that are useful for managing a changing collection. Furthermore, it
solves previous issues with sparse document vectors when using K-tree. The
algorithms and data structures are defined, explained and motivated. Specific
modifications to K-tree are made for use with RI. Experiments have been
executed to measure quality. The results indicate that RI K-tree improves
document cluster quality over the original K-tree algorithm.
</summary>
    <author>
      <name>Christopher M. De Vries</name>
    </author>
    <author>
      <name>Lance De Vine</name>
    </author>
    <author>
      <name>Shlomo Geva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, ADCS 2009; Hyperref and cleveref LaTeX packages conflicted.
  Removed cleveref</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0833v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0833v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1001.0830">
    <id>http://arxiv.org/abs/1001.0830v1</id>
    <updated>2010-01-06T07:43:31Z</updated>
    <published>2010-01-06T07:43:31Z</published>
    <title>K-tree: Large Scale Document Clustering</title>
    <summary>  We introduce K-tree in an information retrieval context. It is an efficient
approximation of the k-means clustering algorithm. Unlike k-means it forms a
hierarchy of clusters. It has been extended to address issues with sparse
representations. We compare performance and quality to CLUTO using document
collections. The K-tree has a low time complexity that is suitable for large
document collections. This tree structure allows for efficient disk based
implementations where space requirements exceed that of main memory.
</summary>
    <author>
      <name>Christopher M. De Vries</name>
    </author>
    <author>
      <name>Shlomo Geva</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1571941.1572094</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1571941.1572094" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, SIGIR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1006.5354">
    <id>http://arxiv.org/abs/1006.5354v1</id>
    <updated>2010-06-28T14:05:05Z</updated>
    <published>2010-06-28T14:05:05Z</published>
    <title>Optimal Trade-Off for Succinct String Indexes</title>
    <summary>  Let s be a string whose symbols are solely available through access(i), a
read-only operation that probes s and returns the symbol at position i in s.
Many compressed data structures for strings, trees, and graphs, require two
kinds of queries on s: select(c, j), returning the position in s containing the
jth occurrence of c, and rank(c, p), counting how many occurrences of c are
found in the first p positions of s. We give matching upper and lower bounds
for this problem, improving the lower bounds given by Golynski [Theor. Comput.
Sci. 387 (2007)] [PhD thesis] and the upper bounds of Barbay et al. [SODA
2007]. We also present new results in another model, improving on Barbay et al.
[SODA 2007] and matching a lower bound of Golynski [SODA 2009]. The main
contribution of this paper is to introduce a general technique for proving
lower bounds on succinct data structures, that is based on the access patterns
of the supported operations, abstracting from the particular operations at
hand. For this, it may find application to other interesting problems on
succinct data structures.
</summary>
    <author>
      <name>Roberto Grossi</name>
    </author>
    <author>
      <name>Alessio Orlandi</name>
    </author>
    <author>
      <name>Rajeev Raman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICALP 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.5354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.5354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1006.4828">
    <id>http://arxiv.org/abs/1006.4828v1</id>
    <updated>2010-06-24T16:35:47Z</updated>
    <published>2010-06-24T16:35:47Z</published>
    <title>An Efficient Algorithm For Chinese Postman Walk on Bi-directed de Bruijn
  Graphs</title>
    <summary>  Sequence assembly from short reads is an important problem in biology. It is
known that solving the sequence assembly problem exactly on a bi-directed de
Bruijn graph or a string graph is intractable. However finding a Shortest
Double stranded DNA string (SDDNA) containing all the k-long words in the reads
seems to be a good heuristic to get close to the original genome. This problem
is equivalent to finding a cyclic Chinese Postman (CP) walk on the underlying
un-weighted bi-directed de Bruijn graph built from the reads. The Chinese
Postman walk Problem (CPP) is solved by reducing it to a general bi-directed
flow on this graph which runs in O(|E|2 log2(|V |)) time. In this paper we show
that the cyclic CPP on bi-directed graphs can be solved without reducing it to
bi-directed flow. We present a ?(p(|V | + |E|) log(|V |) + (dmaxp)3) time
algorithm to solve the cyclic CPP on a weighted bi-directed de Bruijn graph,
where p = max{|{v|din(v) - dout(v) > 0}|, |{v|din(v) - dout(v) &lt; 0}|} and dmax
= max{|din(v) - dout(v)}. Our algorithm performs asymptotically better than the
bidirected flow algorithm when the number of imbalanced nodes p is much less
than the nodes in the bi-directed graph. From our experimental results on
various datasets, we have noticed that the value of p/|V | lies between 0.08%
and 0.13% with 95% probability.
</summary>
    <author>
      <name>Vamsi Kundeti</name>
    </author>
    <author>
      <name>Sanguthevar Rajasekaran</name>
    </author>
    <author>
      <name>Hieu Dinh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-17458-2_16</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-17458-2_16" rel="related"/>
    <link href="http://arxiv.org/abs/1006.4828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.4828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1006.3715">
    <id>http://arxiv.org/abs/1006.3715v1</id>
    <updated>2010-06-18T15:08:28Z</updated>
    <published>2010-06-18T15:08:28Z</published>
    <title>Should Static Search Trees Ever Be Unbalanced?</title>
    <summary>  In this paper we study the question of whether or not a static search tree
should ever be unbalanced. We present several methods to restructure an
unbalanced k-ary search tree $T$ into a new tree $R$ that preserves many of the
properties of $T$ while having a height of $\log_k n +1$ which is one unit off
of the optimal height. More specifically, we show that it is possible to ensure
that the depth of the elements in $R$ is no more than their depth in $T$ plus
at most $\log_k \log_k n +2$. At the same time it is possible to guarantee that
the average access time $P(R)$ in tree $R$ is no more than the average access
time $P(T)$ in tree $T$ plus $O(\log_k P(T))$. This suggests that for most
applications, a balanced tree is always a better option than an unbalanced one
since the balanced tree has similar average access time and much better worst
case access time.
</summary>
    <author>
      <name>Prosenjit Bose</name>
    </author>
    <author>
      <name>Karim Douïeb</name>
    </author>
    <link href="http://arxiv.org/abs/1006.3715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.3715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1006.0809">
    <id>http://arxiv.org/abs/1006.0809v2</id>
    <updated>2011-09-06T12:46:56Z</updated>
    <published>2010-06-04T08:41:22Z</published>
    <title>Tight and simple Web graph compression</title>
    <summary>  Analysing Web graphs has applications in determining page ranks, fighting Web
spam, detecting communities and mirror sites, and more. This study is however
hampered by the necessity of storing a major part of huge graphs in the
external memory, which prevents efficient random access to edge (hyperlink)
lists. A number of algorithms involving compression techniques have thus been
presented, to represent Web graphs succinctly but also providing random access.
Those techniques are usually based on differential encodings of the adjacency
lists, finding repeating nodes or node regions in the successive lists, more
general grammar-based transformations or 2-dimensional representations of the
binary matrix of the graph. In this paper we present two Web graph compression
algorithms. The first can be seen as engineering of the Boldi and Vigna (2004)
method. We extend the notion of similarity between link lists, and use a more
compact encoding of residuals. The algorithm works on blocks of varying size
(in the number of input lines) and sacrifices access time for better
compression ratio, achieving more succinct graph representation than other
algorithms reported in the literature. The second algorithm works on blocks of
the same size, in the number of input lines, and its key mechanism is merging
the block into a single ordered list. This method achieves much more attractive
space-time tradeoffs.
</summary>
    <author>
      <name>Szymon Grabowski</name>
    </author>
    <author>
      <name>Wojciech Bieniecki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.0809v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.0809v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U35" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1005.0662">
    <id>http://arxiv.org/abs/1005.0662v1</id>
    <updated>2010-05-05T01:44:41Z</updated>
    <published>2010-05-05T01:44:41Z</published>
    <title>The B-Skip-List: A Simpler Uniquely Represented Alternative to B-Trees</title>
    <summary>  In previous work, the author introduced the B-treap, a uniquely represented
B-tree analogue, and proved strong performance guarantees for it. However, the
B-treap maintains complex invariants and is very complex to implement. In this
paper we introduce the B-skip-list, which has most of the guarantees of the
B-treap, but is vastly simpler and easier to implement. Like the B-treap, the
B-skip-list may be used to construct strongly history-independent index
structures and filesystems; such constructions reveal no information about the
historical sequence of operations that led to the current logical state. For
example, a uniquely represented filesystem would support the deletion of a file
in a way that, in a strong information-theoretic sense, provably removes all
evidence that the file ever existed. Like the B-tree, the B-skip-list has depth
O(log_B (n)) where B is the block transfer size of the external memory, uses
linear space with high probability, and supports efficient one-dimensional
range queries.
</summary>
    <author>
      <name>Daniel Golovin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.0662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1005.0239">
    <id>http://arxiv.org/abs/1005.0239v1</id>
    <updated>2010-05-03T09:23:01Z</updated>
    <published>2010-05-03T09:23:01Z</published>
    <title>On Finding Frequent Patterns in Directed Acyclic Graphs</title>
    <summary>  Given a directed acyclic graph with labeled vertices, we consider the problem
of finding the most common label sequences ("traces") among all paths in the
graph (of some maximum length m). Since the number of paths can be huge, we
propose novel algorithms whose time complexity depends only on the size of the
graph, and on the relative frequency epsilon of the most frequent traces. In
addition, we apply techniques from streaming algorithms to achieve space usage
that depends only on epsilon, and not on the number of distinct traces. The
abstract problem considered models a variety of tasks concerning finding
frequent patterns in event sequences. Our motivation comes from working with a
data set of 2 million RFID readings from baggage trolleys at Copenhagen
Airport. The question of finding frequent passenger movement patterns is mapped
to the above problem. We report on experimental findings for this data set.
</summary>
    <author>
      <name>Andrea Campagna</name>
    </author>
    <author>
      <name>Rasmus Pagh</name>
    </author>
    <link href="http://arxiv.org/abs/1005.0239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1004.3115">
    <id>http://arxiv.org/abs/1004.3115v1</id>
    <updated>2010-04-19T07:52:08Z</updated>
    <published>2010-04-19T07:52:08Z</published>
    <title>Some long-period random number generators using shifts and xors</title>
    <summary>  Marsaglia recently introduced a class of xorshift random number generators
(RNGs) with periods 2n-1 for n = 32, 64, etc. Here we give a generalisation of
Marsaglia's xorshift generators in order to obtain fast and high-quality RNGs
with extremely long periods. RNGs based on primitive trinomials may be
unsatisfactory because a trinomial has very small weight. In contrast, our
generators can be chosen so that their minimal polynomials have large weight
(number of nonzero terms). A computer search using Magma has found good
generators for n a power of two up to 4096. These have been implemented in a
free software package xorgens.
</summary>
    <author>
      <name>Richard P. Brent</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ANZIAM Journal 48 (CTAC2006), C188-C202, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11K45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry><entry id="1004.3108">
    <id>http://arxiv.org/abs/1004.3108v2</id>
    <updated>2010-04-20T01:11:53Z</updated>
    <published>2010-04-19T07:05:35Z</published>
    <title>Uses of randomness in computation</title>
    <summary>  Random number generators are widely used in practical algorithms. Examples
include simulation, number theory (primality testing and integer
factorization), fault tolerance, routing, cryptography, optimization by
simulated annealing, and perfect hashing. Complexity theory usually considers
the worst-case behaviour of deterministic algorithms, but it can also consider
average-case behaviour if it is assumed that the input data is drawn randomly
from a given distribution. Rabin popularised the idea of "probabilistic"
algorithms, where randomness is incorporated into the algorithm instead of
being assumed in the input data. Yao showed that there is a close connection
between the complexity of probabilistic algorithms and the average-case
complexity of deterministic algorithms. We give examples of the uses of
randomness in computation, discuss the contributions of Rabin, Yao and others,
and mention some open questions. This is the text of an invited talk presented
at "Theory Day", University of NSW, Sydney, 22 April 1994.
</summary>
    <author>
      <name>Richard P. Brent</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An old Technical Report, not published elsewhere. 14 pages. For
  further details see http://wwwmaths.anu.edu.au/~brent/pub/pub147.html</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.3108v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3108v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W20 (Primary) 68Q25 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.1; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry></articles>